{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b782b18d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Clean ways to introduce SQL CTEs into a python based project\n",
    "</summary>\n",
    "\n",
    "Yep ‚Äî **very easily**, and it fits your project style (config-driven, artifacted, reproducible).\n",
    "\n",
    "### 3 clean ways to add CTE SQL to your project\n",
    "\n",
    "#### 1) Save CTE queries as `.sql` files in a dedicated folder (recommended)\n",
    "\n",
    "Add something like:\n",
    "\n",
    "* `resources/sql/section2/`\n",
    "\n",
    "  * `2_5_structural_integrity.sql`\n",
    "  * `2_4_model_readiness.sql`\n",
    "  * `common_ctes.sql`\n",
    "\n",
    "Then your notebook can do:\n",
    "\n",
    "* read SQL text from file\n",
    "* parameterize it (dates, table names)\n",
    "* run it (BigQuery / DuckDB / Postgres)\n",
    "* save results to your existing `SEC2_*_DIR` outputs\n",
    "\n",
    "This keeps the SQL version-controlled and not buried in notebooks.\n",
    "\n",
    "#### 2) Keep CTE SQL in your CONFIG (good for small queries)\n",
    "\n",
    "Add a config key like `SQL.QUERIES.<name>` and load it with your `C()` helper. Great for short queries, but big multi-CTE queries get hard to read inside YAML/JSON.\n",
    "\n",
    "#### 3) Generate ‚Äúderived tables‚Äù with CTEs and materialize them\n",
    "\n",
    "Use CTEs to build ‚Äúclean‚Äù or ‚Äúmodel-ready‚Äù datasets and write them into:\n",
    "\n",
    "* `PROCESSED_DIR` (CSV/parquet)\n",
    "* or a database schema (e.g., `analytics.section2_*`)\n",
    "\n",
    "That‚Äôs basically the SQL analog of your Python artifact pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Where it plugs into *your* pipeline\n",
    "\n",
    "Given your structure, I‚Äôd add:\n",
    "\n",
    "* `SQL_DIR = PROJECT_ROOT / \"resources\" / \"sql\"` (or `SEC2_DIR.parent / \"sql\"` if no project root)\n",
    "* `SQL_SECTION2_DIR = SQL_DIR / \"section2\"`\n",
    "\n",
    "and treat SQL files as **inputs** and query results as **artifacts** saved under `SEC2_2X_DIR` / `NUMERIC_DIR` / `CATEGORICAL_DIR`.\n",
    "\n",
    "üí°üí° If you tell me what engine you‚Äôll run SQL on (BigQuery? DuckDB? Postgres?), I‚Äôll give you a drop-in ‚Äúrun_sql_and_save_artifact()‚Äù pattern that matches your `atomic write + append_sec2` style.\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "running SQL using (BigQuery, DuckDB, Postgres) and isolated to show off a robust demonstration of abilities for job portfolios\n",
    "</summary>\n",
    "\n",
    "\n",
    "Perfect. The clean portfolio move is: **one SQL ‚Äúcontract‚Äù + three isolated runners** (BigQuery / DuckDB / Postgres), all producing the *same artifacts* so you can prove portability + engineering discipline.\n",
    "\n",
    "## The core idea\n",
    "\n",
    "* You write **one canonical query** (CTEs included) in `resources/sql/...`\n",
    "* You run it through **three adapters** that are isolated:\n",
    "\n",
    "  * **BigQuery adapter** (cloud)\n",
    "  * **DuckDB adapter** (local, file-based)\n",
    "  * **Postgres adapter** (local DB container)\n",
    "* Each run writes:\n",
    "\n",
    "  * results artifact (CSV/parquet)\n",
    "  * run metadata (engine, rowcount, duration, git hash, query hash)\n",
    "  * (optional) EXPLAIN plan artifact\n",
    "\n",
    "This gives you a killer portfolio storyline: **‚Äúsame logic, three engines, reproducible outputs.‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "## Directory layout (portfolio-friendly)\n",
    "\n",
    "```text\n",
    "project/\n",
    "  resources/\n",
    "    sql/\n",
    "      section2/\n",
    "        2_5_structural_integrity.sql\n",
    "        2_4_model_readiness.sql\n",
    "      macros/\n",
    "        dialect/\n",
    "          bigquery.sql\n",
    "          postgres.sql\n",
    "          duckdb.sql\n",
    "    data/\n",
    "      raw/                 # optional\n",
    "      seeds/               # tiny demo datasets (CSV)\n",
    "      processed/           # shared processed outputs\n",
    "  src/\n",
    "    sqlrunner/\n",
    "      __init__.py\n",
    "      config.py\n",
    "      common.py            # hashing, timing, artifact writer\n",
    "      adapters/\n",
    "        bigquery.py\n",
    "        duckdb.py\n",
    "        postgres.py\n",
    "  artifacts/\n",
    "    sql_runs/\n",
    "      bigquery/\n",
    "      duckdb/\n",
    "      postgres/\n",
    "  notebooks/\n",
    "  docker/\n",
    "    compose.postgres.yml\n",
    "```\n",
    "\n",
    "üí°üí° In your portfolio README you can literally show: *‚ÄúSame SQL file ‚Üí three engines ‚Üí identical outputs + metadata.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "## Isolation rules (so it‚Äôs a ‚Äúrobust demo‚Äù)\n",
    "\n",
    "### BigQuery isolation\n",
    "\n",
    "* Uses **GCP project + dataset** names from env vars\n",
    "* Writes outputs to `artifacts/sql_runs/bigquery/...` locally\n",
    "* Never depends on local DB state\n",
    "\n",
    "### DuckDB isolation\n",
    "\n",
    "* Uses only local files in `resources/data/seeds` or `resources/data/processed`\n",
    "* DuckDB file lives in `artifacts/sql_runs/duckdb/demo.duckdb` (ephemeral, reproducible)\n",
    "* No network required\n",
    "\n",
    "### Postgres isolation\n",
    "\n",
    "* Run Postgres via docker compose\n",
    "* Load the same seed data on startup (or a one-shot script)\n",
    "* Outputs written locally like the others\n",
    "\n",
    "---\n",
    "\n",
    "## One ‚Äúcanonical‚Äù SQL file, with light dialect handling\n",
    "\n",
    "You have two good options:\n",
    "\n",
    "### Option A (most portfolio-friendly): ‚ÄúOne query + three small dialect macro files‚Äù\n",
    "\n",
    "In your SQL you use placeholders like:\n",
    "\n",
    "* `{{ ref(\"table_name\") }}`\n",
    "* `{{ cast_int(\"col\") }}`\n",
    "* `{{ safe_divide(\"a\",\"b\") }}`\n",
    "\n",
    "Then each adapter injects the right macro definitions.\n",
    "\n",
    "### Option B: Keep SQL fully portable (simpler)\n",
    "\n",
    "Avoid engine-specific functions. Use:\n",
    "\n",
    "* standard CTEs\n",
    "* standard `CASE WHEN`\n",
    "* standard `COALESCE`\n",
    "* avoid BigQuery-only things like `SAFE_CAST`, `QUALIFY`, `ARRAY_*`\n",
    "\n",
    "If you want ‚Äúwow factor‚Äù, Option A is better.\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal adapter interface (what you‚Äôll implement once)\n",
    "\n",
    "Each engine adapter should expose the same method:\n",
    "\n",
    "* `run_query(sql_text: str, params: dict) -> pandas.DataFrame`\n",
    "\n",
    "‚Ä¶and your runner wraps it to:\n",
    "\n",
    "* write result artifact\n",
    "* write metadata json\n",
    "* append to your `SECTION2_REPORT_PATH` if you want\n",
    "\n",
    "üí°üí° This mirrors your Section 2 philosophy: **compute ‚Üí artifact ‚Üí summary row**.\n",
    "\n",
    "---\n",
    "\n",
    "## A concrete ‚ÄúCTE demo query‚Äù that works everywhere\n",
    "\n",
    "Example: ‚ÄúStructural integrity summary by key‚Äù (works in all 3 engines with minimal changes)\n",
    "\n",
    "```sql\n",
    "-- resources/sql/section2/2_5_key_integrity_summary.sql\n",
    "WITH base AS (\n",
    "  SELECT *\n",
    "  FROM {{ ref(\"fact_table\") }}\n",
    "),\n",
    "null_keys AS (\n",
    "  SELECT\n",
    "    COUNT(*) AS n_null_key_rows\n",
    "  FROM base\n",
    "  WHERE {{ key_is_null(\"pk_col\") }}\n",
    "),\n",
    "dupes AS (\n",
    "  SELECT\n",
    "    COUNT(*) AS n_duplicate_keys\n",
    "  FROM (\n",
    "    SELECT pk_col\n",
    "    FROM base\n",
    "    WHERE pk_col IS NOT NULL\n",
    "    GROUP BY pk_col\n",
    "    HAVING COUNT(*) > 1\n",
    "  ) d\n",
    ")\n",
    "SELECT\n",
    "  (SELECT n_null_key_rows FROM null_keys) AS n_null_key_rows,\n",
    "  (SELECT n_duplicate_keys FROM dupes)     AS n_duplicate_keys;\n",
    "```\n",
    "\n",
    "Then your dialect macro files implement `ref()` and `key_is_null()` (or you inline it if you choose Option B).\n",
    "\n",
    "---\n",
    "\n",
    "## How to demonstrate ‚Äúisolation‚Äù in your portfolio\n",
    "\n",
    "### 1) Make one command per engine\n",
    "\n",
    "* `python -m sqlrunner.run --engine duckdb --query 2_5_key_integrity_summary`\n",
    "* `python -m sqlrunner.run --engine postgres --query 2_5_key_integrity_summary`\n",
    "* `python -m sqlrunner.run --engine bigquery --query 2_5_key_integrity_summary`\n",
    "\n",
    "### 2) Make them all write the same artifact schema\n",
    "\n",
    "Every run writes:\n",
    "\n",
    "* `result.csv` (or parquet)\n",
    "* `meta.json` containing:\n",
    "\n",
    "  * engine\n",
    "  * query_name\n",
    "  * query_sha256\n",
    "  * started_at, finished_at, duration_ms\n",
    "  * row_count\n",
    "  * params (sanitized)\n",
    "  * git_commit (if available)\n",
    "\n",
    "### 3) Add a tiny ‚Äúcompare outputs‚Äù check\n",
    "\n",
    "A script that asserts:\n",
    "\n",
    "* same columns\n",
    "* same row count\n",
    "* same checksums (or close enough if types differ)\n",
    "\n",
    "That ‚Äúproves‚Äù portability.\n",
    "\n",
    "---\n",
    "\n",
    "## What you should do next (no extra questions, just a clean plan)\n",
    "\n",
    "üí°üí° Pick your ‚Äúsource-of-truth dataset‚Äù for the demo:\n",
    "\n",
    "* **Seeds (CSV)** ‚Üí load into DuckDB + Postgres; BigQuery loads via `bq load` or Python client.\n",
    "* Or **BigQuery as truth** ‚Üí export a small slice to seeds for local engines (best for realism).\n",
    "\n",
    "üí°üí° Then implement in this order:\n",
    "\n",
    "1. DuckDB adapter (fastest to get working, zero infra)\n",
    "2. Postgres adapter (dockerized, shows real DB skills)\n",
    "3. BigQuery adapter (shows cloud/data warehouse skills)\n",
    "\n",
    "---\n",
    "\n",
    "## Small naming guidance to match your bootstrap style\n",
    "\n",
    "* SQL inputs are **global**: `SQL_DIR`, `SQL_SECTION2_DIR`\n",
    "* SQL run outputs are **engine-scoped**:\n",
    "\n",
    "  * `SQL_RUNS_DIR / duckdb / ...`\n",
    "  * `SQL_RUNS_DIR / postgres / ...`\n",
    "  * `SQL_RUNS_DIR / bigquery / ...`\n",
    "\n",
    "This matches your ‚ÄúSEC2_DIR is canonical, section dirs are rollups‚Äù thinking.\n",
    "\n",
    "---\n",
    "\n",
    "If you tell me **what you‚Äôre using to connect** (for each):\n",
    "\n",
    "* BigQuery: `google-cloud-bigquery` vs `pandas-gbq`?\n",
    "* Postgres: `psycopg2` vs `sqlalchemy`?\n",
    "* DuckDB: `duckdb` Python package?\n",
    "\n",
    "‚Ä¶I‚Äôll give you a drop-in `src/sqlrunner/` implementation that:\n",
    "\n",
    "* loads `resources/sql/...`\n",
    "* injects dialect macros\n",
    "* runs the query\n",
    "* writes artifacts atomically (like your `.tmp` pattern)\n",
    "* optionally appends a Section 2 summary row.\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor:pointer; background:#f7f7fb; border:2px solid #297be7ff; padding:10px 12px;border-radius:10px font-weight:700;\">\n",
    "SQL\n",
    "</summary>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc794441",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Load CUSTOMER DIMENSIONAL table\n",
    "CUSTOMER_DIM = customer_dim_df\n",
    "\n",
    "# Load REFERENCE table\n",
    "REF_TABLES = {\"CUSTOMER_DIM\": customer_dim_df}\n",
    "\n",
    "##\n",
    "if \"REF_TABLES\" in globals() and isinstance(REF_TABLES, dict):\n",
    "    ref_df_2502 = REF_TABLES.get(ref_table_name_2502)\n",
    "\n",
    "##\n",
    "cand_names_2502 = [\n",
    "    ref_table_name_2502,\n",
    "    f\"{ref_table_name_2502}_df\",\n",
    "    f\"df_{ref_table_name_2502}\",\n",
    "    ref_table_name_2502.upper(),\n",
    "]\n",
    "\n",
    "for _cand_2502 in cand_names_2502:\n",
    "    if _cand_2502 in globals() and isinstance(globals()[_cand_2502], pd.DataFrame):\n",
    "        ref_df_2502 = globals()[_cand_2502]\n",
    "        break\n",
    "\n",
    "## Example: build or load your dim table\n",
    "customer_dim_df = (\n",
    "    df[[\"customerID\"]]  # or whatever columns you want\n",
    "      .drop_duplicates()\n",
    "      .rename(columns={\"customerID\": \"customer_id\"})\n",
    ")\n",
    "\n",
    "# Option A: expose it as a named DataFrame\n",
    "CUSTOMER_DIM = customer_dim_df\n",
    "\n",
    "# Option B (more explicit): register in REF_TABLES\n",
    "REF_TABLES = {\n",
    "    \"CUSTOMER_DIM\": customer_dim_df,\n",
    "}\n",
    "\n",
    "##\n",
    "customer_dim_df = (\n",
    "    df[[\"customerID\"]]\n",
    "      .drop_duplicates()\n",
    "      .rename(columns={\"customerID\": \"customer_id\"})\n",
    ")\n",
    "\n",
    "REF_TABLES = {\n",
    "    \"CUSTOMER_DIM\": customer_dim_df\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4abee",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "SQL\n",
    "</summary>\n",
    "\n",
    "Excellent question again, B ‚Äî you‚Äôre thinking about your project architecture like a real **data engineer / analytics engineer** now üëè\n",
    "\n",
    "SQL is one of those foundational skills that fits beautifully into your Telco roadmap ‚Äî **but it needs to appear right when you transition from pure exploration (pandas) to structured data thinking (schemas, queries, joins, aggregations).**\n",
    "\n",
    "Let‚Äôs break it down clearly üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **When to Introduce SQL**\n",
    "\n",
    "| Level                                          | Stage                                    | SQL Role                                   | Recommendation                                                                         |\n",
    "| ---------------------------------------------- | ---------------------------------------- | ------------------------------------------ | -------------------------------------------------------------------------------------- |\n",
    "| **Level_0 ‚Äî Bare-Bones Analysis**              | Basic EDA                                | ‚ùå *Skip for now*                           | Stick with pandas to stay focused on fundamentals.                                     |\n",
    "| **Level_1 ‚Äî Structured EDA**                   | Column profiling, summary stats          | ‚öôÔ∏è *Optional Intro*                        | You can show ‚Äúwhat if this were a SQL table‚Äù as conceptual practice.                   |\n",
    "| **Level_2 ‚Äî Data Cleaning & Validation**       | Quality checks, schema design            | ‚öôÔ∏è *Soft introduction*                     | Define your column data types as if designing a SQL schema (build intuition).          |\n",
    "| **‚úÖ Level_3 ‚Äî Visualization & Insights**       | Aggregations, grouping, joining metadata | ‚≠ê *Best time to introduce actual SQL code* | Connect to SQLite or BigQuery, run SELECT/GROUP BY queries, compare to pandas outputs. |\n",
    "| **Level_4‚Äì5 ‚Äî Feature Engineering ‚Üí Modeling** | Feature aggregation pipelines            | ‚úÖ *Integrate SQL fully*                    | Pull curated data subsets via SQL joins or CTEs.                                       |\n",
    "| **Level_7+ ‚Äî Production Code Structure**       | ETL pipelines                            | üöÄ *SQL essential*                         | Queries live in `.sql` files or orchestrated jobs (Airflow, dbt, etc.).                |\n",
    "\n",
    "---\n",
    "\n",
    "## üéì **Why Level_3 Is the Perfect Starting Point**\n",
    "\n",
    "By Level_3, you‚Äôve already:\n",
    "\n",
    "* Cleaned the data (so it‚Äôs safe to import into a SQL engine)\n",
    "* Know what your columns mean\n",
    "* Are beginning to group, aggregate, and join data\n",
    "\n",
    "That‚Äôs exactly when SQL becomes *practically useful*, not just theoretical.\n",
    "\n",
    "It‚Äôs also where you can start showing **dual fluency**:\n",
    "\n",
    "```sql\n",
    "-- SQL way\n",
    "SELECT gender, Contract, COUNT(*) AS customers\n",
    "FROM telco_churn\n",
    "GROUP BY gender, Contract;\n",
    "```\n",
    "\n",
    "vs\n",
    "\n",
    "```python\n",
    "# Pandas way\n",
    "df.groupby(['gender', 'Contract']).size().reset_index(name='customers')\n",
    "```\n",
    "\n",
    "That comparison helps you demonstrate bilingual data fluency (Python + SQL) ‚Äî *hugely valuable* on resumes.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **How to Integrate It at Level_3**\n",
    "\n",
    "1. Export cleaned data to a local SQLite database:\n",
    "\n",
    "   ```python\n",
    "   import sqlite3\n",
    "   conn = sqlite3.connect(\"telco_clean.db\")\n",
    "   df.to_sql(\"telco_churn\", conn, if_exists=\"replace\", index=False)\n",
    "   ```\n",
    "2. Run SQL queries directly:\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   query = \"\"\"\n",
    "   SELECT Contract, InternetService, COUNT(*) AS customers, AVG(MonthlyCharges) AS avg_charge\n",
    "   FROM telco_churn\n",
    "   GROUP BY Contract, InternetService\n",
    "   ORDER BY avg_charge DESC;\n",
    "   \"\"\"\n",
    "   pd.read_sql_query(query, conn)\n",
    "   ```\n",
    "3. Visualize the query output ‚Äî clean, modular, reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **Level-Based Summary**\n",
    "\n",
    "| Level | SQL Skill Added                                |\n",
    "| ----- | ---------------------------------------------- |\n",
    "| 1     | Conceptually relate columns to database tables |\n",
    "| 2     | Schema mapping, data types                     |\n",
    "| 3     | **Query execution (SQLite or DuckDB)**         |\n",
    "| 4     | Joins, CTEs, aggregations for feature creation |\n",
    "| 7+    | SQL in pipelines / dbt / productionized ETL    |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **TL;DR**\n",
    "\n",
    "> Introduce SQL at **Level 3 (Visualization & Insights)** ‚Äî when you‚Äôre ready to query and aggregate clean data.\n",
    "> Keep Levels 0‚Äì2 pure Python/pandas to build foundations, then bridge into SQL once your data is structured and validated.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show what a **Level 3 Telco folder with integrated SQL setup** would look like (including an example `sql/queries/` directory and notebook connection cell)?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Awesome ‚Äî here‚Äôs a clean, recruiter-friendly **Level_3** layout with integrated SQL. It includes a `sql/queries/` folder, plus ready-to-run notebook cells for **SQLite** (default) and **DuckDB** (optional).\n",
    "\n",
    "---\n",
    "\n",
    "# üìÅ Level_3 ‚Äî with SQL integrated\n",
    "\n",
    "```\n",
    "Level_3/\n",
    "‚îú‚îÄ notebooks/\n",
    "‚îÇ  ‚îú‚îÄ 03_visualization_insights.ipynb\n",
    "‚îÇ  ‚îî‚îÄ 03_sql_analysis.ipynb\n",
    "‚îú‚îÄ data/\n",
    "‚îÇ  ‚îî‚îÄ telco_clean.parquet              # output from Level_2 (cleaned)\n",
    "‚îú‚îÄ sql/\n",
    "‚îÇ  ‚îú‚îÄ schema/\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ create_telco_table.sql\n",
    "‚îÇ  ‚îî‚îÄ queries/\n",
    "‚îÇ     ‚îú‚îÄ churn_by_contract.sql\n",
    "‚îÇ     ‚îú‚îÄ revenue_by_tenure.sql\n",
    "‚îÇ     ‚îú‚îÄ churn_rate_by_internet_service.sql\n",
    "‚îÇ     ‚îî‚îÄ top_payment_methods.sql\n",
    "‚îú‚îÄ db/\n",
    "‚îÇ  ‚îî‚îÄ telco_clean.sqlite               # generated locally (ignored by git)\n",
    "‚îú‚îÄ requirements.txt\n",
    "‚îî‚îÄ README.md\n",
    "```\n",
    "\n",
    "### `.gitignore` additions (Level_3 root)\n",
    "\n",
    "```\n",
    "db/*.sqlite\n",
    "db/*.duckdb\n",
    "*.db\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Level_3 `requirements.txt` (incremental over Level_0)\n",
    "\n",
    "```text\n",
    "pandas>=2.0\n",
    "numpy>=1.23\n",
    "matplotlib>=3.7\n",
    "seaborn>=0.12\n",
    "ipykernel>=6.25\n",
    "pyarrow>=14.0            # for parquet (nice with pandas)\n",
    "duckdb>=1.0.0            # optional (in-process SQL engine)\n",
    "```\n",
    "\n",
    "*(No extra package needed for SQLite ‚Äî it‚Äôs in the Python stdlib via `sqlite3`.)*\n",
    "\n",
    "---\n",
    "\n",
    "## üß± SQL files (drop in `sql/queries/`)\n",
    "\n",
    "**`sql/queries/churn_by_contract.sql`**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  Contract,\n",
    "  COUNT(*)                         AS customers,\n",
    "  SUM(CASE WHEN Churn = 'Yes' THEN 1 ELSE 0 END) AS churned,\n",
    "  ROUND(100.0 * SUM(CASE WHEN Churn = 'Yes' THEN 1 ELSE 0 END) / COUNT(*), 2) AS churn_rate_pct\n",
    "FROM telco_churn\n",
    "GROUP BY Contract\n",
    "ORDER BY churn_rate_pct DESC;\n",
    "```\n",
    "\n",
    "**`sql/queries/revenue_by_tenure.sql`**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  tenure,\n",
    "  COUNT(*) AS customers,\n",
    "  AVG(MonthlyCharges) AS avg_monthly,\n",
    "  SUM(MonthlyCharges) AS total_monthly_revenue_est\n",
    "FROM telco_churn\n",
    "GROUP BY tenure\n",
    "ORDER BY tenure;\n",
    "```\n",
    "\n",
    "**`sql/queries/churn_rate_by_internet_service.sql`**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  InternetService,\n",
    "  COUNT(*) AS customers,\n",
    "  ROUND(100.0 * AVG(CASE WHEN Churn = 'Yes' THEN 1 ELSE 0 END), 2) AS churn_rate_pct\n",
    "FROM telco_churn\n",
    "GROUP BY InternetService\n",
    "ORDER BY churn_rate_pct DESC;\n",
    "```\n",
    "\n",
    "**`sql/schema/create_telco_table.sql`**\n",
    "\n",
    "```sql\n",
    "DROP TABLE IF EXISTS telco_churn;\n",
    "\n",
    "CREATE TABLE telco_churn (\n",
    "  customerID TEXT PRIMARY KEY,\n",
    "  gender TEXT,\n",
    "  SeniorCitizen INTEGER,\n",
    "  Partner TEXT,\n",
    "  Dependents TEXT,\n",
    "  tenure INTEGER,\n",
    "  PhoneService TEXT,\n",
    "  MultipleLines TEXT,\n",
    "  InternetService TEXT,\n",
    "  OnlineSecurity TEXT,\n",
    "  OnlineBackup TEXT,\n",
    "  DeviceProtection TEXT,\n",
    "  TechSupport TEXT,\n",
    "  StreamingTV TEXT,\n",
    "  StreamingMovies TEXT,\n",
    "  Contract TEXT,\n",
    "  PaperlessBilling TEXT,\n",
    "  PaymentMethod TEXT,\n",
    "  MonthlyCharges REAL,\n",
    "  TotalCharges REAL,\n",
    "  Churn TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Notebook cells ‚Äî **Option A: SQLite** (simple & built-in)\n",
    "\n",
    "**1) Load cleaned data and write SQLite DB**\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "L3 = Path.cwd()  # adjust if running from repo root\n",
    "df = pd.read_parquet(L3 / \"data\" / \"telco_clean.parquet\")\n",
    "\n",
    "# Create / connect DB\n",
    "db_path = L3 / \"db\" / \"telco_clean.sqlite\"\n",
    "db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "conn = sqlite3.connect(db_path.as_posix())\n",
    "\n",
    "# (Optional) enforce schema explicitly\n",
    "schema_sql = (L3 / \"sql\" / \"schema\" / \"create_telco_table.sql\").read_text()\n",
    "conn.executescript(schema_sql)\n",
    "\n",
    "# Insert data (replace if table exists)\n",
    "df.to_sql(\"telco_churn\", conn, if_exists=\"replace\", index=False)\n",
    "print(\"‚úÖ Wrote telco_churn to SQLite:\", db_path)\n",
    "```\n",
    "\n",
    "**2) Helper to run `.sql` files**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def run_sql_file(path, conn):\n",
    "    sql = Path(path).read_text()\n",
    "    return pd.read_sql_query(sql, conn)\n",
    "\n",
    "# Example: churn by contract\n",
    "res = run_sql_file(L3 / \"sql\" / \"queries\" / \"churn_by_contract.sql\", conn)\n",
    "res.head()\n",
    "```\n",
    "\n",
    "**3) Quick viz from SQL result**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res.plot(kind=\"bar\", x=\"Contract\", y=\"churn_rate_pct\", figsize=(6,3))\n",
    "plt.title(\"Churn Rate by Contract (%)\")\n",
    "plt.ylabel(\"%\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Notebook cells ‚Äî **Option B: DuckDB** (fast, zero setup)\n",
    "\n",
    "**Why DuckDB?** In-process, no server, can query Parquet directly (skip a DB file entirely).\n",
    "\n",
    "**1) Query Parquet directly (no load step)**\n",
    "\n",
    "```python\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "L3 = Path.cwd()\n",
    "parq = (L3 / \"data\" / \"telco_clean.parquet\").as_posix()\n",
    "\n",
    "# Register Parquet as a view/table\n",
    "con.execute(f\"CREATE OR REPLACE VIEW telco_churn AS SELECT * FROM parquet_scan('{parq}');\")\n",
    "\n",
    "# Run a quick query inline\n",
    "q = \"\"\"\n",
    "SELECT Contract,\n",
    "       COUNT(*) AS customers,\n",
    "       ROUND(100.0 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_rate_pct\n",
    "FROM telco_churn\n",
    "GROUP BY Contract\n",
    "ORDER BY churn_rate_pct DESC;\n",
    "\"\"\"\n",
    "duckdf = con.execute(q).df()\n",
    "duckdf\n",
    "```\n",
    "\n",
    "**2) Reuse the same `.sql` files**\n",
    "\n",
    "```python\n",
    "sql = (L3 / \"sql\" / \"queries\" / \"revenue_by_tenure.sql\").read_text()\n",
    "con.execute(sql).df().head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ README.md (Level_3) ‚Äî key lines to include\n",
    "\n",
    "* What SQL engine you support (SQLite by default; DuckDB optional).\n",
    "* How to generate the DB (one cell) and how to run queries from `sql/queries/`.\n",
    "* A small screenshot or table preview of a query result.\n",
    "* A sentence like:\n",
    "  *‚ÄúThis level demonstrates bilingual analytics: each insight can be produced in **pandas** or **SQL**, reinforcing real-world team workflows.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Pandas ‚Üî SQL ‚Äúbilingual‚Äù example (show both)\n",
    "\n",
    "```python\n",
    "# Pandas\n",
    "df.groupby(['Contract','InternetService']).size().reset_index(name='customers')\n",
    "\n",
    "# SQL (SQLite or DuckDB)\n",
    "sql = \"\"\"\n",
    "SELECT Contract, InternetService, COUNT(*) AS customers\n",
    "FROM telco_churn\n",
    "GROUP BY Contract, InternetService\n",
    "ORDER BY customers DESC;\n",
    "\"\"\"\n",
    "run_sql_file(L3 / \"sql\" / \"queries\" / \"top_payment_methods.sql\", conn)  # or con.execute(sql).df() for DuckDB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ TL;DR\n",
    "\n",
    "* **Introduce SQL at Level_3** with a tidy `sql/queries/` directory.\n",
    "* Default to **SQLite** (no extra deps) and optionally showcase **DuckDB** (query Parquet directly).\n",
    "* Keep DB artifacts under `db/` and **ignore them in git**.\n",
    "* Demonstrate **pandas vs SQL** for the same insight to signal bilingual fluency.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "Here‚Äôs a concrete star schema you could use for the IBM Telco Churn dataset ‚Äî both a **‚Äúreal-world ideal‚Äù** and a **‚Äúpractical from this CSV‚Äù** version.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Business process & grain\n",
    "\n",
    "**Business process:**\n",
    "\n",
    "> *‚ÄúTrack customer subscription status and revenue, and whether the customer has churned.‚Äù*\n",
    "\n",
    "**Grain (practical with IBM dataset):**\n",
    "\n",
    "> **One row per customer snapshot** at the time the dataset was pulled.\n",
    "\n",
    "So your main fact table is a **customer status snapshot** with measures like monthly charges, total charges, tenure, and a churn flag.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Star schema overview\n",
    "\n",
    "### Fact table\n",
    "\n",
    "#### `fact_customer_snapshot`\n",
    "\n",
    "**Grain:** one row per customer at snapshot date.\n",
    "\n",
    "**Keys (FKs):**\n",
    "\n",
    "* `customer_key` ‚Üí `dim_customer`\n",
    "* `snapshot_date_key` ‚Üí `dim_date`\n",
    "* `contract_key` ‚Üí `dim_contract`\n",
    "* `internet_service_key` ‚Üí `dim_internet_service`\n",
    "* `phone_service_key` ‚Üí `dim_phone_service`\n",
    "* `payment_key` ‚Üí `dim_payment`\n",
    "* `geography_key` ‚Üí `dim_geography` (if you add city/state/zip later)\n",
    "\n",
    "**Measures:**\n",
    "\n",
    "* `monthly_charges_amt`\n",
    "* `total_charges_amt`\n",
    "* `tenure_months`\n",
    "* `num_services` (derived: count of ‚ÄúYes‚Äù service flags)\n",
    "* `is_churned` (0/1 from `Churn`)\n",
    "* `is_senior_citizen` (can be a degenerate measure or in `dim_customer`)\n",
    "\n",
    "**Degenerate dimensions (optional):**\n",
    "\n",
    "* `customer_id` (raw `customerID` from source, kept here as a degenerate dimension even though you also have a surrogate `customer_key`)\n",
    "\n",
    "---\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "#### 1. `dim_customer`\n",
    "\n",
    "Relatively static customer attributes.\n",
    "\n",
    "* `customer_key` (PK, surrogate)\n",
    "* `customer_id` (natural key from source)\n",
    "* `gender`\n",
    "* `senior_citizen_flag`\n",
    "* `partner_flag`\n",
    "* `dependents_flag`\n",
    "* `customer_since_date_key` (if you can derive it from tenure & snapshot date)\n",
    "* `customer_segment` (optional derived attribute, e.g., ‚ÄúYoung Single‚Äù, ‚ÄúSenior Couple‚Äù)\n",
    "\n",
    "*Source columns*: `customerID`, `gender`, `SeniorCitizen`, `Partner`, `Dependents`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `dim_contract`\n",
    "\n",
    "Contract-related attributes.\n",
    "\n",
    "* `contract_key` (PK)\n",
    "* `contract_type`  (e.g., Month-to-month, One year, Two year)\n",
    "* `paperless_billing_flag` (Yes/No)\n",
    "* `billing_type` (derived: ‚ÄúElectronic‚Äù, ‚ÄúPaper‚Äù)\n",
    "* `contract_risk_level` (derived: e.g., ‚ÄúHigh churn risk‚Äù for month-to-month)\n",
    "\n",
    "*Source columns*: `Contract`, `PaperlessBilling`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `dim_payment`\n",
    "\n",
    "Payment method details.\n",
    "\n",
    "* `payment_key` (PK)\n",
    "* `payment_method` (e.g., Electronic check, Mailed check)\n",
    "* `is_electronic_flag`\n",
    "* `card_vs_bank_flag` (if you want to group: credit card vs bank vs check)\n",
    "\n",
    "*Source columns*: `PaymentMethod`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. `dim_internet_service`\n",
    "\n",
    "Internet and online add-ons.\n",
    "\n",
    "* `internet_service_key` (PK)\n",
    "* `internet_service_type` (DSL, Fiber optic, No)\n",
    "* `online_security_flag` (Yes/No/No internet)\n",
    "* `online_backup_flag`\n",
    "* `device_protection_flag`\n",
    "* `tech_support_flag`\n",
    "* `streaming_tv_flag`\n",
    "* `streaming_movies_flag`\n",
    "* `is_internet_customer_flag` (derived from `InternetService <> 'No'`)\n",
    "* `internet_bundle_type` (derived; e.g., ‚ÄúTriple-play‚Äù, ‚ÄúStreaming only‚Äù)\n",
    "\n",
    "*Source columns*:\n",
    "`InternetService`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. `dim_phone_service`\n",
    "\n",
    "Phone/voice details.\n",
    "\n",
    "* `phone_service_key` (PK)\n",
    "* `phone_service_flag` (Yes/No)\n",
    "* `multiple_lines_flag` (Yes/No/No phone)\n",
    "\n",
    "*Source columns*: `PhoneService`, `MultipleLines`\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. `dim_geography` (optional now, future-proof)\n",
    "\n",
    "If/when you get location-level data (e.g., region, city):\n",
    "\n",
    "* `geography_key` (PK)\n",
    "* `country`\n",
    "* `state`\n",
    "* `city`\n",
    "* `zip_code`\n",
    "* `market_region` (e.g., ‚ÄúWest‚Äù, ‚ÄúMidwest‚Äù)\n",
    "* `urban_rural_code`\n",
    "\n",
    "Right now IBM sample doesn‚Äôt have this, but you can add if you enrich later.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. `dim_date`\n",
    "\n",
    "Standard date dimension for reporting by time.\n",
    "\n",
    "* `date_key` (PK, `YYYYMMDD` int)\n",
    "* `date`\n",
    "* `day_of_week`\n",
    "* `day_name`\n",
    "* `week_of_year`\n",
    "* `month`\n",
    "* `month_name`\n",
    "* `quarter`\n",
    "* `year`\n",
    "* `is_month_end_flag`\n",
    "* `is_weekend_flag`\n",
    "\n",
    "You‚Äôll at least have:\n",
    "\n",
    "* `snapshot_date_key` in `fact_customer_snapshot`\n",
    "* later you might add `contract_start_date_key`, `churn_date_key` if you get longitudinal data.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Column mapping from the raw IBM Telco dataset\n",
    "\n",
    "Here‚Äôs how the CSV columns roughly map into your dimensional model:\n",
    "\n",
    "**Raw ‚Üí Dimensional**\n",
    "\n",
    "* `customerID` ‚Üí `dim_customer.customer_id` & `fact_customer_snapshot.customer_id` (degenerate)\n",
    "* `gender`, `SeniorCitizen`, `Partner`, `Dependents` ‚Üí `dim_customer`\n",
    "* `tenure` ‚Üí `fact_customer_snapshot.tenure_months` (and maybe used to back-calc `customer_since_date_key`)\n",
    "* `PhoneService`, `MultipleLines` ‚Üí `dim_phone_service`\n",
    "* `InternetService`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies` ‚Üí `dim_internet_service`\n",
    "* `Contract`, `PaperlessBilling` ‚Üí `dim_contract`\n",
    "* `PaymentMethod` ‚Üí `dim_payment`\n",
    "* `MonthlyCharges`, `TotalCharges` ‚Üí `fact_customer_snapshot.monthly_charges_amt`, `total_charges_amt`\n",
    "* `Churn` ‚Üí `fact_customer_snapshot.is_churned` (Yes=1, No=0)\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Example DDL (BigQuery-ish SQL)\n",
    "\n",
    "You can tweak types, but this gives you a concrete starting point.\n",
    "\n",
    "```sql\n",
    "-- DIMENSIONS\n",
    "\n",
    "CREATE TABLE dim_customer (\n",
    "  customer_key INT64 GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  customer_id STRING,\n",
    "  gender STRING,\n",
    "  senior_citizen_flag BOOL,\n",
    "  partner_flag BOOL,\n",
    "  dependents_flag BOOL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_contract (\n",
    "  contract_key INT64 GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  contract_type STRING,\n",
    "  paperless_billing_flag BOOL,\n",
    "  contract_risk_level STRING\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_payment (\n",
    "  payment_key INT64 GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  payment_method STRING,\n",
    "  is_electronic_flag BOOL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_internet_service (\n",
    "  internet_service_key INT64 GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  internet_service_type STRING,\n",
    "  online_security_flag STRING,\n",
    "  online_backup_flag STRING,\n",
    "  device_protection_flag STRING,\n",
    "  tech_support_flag STRING,\n",
    "  streaming_tv_flag STRING,\n",
    "  streaming_movies_flag STRING,\n",
    "  is_internet_customer_flag BOOL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_phone_service (\n",
    "  phone_service_key INT64 GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  phone_service_flag STRING,\n",
    "  multiple_lines_flag STRING\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_date (\n",
    "  date_key INT64 PRIMARY KEY,\n",
    "  date DATE,\n",
    "  year INT64,\n",
    "  quarter INT64,\n",
    "  month INT64,\n",
    "  month_name STRING,\n",
    "  day_of_month INT64,\n",
    "  day_of_week INT64,\n",
    "  day_name STRING,\n",
    "  is_weekend_flag BOOL\n",
    ");\n",
    "\n",
    "-- FACT\n",
    "\n",
    "CREATE TABLE fact_customer_snapshot (\n",
    "  customer_key INT64,\n",
    "  snapshot_date_key INT64,\n",
    "  contract_key INT64,\n",
    "  payment_key INT64,\n",
    "  internet_service_key INT64,\n",
    "  phone_service_key INT64,\n",
    "  geography_key INT64,\n",
    "\n",
    "  customer_id STRING,        -- degenerate dim\n",
    "  tenure_months INT64,\n",
    "  monthly_charges_amt NUMERIC,\n",
    "  total_charges_amt NUMERIC,\n",
    "  num_services INT64,\n",
    "  is_churned BOOL,\n",
    "\n",
    "  FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key),\n",
    "  FOREIGN KEY (snapshot_date_key) REFERENCES dim_date(date_key),\n",
    "  FOREIGN KEY (contract_key) REFERENCES dim_contract(contract_key),\n",
    "  FOREIGN KEY (payment_key) REFERENCES dim_payment(payment_key),\n",
    "  FOREIGN KEY (internet_service_key) REFERENCES dim_internet_service(internet_service_key),\n",
    "  FOREIGN KEY (phone_service_key) REFERENCES dim_phone_service(phone_service_key)\n",
    ");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ How you‚Äôd use this model (examples)\n",
    "\n",
    "Once populated, you can easily answer:\n",
    "\n",
    "* **Churn rate by contract type & payment method:**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  dc.contract_type,\n",
    "  dp.payment_method,\n",
    "  AVG(CASE WHEN f.is_churned THEN 1 ELSE 0 END) AS churn_rate\n",
    "FROM fact_customer_snapshot f\n",
    "JOIN dim_contract dc ON f.contract_key = dc.contract_key\n",
    "JOIN dim_payment dp ON f.payment_key = dp.payment_key\n",
    "GROUP BY 1, 2;\n",
    "```\n",
    "\n",
    "* **ARPU and churn by internet bundle:**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  di.internet_service_type,\n",
    "  di.internet_bundle_type,\n",
    "  AVG(f.monthly_charges_amt) AS avg_monthly_revenue,\n",
    "  AVG(CASE WHEN f.is_churned THEN 1 ELSE 0 END) AS churn_rate\n",
    "FROM fact_customer_snapshot f\n",
    "JOIN dim_internet_service di ON f.internet_service_key = di.internet_service_key\n",
    "GROUP BY 1, 2;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next step I can:\n",
    "\n",
    "* Turn this into a **dbt-style model plan** (`stg_telco`, `dim_*`, `fact_*`), or\n",
    "* Show an **exact SQL pipeline** from the original `telco_churn_raw` table into these dimensions and fact.\n",
    "\n",
    "Yes ‚Äî the structures I gave you **are SQL table definitions (DDL statements)**.\n",
    "\n",
    "They show how you would build a **dimensional star schema** for the Telco Churn dataset inside a database such as:\n",
    "\n",
    "* **BigQuery**\n",
    "* **PostgreSQL**\n",
    "* **MySQL**\n",
    "* **Snowflake**\n",
    "* **SQL Server**\n",
    "* **DuckDB**\n",
    "* **Databricks SQL**\n",
    "\n",
    "(Syntax may vary slightly between engines, but the concept stays the same.)\n",
    "\n",
    "---\n",
    "\n",
    "### Clarifying clearly:\n",
    "\n",
    "| Term                 | Meaning                                                                                     |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Fact table**       | Holds numeric measures and foreign keys to dimensions ‚Äî center of the star schema           |\n",
    "| **Dimension tables** | Contain descriptive attributes such as contract, customer, payment method, internet service |\n",
    "| **SQL DDL**          | The actual `CREATE TABLE ...` statements used to build the schema                           |\n",
    "\n",
    "---\n",
    "\n",
    "### What I gave you previously = these kinds of SQL objects üëá\n",
    "\n",
    "```sql\n",
    "CREATE TABLE dim_customer (...);       -- dimension\n",
    "CREATE TABLE dim_contract (...);       -- dimension\n",
    "CREATE TABLE dim_payment (...);        -- dimension\n",
    "CREATE TABLE dim_internet_service (...);  -- dimension\n",
    "CREATE TABLE dim_phone_service (...);     -- dimension\n",
    "CREATE TABLE dim_date (...);           -- standard date dimension\n",
    "CREATE TABLE fact_customer_snapshot (...);  -- fact table\n",
    "```\n",
    "\n",
    "These statements define the shape of the data warehouse you're building.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can now:\n",
    "\n",
    "üìå **Option A ‚Äî Build full SQL ETL to populate these tables**\n",
    "üìå **Option B ‚Äî Convert the schema into dbt models (with DIM/FACT folder structure)**\n",
    "üìå **Option C ‚Äî Write Python/BigQuery code to ingest & load from the CSV automatically**\n",
    "üìå **Option D ‚Äî Generate diagrams (star schema chart) for documentation**\n",
    "\n",
    "Just tell me which direction you want next.\n",
    "\n",
    "Is dimensional modeling used for warehousing \n",
    "\n",
    "Yes ‚Äî **dimensional modeling is one of the core methods used in data warehousing.**\n",
    "\n",
    "In fact, when people say *‚Äúdata warehouse‚Äù*, they're often referring to a system structured around:\n",
    "\n",
    "üîπ **Fact tables** (measurable events like sales, churn, revenue)\n",
    "üîπ **Dimension tables** (descriptions like customer, product, time, geography)\n",
    "\n",
    "This star/snowflake layout is what makes warehouses fast for analytics.\n",
    "\n",
    "---\n",
    "\n",
    "# Why dimensional modeling fits data warehouses\n",
    "\n",
    "| Reason                          | Meaning                                                                        |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| ‚≠ê Optimized for queries         | Analysts can slice and drill into data quickly                                 |\n",
    "| üìä Great for BI tools           | Tableau, Power BI, Looker, etc. work best in star schemas                      |\n",
    "| üß± Supports large history       | Warehouses store years of data ‚Äî dimensions help track changes (SCD)           |\n",
    "| üìà Analytical not transactional | Warehouses exist to explore trends, not run applications                       |\n",
    "| üí° Clear business meaning       | Facts and dimensions mirror how businesses think (\"sales by product by month\") |\n",
    "\n",
    "A warehouse isn't just storage ‚Äî dimensional modeling **turns data into something usable**.\n",
    "\n",
    "---\n",
    "\n",
    "# Quick phrasing you could use anywhere\n",
    "\n",
    "> **Dimensional modeling is the design pattern used to structure data warehouses for reporting and analytics.**\n",
    "> Instead of normalized tables meant for transactions, it organizes data into **facts + dimensions** so analysis becomes fast, intuitive, and scalable.\n",
    "\n",
    "---\n",
    "\n",
    "# How it connects back to your Telco churn project\n",
    "\n",
    "You‚Äôre building:\n",
    "\n",
    "‚úî dimensional model (fact_customer_snapshot + dim_customer, dim_contract, etc.)\n",
    "‚úî churn metrics\n",
    "‚úî customer profiles\n",
    "‚úî historical analysis ability\n",
    "\n",
    "That‚Äôs exactly what a data warehouse is meant for.\n",
    "\n",
    "So your project isn't just a dataset ‚Äî it's the **foundation of an analytical warehouse**.\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can show you:\n",
    "\n",
    "üìå a visual warehouse diagram for your Telco model\n",
    "üìå what your final warehouse layer would look like in dbt\n",
    "üìå how to turn it into a feature store for churn prediction\n",
    "üìå how the next step becomes ML forecasting\n",
    "\n",
    "Just tell me where you want to take it.\n",
    "\n",
    "What are examples of flattening the truth\n",
    "\n",
    "**Flattening the truth** means presenting a situation in a simpler, cleaner, or more convenient way than reality ‚Äî not necessarily lying, but **removing nuance** so it‚Äôs easier to digest.\n",
    "\n",
    "Here are clear examples:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Reducing complexity into a single cause**\n",
    "\n",
    "Real world:\n",
    "A customer churns due to *price + service issues + lack of loyalty + competitor offers*.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúThey left because our prices were too high.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Taking a gradient and making it binary**\n",
    "\n",
    "Real world:\n",
    "Someone is *nervous, unsure, hopeful, conflicted, afraid of commitment, and overwhelmed*.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúShe didn‚Äôt want to date me.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ignoring outliers for a clean narrative**\n",
    "\n",
    "Real world:\n",
    "Most subscribers churn in month 1‚Äì3, but some churn after 40 months for unrelated reasons.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúNew customers churn early ‚Äî that‚Äôs the whole story.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Turning a spectrum into a category**\n",
    "\n",
    "Real world:\n",
    "Support quality ranges from amazing ‚Üí decent ‚Üí sloppy ‚Üí unresponsive over time.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúSupport is either good or bad.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Oversimplifying responsibility**\n",
    "\n",
    "Real world:\n",
    "A project fails due to miscommunication, unclear ownership, lack of data, shifting priorities.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúIt failed because no one cared.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Treating feelings as facts**\n",
    "\n",
    "Real world:\n",
    "You *think* someone is upset, busy, confused, or uncertain.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúShe doesn‚Äôt like me anymore.‚Äù\n",
    "\n",
    "Even if it might be partly true, the complexity behind it gets compressed.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Shrinking multifaceted outcomes into a label**\n",
    "\n",
    "Real world:\n",
    "A person is *good at technical skills, average at communication, weak at deadlines*.\n",
    "\n",
    "Flattened truth:\n",
    "\n",
    "> ‚ÄúHe‚Äôs a bad engineer.‚Äù\n",
    "> or\n",
    "> ‚ÄúHe‚Äôs a rockstar.‚Äù\n",
    "\n",
    "Both discard detail.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why people flatten the truth**\n",
    "\n",
    "‚úî to make stories cleaner\n",
    "‚úî to avoid uncertainty or ambiguity\n",
    "‚úî to communicate faster\n",
    "‚úî to emotionally self-protect\n",
    "‚úî to avoid cognitive overload\n",
    "‚úî because nuance requires effort\n",
    "\n",
    "Flattening isn‚Äôt always wrong ‚Äî but it hides the full shape of reality.\n",
    "Data engineers deal with this constantly (metrics distill complexity).\n",
    "Relationships deal with it even more (feelings get compressed into interpretations).\n",
    "\n",
    "---\n",
    "\n",
    "### If you want, I can go further:\n",
    "\n",
    "1. How to detect when you‚Äôre flattening truth\n",
    "2. How to expand flattened truths into nuance again\n",
    "3. How flattened truths shape analytics, relationships, and decisions\n",
    "4. How to communicate complexity **clearly without oversimplifying**\n",
    "\n",
    "Just tell me which direction.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
