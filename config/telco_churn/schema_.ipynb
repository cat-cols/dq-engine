{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ec5d7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "># Generate a **YAML** version of the same schema (some teams prefer human-editable YAML in `config/`), plus a helper to **validate** that the current dataset still matches the schema before training.\n",
    "\n",
    "Awesome — here’s a **YAML-first** setup plus a **validator/helper** so your preprocessing is driven by a human-readable config.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) `config/feature_schema.yaml`\n",
    "\n",
    "```yaml\n",
    "# Telco Churn — feature schema & strategies\n",
    "version: 1\n",
    "target: Churn_flag\n",
    "\n",
    "binary:\n",
    "  columns:\n",
    "    - SeniorCitizen\n",
    "    - Churn_flag\n",
    "  encode: passthrough          # keep as 0/1\n",
    "  dtype: int8                  # optional: cast in preprocessing\n",
    "  missing: most_frequent\n",
    "\n",
    "continuous:\n",
    "  columns:\n",
    "    - tenure\n",
    "    - MonthlyCharges\n",
    "    - TotalCharges\n",
    "  default_scale: StandardScaler\n",
    "  missing: median\n",
    "\n",
    "categorical:\n",
    "  columns:\n",
    "    - gender\n",
    "    - Partner\n",
    "    - Dependents\n",
    "    - PhoneService\n",
    "    - MultipleLines\n",
    "    - InternetService\n",
    "    - OnlineSecurity\n",
    "    - OnlineBackup\n",
    "    - DeviceProtection\n",
    "    - TechSupport\n",
    "    - StreamingTV\n",
    "    - StreamingMovies\n",
    "    - Contract\n",
    "    - PaperlessBilling\n",
    "    - PaymentMethod\n",
    "  encode: OneHotEncoder\n",
    "  missing: most_frequent\n",
    "  onehot_params:\n",
    "    handle_unknown: ignore\n",
    "    sparse_output: false\n",
    "\n",
    "# Column-level overrides (take precedence over defaults)\n",
    "overrides:\n",
    "  TotalCharges:\n",
    "    scale: QuantileTransformer\n",
    "    params:\n",
    "      output_distribution: normal\n",
    "  MonthlyCharges:\n",
    "    scale: RobustScaler\n",
    "```\n",
    "\n",
    "> Edit the lists to match your dataset if columns differ.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Validator + Preprocessor Builder (`src/pipeline/schema_loader.py`)\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, QuantileTransformer\n",
    "\n",
    "\n",
    "# ---------- Loader ----------\n",
    "\n",
    "def load_schema(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "# ---------- Validator ----------\n",
    "\n",
    "@dataclass\n",
    "class SchemaReport:\n",
    "    missing_columns: List[str]\n",
    "    unexpected_columns: List[str]\n",
    "    binary_not_two_unique: List[str]\n",
    "    notes: List[str]\n",
    "\n",
    "    def ok(self) -> bool:\n",
    "        return not (self.missing_columns or self.binary_not_two_unique)\n",
    "\n",
    "def validate_dataset_against_schema(df: pd.DataFrame, schema: Dict[str, Any]) -> SchemaReport:\n",
    "    tgt = schema[\"target\"]\n",
    "    groups = [\"binary\", \"continuous\", \"categorical\"]\n",
    "    expected_cols = set([tgt])\n",
    "    for g in groups:\n",
    "        expected_cols.update(schema.get(g, {}).get(\"columns\", []))\n",
    "\n",
    "    missing = sorted(col for col in expected_cols if col not in df.columns)\n",
    "    unexpected = sorted(col for col in df.columns if col not in expected_cols)\n",
    "\n",
    "    binary_cols = schema.get(\"binary\", {}).get(\"columns\", [])\n",
    "    binary_not_two = []\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            nunq = df[col].dropna().nunique()\n",
    "            if nunq != 2:\n",
    "                binary_not_two.append(f\"{col} (nunique={nunq})\")\n",
    "\n",
    "    notes = []\n",
    "    if tgt in df.columns and df[tgt].dropna().nunique() != 2:\n",
    "        notes.append(f\"Target {tgt} has nunique={df[tgt].dropna().nunique()} (expected 2)\")\n",
    "\n",
    "    return SchemaReport(missing_columns=missing,\n",
    "                        unexpected_columns=unexpected,\n",
    "                        binary_not_two_unique=binary_not_two,\n",
    "                        notes=notes)\n",
    "\n",
    "\n",
    "# ---------- Preprocessor Builder ----------\n",
    "\n",
    "def _scaler_from_name(name: str, params: Dict[str, Any] | None = None):\n",
    "    params = params or {}\n",
    "    if name == \"StandardScaler\":\n",
    "        return StandardScaler()\n",
    "    if name == \"RobustScaler\":\n",
    "        return RobustScaler()\n",
    "    if name == \"QuantileTransformer\":\n",
    "        return QuantileTransformer(**({\"output_distribution\": \"normal\"} | params))\n",
    "    raise ValueError(f\"Unknown scaler: {name}\")\n",
    "\n",
    "def build_preprocessor_from_schema(schema: Dict[str, Any]) -> ColumnTransformer:\n",
    "    bin_spec = schema.get(\"binary\", {})\n",
    "    cont_spec = schema.get(\"continuous\", {})\n",
    "    cat_spec = schema.get(\"categorical\", {})\n",
    "    overrides = schema.get(\"overrides\", {})\n",
    "\n",
    "    bin_cols = bin_spec.get(\"columns\", [])\n",
    "    cont_cols = cont_spec.get(\"columns\", [])\n",
    "    cat_cols = cat_spec.get(\"columns\", [])\n",
    "\n",
    "    # Binary: impute then passthrough\n",
    "    bin_pipe = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=bin_spec.get(\"missing\", \"most_frequent\"))),\n",
    "        (\"passthrough\", \"passthrough\")\n",
    "    ])\n",
    "\n",
    "    # Continuous: per-column scaling (allows overrides)\n",
    "    cont_transformers = []\n",
    "    for c in cont_cols:\n",
    "        ov = overrides.get(c, {})\n",
    "        scaler_name = ov.get(\"scale\", cont_spec.get(\"default_scale\", \"StandardScaler\"))\n",
    "        scaler = _scaler_from_name(scaler_name, ov.get(\"params\"))\n",
    "        cont_transformers.append((\n",
    "            f\"cont__{c}\",\n",
    "            Pipeline(steps=[\n",
    "                (\"impute\", SimpleImputer(strategy=cont_spec.get(\"missing\", \"median\"))),\n",
    "                (\"scale\", scaler)\n",
    "            ]),\n",
    "            [c]\n",
    "        ))\n",
    "\n",
    "    # Categorical: impute + onehot\n",
    "    oh_params = cat_spec.get(\"onehot_params\", {\"handle_unknown\": \"ignore\", \"sparse_output\": False})\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=cat_spec.get(\"missing\", \"most_frequent\"))),\n",
    "        (\"onehot\", OneHotEncoder(**oh_params))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"binary\", bin_pipe, bin_cols),\n",
    "            (\"categorical\", cat_pipe, cat_cols),\n",
    "            *cont_transformers\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return pre\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Usage Example in Modeling Notebook\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data (ensure Churn_flag exists)\n",
    "# df = ...\n",
    "\n",
    "# Load + validate\n",
    "schema = load_schema(\"config/feature_schema.yaml\")\n",
    "report = validate_dataset_against_schema(df, schema)\n",
    "\n",
    "if not report.ok():\n",
    "    print(\"❌ Schema validation issues:\")\n",
    "    if report.missing_columns:   print(\"  - Missing:\", report.missing_columns)\n",
    "    if report.binary_not_two_unique: print(\"  - Binary not 2 unique:\", report.binary_not_two_unique)\n",
    "    if report.notes:             print(\"  - Notes:\", report.notes)\n",
    "    # decide: raise or continue\n",
    "    # raise ValueError(\"Schema validation failed.\")\n",
    "\n",
    "print(\"✅ Schema looks good.\")\n",
    "\n",
    "# Build preprocessor\n",
    "pre = build_preprocessor_from_schema(schema)\n",
    "\n",
    "X = df.drop(columns=[schema[\"target\"]])\n",
    "y = df[schema[\"target\"]]\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "clf.fit(X, y)\n",
    "print(\"✅ Model trained.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Pro tips\n",
    "\n",
    "* Keep the YAML as your **single source of truth**; update overrides as EDA reveals skew/heavy tails.\n",
    "* Pin schema versions (`version:`) and save `describe().T`/`skew`/`kurtosis` snapshots to `reports/` for auditability.\n",
    "* If you later add **imputation rules per column** (e.g., specific medians), add them under `overrides:<col>:impute_value` and swap `SimpleImputer` to `strategy=\"constant\"` with that value.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
