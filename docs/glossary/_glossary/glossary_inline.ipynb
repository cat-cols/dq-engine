{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38004a6a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>FIXES</summary>\n",
    "\n",
    "```python\n",
    "import ssl, nltk, os\n",
    "ssl._create_default_https_context = ssl._create_unverified_context  # TEMP: bypass SSL\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "for pkg in [\"punkt\", \"punkt_tab\", \"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\"]:\n",
    "    nltk.download(pkg)\n",
    "\n",
    "print(\"‚úÖ NLTK data downloaded. You can restart the kernel and remove the SSL bypass line.\")\n",
    "\n",
    "import ssl, certifi, urllib.request\n",
    "print(\"certifi:\", certifi.where())\n",
    "print(\"ssl cafile:\", ssl.get_default_verify_paths().cafile)\n",
    "urllib.request.urlopen(\"https://www.python.org\").read(1)  # should not raise\n",
    "# ERROR [1]\n",
    "# ERROR [1]: still using the system‚Äôs default SSL context, so the request isn‚Äôt picking up certifi‚Äôs CA bundle. Do one of these (fastest first):\n",
    "\n",
    "### A) Quick kernel patch (works immediately in this notebook)\n",
    "# Run this **in a cell once**, then try the urllib/NLTK downloads again in the *same* kernel:\n",
    "# Then your NLTK bootstrap/download calls should succeed.\n",
    "\n",
    "import os, ssl, certifi\n",
    "\n",
    "# Route ALL new HTTPS contexts to certifi's CA bundle\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "ssl._create_default_https_context = lambda *a, **k: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# **Test:**\n",
    "import urllib.request\n",
    "urllib.request.urlopen(\"https://www.python.org\").read(1)  # should NOT raise now\n",
    "\n",
    "### B) Make it persistent for this venv (recommended)\n",
    "\n",
    "# Append these lines to **`.venv/bin/activate`**:\n",
    "export SSL_CERT_FILE=\"$(python -c 'import certifi; print(certifi.where())')\"\n",
    "export REQUESTS_CA_BUNDLE=\"$SSL_CERT_FILE\"\n",
    "\n",
    "# Then restart:\n",
    "deactivate 2>/dev/null || true\n",
    "source .venv/bin/activate\n",
    "python - <<'PY'\n",
    "import ssl, urllib.request, certifi\n",
    "print(\"certifi:\", certifi.where())\n",
    "print(\"cafile:\", ssl.get_default_verify_paths().cafile)\n",
    "urllib.request.urlopen(\"https://www.python.org\").read(1)\n",
    "print(\"OK\")\n",
    "PY\n",
    "\n",
    "### C) Run Apple‚Äôs cert installer (if you have the python.org build)\n",
    "\n",
    "# Your path shows a **Frameworks** install (`/Library/Frameworks/...`), which usually also installs this script\n",
    "# open \"/Applications/Python 3.12/Install Certificates.command\"\n",
    "\n",
    "# If unsure of the exact version folder, let macOS find it:\n",
    "# !find /Applications -maxdepth 2 -name \"Install Certificates.command\" -print\n",
    "# then:\n",
    "# open \"/Applications/Python 3.XX/Install Certificates.command\"\n",
    "# After it runs, restart the terminal, reactivate the venv, and the SSL error should be gone without any code patches.\n",
    "# If you still hit issues after A)  hard-wire the NLTK downloader to your `~/nltk_data` directory as a fallback\n",
    "\n",
    "import os, ssl, certifi\n",
    "\n",
    "# Route ALL new HTTPS contexts to certifi's CA bundle\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "ssl._create_default_https_context = lambda *a, **k: ssl.create_default_context(cafile=certifi.where())\n",
    "import urllib.request\n",
    "urllib.request.urlopen(\"https://www.python.org\").read(1)\n",
    "# üìö Inline Sectioned Glossary Builder ‚Äî with Noun-Phrase Bigrams: \"e.g., customer churn, monthly charges\"\n",
    "#    - Captures unigrams (NN/JJ) and NP bigrams like \"customer churn\"\n",
    "#    - Looks up definitions (tries underscore form for WordNet)\n",
    "\n",
    "# --- NLTK bootstrap: handle macOS SSL + local cache ---\n",
    "# import os, ssl, nltk\n",
    "# NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "# os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "# # Make sure NLTK looks here first\n",
    "# if NLTK_DIR not in nltk.data.path:\n",
    "#     nltk.data.path.insert(0, NLTK_DIR)\n",
    "\n",
    "# # TEMP: bypass SSL verification only for these downloads\n",
    "# try:\n",
    "#     _orig_ctx = ssl._create_default_https_context\n",
    "#     ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# except Exception:\n",
    "#     _orig_ctx = None\n",
    "\n",
    "# # Download required packages if missing, into ~/nltk_data\n",
    "# for pkg, kind in [\n",
    "#     (\"punkt\", \"tokenizers\"), (\"punkt_tab\", \"tokenizers\"),\n",
    "#     (\"averaged_perceptron_tagger\", \"taggers\"),\n",
    "#     (\"wordnet\", \"corpora\"), (\"omw-1.4\", \"corpora\")\n",
    "# ]:\n",
    "#     try:\n",
    "#         nltk.data.find(f\"{kind}/{pkg}\")\n",
    "#     except LookupError:\n",
    "#         nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "\n",
    "# # Restore SSL context\n",
    "# if _orig_ctx:\n",
    "#     ssl._create_default_https_context = _orig_ctx\n",
    "\n",
    "###\n",
    "\n",
    "# --- Use certifi CA bundle for all HTTPS in this kernel ---\n",
    "import os, ssl, certifi, nltk\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()      # honored by Python ssl\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where() # for requests, if used\n",
    "\n",
    "# ensure new contexts use certifi's bundle\n",
    "ssl._create_default_https_context = \\\n",
    "    (lambda *a, **kw: ssl.create_default_context(cafile=certifi.where()))\n",
    "\n",
    "# now downloads should succeed:\n",
    "for pkg, kind in [\n",
    "    (\"punkt\", \"tokenizers\"), (\"punkt_tab\", \"tokenizers\"),\n",
    "    (\"averaged_perceptron_tagger\", \"taggers\"),\n",
    "    (\"wordnet\", \"corpora\"), (\"omw-1.4\", \"corpora\")\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(f\"{kind}/{pkg}\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=False)  # first run may take a minute\n",
    "print(\"‚úÖ NLTK resources ready\")\n",
    "\n",
    "###\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "# Ensure required NLTK data (handles new punkt_tab too)\n",
    "for pkg in [\"punkt\", \"punkt_tab\", \"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\"]:\n",
    "    try:\n",
    "        if pkg in (\"punkt\",\"punkt_tab\"):\n",
    "            nltk.data.find(f\"tokenizers/{pkg}\")\n",
    "        else:\n",
    "            nltk.data.find(f\"corpora/{pkg}\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "NOTEBOOK_PATH = Path(\"01_EDA.ipynb\")\n",
    "TOP_K_PER_SECTION = 80\n",
    "MIN_FREQ_UNI = 2                  # min unigram frequency\n",
    "MIN_FREQ_BI  = 2                  # min bigram frequency\n",
    "ALLOW_POS_UNI = {\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\"}  # nouns + adjectives\n",
    "MIN_LEN = 3\n",
    "INCLUDE_BIGRAMS = True\n",
    "SAVE_JSON = Path(\"outputs/notebook_glossary_by_section.json\")\n",
    "SAVE_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "# ---------------------------------\n",
    "\n",
    "# Detect section headings / comment headers\n",
    "SEC_RE = re.compile(r\"^\\s{0,3}#{1,6}\\s*(?:(?P<num>(?:\\d+\\.)*\\d+)\\s*)?(?P<title>.*)$\")\n",
    "COMMENT_SEC_RE = re.compile(r\"^\\s*#\\s*(?:(?P<num>(?:\\d+\\.)*\\d+)\\s*)?(?P<title>.*)$\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def map_pos(tag):\n",
    "    if tag.startswith(\"NN\"): return \"n\"\n",
    "    if tag.startswith(\"JJ\"): return \"a\"\n",
    "    return None\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = re.sub(r\"[`*_<>]+\", \" \", text)\n",
    "    text = re.sub(r\"[\\u2000-\\u206F]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def extract_tokens(text: str):\n",
    "    \"\"\"Return tokens + POS tags filtered to A/N (kept raw for bigram logic too).\"\"\"\n",
    "    tokens = [t for t in word_tokenize(text) if re.search(r\"[A-Za-z]\", t)]\n",
    "    tagged  = pos_tag(tokens)\n",
    "    return tagged\n",
    "\n",
    "def lemmatize_if_needed(tok: str, tag: str) -> str:\n",
    "    t = re.sub(r\"[^A-Za-z\\-]\", \"\", tok).lower()\n",
    "    if len(t) < MIN_LEN: \n",
    "        return \"\"\n",
    "    wnpos = map_pos(tag)\n",
    "    return lem.lemmatize(t, wnpos) if wnpos else t\n",
    "\n",
    "def noun_phrase_bigrams(tagged):\n",
    "    \"\"\"\n",
    "    Capture bigrams that look like NP chunks:\n",
    "      (Adj|Noun) + Noun\n",
    "      e.g., 'monthly charges', 'customer churn', 'fiber optic'\n",
    "    Returns lower-cased space-joined bigrams, lemmatizing the head noun.\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    for (w1, t1), (w2, t2) in zip(tagged, tagged[1:]):\n",
    "        if not (t2.startswith(\"NN\") and (t1.startswith(\"JJ\") or t1.startswith(\"NN\"))):\n",
    "            continue\n",
    "        w1n = re.sub(r\"[^A-Za-z\\-]\", \"\", w1).lower()\n",
    "        w2n = lemmatize_if_needed(w2, t2)  # lemmatize head noun\n",
    "        if len(w1n) >= MIN_LEN and len(w2n) >= MIN_LEN:\n",
    "            bigrams.append(f\"{w1n} {w2n}\")\n",
    "    return bigrams\n",
    "\n",
    "def define_term(term: str) -> str:\n",
    "    \"\"\"\n",
    "    Lookup definition in WordNet.\n",
    "    - Try underscore form first for MWEs (e.g., 'customer_churn').\n",
    "    - Fall back to space form.\n",
    "    Prefer noun, then adjective.\n",
    "    \"\"\"\n",
    "    candidates = [term.replace(\" \", \"_\"), term]\n",
    "    for cand in candidates:\n",
    "        syns = wn.synsets(cand)\n",
    "        if syns:\n",
    "            noun_first = [s for s in syns if s.pos() == 'n'] + [s for s in syns if s.pos() == 'a'] + syns\n",
    "            return noun_first[0].definition()\n",
    "    return \"(definition not found)\"\n",
    "\n",
    "# --- 1) Read notebook & gather text chunks by section\n",
    "nb = json.loads(NOTEBOOK_PATH.read_text(encoding=\"utf-8\"))\n",
    "sections_text = defaultdict(list)\n",
    "current_key = \"0.0 Unsectioned\"\n",
    "\n",
    "def make_key(num: str|None, title: str) -> str:\n",
    "    title = (title or \"\").strip()\n",
    "    return f\"{num} {title}\".strip() if num else (title or \"Unsectioned\")\n",
    "\n",
    "for cell in nb.get(\"cells\", []):\n",
    "    ctype = cell.get(\"cell_type\")\n",
    "    src_list = cell.get(\"source\", [])\n",
    "    src = \"\".join(src_list)\n",
    "\n",
    "    if ctype == \"markdown\":\n",
    "        set_key = None\n",
    "        for line in src.splitlines():\n",
    "            m = SEC_RE.match(line)\n",
    "            if m:\n",
    "                set_key = make_key(m.group(\"num\"), m.group(\"title\"))\n",
    "                current_key = set_key\n",
    "                break\n",
    "        sections_text[current_key].append(src)\n",
    "\n",
    "    elif ctype == \"code\":\n",
    "        # top-of-cell comments as potential section headers\n",
    "        lines = src_list\n",
    "        comments = []\n",
    "        comment_key = None\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                comments.append(re.sub(r\"^#+\\s?\", \"\", line.strip()))\n",
    "                if comment_key is None:\n",
    "                    m2 = COMMENT_SEC_RE.match(line)\n",
    "                    if m2:\n",
    "                        comment_key = make_key(m2.group(\"num\"), m2.group(\"title\"))\n",
    "            elif line.strip() == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if comment_key:\n",
    "            current_key = comment_key\n",
    "        if comments:\n",
    "            sections_text[current_key].append(\"\\n\".join(comments))\n",
    "        # optional: top-of-cell docstring\n",
    "        m3 = re.match(r'\\s*(?P<q>\"\"\"|\\'\\'\\')(?P<doc>.*?)(?P=q)', src, flags=re.DOTALL)\n",
    "        if m3:\n",
    "            sections_text[current_key].append(m3.group(\"doc\"))\n",
    "\n",
    "# --- 2) Build per-section term frequencies (unigrams + bigrams)\n",
    "by_section_defs = {}\n",
    "global_counter = Counter()\n",
    "\n",
    "for sec, chunks in sections_text.items():\n",
    "    text = normalize_text(\"\\n\\n\".join(chunks))\n",
    "    tagged = extract_tokens(text)\n",
    "\n",
    "    # Unigrams (nouns/adjectives)\n",
    "    uni_terms = []\n",
    "    for tok, tag in tagged:\n",
    "        if tag in ALLOW_POS_UNI:\n",
    "            lemmed = lemmatize_if_needed(tok, tag)\n",
    "            if lemmed:\n",
    "                uni_terms.append(lemmed)\n",
    "\n",
    "    uni_freq = Counter(uni_terms)\n",
    "    uni_kept = [(t, c) for t, c in uni_freq.items() if c >= MIN_FREQ_UNI]\n",
    "\n",
    "    # Bigrams (NP patterns)\n",
    "    bi_kept = []\n",
    "    if INCLUDE_BIGRAMS:\n",
    "        bi_terms = noun_phrase_bigrams(tagged)\n",
    "        bi_freq = Counter(bi_terms)\n",
    "        bi_kept = [(t, c) for t, c in bi_freq.items() if c >= MIN_FREQ_BI]\n",
    "\n",
    "    # Merge & cap by TOP_K_PER_SECTION (prioritize bigrams, then unigrams)\n",
    "    merged = sorted(bi_kept, key=lambda x: (-x[1], x[0])) \\\n",
    "           + sorted(uni_kept, key=lambda x: (-x[1], x[0]))\n",
    "    merged = merged[:TOP_K_PER_SECTION]\n",
    "\n",
    "    # Definitions + frequencies\n",
    "    sec_map = {}\n",
    "    for term, count in merged:\n",
    "        defn = define_term(term)\n",
    "        sec_map[term] = {\"definition\": defn, \"frequency\": count}\n",
    "        global_counter[term] += count\n",
    "\n",
    "    by_section_defs[sec] = sec_map\n",
    "\n",
    "# --- 3) Build overall top terms\n",
    "overall_top = {}\n",
    "for term, count in global_counter.most_common(200):\n",
    "    overall_top[term] = {\"definition\": define_term(term), \"frequency\": count}\n",
    "\n",
    "# --- 4) Save JSON\n",
    "payload = {\n",
    "    \"source_notebook\": str(NOTEBOOK_PATH),\n",
    "    \"min_freq_unigram\": MIN_FREQ_UNI,\n",
    "    \"min_freq_bigram\": MIN_FREQ_BI,\n",
    "    \"top_k_per_section\": TOP_K_PER_SECTION,\n",
    "    \"include_bigrams\": INCLUDE_BIGRAMS,\n",
    "    \"by_section\": by_section_defs,\n",
    "    \"overall_top_terms\": overall_top\n",
    "}\n",
    "SAVE_JSON.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# --- 5) Preview\n",
    "print(f\"‚úÖ Glossary (unigrams + bigrams) built from {NOTEBOOK_PATH.name}\")\n",
    "print(f\"üíæ Saved ‚Üí {SAVE_JSON}\\n\")\n",
    "\n",
    "def preview(sec_key, n=12):\n",
    "    sec = by_section_defs.get(sec_key, {})\n",
    "    items = sorted(sec.items(), key=lambda kv: (-kv[1]['frequency'], kv[0]))[:n]\n",
    "    print(f\"--- {sec_key} (showing {len(items)}/{len(sec)}) ---\")\n",
    "    for i, (term, meta) in enumerate(items, 1):\n",
    "        print(f\"{i:>2}. {term} [{meta['frequency']}]: {meta['definition']}\")\n",
    "    print()\n",
    "\n",
    "shown = 0\n",
    "for sec_key in sorted(by_section_defs.keys(), key=lambda k: (k.split()[0], k)):\n",
    "    preview(sec_key, n=10)\n",
    "    shown += 1\n",
    "    if shown >= 2:\n",
    "        break\n",
    "\n",
    "print(\"--- Overall Top Terms (first 15) ---\")\n",
    "for i, (t, meta) in enumerate(list(overall_top.items())[:15], 1):\n",
    "    print(f\"{i:>2}. {t} [{meta['frequency']}]: {meta['definition']}\")\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Force HTTPS to use certifi in THIS kernel\n",
    "import os, ssl, certifi, urllib.request, sys\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"certifi bundle:\", certifi.where())\n",
    "\n",
    "# 1) Make every new SSL context use certifi's CA bundle\n",
    "ssl._create_default_https_context = lambda *a, **k: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# 2) Install a global urllib opener that uses that context\n",
    "_ctx = ssl.create_default_context(cafile=certifi.where())\n",
    "_opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=_ctx))\n",
    "urllib.request.install_opener(_opener)\n",
    "\n",
    "# 3) (optional) also set env vars some libs honor\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# 4) Test a real HTTPS fetch\n",
    "print(\"Fetching 1 byte from python.org over HTTPS‚Ä¶\")\n",
    "print(urllib.request.urlopen(\"https://www.python.org\", timeout=10).read(1))\n",
    "print(\"‚úÖ HTTPS OK\")\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Make it persistent (no more patches in notebooks)\n",
    "# Create a sitecustomize.py inside your venv so every Python process in that venv uses certifi:\n",
    "\n",
    "# python - <<'PY'\n",
    "\n",
    "import certifi, sys, pathlib, textwrap\n",
    "site_dir = next(p for p in sys.path if p.endswith(\"site-packages\"))\n",
    "target = pathlib.Path(site_dir) / \"sitecustomize.py\"\n",
    "code = textwrap.dedent(f\"\"\"\n",
    "import os, ssl, certifi\n",
    "os.environ.setdefault(\"SSL_CERT_FILE\", certifi.where())\n",
    "os.environ.setdefault(\"REQUESTS_CA_BUNDLE\", certifi.where())\n",
    "ssl._create_default_https_context = lambda *a, **k: ssl.create_default_context(cafile=certifi.where())\n",
    "\"\"\")\n",
    "target.write_text(code)\n",
    "print(\"Wrote\", target)\n",
    "PY\n",
    "\n",
    "# Then restart your terminal/kernel and retest the tiny fetch:\n",
    "import urllib.request\n",
    "urllib.request.urlopen(\"https://www.python.org\").read(1)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "import nltk, ssl, certifi\n",
    "# contexts created inside nltk should now inherit the patched default\n",
    "for pkg, kind in [\n",
    "    (\"punkt\", \"tokenizers\"), (\"punkt_tab\", \"tokenizers\"),\n",
    "    (\"averaged_perceptron_tagger\", \"taggers\"),\n",
    "    (\"wordnet\", \"corpora\"), (\"omw-1.4\", \"corpora\")\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(f\"{kind}/{pkg}\")\n",
    "        print(f\"‚úì {pkg} already present\")\n",
    "    except LookupError:\n",
    "        print(f\"‚Üì downloading {pkg} ‚Ä¶\")\n",
    "        nltk.download(pkg, quiet=False)\n",
    "print(\"‚úÖ NLTK resources ready\")\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "You‚Äôve got the **new NLTK resource name** error. Recent NLTK versions look for\n",
    "`taggers/averaged_perceptron_tagger_eng` (not the old `‚Ä¶_tagger`). Fix it by (1) forcing HTTPS to use **certifi** (so downloads work) and (2) downloading **both** the new and legacy tagger names.\n",
    "\n",
    "### Drop-in cell to run **before** your glossary code\n",
    "\n",
    "```python\n",
    "# --- Make NLTK downloads work + fetch the right tagger resources ---\n",
    "import os, ssl, certifi, nltk\n",
    "\n",
    "# Route HTTPS to certifi bundle (works inside this kernel)\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "ssl._create_default_https_context = lambda *a, **k: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Ensure a local cache dir is used (so it won't re-download every time)\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, NLTK_DIR)\n",
    "\n",
    "# Try to find or download required packages\n",
    "def need(path): \n",
    "    try: nltk.data.find(path); return False\n",
    "    except LookupError: return True\n",
    "\n",
    "to_get = []\n",
    "if need(\"tokenizers/punkt\"): to_get += [\"punkt\"]\n",
    "# some installs want punkt_tab as well\n",
    "if need(\"tokenizers/punkt_tab\"): to_get += [\"punkt_tab\"]\n",
    "# POS tagger: new & legacy names (grab both to be safe)\n",
    "if need(\"taggers/averaged_perceptron_tagger_eng\"): to_get += [\"averaged_perceptron_tagger_eng\"]\n",
    "if need(\"taggers/averaged_perceptron_tagger\"):     to_get += [\"averaged_perceptron_tagger\"]\n",
    "# WordNet for definitions\n",
    "if need(\"corpora/wordnet\"): to_get += [\"wordnet\"]\n",
    "if need(\"corpora/omw-1.4\"): to_get += [\"omw-1.4\"]\n",
    "\n",
    "if to_get:\n",
    "    print(\"Downloading NLTK data:\", \", \".join(to_get))\n",
    "    for pkg in to_get:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=False)\n",
    "else:\n",
    "    print(\"‚úÖ All required NLTK resources already present\")\n",
    "```\n",
    "\n",
    "Now re-run your glossary cell. The `pos_tag` call should succeed because the **ENG** tagger is present.\n",
    "\n",
    "---\n",
    "\n",
    "### If downloads still fail (corporate proxy / SSL weirdness)\n",
    "\n",
    "Use the **no-internet fallback** for bigrams (no POS tagger, no WordNet). It‚Äôs surprisingly decent for study notes:\n",
    "\n",
    "```python\n",
    "import re, json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"../../../Level_3/notebooks/01_EDA.ipynb\").resolve()\n",
    "\n",
    "def simple_tokens(t):\n",
    "    return [w.lower() for w in re.findall(r\"[A-Za-z][A-Za-z\\-]+\", t)]\n",
    "\n",
    "def simple_bigrams(tokens):\n",
    "    # keep (adj|noun)-ish approximations: exclude common stopwords\n",
    "    stop = set(\"a an the of to and or in on for with from by as at is are was were be been being this that these those it its\".split())\n",
    "    keep = [w for w in tokens if w not in stop and len(w) >= 3]\n",
    "    return [\" \".join(pair) for pair in zip(keep, keep[1:])]\n",
    "\n",
    "nb = json.loads(NOTEBOOK_PATH.read_text(encoding=\"utf-8\"))\n",
    "sections = defaultdict(list)\n",
    "sec = \"Unsectioned\"\n",
    "for cell in nb.get(\"cells\", []):\n",
    "    src = \"\".join(cell.get(\"source\", []))\n",
    "    if cell.get(\"cell_type\") == \"markdown\":\n",
    "        # crude section split: if a heading appears, switch section\n",
    "        for line in src.splitlines():\n",
    "            if line.lstrip().startswith(\"#\"):\n",
    "                sec = re.sub(r\"^\\s*#+\\s*\", \"\", line).strip() or \"Unsectioned\"\n",
    "                break\n",
    "        sections[sec].append(src)\n",
    "    else:\n",
    "        sections[sec].append(src)\n",
    "\n",
    "gloss = {}\n",
    "for sec, chunks in sections.items():\n",
    "    text = \"\\n\\n\".join(chunks)\n",
    "    toks = simple_tokens(text)\n",
    "    uni = Counter(toks)\n",
    "    bi  = Counter(simple_bigrams(toks))\n",
    "    # keep top items; no definitions in offline mode\n",
    "    terms = {}\n",
    "    for t, c in bi.most_common(60):\n",
    "        terms[t] = {\"definition\": \"(offline mode ‚Äî no WordNet)\", \"frequency\": c}\n",
    "    for t, c in uni.most_common(60):\n",
    "        terms.setdefault(t, {\"definition\": \"(offline mode ‚Äî no WordNet)\", \"frequency\": c})\n",
    "    gloss[sec] = terms\n",
    "\n",
    "out = Path(\"outputs/notebook_glossary_by_section.json\")\n",
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.write_text(json.dumps({\"by_section\": gloss}, indent=2), encoding=\"utf-8\")\n",
    "print(\"‚úÖ Offline glossary saved ‚Üí\", out)\n",
    "```\n",
    "\n",
    "This lets you keep moving; when your SSL is sorted, switch back to the NLTK-powered version for POS-aware bigrams and definitions.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make NLTK downloads work + fetch the right tagger resources ---\n",
    "import os, ssl, certifi, nltk\n",
    "\n",
    "# Route HTTPS to certifi bundle (works inside this kernel)\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "ssl._create_default_https_context = lambda *a, **k: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Ensure a local cache dir is used (so it won't re-download every time)\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, NLTK_DIR)\n",
    "\n",
    "# Try to find or download required packages\n",
    "def need(path): \n",
    "    try: nltk.data.find(path); return False\n",
    "    except LookupError: return True\n",
    "\n",
    "to_get = []\n",
    "if need(\"tokenizers/punkt\"): to_get += [\"punkt\"]\n",
    "# some installs want punkt_tab as well\n",
    "if need(\"tokenizers/punkt_tab\"): to_get += [\"punkt_tab\"]\n",
    "# POS tagger: new & legacy names (grab both to be safe)\n",
    "if need(\"taggers/averaged_perceptron_tagger_eng\"): to_get += [\"averaged_perceptron_tagger_eng\"]\n",
    "if need(\"taggers/averaged_perceptron_tagger\"):     to_get += [\"averaged_perceptron_tagger\"]\n",
    "# WordNet for definitions\n",
    "if need(\"corpora/wordnet\"): to_get += [\"wordnet\"]\n",
    "if need(\"corpora/omw-1.4\"): to_get += [\"omw-1.4\"]\n",
    "\n",
    "if to_get:\n",
    "    print(\"Downloading NLTK data:\", \", \".join(to_get))\n",
    "    for pkg in to_get:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=False)\n",
    "else:\n",
    "    print(\"‚úÖ All required NLTK resources already present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Inline Glossary Builder ‚Äî Jupyter Study Tool: V1\n",
    "# - Extracts important words from your notebook and\n",
    "# - builds a dictionary of definitions using NLTK WordNet.\n",
    "\n",
    "# üîπ ensure required nltk data\n",
    "# üí° Quick repair inside your current notebook\n",
    "# The new tokenizer now separates its data into two resources:   (a **recent NLTK change**)\n",
    "# 1. `\"punkt\"` ‚Üí old base tokenizer\n",
    "# 2. `\"punkt_tab\"` ‚Üí new tokenization tables required by recent versions\n",
    "# add `\"punkt_tab\"` to the download list at the top of the script:\n",
    "# You can also just run this **once** in a separate Jupyter cell:\n",
    "# import nltk\n",
    "# nltk.download(\"punkt_tab\")\n",
    "# Then re-run the glossary cell.\n",
    "# That will permanently add the new tokenizer tables to your NLTK data directory, and the rest of the script will work fine.\n",
    "\n",
    "# %pip install nltk\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# üîπ ensure required nltk data\n",
    "for pkg in [\"punkt\", \"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\"]:\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{pkg}\") if pkg==\"punkt\" else nltk.data.find(f\"corpora/{pkg}\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- CONFIG ---\n",
    "\n",
    "# 1) Correct path reference:\n",
    "# NOTEBOOK_PATH = Path(__file__).resolve().parents[3] / \"Level_3\" / \"notebooks\" / \"01_EDA.ipynb\"\n",
    "\n",
    "# 2) If you‚Äôre running it interactively in Jupyter (so __file__ isn‚Äôt defined), use:\n",
    "from pathlib import Path\n",
    "NOTEBOOK_PATH = Path.cwd().resolve().parents[3]/ \"Telco\" / \"Level_3\" / \"notebooks\" / \"01_EDA.ipynb\"\n",
    "\n",
    "# 3) if you just want a clean relative path from the glossary notebook‚Äôs folder:\n",
    "# NOTEBOOK_PATH = Path(\"../../../Level_3/notebooks/01_EDA.ipynb\").resolve()\n",
    "\n",
    "#4) incorrect?\n",
    "# NOTEBOOK_PATH = Path(\"./Level_3/notebooks/01_EDA.ipynb\")\n",
    "\n",
    "TOP_K = 1000                             # how many terms to keep\n",
    "MIN_FREQ = 2                            # minimum frequency\n",
    "ALLOW_POS = {\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\"}  # nouns & adjectives only\n",
    "# ---------------\n",
    "\n",
    "# --- 1Ô∏è‚É£ Extract text from notebook ---\n",
    "nb = json.loads(NOTEBOOK_PATH.read_text(encoding=\"utf-8\"))\n",
    "texts = []\n",
    "for cell in nb[\"cells\"]:\n",
    "    if cell[\"cell_type\"] == \"markdown\":\n",
    "        texts.append(\"\".join(cell[\"source\"]))\n",
    "    elif cell[\"cell_type\"] == \"code\":\n",
    "        # capture top comments\n",
    "        lines = cell[\"source\"]\n",
    "        comments = []\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                comments.append(line.strip(\"#\").strip())\n",
    "            elif line.strip() == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if comments:\n",
    "            texts.append(\"\\n\".join(comments))\n",
    "\n",
    "text = \"\\n\".join(texts)\n",
    "\n",
    "# --- 2Ô∏è‚É£ Tokenize and filter important words ---\n",
    "lem = WordNetLemmatizer()\n",
    "tokens = [t for t in word_tokenize(text) if re.search(r\"[A-Za-z]\", t)]\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "def map_pos(tag):\n",
    "    if tag.startswith(\"NN\"): return \"n\"\n",
    "    if tag.startswith(\"JJ\"): return \"a\"\n",
    "    return None\n",
    "\n",
    "terms = []\n",
    "for word, tag in tagged:\n",
    "    if tag not in ALLOW_POS or len(word) < 3: \n",
    "        continue\n",
    "    pos = map_pos(tag)\n",
    "    word = word.lower()\n",
    "    lemma = lem.lemmatize(word, pos) if pos else word\n",
    "    terms.append(lemma)\n",
    "\n",
    "# --- 3Ô∏è‚É£ Count and select top terms ---\n",
    "freq = Counter(terms)\n",
    "top_terms = [w for w, c in freq.items() if c >= MIN_FREQ]\n",
    "top_terms = sorted(top_terms, key=lambda x: (-freq[x], x))[:TOP_K]\n",
    "\n",
    "# --- 4Ô∏è‚É£ Lookup definitions from WordNet ---\n",
    "glossary = {}\n",
    "for term in top_terms:\n",
    "    syns = wn.synsets(term)\n",
    "    if syns:\n",
    "        glossary[term] = syns[0].definition()\n",
    "    else:\n",
    "        glossary[term] = \"(definition not found)\"\n",
    "\n",
    "# --- 5Ô∏è‚É£ Display glossary preview ---\n",
    "print(f\"‚úÖ Extracted {len(glossary)} terms from {NOTEBOOK_PATH.name}\")\n",
    "for i, (k, v) in enumerate(list(glossary.items())[:15], 1):\n",
    "    print(f\"{i:>2}. {k}: {v}\")\n",
    "\n",
    "# --- 6Ô∏è‚É£ Optional: save as JSON for reuse ---\n",
    "out_path = Path(\"notebook_glossary.json\")\n",
    "out_path.write_text(json.dumps(glossary, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(f\"\\nüíæ Saved glossary ‚Üí {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline glossary builder - grouped by notebook section - V2\n",
    "# (e.g., ‚Äú2.1 Missing Scan‚Äù, ‚Äú2.2 Constant Columns‚Äù). \n",
    "# It scans markdown headings and top-of-cell comments to track the current section, \n",
    "# extracts key terms per section,\n",
    "# pulls **WordNet** definitions, and saves a nested JSON:\n",
    "\n",
    "# TODO: make it runnable both ways ‚Äî as a CLI tool and as an importable module (with a main() guard and docstring header)\n",
    "# ==========================================================\n",
    "# üìö Inline Sectioned Glossary Builder ‚Äî Jupyter Study Tool\n",
    "#    Groups terms by notebook sections like \"2.1 ‚Ä¶\", \"2.2 ‚Ä¶\"\n",
    "# ==========================================================\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# üîπ Ensure required NLTK data (quiet, idempotent)\n",
    "# for pkg in [\"punkt\", \"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\"]:\n",
    "#     try:\n",
    "#         nltk.data.find(f\"tokenizers/{pkg}\") if pkg==\"punkt\" else nltk.data.find(f\"corpora/{pkg}\")\n",
    "#     except LookupError:\n",
    "#         nltk.download(pkg, quiet=True)\n",
    "\n",
    "for pkg in [\"punkt\", \"punkt_tab\", \"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\"]:\n",
    "    try:\n",
    "        if pkg in [\"punkt\", \"punkt_tab\"]:\n",
    "            nltk.data.find(f\"tokenizers/{pkg}\")\n",
    "        else:\n",
    "            nltk.data.find(f\"corpora/{pkg}\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "NOTEBOOK_PATH = Path(\"01_EDA.ipynb\")         # ‚Üê your notebook file\n",
    "TOP_K_PER_SECTION = 60                       # max terms per section\n",
    "MIN_FREQ = 2                                 # min frequency per section\n",
    "ALLOW_POS = {\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\"}   # nouns + adjectives\n",
    "MIN_LEN = 3\n",
    "SAVE_JSON = Path(\"outputs/notebook_glossary_by_section.json\")\n",
    "SAVE_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "# ---------------------------------\n",
    "\n",
    "# Detect section from a line like:\n",
    "#   \"# 2.1 Missing / Null / Blank Scan\"\n",
    "#   \"## 2.0.1 üß©üîí Dataset Guard ‚Ä¶\"\n",
    "#   \"# 3Ô∏è‚É£ DESCRIPTIVE STATISTICS\" (fallback keeps emoji titles too)\n",
    "SEC_RE = re.compile(\n",
    "    r\"^\\s{0,3}#{1,6}\\s*(?P<num>(?:\\d+\\.)*\\d+)\\s*(?P<title>.*)$\"\n",
    ")\n",
    "\n",
    "# Also detect from top-of-cell comments:\n",
    "#   \"# 2.1 Missing ‚Ä¶\"\n",
    "COMMENT_SEC_RE = re.compile(\n",
    "    r\"^\\s*#\\s*(?P<num>(?:\\d+\\.)*\\d+)\\s*(?P<title>.*)$\"\n",
    ")\n",
    "\n",
    "def normalize_terms(text):\n",
    "    # strip markup-y artifacts, then tokenize & POS tag\n",
    "    text = re.sub(r\"[`*_<>]+\", \" \", text)\n",
    "    text = re.sub(r\"[\\u2000-\\u206F]\", \" \", text)\n",
    "    toks = [t for t in word_tokenize(text) if re.search(r\"[A-Za-z]\", t)]\n",
    "    tagged = pos_tag(toks)\n",
    "\n",
    "    def to_wnpos(tag):\n",
    "        if tag.startswith(\"NN\"): return \"n\"\n",
    "        if tag.startswith(\"JJ\"): return \"a\"\n",
    "        return None\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    terms = []\n",
    "    for tok, tag in tagged:\n",
    "        if tag not in ALLOW_POS:\n",
    "            continue\n",
    "        tok_clean = re.sub(r\"[^A-Za-z\\-]\", \"\", tok).lower()\n",
    "        if len(tok_clean) < MIN_LEN:\n",
    "            continue\n",
    "        wnpos = to_wnpos(tag)\n",
    "        tok_lem = lem.lemmatize(tok_clean, wnpos) if wnpos else tok_clean\n",
    "        terms.append(tok_lem)\n",
    "    return terms\n",
    "\n",
    "# --- 1) Read notebook & walk cells, maintaining current section\n",
    "nb = json.loads(NOTEBOOK_PATH.read_text(encoding=\"utf-8\"))\n",
    "sections_text = defaultdict(list)   # section_key -> list of text chunks\n",
    "current_key = \"0.0 Unsectioned\"\n",
    "\n",
    "def make_key(num: str, title: str) -> str:\n",
    "    title = title.strip()\n",
    "    # Compact overly long titles\n",
    "    return f\"{num} {title}\" if num else (title or \"Unsectioned\")\n",
    "\n",
    "for cell in nb.get(\"cells\", []):\n",
    "    ctype = cell.get(\"cell_type\")\n",
    "    src_list = cell.get(\"source\", [])\n",
    "    src = \"\".join(src_list)\n",
    "\n",
    "    # Check for a heading in markdown\n",
    "    if ctype == \"markdown\":\n",
    "        # look line-by-line for the first heading\n",
    "        heading_key = None\n",
    "        for line in src.splitlines():\n",
    "            m = SEC_RE.match(line)\n",
    "            if m:\n",
    "                heading_key = make_key(m.group(\"num\"), m.group(\"title\"))\n",
    "                current_key = heading_key\n",
    "                break\n",
    "        # record full markdown text under the *current* section\n",
    "        sections_text[current_key].append(src)\n",
    "\n",
    "    elif ctype == \"code\":\n",
    "        # Try to capture a section from top-of-cell comments (first contiguous comment block)\n",
    "        lines = src_list\n",
    "        comments = []\n",
    "        comment_section_key = None\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                comments.append(re.sub(r\"^#+\\s?\", \"\", line.strip()))\n",
    "                # also see if the very first comment line defines a section number\n",
    "                if comment_section_key is None:\n",
    "                    m2 = COMMENT_SEC_RE.match(line)\n",
    "                    if m2:\n",
    "                        comment_section_key = make_key(m2.group(\"num\"), m2.group(\"title\"))\n",
    "            elif line.strip() == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if comment_section_key:\n",
    "            current_key = comment_section_key\n",
    "\n",
    "        if comments:\n",
    "            sections_text[current_key].append(\"\\n\".join(comments))\n",
    "\n",
    "        # (Optional) capture top-of-cell docstring if present at the very start\n",
    "        m3 = re.match(r'\\s*(?P[q]\"\"\"|\\'\\'\\')(?P<doc>.*?)(?P=q)', src, flags=re.DOTALL)\n",
    "        if m3:\n",
    "            sections_text[current_key].append(m3.group(\"doc\"))\n",
    "\n",
    "# --- 2) Build per-section term frequencies\n",
    "section_term_freqs = {}\n",
    "for sec, chunks in sections_text.items():\n",
    "    text = \"\\n\\n\".join(chunks)\n",
    "    terms = normalize_terms(text)\n",
    "    freq = Counter(terms)\n",
    "    # filter by MIN_FREQ and pick top K\n",
    "    kept = [(t, c) for t, c in freq.items() if c >= MIN_FREQ]\n",
    "    kept.sort(key=lambda x: (-x[1], x[0]))\n",
    "    section_term_freqs[sec] = kept[:TOP_K_PER_SECTION]\n",
    "\n",
    "# --- 3) Look up definitions per section; also build global view\n",
    "by_section_defs = {}\n",
    "global_terms = Counter()\n",
    "\n",
    "for sec, pairs in section_term_freqs.items():\n",
    "    sec_defs = {}\n",
    "    for term, count in pairs:\n",
    "        global_terms[term] += count\n",
    "        syns = wn.synsets(term)\n",
    "        if syns:\n",
    "            # Prefer noun first, then adjective\n",
    "            noun_first = [s for s in syns if s.pos() == 'n'] + [s for s in syns if s.pos() == 'a'] + syns\n",
    "            definition = noun_first[0].definition()\n",
    "        else:\n",
    "            definition = \"(definition not found)\"\n",
    "        sec_defs[term] = {\n",
    "            \"definition\": definition,\n",
    "            \"frequency\": count\n",
    "        }\n",
    "    by_section_defs[sec] = sec_defs\n",
    "\n",
    "overall_top = dict()\n",
    "for term, count in global_terms.most_common(200):\n",
    "    syns = wn.synsets(term)\n",
    "    if syns:\n",
    "        noun_first = [s for s in syns if s.pos() == 'n'] + [s for s in syns if s.pos() == 'a'] + syns\n",
    "        definition = noun_first[0].definition()\n",
    "    else:\n",
    "        definition = \"(definition not found)\"\n",
    "    overall_top[term] = {\"definition\": definition, \"frequency\": count}\n",
    "\n",
    "# --- 4) Save JSON (nested: by_section + overall)\n",
    "payload = {\n",
    "    \"source_notebook\": str(NOTEBOOK_PATH),\n",
    "    \"min_freq\": MIN_FREQ,\n",
    "    \"top_k_per_section\": TOP_K_PER_SECTION,\n",
    "    \"by_section\": by_section_defs,\n",
    "    \"overall_top_terms\": overall_top\n",
    "}\n",
    "SAVE_JSON.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# --- 5) Pretty preview\n",
    "print(f\"‚úÖ Glossary built by section from {NOTEBOOK_PATH.name}\")\n",
    "print(f\"üíæ Saved ‚Üí {SAVE_JSON}\\n\")\n",
    "\n",
    "def preview(sec_key, n=10):\n",
    "    terms = list(by_section_defs.get(sec_key, {}).items())[:n]\n",
    "    print(f\"--- {sec_key} (showing {len(terms)}/{len(by_section_defs.get(sec_key, {}))}) ---\")\n",
    "    for i, (term, data) in enumerate(terms, 1):\n",
    "        print(f\"{i:>2}. {term} [{data['frequency']}]: {data['definition']}\")\n",
    "    print()\n",
    "\n",
    "# Show the first 2 sections found (if any)\n",
    "shown = 0\n",
    "for sec_key in sorted(by_section_defs.keys(), key=lambda k: (k.split()[0], k)):\n",
    "    preview(sec_key, n=10)\n",
    "    shown += 1\n",
    "    if shown >= 2:\n",
    "        break\n",
    "\n",
    "# Show overall top terms\n",
    "print(\"--- Overall Top Terms (first 15) ---\")\n",
    "for i, (term, data) in enumerate(list(overall_top.items())[:15], 1):\n",
    "    print(f\"{i:>2}. {term} [{data['frequency']}]: {data['definition']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa62cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### What‚Äôs new vs. the previous version\n",
    "# **Section tracking** from:\n",
    "#   * Markdown headings like `## 2.1 Missing / Null / Blank Scan`\n",
    "#   * *or* the first top-of-cell **comment** like `# 2.1 ‚Ä¶`\n",
    "# * **Per-section term frequency** (filtered by `MIN_FREQ`, capped by `TOP_K_PER_SECTION`)\n",
    "# * **Definitions** stored **per section** and an **overall** roll-up\n",
    "# * Saves a nested JSON you can search or render later\n",
    "\n",
    "### Tips\n",
    "# * If some sections don‚Äôt start with a numeric heading (e.g., ‚Äúüß≠ Intro‚Äù), they‚Äôll fall under **‚Äú0.0 Unsectioned‚Äù** until the next numeric heading/comment appears.\n",
    "# * To include **verbs**, add POS tags to `ALLOW_POS` (e.g., `{\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"}`).\n",
    "\n",
    "\n",
    "###\n",
    "### \n",
    "\n",
    "# ==========================================\n",
    "# üìñ Collapsible Glossary Renderer (inline)\n",
    "# ==========================================\n",
    "from pathlib import Path\n",
    "import json, html, re\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "JSON_PATH = Path(\"outputs/notebook_glossary_by_section.json\")  # adjust if needed\n",
    "\n",
    "if not JSON_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Glossary JSON not found at: {JSON_PATH}\")\n",
    "\n",
    "data = json.loads(JSON_PATH.read_text(encoding=\"utf-8\"))\n",
    "by_section = data.get(\"by_section\", {})\n",
    "overall = data.get(\"overall_top_terms\", {})\n",
    "\n",
    "def _sec_sort_key(s):\n",
    "    # sort by numeric prefix if present, else lexicographically\n",
    "    m = re.match(r\"^\\s*((?:\\d+\\.)*\\d+)\", s)\n",
    "    if not m: \n",
    "        return (9999, s.lower())\n",
    "    parts = [int(p) for p in m.group(1).split(\".\")]\n",
    "    return (parts + [0]*5)[:5]  # pad for consistent length\n",
    "\n",
    "def _render_table(rows):\n",
    "    # rows: list of (term, {definition, frequency})\n",
    "    if not rows:\n",
    "        return \"<p class='muted'>No terms.</p>\"\n",
    "    rows_html = []\n",
    "    for term, meta in rows:\n",
    "        defn = html.escape(str(meta.get(\"definition\",\"(no definition)\")))\n",
    "        freq = meta.get(\"frequency\", \"\")\n",
    "        term_esc = html.escape(term)\n",
    "        rows_html.append(\n",
    "            f\"<tr class='term-row'><td class='term'>{term_esc}</td>\"\n",
    "            f\"<td class='def'>{defn}</td><td class='freq'>{freq}</td></tr>\"\n",
    "        )\n",
    "    return (\n",
    "        \"<table class='glossary-table'>\"\n",
    "        \"<thead><tr><th>Term</th><th>Definition</th><th>Freq</th></tr></thead>\"\n",
    "        f\"<tbody>{''.join(rows_html)}</tbody></table>\"\n",
    "    )\n",
    "\n",
    "# Build HTML\n",
    "sections_sorted = sorted(by_section.items(), key=lambda kv: _sec_sort_key(kv[0]))\n",
    "\n",
    "overall_rows = list(overall.items())\n",
    "overall_html = _render_table(overall_rows[:30])  # show first 30 overall\n",
    "\n",
    "sec_blocks = []\n",
    "for sec_title, term_map in sections_sorted:\n",
    "    rows = sorted(term_map.items(), key=lambda kv: (-kv[1].get(\"frequency\",0), kv[0]))\n",
    "    block = (\n",
    "        f\"<details class='sec' open>\"\n",
    "        f\"<summary><span class='sec-title'>{html.escape(sec_title)}</span>\"\n",
    "        f\"<span class='count'>({len(rows)} terms)</span></summary>\"\n",
    "        f\"{_render_table(rows)}\"\n",
    "        f\"</details>\"\n",
    "    )\n",
    "    sec_blocks.append(block)\n",
    "\n",
    "css = \"\"\"\n",
    "<style>\n",
    "  .glossary-wrap {font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; line-height:1.45;}\n",
    "  .header {display:flex; align-items:center; gap:.75rem; margin-bottom:.5rem;}\n",
    "  .header h2 {margin:0; font-size:1.25rem;}\n",
    "  .search {margin: .25rem 0 1rem 0;}\n",
    "  .search input {width:100%; max-width:720px; padding:.6rem .75rem; border:1px solid #d0d7de; border-radius:8px; font-size:.95rem;}\n",
    "  details.sec {margin-bottom:.6rem; border:1px solid #e5e7eb; border-radius:10px; padding:.4rem .8rem; background:#fff;}\n",
    "  details.sec > summary {cursor:pointer; font-weight:600; display:flex; justify-content:space-between; list-style:none; outline:none;}\n",
    "  details.sec > summary::-webkit-details-marker {display:none;}\n",
    "  .sec-title {font-size:1rem;}\n",
    "  .count {color:#64748b; font-weight:500;}\n",
    "  .muted {color:#94a3b8;}\n",
    "  .glossary-table {border-collapse:collapse; width:100%; margin:.5rem 0 .75rem;}\n",
    "  .glossary-table th, .glossary-table td {border-top:1px solid #e5e7eb; padding:.5rem .6rem; vertical-align:top;}\n",
    "  .glossary-table thead th {background:#f8fafc; font-weight:700; text-align:left;}\n",
    "  .glossary-table td.term {white-space:nowrap; font-weight:600;}\n",
    "  .glossary-table td.freq {text-align:right; color:#475569; width:70px;}\n",
    "  .hint {color:#64748b; font-size:.9rem; margin:.25rem 0 1rem;}\n",
    "  .overall {margin: .2rem 0 1rem; border:1px dashed #cbd5e1; border-radius:10px; padding:.6rem .8rem; background:#f8fafc;}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "js = \"\"\"\n",
    "<script>\n",
    "(function(){\n",
    "  const input = document.getElementById('glossary-search');\n",
    "  if(!input) return;\n",
    "  const wrap = document.querySelector('.glossary-wrap');\n",
    "  const rowsSelector = 'table.glossary-table tbody tr.term-row';\n",
    "  const secDetails = Array.from(document.querySelectorAll('details.sec'));\n",
    "\n",
    "  function normalize(s){\n",
    "    return (s || '').toLowerCase().normalize('NFKD').replace(/[\\\\u0300-\\\\u036f]/g,'');\n",
    "  }\n",
    "\n",
    "  function applyFilter(){\n",
    "    const q = normalize(input.value.trim());\n",
    "    if(!q){\n",
    "      // reset: show all\n",
    "      secDetails.forEach(d => d.style.display = '');\n",
    "      wrap.querySelectorAll(rowsSelector).forEach(tr => tr.style.display = '');\n",
    "      return;\n",
    "    }\n",
    "    secDetails.forEach(d => d.style.display = ''); // show all sections by default\n",
    "    const rows = wrap.querySelectorAll(rowsSelector);\n",
    "    rows.forEach(tr => {\n",
    "      const term = normalize(tr.querySelector('.term')?.textContent || '');\n",
    "      const def  = normalize(tr.querySelector('.def')?.textContent || '');\n",
    "      const match = term.includes(q) || def.includes(q);\n",
    "      tr.style.display = match ? '' : 'none';\n",
    "    });\n",
    "    // If a section has no visible rows, hide that section\n",
    "    secDetails.forEach(d => {\n",
    "      const visible = d.querySelectorAll('tbody tr.term-row:not([style*=\"display: none\"])').length;\n",
    "      d.style.display = visible ? '' : 'none';\n",
    "    });\n",
    "  }\n",
    "\n",
    "  input.addEventListener('input', applyFilter);\n",
    "})();\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "html_doc = f\"\"\"\n",
    "<div class=\"glossary-wrap\">\n",
    "  <div class=\"header\">\n",
    "    <h2>Notebook Glossary</h2>\n",
    "  </div>\n",
    "  <div class=\"search\">\n",
    "    <input id=\"glossary-search\" type=\"search\" placeholder=\"Search terms & definitions‚Ä¶\" aria-label=\"Search glossary\" />\n",
    "    <div class=\"hint\">Tip: search works across all sections; hidden sections reappear when you clear the search.</div>\n",
    "  </div>\n",
    "\n",
    "  <details class=\"overall\" open>\n",
    "    <summary><strong>Overall Top Terms</strong> (first 30)</summary>\n",
    "    {overall_html}\n",
    "  </details>\n",
    "\n",
    "  {''.join(sec_blocks)}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(css + html_doc + js))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
