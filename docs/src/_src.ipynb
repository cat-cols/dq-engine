{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c8c960",
   "metadata": {},
   "source": [
    "Think of `telco_churn` as **the brain of the project**: all the reusable logic that your Level_ notebooks call into.\n",
    "\n",
    "You donâ€™t need a million folders, but you *do* want the big responsibilities separated so it feels â€œreal project, not just a notebook dump.â€\n",
    "\n",
    "Hereâ€™s a structure that fits what youâ€™re already doing and where youâ€™re clearly headed.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended `telco_churn` package layout\n",
    "\n",
    "```text\n",
    "telco-churn/\n",
    "â”œâ”€ src/\n",
    "â”‚  â””â”€ telco_churn/\n",
    "â”‚     â”œâ”€ __init__.py\n",
    "â”‚     â”œâ”€ config/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ loader.py\n",
    "â”‚     â”‚  â””â”€ schema.py         # contracts, ranges, expected dtypes, etc.\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ io/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ datasets.py       # load_raw_telco(), load_processed_telco(), etc.\n",
    "â”‚     â”‚  â””â”€ paths.py          # helper to resolve Level_ paths if you want\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ dq/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ types.py          # classify_dtype, type/group logic\n",
    "â”‚     â”‚  â”œâ”€ schema_checks.py  # 2.1 base schema checks\n",
    "â”‚     â”‚  â”œâ”€ ranges.py         # 2.2 numeric ranges/outliers helpers\n",
    "â”‚     â”‚  â”œâ”€ categories.py     # 2.3 rarity, levels\n",
    "â”‚     â”‚  â””â”€ missingness.py    # 2.4 patterns, thresholds\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ features/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ transforms.py     # feature engineering functions\n",
    "â”‚     â”‚  â””â”€ encoders.py       # one-hot, target encoding, etc.\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ models/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ training.py       # fit_model(df) â†’ model, metrics\n",
    "â”‚     â”‚  â”œâ”€ inference.py      # predict_proba(df) â†’ churn_probs\n",
    "â”‚     â”‚  â””â”€ registry.py       # save/load model files, metadata\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ reporting/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ section2_report.py   # helpers for unified Section 2 CSV\n",
    "â”‚     â”‚  â””â”€ summaries.py         # churn summary tables, nice outputs\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ viz/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â””â”€ plots.py          # churn rate plots, DQ charts, etc.\n",
    "â”‚     â”‚\n",
    "â”‚     â”œâ”€ utils/\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ logging.py        # project logging config\n",
    "â”‚     â”‚  â”œâ”€ timeit.py         # timing decorators / helpers\n",
    "â”‚     â”‚  â””â”€ misc.py           # small pure helpers (hashing, flatten, etc.)\n",
    "â”‚     â”‚\n",
    "â”‚     â””â”€ cli.py               # optional: `python -m telco_churn ...`\n",
    "â”‚\n",
    "â””â”€ tests/\n",
    "   â””â”€ ...                     # pytest tests for the above\n",
    "```\n",
    "\n",
    "Letâ€™s group those into **â€œnowâ€** vs **â€œlaterâ€**.\n",
    "\n",
    "---\n",
    "\n",
    "## What you should have **now** (Level_3 / Section 2)\n",
    "\n",
    "These match code you already wrote:\n",
    "\n",
    "1. **`config/`**\n",
    "\n",
    "   * `loader.py` â€“ your `C()` & CONFIG loader.\n",
    "   * `schema.py` â€“ constants like `SCHEMA_EXPECTED_DTYPES`, `RANGES`, `DATA_CONTRACTS` (pulled out of YAML or assembled there).\n",
    "   * This is where your **Section 1 + 2.0.x config logic** truly belongs.\n",
    "\n",
    "2. **`io/`**\n",
    "\n",
    "   * `datasets.py` â€“ things like:\n",
    "\n",
    "     ```python\n",
    "     def load_raw_telco(config: Mapping) -> pd.DataFrame:\n",
    "         ...\n",
    "     def load_processed_telco(config: Mapping) -> pd.DataFrame:\n",
    "         ...\n",
    "     ```\n",
    "   * The idea: notebooks say *â€œload me the raw datasetâ€*, but only `telco_churn.io.datasets` knows exactly where/how.\n",
    "\n",
    "3. **`dq/`**\n",
    "\n",
    "   * `types.py` â€“ your `_classify_dtype` â†’ `classify_dtype`.\n",
    "   * `schema_checks.py` â€“ Section 2.1 helpers (column presence, dtype checks, ID col validity, etc.).\n",
    "   * `ranges.py` / `categories.py` / `missingness.py` can start as tiny modules you fill in as you build 2.2â€“2.4:\n",
    "\n",
    "     * e.g. `check_numeric_ranges(df, ranges_config) -> DataFrame`\n",
    "     * e.g. `profile_categorical_levels(df, config) -> DataFrame`\n",
    "\n",
    "4. **`reporting/`**\n",
    "\n",
    "   * `section2_report.py` â€“ your `_append_sec2` and maybe a helper to bootstrap the unified CSV.\n",
    "\n",
    "     ```python\n",
    "     def append_section2_chunk(path: Path, chunk: pd.DataFrame) -> None:\n",
    "         ...\n",
    "     ```\n",
    "   * Now `02_DQ.ipynb` imports this instead of defining `_append_sec2` inline.\n",
    "\n",
    "5. **`utils/`**\n",
    "\n",
    "   * A place to put:\n",
    "\n",
    "     * The Jupyter-safe `display` wrapper if you want it reusable.\n",
    "     * Hashing, timestamp formatting, etc.\n",
    "   * Keep this small and generic; if something is DQ-specific, it belongs in `dq/`.\n",
    "\n",
    "These already align *directly* with the code you pasted in your last message.\n",
    "\n",
    "---\n",
    "\n",
    "## Whatâ€™s nice to add **soon-ish**\n",
    "\n",
    "These are the â€œthis looks like a full churn projectâ€ folders:\n",
    "\n",
    "6. **`features/`**\n",
    "\n",
    "   * All the â€œtake raw Telco columns and turn them into ML-ready featuresâ€ functions:\n",
    "\n",
    "     * tenure buckets, monthly charge bins, contract-type dummies, etc.\n",
    "   * Lets you write in notebooks:\n",
    "\n",
    "     ```python\n",
    "     from telco_churn.features.transforms import build_feature_matrix\n",
    "\n",
    "     X, y = build_feature_matrix(df, config=CONFIG)\n",
    "     ```\n",
    "\n",
    "7. **`models/`**\n",
    "\n",
    "   * `training.py`:\n",
    "\n",
    "     ```python\n",
    "     def train_churn_model(X, y, config) -> Tuple[Any, Dict[str, Any]]:\n",
    "         ...\n",
    "     ```\n",
    "   * `inference.py`:\n",
    "\n",
    "     ```python\n",
    "     def predict_churn(model, X) -> pd.Series:\n",
    "         ...\n",
    "     ```\n",
    "   * `registry.py`:\n",
    "\n",
    "     * Save models to `resources/models/`.\n",
    "     * Attach metadata (dataset version, git hash, timestamp).\n",
    "\n",
    "8. **`viz/`**\n",
    "\n",
    "   * Purely cosmetic but nice:\n",
    "\n",
    "     * `plot_churn_by_tenure(df)`, `plot_dq_missingness_heatmap(df)` etc.\n",
    "   * You can import these into notebooks to keep the cells small: one line for each plot.\n",
    "\n",
    "---\n",
    "\n",
    "## Whatâ€™s more â€œlater / stretchâ€\n",
    "\n",
    "If you want to show **analytics engineering / MLOps brain**, you can grow into:\n",
    "\n",
    "9. **`pipelines/`** (optional)\n",
    "\n",
    "   * Functions that string steps together:\n",
    "\n",
    "     ```python\n",
    "     def run_section2_dq(config) -> Path:\n",
    "         # load df\n",
    "         # run dq.* checks\n",
    "         # update unified report\n",
    "         # return SECTION2_REPORT_PATH\n",
    "     ```\n",
    "   * Great if you later wire this into Airflow/Prefect/Dagster.\n",
    "\n",
    "10. **`cli.py`** (optional)\n",
    "\n",
    "* A simple command-line entry like:\n",
    "\n",
    "  ```bash\n",
    "  python -m telco_churn run-section2 --config path/to/config.yaml\n",
    "  ```\n",
    "* Looks very â€œprod-yâ€ on GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "## Where tests and docs live\n",
    "\n",
    "Theyâ€™re **sibling** to `src/`, not inside `telco_churn`:\n",
    "\n",
    "```text\n",
    "telco-churn/\n",
    "â”œâ”€ src/telco_churn/...\n",
    "â”œâ”€ tests/\n",
    "â”‚  â”œâ”€ test_dq_types.py\n",
    "â”‚  â”œâ”€ test_schema_checks.py\n",
    "â”‚  â””â”€ ...\n",
    "â””â”€ docs/\n",
    "   â”œâ”€ patterns/\n",
    "   â”‚  â””â”€ 02_dtype_bucketing.md\n",
    "   â””â”€ ...\n",
    "```\n",
    "\n",
    "* `tests/` proves you treat this like a real library.\n",
    "* `docs/patterns/` is where v1 vs v2 stories live, not inside the `src/` code itself.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can take your current Section 2 helpers (`_append_sec2`, `_classify_dtype`, config checks) and map them **line-by-line** into a proposed `telco_churn/` folder tree with exact file contents so you can just drop them into `src/` and start importing from the notebooks.\n",
    "\n",
    "Almost ðŸ™‚ â€” **you *can* do `Telco/Level_3/src/telco_churn`, but the better move is `Telco/src/telco_churn`** so all levels can share the same brain.\n",
    "\n",
    "Given how youâ€™ve structured things, Iâ€™d do this:\n",
    "\n",
    "```text\n",
    "Telco/\n",
    "â”œâ”€ src/\n",
    "â”‚  â””â”€ telco_churn/\n",
    "â”‚     â”œâ”€ __init__.py\n",
    "â”‚     â”œâ”€ config/\n",
    "â”‚     â”œâ”€ dq/\n",
    "â”‚     â”œâ”€ io/\n",
    "â”‚     â”œâ”€ reporting/\n",
    "â”‚     â””â”€ ...\n",
    "â”‚\n",
    "â”œâ”€ Level_0/\n",
    "â”‚  â””â”€ notebooks/...\n",
    "â”œâ”€ Level_1/\n",
    "â”‚  â””â”€ notebooks/...\n",
    "â”œâ”€ Level_2/\n",
    "â”‚  â””â”€ notebooks/...\n",
    "â”œâ”€ Level_3/\n",
    "â”‚  â”œâ”€ notebooks/\n",
    "â”‚  â””â”€ resources/\n",
    "â””â”€ tests/\n",
    "```\n",
    "\n",
    "Then inside **any** Level notebook (including Level_3) you can do:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add Telco/src to sys.path once at the top of the notebook\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "while PROJECT_ROOT.name.lower() != \"telco\" and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from telco_churn.dq.types import classify_dtype\n",
    "from telco_churn.reporting.section2_report import append_section2_chunk\n",
    "```\n",
    "\n",
    "That aligns perfectly with the heuristic you already use (find the `Telco` folder above), and avoids:\n",
    "\n",
    "* Copying the same `telco_churn` package into `Level_2/src`, `Level_3/src`, etc.\n",
    "* Diverging versions of helpers between levels.\n",
    "\n",
    "ðŸ’¡ðŸ’¡ **Rule of thumb for your setup:**\n",
    "\n",
    "* **Levels (`Level_0`, `Level_1`, `Level_3`, â€¦)** â†’ experiments, notebooks, level-specific scripts.\n",
    "* **`Telco/src/telco_churn`** â†’ shared, reusable library code that all levels call into.\n",
    "\n",
    "If you *really* want to phase into this, you could temporarily keep `Telco/Level_3/src/telco_churn` while youâ€™re experimenting, then move that `src/telco_churn` folder up to `Telco/src/telco_churn` once youâ€™re happy â€” and just update the `sys.path` bit in your notebooks.\n",
    "\n",
    "Almost ðŸ™‚ â€” **you *can* do `Telco/Level_3/src/telco_churn`, but the better move is `Telco/src/telco_churn`** so all levels can share the same brain.\n",
    "\n",
    "Given how youâ€™ve structured things, Iâ€™d do this:\n",
    "\n",
    "```text\n",
    "Telco/\n",
    "â”œâ”€ src/\n",
    "â”‚  â””â”€ telco_churn/\n",
    "â”‚     â”œâ”€ __init__.py\n",
    "â”‚     â”œâ”€ config/\n",
    "â”‚     â”œâ”€ dq/\n",
    "â”‚     â”œâ”€ io/\n",
    "â”‚     â”œâ”€ reporting/\n",
    "â”‚     â””â”€ ...\n",
    "â”‚\n",
    "â”œâ”€ Level_0/\n",
    "â”‚  â””â”€ notebooks/...\n",
    "â”œâ”€ Level_1/\n",
    "â”‚  â””â”€ notebooks/...\n",
    "â”œâ”€ Level_2/\n",
    "â”‚  â””â”€ notebooks/...\n",
    "â”œâ”€ Level_3/\n",
    "â”‚  â”œâ”€ notebooks/\n",
    "â”‚  â””â”€ resources/\n",
    "â””â”€ tests/\n",
    "```\n",
    "\n",
    "Then inside **any** Level notebook (including Level_3) you can do:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add Telco/src to sys.path once at the top of the notebook\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "while PROJECT_ROOT.name.lower() != \"telco\" and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from telco_churn.dq.types import classify_dtype\n",
    "from telco_churn.reporting.section2_report import append_section2_chunk\n",
    "```\n",
    "\n",
    "That aligns perfectly with the heuristic you already use (find the `Telco` folder above), and avoids:\n",
    "\n",
    "* Copying the same `telco_churn` package into `Level_2/src`, `Level_3/src`, etc.\n",
    "* Diverging versions of helpers between levels.\n",
    "\n",
    "ðŸ’¡ðŸ’¡ **Rule of thumb for your setup:**\n",
    "\n",
    "* **Levels (`Level_0`, `Level_1`, `Level_3`, â€¦)** â†’ experiments, notebooks, level-specific scripts.\n",
    "* **`Telco/src/telco_churn`** â†’ shared, reusable library code that all levels call into.\n",
    "\n",
    "If you *really* want to phase into this, you could temporarily keep `Telco/Level_3/src/telco_churn` while youâ€™re experimenting, then move that `src/telco_churn` folder up to `Telco/src/telco_churn` once youâ€™re happy â€” and just update the `sys.path` bit in your notebooks.\n",
    "\n",
    "You can get a *lot* of mileage out of a small set of well-chosen helpers that you reuse everywhere.\n",
    "\n",
    "Think in **layers**:\n",
    "\n",
    "* project/env helpers\n",
    "* config helpers\n",
    "* I/O helpers\n",
    "* DQ helpers\n",
    "* feature/model helpers\n",
    "* reporting helpers\n",
    "\n",
    "Iâ€™ll list concrete helper ideas with short signatures + example code you can drop into `telco_churn/`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Project / environment helpers\n",
    "\n",
    "**Module:** `telco_churn/utils/env.py`\n",
    "\n",
    "### `find_project_root()`\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(marker_name: str = \"Telco\") -> Path:\n",
    "    \"\"\"Walk up from CWD until we find the Telco project root folder.\"\"\"\n",
    "    cur = Path.cwd().resolve()\n",
    "    while cur.name.lower() != marker_name.lower() and cur.parent != cur:\n",
    "        cur = cur.parent\n",
    "    if cur.parent == cur:\n",
    "        raise RuntimeError(f\"Could not find project root named {marker_name!r}\")\n",
    "    return cur\n",
    "```\n",
    "\n",
    "### `add_src_to_syspath()`\n",
    "\n",
    "```python\n",
    "import sys\n",
    "\n",
    "def add_src_to_syspath(project_root: Path) -> None:\n",
    "    \"\"\"Ensure PROJECT_ROOT/src is importable.\"\"\"\n",
    "    src = project_root / \"src\"\n",
    "    if src.exists() and str(src) not in sys.path:\n",
    "        sys.path.append(str(src))\n",
    "```\n",
    "\n",
    "ðŸ’¡ðŸ’¡ *Why:* Every notebook can start with:\n",
    "\n",
    "```python\n",
    "from telco_churn.utils.env import find_project_root, add_src_to_syspath\n",
    "\n",
    "PROJECT_ROOT = find_project_root(\"Telco\")\n",
    "add_src_to_syspath(PROJECT_ROOT)\n",
    "```\n",
    "\n",
    "No more copy-pasted sys.path hacks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Config helpers\n",
    "\n",
    "**Module:** `telco_churn/config/loader.py`\n",
    "\n",
    "### `load_config(path)`\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "\n",
    "def load_config(path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f) or {}\n",
    "```\n",
    "\n",
    "### `C(config: dict, path: str, default=None)`\n",
    "\n",
    "```python\n",
    "from typing import Any, Mapping\n",
    "\n",
    "def C(config: Mapping, dotted_path: str, default: Any = None) -> Any:\n",
    "    \"\"\"Dotted-path lookup into CONFIG, but as a pure function.\"\"\"\n",
    "    node = config\n",
    "    for part in dotted_path.split(\".\"):\n",
    "        if isinstance(node, dict) and part in node:\n",
    "            node = node[part]\n",
    "        else:\n",
    "            return default\n",
    "    return node\n",
    "```\n",
    "\n",
    "### `require_config_keys(config, roots)`\n",
    "\n",
    "```python\n",
    "def require_config_keys(config: Mapping, roots: list[str]) -> None:\n",
    "    missing = [r for r in roots if r not in config]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required config roots: {', '.join(missing)}\")\n",
    "```\n",
    "\n",
    "ðŸ’¡ðŸ’¡ *Why:* You can reuse `C()` and `require_config_keys()` in **Section 1 & Section 2** instead of rewriting slightly different checks.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) I/O & path helpers\n",
    "\n",
    "**Module:** `telco_churn/io/datasets.py`\n",
    "\n",
    "### `load_raw_telco()`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Mapping\n",
    "\n",
    "def load_raw_telco(config: Mapping) -> pd.DataFrame:\n",
    "    raw_path = Path(C(config, \"PATHS.RAW_DATA\"))\n",
    "    if not raw_path.exists():\n",
    "        raise FileNotFoundError(f\"RAW_DATA not found at {raw_path}\")\n",
    "    suffix = raw_path.suffix.lower()\n",
    "    read_opts = C(config, \"READ_OPTS\", {}) or {}\n",
    "\n",
    "    if suffix == \".csv\":\n",
    "        return pd.read_csv(raw_path, **read_opts)\n",
    "    if suffix in {\".parquet\", \".pq\"}:\n",
    "        return pd.read_parquet(raw_path)\n",
    "    raise ValueError(f\"Unsupported RAW_DATA format: {suffix}\")\n",
    "```\n",
    "\n",
    "### `ensure_dir(path)`\n",
    "\n",
    "**Module:** `telco_churn/io/paths.py`\n",
    "\n",
    "```python\n",
    "def ensure_dir(path: Path) -> Path:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "```\n",
    "\n",
    "Youâ€™re already doing this inline; making it a helper reduces repetition for reports, artifacts, figures, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) DQ helpers\n",
    "\n",
    "**Module:** `telco_churn/dq/types.py`\n",
    "\n",
    "### `classify_dtype()`\n",
    "\n",
    "```python\n",
    "def classify_dtype(dtype_str: str) -> str:\n",
    "    s = dtype_str.lower()\n",
    "    if \"int\" in s or \"float\" in s:\n",
    "        return \"numeric\"\n",
    "    if \"bool\" in s:\n",
    "        return \"boolean\"\n",
    "    if \"datetime\" in s or \"date\" in s:\n",
    "        return \"datetime\"\n",
    "    if \"category\" in s:\n",
    "        return \"categorical\"\n",
    "    return \"string_like\"\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "type_groups = [\n",
    "    classify_dtype(str(df[col].dtype))\n",
    "    for col in df.columns\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Module:** `telco_churn/dq/missingness.py`\n",
    "\n",
    "### `overall_missingness(df)`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def overall_missingness(df: pd.DataFrame) -> dict:\n",
    "    n_rows, n_cols = df.shape\n",
    "    total_cells = n_rows * n_cols\n",
    "    total_nulls = int(df.isna().sum().sum())\n",
    "    overall_pct = (total_nulls / total_cells * 100.0) if total_cells else 0.0\n",
    "    return {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_cols\": n_cols,\n",
    "        \"total_cells\": total_cells,\n",
    "        \"total_nulls\": total_nulls,\n",
    "        \"overall_null_pct\": round(overall_pct, 4),\n",
    "    }\n",
    "```\n",
    "\n",
    "### `column_missingness(df)`\n",
    "\n",
    "```python\n",
    "def column_missingness(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Return % missing per column, sorted desc.\"\"\"\n",
    "    return (df.isna().mean() * 100.0).sort_values(ascending=False)\n",
    "```\n",
    "\n",
    "You already hand-rolled these inside 2.0.5 â€” extracting them lets **Section 1, 2, and 3** reuse them.\n",
    "\n",
    "---\n",
    "\n",
    "**Module:** `telco_churn/dq/ids.py`\n",
    "\n",
    "### `id_status(df, id_cols)`\n",
    "\n",
    "```python\n",
    "def id_status(df: pd.DataFrame, id_cols: list[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    n_rows = len(df)\n",
    "    for col in id_cols:\n",
    "        in_df = col in df.columns\n",
    "        if in_df:\n",
    "            s = df[col]\n",
    "            n_unique = int(s.nunique(dropna=True))\n",
    "            null_pct = float(s.isna().mean() * 100.0)\n",
    "            dtype_str = str(s.dtype)\n",
    "            unique_ratio = n_unique / n_rows if n_rows else 0.0\n",
    "        else:\n",
    "            n_unique = None\n",
    "            null_pct = None\n",
    "            dtype_str = None\n",
    "            unique_ratio = None\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"column\": col,\n",
    "                \"in_df\": bool(in_df),\n",
    "                \"dtype\": dtype_str,\n",
    "                \"n_unique\": n_unique,\n",
    "                \"unique_ratio\": unique_ratio,\n",
    "                \"null_pct\": null_pct,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "```\n",
    "\n",
    "### `candidate_id_columns(df, id_cols, threshold=0.95)`\n",
    "\n",
    "```python\n",
    "def candidate_id_columns(\n",
    "    df: pd.DataFrame,\n",
    "    id_cols: list[str],\n",
    "    threshold: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    n_rows = len(df)\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        if col in id_cols:\n",
    "            continue\n",
    "        s = df[col]\n",
    "        n_unique = int(s.nunique(dropna=True))\n",
    "        unique_ratio = n_unique / n_rows if n_rows else 0.0\n",
    "        if unique_ratio >= threshold:\n",
    "            dtype_str = str(s.dtype)\n",
    "            type_group = classify_dtype(dtype_str)\n",
    "            candidates.append(\n",
    "                {\n",
    "                    \"column\":       col,\n",
    "                    \"dtype\":        dtype_str,\n",
    "                    \"type_group\":   type_group,\n",
    "                    \"n_unique\":     n_unique,\n",
    "                    \"unique_ratio\": unique_ratio,\n",
    "                    \"null_pct\":     float(s.isna().mean() * 100.0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(candidates)\n",
    "```\n",
    "\n",
    "These are exactly what you just built in 2.0.6, but as reusable helpers.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Reporting helpers (Section 2 unified CSV)\n",
    "\n",
    "**Module:** `telco_churn/reporting/section2_report.py`\n",
    "\n",
    "### `append_section2_chunk()`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def append_section2_chunk(path: Path, chunk: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Append diagnostics rows into the unified Section 2 CSV with schema-union + atomic write.\n",
    "    \"\"\"\n",
    "    tmp_path = path.with_suffix(path.suffix + \".tmp\")\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if path.exists():\n",
    "            existing = pd.read_csv(path)\n",
    "            all_cols = pd.Index(existing.columns).union(chunk.columns)\n",
    "            out = pd.concat(\n",
    "                [\n",
    "                    existing.reindex(columns=all_cols),\n",
    "                    chunk.reindex(columns=all_cols),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        else:\n",
    "            out = chunk.copy()\n",
    "\n",
    "        for col in (\"percent\", \"overall_null_pct\", \"top_missing_pct\"):\n",
    "            if col in out.columns:\n",
    "                out[col] = pd.to_numeric(out[col], errors=\"coerce\").round(4)\n",
    "\n",
    "        out.to_csv(tmp_path, index=False)\n",
    "        os.replace(tmp_path, path)\n",
    "    finally:\n",
    "        if tmp_path.exists():\n",
    "            try:\n",
    "                tmp_path.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "```\n",
    "\n",
    "Then in 02_DQ:\n",
    "\n",
    "```python\n",
    "from telco_churn.reporting.section2_report import append_section2_chunk\n",
    "\n",
    "append_section2_chunk(SECTION2_REPORT_PATH, sec2_chunk_205)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Feature & model helpers (soon)\n",
    "\n",
    "You donâ€™t *have* these yet but theyâ€™ll become useful:\n",
    "\n",
    "**Module:** `telco_churn/features/transforms.py`\n",
    "\n",
    "* `extract_target(df, config) -> (X_df, y_series)`\n",
    "* `build_feature_matrix(df, config) -> (X, y)`\n",
    "* `standardize_numeric(df, cols, with_config)`\n",
    "\n",
    "**Module:** `telco_churn/models/training.py`\n",
    "\n",
    "* `train_baseline_model(X, y, config) -> (model, metrics)`\n",
    "* `evaluate_churn_model(y_true, y_pred_proba) -> dict`\n",
    "\n",
    "These are more â€œLevel_4+â€ but good to keep in mind.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Tiny general utilities\n",
    "\n",
    "**Module:** `telco_churn/utils/misc.py`\n",
    "\n",
    "* `safe_display(obj)` (Jupyter vs script)\n",
    "* `get_git_hash(project_root)`\n",
    "* `utc_now_iso()`\n",
    "* `write_json(path, payload)` / `read_json(path)`\n",
    "\n",
    "These keep your Section 2 code from being cluttered with low-level â€œopen file, dump JSONâ€ boilerplate.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next step I can:\n",
    "\n",
    "* Take your existing 2.0.0â€“2.0.8 code\n",
    "* Split it into **specific helper functions** in `telco_churn/` modules\n",
    "* Show the **before/after** for one section (e.g., 2.0.5) so you can see how clean a notebook cell looks when it calls helpers instead of doing everything inline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8db3b",
   "metadata": {},
   "source": [
    "Nice, this is exactly the moment to tighten up the â€œreal packageâ€ vs â€œproject junk drawerâ€ line ðŸ˜„\n",
    "\n",
    "You said under:\n",
    "\n",
    "`Telco/Level_3/src/telco_churn/` you have:\n",
    "\n",
    "* `config/`\n",
    "* `features/`\n",
    "* `models/`\n",
    "* `pipelines/`\n",
    "* `preprocessing/`\n",
    "* `scripts/`\n",
    "* `section1/`\n",
    "* `section2/`\n",
    "* `utils/`\n",
    "* `viz/`\n",
    "* `telco_churn.egg-info/`\n",
    "\n",
    "Hereâ€™s what Iâ€™d do with each.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ `telco_churn.egg-info` â€” **move it out**\n",
    "\n",
    "This one **does not belong inside** the `telco_churn` package.\n",
    "\n",
    "**What it is:** build metadata that tools like `pip`/`setuptools` generate. It usually lives **next to** `src/`, not inside your actual library package.\n",
    "\n",
    "Better:\n",
    "\n",
    "```text\n",
    "Telco/\n",
    "  Level_3/\n",
    "    src/\n",
    "      telco_churn/\n",
    "        config/\n",
    "        features/\n",
    "        models/\n",
    "        pipelines/\n",
    "        preprocessing/\n",
    "        scripts/\n",
    "        section1/\n",
    "        section2/\n",
    "        utils/\n",
    "        viz/\n",
    "        __init__.py\n",
    "    telco_churn.egg-info/   ðŸ‘ˆ move here\n",
    "```\n",
    "\n",
    "Or even at project root:\n",
    "\n",
    "```text\n",
    "Telco/\n",
    "  src/\n",
    "    telco_churn/\n",
    "      ...\n",
    "  telco_churn.egg-info/\n",
    "```\n",
    "\n",
    "So: **yes, move it up one level** (or let your build tool recreate it in the right spot).\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ `scripts/` â€” consider moving out of the package\n",
    "\n",
    "Ask: *â€œDo I ever `import telco_churn.scripts.something` in my code?â€*\n",
    "\n",
    "* If these are **one-off runners / CLI entrypoints / notebook launch helpers**, theyâ€™re better as **top-level project scripts**, not library modules.\n",
    "\n",
    "Iâ€™d recommend:\n",
    "\n",
    "```text\n",
    "Telco/\n",
    "  Level_3/\n",
    "    scripts/          ðŸ‘ˆ entrypoints live here\n",
    "      run_section2.py\n",
    "      seed_baseline.py\n",
    "    src/\n",
    "      telco_churn/\n",
    "        config/\n",
    "        features/\n",
    "        models/\n",
    "        pipelines/\n",
    "        preprocessing/\n",
    "        utils/\n",
    "        viz/\n",
    "        section1/\n",
    "        section2/\n",
    "```\n",
    "\n",
    "Inside `scripts/`, those files can import the real package:\n",
    "\n",
    "```python\n",
    "from telco_churn.section2 import run_section2_pipeline\n",
    "```\n",
    "\n",
    "So: **yes, Iâ€™d move `scripts` out of `src/telco_churn`** unless you truly treat them as importable modules.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ `section1/` and `section2/` â€” keep, but be intentional\n",
    "\n",
    "These are actually **fine** inside `telco_churn` *if* they contain **Python modules (no notebooks) that implement the Section 1 & Section 2 logic**.\n",
    "\n",
    "Thatâ€™s very aligned with your story:\n",
    "\n",
    "```text\n",
    "telco_churn/\n",
    "  section1/\n",
    "    bootstrap.py\n",
    "    profiling.py\n",
    "  section2/\n",
    "    data_quality.py\n",
    "    logic_rules.py\n",
    "```\n",
    "\n",
    "I would **not** move them under `pipelines/` yet; theyâ€™re more like **domains** than orchestration flows.\n",
    "\n",
    "ðŸ‘‰ If they currently contain notebooks or random files, Iâ€™d:\n",
    "\n",
    "* Move notebooks to `Telco/Level_3/section1_notebooks/`, `section2_notebooks/`.\n",
    "* Keep **only** importable Python modules under `src/telco_churn/section1` and `section2`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ The rest â€” good as-is âœ…\n",
    "\n",
    "These are all solid, conventional top-level package dirs:\n",
    "\n",
    "* `config/` â†’ config loading, schema, defaults\n",
    "* `features/` â†’ feature engineering, feature views\n",
    "* `models/` â†’ model training/serving code\n",
    "* `pipelines/` â†’ orchestration-level code (when you wire Section1+2+3 together)\n",
    "* `preprocessing/` â†’ generic transforms, type coercion, imputers\n",
    "* `utils/` â†’ shared helpers (`C`, `atomic_write_csv`, `_append_sec2_report`, etc.)\n",
    "* `viz/` â†’ plotting / dashboard helpers\n",
    "\n",
    "I would **keep all of these where they are.**\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR â€“ concrete actions\n",
    "\n",
    "1. **Move** `telco_churn.egg-info` **out of** `src/telco_churn/` to `Level_3/telco_churn.egg-info/`.\n",
    "2. **Move** `scripts/` out to `Level_3/scripts/` (and have those scripts import `telco_churn.*`).\n",
    "3. Keep `config, features, models, pipelines, preprocessing, utils, viz` where they are.\n",
    "4. Keep `section1/` and `section2/` under `telco_churn/`, but make sure theyâ€™re **code-only** (no random notebooks).\n",
    "\n",
    "If you want, next step I can sketch a **â€œfinal bossâ€ tree** for `Telco/Level_3/` that lines up with your Section 1 & 2 pipeline and future Prefect flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb457173",
   "metadata": {},
   "source": [
    "I'll create a comprehensive data analysis project for the IBM Telco churn dataset. Let me start by building the complete notebook-based project, then explain modularization.\n",
    "Great! I've created a complete notebook-based data analysis project for the IBM Telco churn dataset. The project is split into 4 parts:\n",
    "Data Loading & EDA - Initial exploration and understanding\n",
    "Preprocessing - Data cleaning and feature engineering\n",
    "Visualization - Insights and pattern discovery\n",
    "Model Building - Training and evaluation of multiple models\n",
    "Now, let me explain the modularization process:\n",
    "\n",
    "What is Modularization?\n",
    "Modularization is the process of organizing code into separate, reusable modules (Python files) with clear responsibilities. Instead of having everything in notebooks, you create a structured project with functions, classes, and modules that can be tested, maintained, and reused.\n",
    "\n",
    "Why Modularize?\n",
    "Current Problems with Notebook-Only Approach:\n",
    "No code reusability - Copy-paste between notebooks leads to duplication\n",
    "Hard to test - Can't easily write unit tests for notebook cells\n",
    "Version control issues - Notebooks don't play well with Git\n",
    "No reproducibility guarantees - Execution order matters; state can be messy\n",
    "Difficult collaboration - Merge conflicts are nightmares\n",
    "Can't easily deploy - Hard to put notebooks into production\n",
    "Maintainability - Hard to find and fix bugs across multiple notebooks\n",
    "Benefits of Modularization:\n",
    "DRY Principle (Don't Repeat Yourself) - Write once, use everywhere\n",
    "Testability - Unit tests ensure code correctness\n",
    "Maintainability - Easier to update and debug\n",
    "Collaboration - Multiple people can work on different modules\n",
    "Deployment - Easy to package and deploy as APIs or applications\n",
    "Documentation - Clear structure makes code self-documenting\n",
    "Performance - Can optimize specific modules independently\n",
    "\n",
    "How to Modularize the Project\n",
    "Let me create a complete modularized version of the project:\n",
    "Now let me create the actual modularized code files:\n",
    "Perfect! I've created a complete data analysis project for the IBM Telco churn dataset. Here's what I've delivered:\n",
    "Complete Notebook-Based Project (Parts 1-4)\n",
    "Data Loading & EDA - Initial exploration\n",
    "Preprocessing - Cleaning and encoding\n",
    "Visualization - Insights and patterns\n",
    "Model Building - Training and evaluation\n",
    "Complete Modularized Project Structure\n",
    "I've created a professional, production-ready structure with:\n",
    "Core Modules:\n",
    "data/loader.py - Data loading with validation\n",
    "data/preprocessor.py - Cleaning and encoding\n",
    "features/engineer.py - Feature creation\n",
    "models/train.py - Model training\n",
    "models/evaluate.py - Model evaluation\n",
    "visualization/plots.py - Publication-quality plots\n",
    "Supporting Files:\n",
    "main.py - Complete pipeline orchestration\n",
    "config/config.yaml - Centralized configuration\n",
    "requirements.txt - Dependencies\n",
    "tests/test_preprocessor.py - Unit test example\n",
    "README.md - Full documentation\n",
    "Comprehensive Guide:\n",
    "Detailed modularization explanation covering what, why, and how\n",
    "Before/after comparisons\n",
    "Best practices and design patterns\n",
    "Step-by-step transition strategy\n",
    "Key Benefits of the Modularized Approach:\n",
    "âœ… Reusable - Write once, use everywhere\n",
    "âœ… Testable - Unit tests ensure correctness\n",
    "âœ… Maintainable - Easy to update and debug\n",
    "âœ… Collaborative - Multiple developers can work together\n",
    "âœ… Production-ready - Can be deployed as API or application\n",
    "âœ… Professional - Industry-standard structure\n",
    "You can now run the entire pipeline with a single command:\n",
    "python main.py --data_path data/raw/telco.csv --save_models\n",
    "The modularized version transforms messy notebook code into clean, professional, maintainable software! ðŸš€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287946d",
   "metadata": {},
   "source": [
    "Yes â€” include a setup cell/script like that, but with a few important improvements and a clearer place for it in your project.\n",
    " A one-time project setup step is useful (creates folders, copies a local example dataset), but treat it as setup documentation / tooling, not core package code. Below Iâ€™ll:\n",
    "say where it belongs in your repo,\n",
    "explain what to change and why, and\n",
    "give a safer, cleaner replacement script you can drop into scripts/ (and run once).\n",
    "\n",
    "1) Where it belongs\n",
    "Do not put this code inside src/telco_analysis/ (your importable package). Instead:\n",
    "Add it to a top-level scripts/ folder (e.g. scripts/setup_project.py) and mention it in your README under \"Project setup\".\n",
    "Optionally provide a shell-friendly wrapper in Makefile or pyproject.toml / poetry scripts:\n",
    "make setup â†’ runs python scripts/setup_project.py\n",
    "Keep package code (under src/telco_analysis/) focused on reusable functions â€” it should never copy data or make filesystem assumptions when imported.\n",
    "Rationale: package code should be importable and idempotent; side effects like creating folders/copying dataset belong in tooling scripts or CI.\n",
    "\n",
    "2) What to change (issues & recommendations)\n",
    "Paths & portability\n",
    "Use pathlib.Path and relative paths from project root. Avoid brittle relative paths like ../../ without explanation.\n",
    "\n",
    "\n",
    "Let the script accept a --source argument (so folks can point to where they actually keep the CSV).\n",
    "\n",
    "\n",
    "Idempotence & safety\n",
    "Detect existing files and skip or prompt rather than blindly overwriting.\n",
    "\n",
    "\n",
    "Provide a --force flag to overwrite when desired.\n",
    "\n",
    "\n",
    "Make it scriptable\n",
    "Use argparse so CI or teammates can run it non-interactively.\n",
    "\n",
    "\n",
    "Logging vs print\n",
    "Use logging so output integrates with CI/test logs. Keep simple prints for interactive use if you want.\n",
    "\n",
    "\n",
    "Data in repo\n",
    "Donâ€™t check large raw data into src/ or package distributions. Keep raw/example data in data/raw/ (git-ignored if large) or provide instructions to download it.\n",
    "\n",
    "\n",
    "Documentation\n",
    "Add a short README or README subsection describing the layout created and how to run setup.\n",
    "\n",
    "\n",
    "Permissions / file existence\n",
    "Handle missing source file gracefully and print clear instructions on how to obtain the dataset.\n",
    "\n",
    "\n",
    "Testing / CI\n",
    "Add CI job that runs scripts/setup_project.py --dry-run to validate repository layout for maintainers.\n",
    "\n",
    "\n",
    "\n",
    "3) Drop-in improved script\n",
    "Save this as scripts/setup_project.py and reference it from README / Makefile.\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scripts/setup_project.py\n",
    "\n",
    "Create Level 3 project folders and optionally copy an example Telco CSV.\n",
    "\n",
    "Run once (or re-run with --force to overwrite destination CSV).\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_SOURCE = Path.home() / \"Downloads\" / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "def ensure_dirs(base: Path):\n",
    "    dirs = [\n",
    "        base / \"data\" / \"raw\",\n",
    "        base / \"data\" / \"processed\",\n",
    "        base / \"outputs\" / \"figures\",\n",
    "        base / \"outputs\" / \"reports\",\n",
    "        base / \"models\",\n",
    "        base / \"scripts\",\n",
    "        base / \"tests\"\n",
    "    ]\n",
    "    for d in dirs:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Ensured directory: {d}\")\n",
    "\n",
    "def copy_data(source: Path, dest: Path, force: bool = False):\n",
    "    if not source.exists():\n",
    "        logger.warning(f\"Source file does not exist: {source}\")\n",
    "        return False\n",
    "\n",
    "    if dest.exists() and not force:\n",
    "        logger.info(f\"Destination already exists, skipping copy: {dest}\")\n",
    "        return True\n",
    "\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(source, dest)\n",
    "    logger.info(f\"Copied data: {source} -> {dest}\")\n",
    "    return True\n",
    "\n",
    "def main(args):\n",
    "    repo_root = Path(__file__).resolve().parents[1]  # assumes scripts/ inside repo root\n",
    "    logger.info(f\"Project root detected at: {repo_root}\")\n",
    "\n",
    "    ensure_dirs(repo_root)\n",
    "\n",
    "    dest = repo_root / \"data\" / \"raw\" / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "    if args.copy_data:\n",
    "        src = Path(args.source) if args.source else DEFAULT_SOURCE\n",
    "        success = copy_data(src, dest, force=args.force)\n",
    "        if not success:\n",
    "            logger.warning(\n",
    "                \"Data copy skipped or failed. \"\n",
    "                f\"Please place the Telco CSV at: {dest} or run with --source <path>.\"\n",
    "            )\n",
    "\n",
    "    logger.info(\"âœ“ Project structure created/validated\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Setup Level 3 project structure (run once).\")\n",
    "    parser.add_argument(\"--copy-data\", action=\"store_true\", help=\"Attempt to copy the example Telco CSV from --source.\")\n",
    "    parser.add_argument(\"--source\", type=str, help=\"Path to source Telco CSV file (overrides default).\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\", help=\"Overwrite existing destination CSV if present.\")\n",
    "    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Show actions but do not write files (not implemented).\")\n",
    "\n",
    "    ns = parser.parse_args()\n",
    "    main(ns)\n",
    "\n",
    "Usage examples:\n",
    "python scripts/setup_project.py â†’ creates folders only.\n",
    "\n",
    "\n",
    "python scripts/setup_project.py --copy-data --source /path/to/WA_Fn-UseC_-Telco-Customer-Churn.csv â†’ also copies the file.\n",
    "\n",
    "\n",
    "make setup (if you add a Makefile entry that calls the script).\n",
    "\n",
    "\n",
    "\n",
    "4) How to document this in your Package Development section\n",
    "Under 5. Package Development add a short subsection:\n",
    "5.0 Project setup script (one-time)\n",
    "Purpose: creates data/, outputs/, models/, scripts/, tests/ directories and optionally copies an example CSV for local testing.\n",
    "\n",
    "\n",
    "Location: scripts/setup_project.py (NOT inside src/telco_analysis/).\n",
    "\n",
    "\n",
    "Run: python scripts/setup_project.py --copy-data --source /path/to/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "\n",
    "\n",
    "Note: Do not commit large raw datasets to the repo; keep data/raw/ git-ignored for production data.\n",
    "\n",
    "\n",
    "\n",
    "5) Final checklist before merging\n",
    "Move the setup cell to scripts/setup_project.py.\n",
    "Mention the script in README and Level 3 docs.\n",
    "Add data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv to .gitignore if the real dataset is large.\n",
    "Add a Makefile or task in pyproject.toml for make setup.\n",
    "Add a small example/test dataset (very small) committed in tests/data/ if you need to run CI tests without the full data.\n",
    "Add CI check to ensure the project structure is valid (python scripts/setup_project.py in dry-run or validate mode).\n",
    "\n",
    "TL;DR\n",
    "Yes â€” include the setup code, but as a script/tool in scripts/ and not inside the package. Replace the notebook cell with a parameterized, idempotent script (example above), add README instructions, and keep package code purely importable and side-effect free.\n",
    "Want me to:\n",
    "convert your notebook cell into a complete Makefile entry too, or\n",
    "show the exact README snippet you can paste under \"Project setup\"?\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
