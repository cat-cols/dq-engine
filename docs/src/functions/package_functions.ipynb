{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d40287",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ’¡ **Why the `telco_analysis/` Package Structure is Better**\n",
    "\n",
    "**âœ… Proper Python Package**: Creates a true importable package with `__init__.py`\n",
    "\n",
    "**âœ… Namespace Management**: All your code lives under `telco_analysis.module_name`\n",
    "\n",
    "**âœ… Future-Proof**: Easily extensible for multiple analysis types\n",
    "\n",
    "**âœ… Professional Standard**: Industry best practice for Python projects\n",
    "\n",
    "**âœ… Testing-Friendly**: Pytest can easily discover and import your package\n",
    "\n",
    "**âœ… Distribution-Ready**: Can be packaged and installed with pip later\n",
    "\n",
    "### **Don't Forget the `__init__.py` Files!**\n",
    "\n",
    "**`src/telco_analysis/__init__.py`**:\n",
    "```python\n",
    "\"\"\"\n",
    "Telco Customer Churn Analysis Package\n",
    "\n",
    "A comprehensive toolkit for analyzing customer churn in telecommunications data.\n",
    "\"\"\"\n",
    "\n",
    "__version__ = \"0.1.0\"\n",
    "__author__ = \"Your Name\"\n",
    "\n",
    "# Import key functions for easy access\n",
    "from .data_utils import load_raw_data, validate_data_schema\n",
    "from .preprocessing import clean_total_charges, prepare_features_target\n",
    "from .model_utils import create_baseline_model, train_model, evaluate_model\n",
    "\n",
    "# Define what's available when someone imports the package\n",
    "__all__ = [\n",
    "    'load_raw_data', \n",
    "    'validate_data_schema',\n",
    "    'clean_total_charges',\n",
    "    'prepare_features_target', \n",
    "    'create_baseline_model',\n",
    "    'train_model',\n",
    "    'evaluate_model'\n",
    "]\n",
    "```\n",
    "\n",
    "This allows cleaner imports later:\n",
    "```python\n",
    "from telco_analysis import load_raw_data, create_baseline_model\n",
    "```\n",
    "\n",
    "Instead of:\n",
    "```python  \n",
    "from telco_analysis.data_utils import load_raw_data\n",
    "from telco_analysis.model_utils import create_baseline_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118f64c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **`src/data_utils.py`**\n",
    "```python\n",
    "\"\"\"\n",
    "Data loading and basic utilities for the telco churn project.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_raw_data(filepath='data/raw/telco_customer_churn.csv'):\n",
    "    \"\"\"\n",
    "    Load the raw telco churn dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the raw CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Raw dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {filepath}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_data_schema(df, expected_columns=None):\n",
    "    \"\"\"\n",
    "    Validate that the dataframe has expected structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset to validate\n",
    "    expected_columns : list, optional\n",
    "        List of expected column names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if validation passes\n",
    "    \"\"\"\n",
    "    if expected_columns is None:\n",
    "        expected_columns = [\n",
    "            'customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "            'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "            'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "            'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "            'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'\n",
    "        ]\n",
    "    \n",
    "    missing_cols = set(expected_columns) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    logger.info(\"Data schema validation passed\")\n",
    "    return True\n",
    "\n",
    "def save_processed_data(df, filename, folder='data/processed'):\n",
    "    \"\"\"\n",
    "    Save processed dataframe to CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Processed dataset\n",
    "    filename : str\n",
    "        Name for the output file\n",
    "    folder : str\n",
    "        Output directory\n",
    "    \"\"\"\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    filepath = Path(folder) / filename\n",
    "    df.to_csv(filepath, index=False)\n",
    "    logger.info(f\"Data saved to: {filepath}\")\n",
    "```\n",
    "\n",
    "### **`src/preprocessing.py`**\n",
    "```python\n",
    "\"\"\"\n",
    "Data preprocessing functions for the telco churn project.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_total_charges(df):\n",
    "    \"\"\"\n",
    "    Clean and convert TotalCharges column to numeric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataframe with cleaned TotalCharges\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert TotalCharges to numeric, coercing errors to NaN\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    \n",
    "    # Handle missing values (likely new customers with tenure=0)\n",
    "    mask = df['TotalCharges'].isna()\n",
    "    df.loc[mask, 'TotalCharges'] = df.loc[mask, 'MonthlyCharges']\n",
    "    \n",
    "    logger.info(f\"Cleaned TotalCharges column. Fixed {mask.sum()} missing values.\")\n",
    "    return df\n",
    "\n",
    "def prepare_features_target(df):\n",
    "    \"\"\"\n",
    "    Separate features and target, and identify column types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (X, y, numeric_features, categorical_features)\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(['customerID', 'Churn'], axis=1)\n",
    "    y = (df['Churn'] == 'Yes').astype(int)  # Convert to binary\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    \n",
    "    # SeniorCitizen is already numeric (0/1)\n",
    "    if 'SeniorCitizen' in X.columns:\n",
    "        numeric_features.append('SeniorCitizen')\n",
    "    \n",
    "    categorical_features = [\n",
    "        col for col in X.columns \n",
    "        if col not in numeric_features\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Features prepared. Numeric: {len(numeric_features)}, Categorical: {len(categorical_features)}\")\n",
    "    return X, y, numeric_features, categorical_features\n",
    "\n",
    "def create_preprocessing_pipeline(numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Create preprocessing pipeline for the features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    numeric_features : list\n",
    "        List of numeric feature names\n",
    "    categorical_features : list\n",
    "        List of categorical feature names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.compose.ColumnTransformer\n",
    "        Preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Numeric preprocessing: just standard scaling for now\n",
    "    numeric_transformer = StandardScaler()\n",
    "    \n",
    "    # Categorical preprocessing: one-hot encoding\n",
    "    categorical_transformer = OneHotEncoder(\n",
    "        drop='first',  # Avoid multicollinearity\n",
    "        handle_unknown='ignore'  # Handle unseen categories gracefully\n",
    "    )\n",
    "    \n",
    "    # Combine preprocessors\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Preprocessing pipeline created\")\n",
    "    return preprocessor\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features\n",
    "    y : pd.Series\n",
    "        Target variable\n",
    "    test_size : float\n",
    "        Proportion of data for testing\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Data split completed. Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "    logger.info(f\"Train churn rate: {y_train.mean():.3f}, Test churn rate: {y_test.mean():.3f}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "```\n",
    "\n",
    "### **`src/model_utils.py`**\n",
    "```python\n",
    "\"\"\"\n",
    "Model training and evaluation utilities.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, \n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_baseline_model(preprocessor, random_state=42):\n",
    "    \"\"\"\n",
    "    Create baseline logistic regression model with preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    preprocessor : sklearn.compose.ColumnTransformer\n",
    "        Preprocessing pipeline\n",
    "    random_state : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Complete modeling pipeline\n",
    "    \"\"\"\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            random_state=random_state,\n",
    "            max_iter=1000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    logger.info(\"Baseline model pipeline created\")\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the model pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn.pipeline.Pipeline\n",
    "        Model pipeline to train\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training target\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Trained model pipeline\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    logger.info(\"Model training completed\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn.pipeline.Pipeline\n",
    "        Trained model\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test target\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Model evaluation completed. ROC-AUC: {roc_auc:.3f}, PR-AUC: {pr_auc:.3f}\")\n",
    "    return results\n",
    "\n",
    "def save_model_artifacts(model, results, model_path='models/baseline_pipeline.joblib', \n",
    "                        metadata_path='models/model_metadata.json'):\n",
    "    \"\"\"\n",
    "    Save model and performance metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn.pipeline.Pipeline\n",
    "        Trained model to save\n",
    "    results : dict\n",
    "        Model evaluation results\n",
    "    model_path : str\n",
    "        Path to save the model\n",
    "    metadata_path : str\n",
    "        Path to save the metadata\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(metadata_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, model_path)\n",
    "    logger.info(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logger.info(f\"Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "def load_model(model_path='models/baseline_pipeline.joblib'):\n",
    "    \"\"\"\n",
    "    Load saved model pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_path : str\n",
    "        Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Loaded model pipeline\n",
    "    \"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    logger.info(f\"Model loaded from: {model_path}\")\n",
    "    return model\n",
    "```\n",
    "\n",
    "### **`src/telco_analysis/config.py`** (Alternative to separate configs/ folder)\n",
    "```python\n",
    "\"\"\"\n",
    "Configuration settings for the telco churn model.\n",
    "\"\"\"\n",
    "\n",
    "# Data paths\n",
    "DATA_PATHS = {\n",
    "    'raw_data': 'data/raw/telco_customer_churn.csv',\n",
    "    'processed_train': 'data/processed/train_clean.csv',\n",
    "    'processed_test': 'data/processed/test_clean.csv'\n",
    "}\n",
    "\n",
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    'baseline_model': 'models/baseline_pipeline.joblib',\n",
    "    'model_metadata': 'models/model_metadata.json'\n",
    "}\n",
    "\n",
    "# Model parameters\n",
    "MODEL_CONFIG = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'logistic_regression': {\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "# Feature definitions\n",
    "FEATURE_CONFIG = {\n",
    "    'numeric_features': ['tenure', 'MonthlyCharges', 'TotalCharges', 'SeniorCitizen'],\n",
    "    'categorical_features': [\n",
    "        'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
    "        'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "        'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',\n",
    "        'PaperlessBilling', 'PaymentMethod'\n",
    "    ],\n",
    "    'target_column': 'Churn',\n",
    "    'id_column': 'customerID'\n",
    "}\n",
    "\n",
    "# Validation thresholds\n",
    "VALIDATION_CONFIG = {\n",
    "    'min_roc_auc': 0.70,  # Minimum acceptable ROC-AUC\n",
    "    'min_pr_auc': 0.50,   # Minimum acceptable PR-AUC\n",
    "    'max_train_test_diff': 0.05  # Max difference in performance between train/test\n",
    "}\n",
    "```\n",
    "\n",
    "### **`04_baseline_model.ipynb` Structure**\n",
    "```python\n",
    "# Cell 1: Setup and Imports\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "# Now import from your package\n",
    "from telco_analysis.data_utils import load_raw_data, validate_data_schema, save_processed_data\n",
    "from telco_analysis.preprocessing import clean_total_charges, prepare_features_target, create_preprocessing_pipeline, split_data\n",
    "from telco_analysis.model_utils import create_baseline_model, train_model, evaluate_model, save_model_artifacts\n",
    "from telco_analysis.config import DATA_PATHS, MODEL_PATHS, MODEL_CONFIG, FEATURE_CONFIG\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cell 2: Load and Prepare Data\n",
    "df = load_raw_data(DATA_PATHS['raw_data'])\n",
    "validate_data_schema(df)\n",
    "df_clean = clean_total_charges(df)\n",
    "\n",
    "# Cell 3: Feature Engineering and Splitting\n",
    "X, y, numeric_features, categorical_features = prepare_features_target(df_clean)\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, **MODEL_CONFIG)\n",
    "\n",
    "# Cell 4: Create and Train Model\n",
    "preprocessor = create_preprocessing_pipeline(numeric_features, categorical_features)\n",
    "model = create_baseline_model(preprocessor, MODEL_CONFIG['random_state'])\n",
    "model = train_model(model, X_train, y_train)\n",
    "\n",
    "# Cell 5: Evaluate and Save\n",
    "results = evaluate_model(model, X_test, y_test)\n",
    "save_model_artifacts(model, results, MODEL_PATHS['baseline_model'], MODEL_PATHS['model_metadata'])\n",
    "\n",
    "# Cell 6: Display Results\n",
    "print(f\"ROC-AUC Score: {results['roc_auc']:.3f}\")\n",
    "print(f\"PR-AUC Score: {results['pr_auc']:.3f}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
