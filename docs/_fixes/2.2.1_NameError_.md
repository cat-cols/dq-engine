># Q: How should I fix the UserWarning and anything else that needs a fix?

```py
# 2.2.1 üß¨ Auto-Detect Data Types (Column Type Map)
print("\n2.2.1 üß¨ Auto-detect data types")

# Basic guards
assert "df" in globals(), "‚ùå df is not defined. Run Section 1 & 2.1 first."
assert "REPORTS_DIR" in globals(), "‚ùå REPORTS_DIR missing."
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing (2.0.1)."

#
SEC2_REPORTS_DIR   = REPORTS_DIR   / "section2"
SEC2_ARTIFACTS_DIR = ARTIFACTS_DIR / "section2"
SEC2_REPORTS_DIR.mkdir(parents=True, exist_ok=True)
SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

#
TYPE_DET_DIR = SEC2_REPORTS_DIR / "type_detection"
TYPE_DET_DIR.mkdir(parents=True, exist_ok=True)

n_rows, n_cols = df.shape
run_ts = datetime.now(timezone.utc).isoformat(timespec="seconds")

# --- Config-backed knobs with safe fallbacks
try:
    id_cols = set(C("ID_COLUMNS", []) or [])
except Exception:
    id_cols = set()

try:
    target_name = C("TARGET.COLUMN")
except Exception:
    target_name = None

try:
    numeric_pattern = C("TYPE_DETECTION.NUMERIC_REGEX", r'^[\+\-]?\d+(\.\d+)?$')
except Exception:
    numeric_pattern = r'^[\+\-]?\d+(\.\d+)?$'

try:
    numeric_threshold = float(C("TYPE_DETECTION.NUMERIC_THRESHOLD", 0.95))
except Exception:
    numeric_threshold = 0.95

try:
    bool_true_cfg = C("TYPE_DETECTION.BOOLEAN_TRUE_VALUES", ["true","t","yes","y","1"])
except Exception:
    bool_true_cfg = ["true","t","yes","y","1"]

try:
    bool_false_cfg = C("TYPE_DETECTION.BOOLEAN_FALSE_VALUES", ["false","f","no","n","0"])
except Exception:
    bool_false_cfg = ["false","f","no","n","0"]

bool_true_vals  = set(str(v).strip().lower() for v in bool_true_cfg)
bool_false_vals = set(str(v).strip().lower() for v in bool_false_cfg)

try:
    boolean_threshold = float(C("TYPE_DETECTION.BOOLEAN_THRESHOLD", 0.95))
except Exception:
    boolean_threshold = 0.95

try:
    datetime_sample_size = int(C("TYPE_DETECTION.DATETIME_SAMPLE_SIZE", 500))
except Exception:
    datetime_sample_size = 500

try:
    datetime_threshold = float(C("TYPE_DETECTION.DATETIME_THRESHOLD", 0.8))
except Exception:
    datetime_threshold = 0.8

# Try to incorporate structural info from 2.1.7 if available
feature_roles_map = {}
feature_group_map = {}
if "feature_roles_df" in globals():
    for _, r in feature_roles_df.iterrows():
        col_ = r["column"]
        feature_roles_map[col_] = str(r.get("role", ""))
        feature_group_map[col_] = str(r.get("feature_group", ""))

# Protected columns snapshot (optional)
if "protected_columns" in globals():
    protected_cols = set(protected_columns)
else:
    protected_cols = set()

rows_221 = []

for col in df.columns:
    s = df[col]
    dtype_str = str(s.dtype)
    dtype_lower = dtype_str.lower()

    # --- base type group from pandas dtype -----------------------------------
    if ("int" in dtype_lower) or ("float" in dtype_lower) or ("complex" in dtype_lower):
        type_group_base = "numeric"
    elif "bool" in dtype_lower:
        type_group_base = "boolean"
    elif ("datetime" in dtype_lower) or ("date" in dtype_lower):
        type_group_base = "datetime"
    elif "category" in dtype_lower:
        type_group_base = "categorical"
    else:
        type_group_base = "string_like"

    non_null = int(s.notna().sum())
    nulls    = int(s.isna().sum())
    null_pct = round((nulls / n_rows) * 100.0, 3) if n_rows else 0.0
    n_unique = int(s.nunique(dropna=True))

    # sample values for human inspection
    sample_values = (
        s.dropna()
         .astype("string")
         .head(5)
         .tolist()
    )

    pct_numeric_like   = 0.0
    pct_boolean_like   = 0.0
    pct_datetime_like  = 0.0
    numeric_like_flag  = False
    boolean_like_flag  = False
    datetime_like_flag = False

    if type_group_base == "string_like":
        s_str = s.astype("string").str.strip()
        non_empty_mask = (s_str != "")
        non_empty_count = int(non_empty_mask.sum())

        if non_empty_count > 0:
            # numeric-like
            is_numeric_like = s_str.str.match(numeric_pattern, na=False)
            pct_numeric_like = float((is_numeric_like & non_empty_mask).sum()) / non_empty_count
            numeric_like_flag = pct_numeric_like >= numeric_threshold

            # boolean-like
            norm = s_str[non_empty_mask].str.lower()
            valid_bool = norm.isin(bool_true_vals | bool_false_vals)
            pct_boolean_like = float(valid_bool.sum()) / non_empty_count
            boolean_like_flag = pct_boolean_like >= boolean_threshold

            # OLD
            # # datetime-like (sampled)
            # sample_dt = s_str[non_empty_mask].dropna().head(datetime_sample_size)
            # if not sample_dt.empty:
            #     parsed = pd.to_datetime(sample_dt, errors="coerce")
            #     pct_datetime_like = float(parsed.notna().sum()) / len(sample_dt)
            #     datetime_like_flag = pct_datetime_like >= datetime_threshold

            # datetime-like (sampled)
            sample_dt = s_str[non_empty_mask].dropna().head(datetime_sample_size)
            if datetime_enabled and not sample_dt.empty:
                with warnings.catch_warnings():
                    # Silence "Could not infer format" noise for generic detection
                    warnings.filterwarnings(
                        "ignore",
                        message="Could not infer format.*",
                        category=UserWarning,
                    )

            parsed = pd.to_datetime(
                sample_dt,
                errors="coerce",
                format=datetime_format,   # None by default; or set in CONFIG
            )

    pct_datetime_like = float(parsed.notna().sum()) / len(sample_dt)
    datetime_like_flag = pct_datetime_like >= datetime_threshold

    # --- inferred type group + semantic type ---------------------------------
    type_group_inferred = type_group_base
    if type_group_base == "string_like":
        # precedence: boolean -> numeric -> datetime -> categorical
        if boolean_like_flag:
            type_group_inferred = "boolean"
        elif numeric_like_flag:
            type_group_inferred = "numeric"
        elif datetime_like_flag:
            type_group_inferred = "datetime"
        else:
            type_group_inferred = "categorical"

    # semantic_type for downstream coercion & semantics
    if type_group_base == "string_like" and numeric_like_flag and type_group_inferred == "numeric":
        semantic_type = "numeric_like_string"
    elif type_group_base == "string_like" and datetime_like_flag and type_group_inferred == "datetime":
        semantic_type = "datetime_like_string"
    elif type_group_base == "string_like" and boolean_like_flag and type_group_inferred == "boolean":
        semantic_type = "boolean_like_string"
    else:
        semantic_type = type_group_inferred

    is_id_col     = col in id_cols
    is_target_col = (col == target_name)

    role_val = feature_roles_map.get(col, "")
    feature_group_val = feature_group_map.get(col, "")
    is_protected = col in protected_cols

    rows_221.append(
        {
            "column":               col,
            "pandas_dtype":         dtype_str,
            "type_group_base":      type_group_base,
            "type_group_inferred":  type_group_inferred,
            "semantic_type":        semantic_type,
            "non_null":             non_null,
            "nulls":                nulls,
            "null_pct":             null_pct,
            "n_unique":             n_unique,
            "pct_numeric_like":     round(pct_numeric_like, 4),
            "numeric_like_flag":    numeric_like_flag,
            "pct_boolean_like":     round(pct_boolean_like, 4),
            "boolean_like_flag":    boolean_like_flag,
            "pct_datetime_like":    round(pct_datetime_like, 4),
            "datetime_like_flag":   datetime_like_flag,
            "sample_values":        json.dumps(sample_values),
            "is_id":                is_id_col,
            "is_target":            is_target_col,
            "role":                 role_val,
            "feature_group":        feature_group_val,
            "is_protected":         is_protected,
            "run_ts":               run_ts,
            "n_rows":               n_rows,
            "n_cols":               n_cols,
        }
    )

type_det_df = (
    pd.DataFrame(rows_221)
    .sort_values(["type_group_inferred", "column"])
    .reset_index(drop=True)
)

print("\nüìä 2.2.1 type detection summary (head):")
display(
    type_det_df[
        [
            "column",
            "pandas_dtype",
            "type_group_base",
            "type_group_inferred",
            "semantic_type",
            "n_unique",
            "null_pct",
            "pct_numeric_like",
            "pct_boolean_like",
            "pct_datetime_like",
        ]
    ].head(20)
)

# Write CSV summary
type_summary_path = TYPE_DET_DIR / "type_detection_summary.csv"
tmp_csv = type_summary_path.with_suffix(".tmp.csv")
type_det_df.to_csv(tmp_csv, index=False)
os.replace(tmp_csv, type_summary_path)
print(f"üíæ Wrote type detection summary ‚Üí {type_summary_path}")

# Write JSON column type map
column_type_map = {}
for _, r in type_det_df.iterrows():
    col = r["column"]
    column_type_map[col] = {
        "raw_dtype":         r["pandas_dtype"],
        "type_group":        r["type_group_inferred"],
        "semantic_type":     r["semantic_type"],
        "role":              r.get("role", ""),
        "feature_group":     r.get("feature_group", ""),
        "is_protected":      bool(r.get("is_protected", False)),
        "is_id":             bool(r.get("is_id", False)),
        "is_target":         bool(r.get("is_target", False)),
        "hints": {
            "n_unique":          int(r["n_unique"]),
            "null_pct":          float(r["null_pct"]),
            "pct_numeric_like":  float(r["pct_numeric_like"]),
            "pct_boolean_like":  float(r["pct_boolean_like"]),
            "pct_datetime_like": float(r["pct_datetime_like"]),
        },
    }

type_map_path = TYPE_DET_DIR / "column_type_map.json"
with open(type_map_path, "w", encoding="utf-8") as f:
    json.dump(column_type_map, f, indent=2)

print(f"üíæ Wrote column type map ‚Üí {type_map_path}")

# Append unified diagnostics row (2.2.1)
summary_221 = {
    "section":            "2.2.1",
    "section_name":       "Auto-detect data types",
    "check":              "Column type detection summary & type map artifact",
    "level":              "info",
    "status":             "OK",
    "n_columns":          n_cols,
    "n_numeric":          int((type_det_df["type_group_inferred"] == "numeric").sum()),
    "n_categorical":      int((type_det_df["type_group_inferred"] == "categorical").sum()),
    "n_boolean":          int((type_det_df["type_group_inferred"] == "boolean").sum()),
    "n_datetime":         int((type_det_df["type_group_inferred"] == "datetime").sum()),
    "timestamp":          pd.Timestamp.utcnow(),
    "detail":             "Type map ‚Üí column_type_map.json; summary ‚Üí type_detection_summary.csv",
}

if "_append_sec2" in globals():
    _append_sec2(pd.DataFrame([summary_221]))
else:
    path = SECTION2_REPORT_PATH
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    chunk = pd.DataFrame([summary_221])
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        if path.exists():
            ex = pd.read_csv(path)
            allc = pd.Index(ex.columns).union(chunk.columns)
            out = pd.concat(
                [ex.reindex(columns=allc), chunk.reindex(columns=allc)],
                ignore_index=True,
            )
        else:
            out = chunk
        out.to_csv(tmp_path, index=False)
        os.replace(tmp_path, path)
        print(f"üßæ Appended 2.2.1 summary ‚Üí {path}")
    except Exception as e:
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        print(f"‚ö†Ô∏è Could not append 2.2.1 summary: {e}")

print("‚úÖ 2.2.1 complete.")

# OUTPUT
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[451], line 152
    142 # OLD
    143 # # datetime-like (sampled)
    144 # sample_dt = s_str[non_empty_mask].dropna().head(datetime_sample_size)
   (...)    149
    150 # datetime-like (sampled)
    151 sample_dt = s_str[non_empty_mask].dropna().head(datetime_sample_size)
--> 152 if datetime_enabled and not sample_dt.empty:
    153     with warnings.catch_warnings():
    154         # Silence "Could not infer format" noise for generic detection
    155         warnings.filterwarnings(
    156             "ignore",
    157             message="Could not infer format.*",
    158             category=UserWarning,
    159         )

NameError: name 'datetime_enabled' is not defined
```

># SOLUTION:
Ah yep, that traceback is exactly what we‚Äôd expect from the half-patched version:

* `datetime_enabled` was never defined
* and the datetime block indentation is a bit off, so `parsed` / `sample_dt` are referenced in the wrong scope

Let‚Äôs fix this cleanly.

---

## 1Ô∏è‚É£ Add the missing config knobs (defines `datetime_enabled`, `datetime_format`)

Right after your existing `datetime_threshold` block:

```python
try:
    datetime_threshold = float(C("TYPE_DETECTION.DATETIME_THRESHOLD", 0.8))
except Exception:
    datetime_threshold = 0.8
```

add **these two blocks**:

```python
# NEW: control whether datetime detection runs at all
try:
    datetime_enabled = bool(C("TYPE_DETECTION.DATETIME_ENABLED", True))
except Exception:
    datetime_enabled = True

# NEW: optional explicit datetime format
try:
    datetime_format = C("TYPE_DETECTION.DATETIME_FORMAT", None)
except Exception:
    datetime_format = None
```

Also make sure at the very top of the notebook (with your imports) you have:

```python
import warnings
```

---

## 2Ô∏è‚É£ Fix the datetime block indentation

Right now, your new code looks like this (problematic part):

```python
            # datetime-like (sampled)
            sample_dt = s_str[non_empty_mask].dropna().head(datetime_sample_size)
            if datetime_enabled and not sample_dt.empty:
                with warnings.catch_warnings():
                    # Silence "Could not infer format" noise for generic detection
                    warnings.filterwarnings(
                        "ignore",
                        message="Could not infer format.*",
                        category=UserWarning,
                    )

            parsed = pd.to_datetime(
                sample_dt,
                errors="coerce",
                format=datetime_format,   # None by default; or set in CONFIG
            )

    pct_datetime_like = float(parsed.notna().sum()) / len(sample_dt)
    datetime_like_flag = pct_datetime_like >= datetime_threshold
```

Notice how:

* `parsed = ...` is **outside** the `if datetime_enabled` block
* `pct_datetime_like = ...` and `datetime_like_flag = ...` are even further dedented, outside the `if type_group_base == "string_like"` block

We want *all* of that work to live inside the `if non_empty_count > 0:` block, under `if type_group_base == "string_like":`.

### Replace your datetime section with this

Inside:

```python
if type_group_base == "string_like":
    s_str = s.astype("string").str.strip()
    non_empty_mask = (s_str != "")
    non_empty_count = int(non_empty_mask.sum())

    if non_empty_count > 0:
        # numeric-like
        ...
        # boolean-like
        ...
        # <<< put the datetime block here >>>
```

Use this **exact block**:

```python
        # datetime-like (sampled)
        sample_dt = s_str[non_empty_mask].dropna().head(datetime_sample_size)
        if datetime_enabled and not sample_dt.empty:
            with warnings.catch_warnings():
                # Silence "Could not infer format" noise for generic detection
                warnings.filterwarnings(
                    "ignore",
                    message="Could not infer format.*",
                    category=UserWarning,
                )

                parsed = pd.to_datetime(
                    sample_dt,
                    errors="coerce",
                    format=datetime_format,   # None by default; or set in CONFIG
                )

            pct_datetime_like = float(parsed.notna().sum()) / len(sample_dt)
            datetime_like_flag = pct_datetime_like >= datetime_threshold
```

And **remove** these lines from after the `if`:

```python
    pct_datetime_like = float(parsed.notna().sum()) / len(sample_dt)
    datetime_like_flag = pct_datetime_like >= datetime_threshold
```

You already initialize:

```python
pct_datetime_like   = 0.0
datetime_like_flag  = False
```

at the top of the loop, so if datetime is disabled or sample is empty, it just stays at 0 / False. That‚Äôs exactly what we want.

---

## 3Ô∏è‚É£ (Optional but nice) ‚Äì turn datetime detection off for Telco

Since Telco has no real datetime columns anyway, you can quiet this whole branch via config.

In `project_config.yaml` add:

```yaml
TYPE_DETECTION:
  DATETIME_ENABLED: false
  DATETIME_FORMAT: null
```

Then `datetime_enabled` will be `False` and that whole datetime probe is skipped (no warnings, no extra work).

---

After these three tweaks:

* ‚úÖ No more `NameError: datetime_enabled`
* ‚úÖ No more `UserWarning: Could not infer format...`
* ‚úÖ Datetime detection still available later if you add real timestamp columns and flip `DATETIME_ENABLED` to `true` or set a `DATETIME_FORMAT`.
