># Q: Why is it not finding the baseline numeric profile?

**OUTPUT:**
2.3.16 üìú Data contracts & threshold enforcement
>/var/folders/dn/wtcqyjbd1hn63s40771sdddm0000gn/T/ipykernel_74374/1866560971.py:336:

> FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.

```py
contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

üíæ Wrote contract violations JSON ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section2/numeric_integrity/data_contract_violations.json

üíæ Wrote contract violations CSV ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section2/numeric_integrity/data_contract_violations.csv

üìä 2.3.16 data_contract_violations (head):

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>scope</th>
      <th>severity</th>
      <th>target</th>
      <th>op</th>
      <th>threshold</th>
      <th>value</th>
      <th>min_fraction</th>
      <th>max_fraction</th>
      <th>n_subjects</th>
      <th>n_violations</th>
      <th>pct_violations</th>
      <th>contract_status</th>
      <th>reason</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>no_high_null_for_key_features</td>
      <td>numeric_profile</td>
      <td>hard</td>
      <td>null_pct</td>
      <td>&lt;=</td>
      <td>5.0</td>
      <td>None</td>
      <td>None</td>
      <td>NaN</td>
      <td>5</td>
      <td>0</td>
      <td>0.0</td>
      <td>OK</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>non_negative_readiness</td>
      <td>readiness</td>
      <td>soft</td>
      <td>readiness_score</td>
      <td>&gt;=</td>
      <td>0.0</td>
      <td>None</td>
      <td>None</td>
      <td>NaN</td>
      <td>5</td>
      <td>0</td>
      <td>0.0</td>
      <td>OK</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>limit_high_drift_low_readiness</td>
      <td>drift</td>
      <td>hard</td>
      <td>high_drift_low_readiness</td>
      <td>fraction_eq</td>
      <td>NaN</td>
      <td>True</td>
      <td>None</td>
      <td>0.05</td>
      <td>5</td>
      <td>0</td>
      <td>0.0</td>
      <td>OK</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>no_hard_contract_failures</td>
      <td>contracts</td>
      <td>hard</td>
      <td>hard_contract_failures</td>
      <td>==</td>
      <td>NaN</td>
      <td>0</td>
      <td>None</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
      <td>0.0</td>
      <td>OK</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div>

üßæ Appended diagnostics ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section2/section2_data_quality_20251211_143117.csv

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>section</th>
      <th>section_name</th>
      <th>check</th>
      <th>level</th>
      <th>status</th>
      <th>n_contracts</th>
      <th>n_contracts_fail_hard</th>
      <th>n_contracts_warn_soft</th>
      <th>detail</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.3.16</td>
      <td>Data contracts &amp; threshold enforcement</td>
      <td>Evaluate configured data contracts against num...</td>
      <td>info</td>
      <td>OK</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>data_contract_violations.json</td>
      <td>2025-12-11 22:31:26.383319+00:00</td>
    </tr>
  </tbody>
</table>
</div>

How can we make this better? Should I handle the FutureWarning?
```py
# 2.3.16 | Data Drift & Monitoring Hooks
print("\n2.3.16 üõ∞Ô∏è Data drift & monitoring hooks")

# üí°üí° Resolve core paths for drift artifacts ‚Äî ties to 2.3.6 (numeric_profile_df.csv) & 2.3.8 (model_readiness_report.csv)
numeric_profile_path_2314 = NUMERIC_DIR / "numeric_profile_df.csv"
model_readiness_path_2314 = NUMERIC_DIR / "model_readiness_report.csv"

if not numeric_profile_path_2314.exists():
    print(f"‚ö†Ô∏è {numeric_profile_path_2314} missing ‚Äî cannot compute drift metrics.")
    data_drift_df_2314 = pd.DataFrame()
else:
    numeric_profile_df_curr_2314 = pd.read_csv(numeric_profile_path_2314)

2.3.16 üõ∞Ô∏è Data drift & monitoring hooks
‚ö†Ô∏è No baseline numeric profile found ‚Äî drift metrics will be empty.
üíæ Wrote data drift metrics ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section2/numeric_integrity/data_drift_metrics.csv

üìä 2.3.14 data_drift_metrics (head):
   (no drift metrics computed)
‚ÑπÔ∏è Wrote stub dashboard drift payload ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section2/numeric_integrity/dashboard_updates.json
üßæ Appended diagnostics ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section2/section2_data_quality_20251211_131144.csv
    # üí°üí° Normalize feature key (use 'column' as canonical; also create 'feature' alias for joins with 2.3.8)
    if "column" not in numeric_profile_df_curr_2314.columns and "feature" in numeric_profile_df_curr_2314.columns:
        numeric_profile_df_curr_2314["column"] = numeric_profile_df_curr_2314["feature"].astype("string")
    if "feature" not in numeric_profile_df_curr_2314.columns and "column" in numeric_profile_df_curr_2314.columns:
        numeric_profile_df_curr_2314["feature"] = numeric_profile_df_curr_2314["column"].astype("string")

    # üí°üí° Locate baseline numeric profile ‚Äî either from CONFIG['DRIFT.BASELINE_NUMERIC_PROFILE'] or common fallback filenames
    baseline_path_2314 = None
    baseline_from_config_2314 = None
    if "C" in globals() and callable(C):
        baseline_from_config_2314 = C("DRIFT.BASELINE_NUMERIC_PROFILE", None)
    if baseline_from_config_2314:
        baseline_candidate_2314 = Path(str(baseline_from_config_2314))
        if not baseline_candidate_2314.is_absolute() and "PROJECT_ROOT" in globals():
            baseline_candidate_2314 = PROJECT_ROOT / baseline_candidate_2314
        if baseline_candidate_2314.exists():
            baseline_path_2314 = baseline_candidate_2314

    if baseline_path_2314 is None:
        # üí°üí° Fallback candidates ‚Äî must stay in sync with 2.3.14.0 seeding
        candidates_2314 = [
            NUMERIC_DIR / "numeric_profile_baseline.csv",   # ‚Üê from 2.3.14.0
            NUMERIC_DIR / "numeric_profile_df_baseline.csv",
            NUMERIC_DIR / "numeric_profile_df_prev.csv",
        ]
        for p in candidates_2314:
            if p.exists():
                baseline_path_2314 = p
                break
    #
    if baseline_path_2314 is None:
        print("‚ö†Ô∏è No baseline numeric profile found ‚Äî drift metrics will be empty.")
        data_drift_df_2314 = pd.DataFrame()
    else:
        print(f"‚ÑπÔ∏è Using baseline numeric profile ‚Üí {baseline_path_2314}")
        numeric_profile_df_base_2314 = pd.read_csv(baseline_path_2314)

        # üí°üí° Normalize baseline key columns to match current (2.3.6 output)
        if "column" not in numeric_profile_df_base_2314.columns and "feature" in numeric_profile_df_base_2314.columns:
            numeric_profile_df_base_2314["column"] = numeric_profile_df_base_2314["feature"].astype("string")
        if "feature" not in numeric_profile_df_base_2314.columns and "column" in numeric_profile_df_base_2314.columns:
            numeric_profile_df_base_2314["feature"] = numeric_profile_df_base_2314["column"].astype("string")

        # üí°üí° Determine monitored features ‚Äî intersection of current & baseline, optionally filtered by DRIFT.MONITORED_FEATURES
        curr_cols_2314 = set(numeric_profile_df_curr_2314["column"].astype("string"))
        base_cols_2314 = set(numeric_profile_df_base_2314["column"].astype("string"))
        intersect_cols_2314 = sorted(curr_cols_2314.intersection(base_cols_2314))

        monitored_features_cfg_2314 = None
        if "C" in globals() and callable(C):
            monitored_features_cfg_2314 = C("DRIFT.MONITORED_FEATURES", None)

        if monitored_features_cfg_2314:
            monitored_set_2314 = set(map(str, monitored_features_cfg_2314))
            monitored_features_2314 = [c for c in intersect_cols_2314 if c in monitored_set_2314]
        else:
            monitored_features_2314 = intersect_cols_2314

        if not monitored_features_2314:
            print("‚ö†Ô∏è No overlapping monitored features between current and baseline numeric profiles.")
            data_drift_df_2314 = pd.DataFrame()
        else:
            # üí°üí° Prepare aligned current & baseline slices keyed by column ‚Äî we rely on null_pct for PSI/KS (2-bucket drift: null vs non-null)
            curr_idx_2314 = (
                numeric_profile_df_curr_2314
                .set_index("column")
                .reindex(monitored_features_2314)
            )
            base_idx_2314 = (
                numeric_profile_df_base_2314
                .set_index("column")
                .reindex(monitored_features_2314)
            )

            # üí°üí° Extract null percentages for both runs ‚Äî uses 2.3.6 unified profile (null_pct or null_pct_221)
            curr_null_col_2314 = "null_pct" if "null_pct" in curr_idx_2314.columns else "null_pct_221"
            base_null_col_2314 = "null_pct" if "null_pct" in base_idx_2314.columns else "null_pct_221"

            curr_pnull_2314 = curr_idx_2314[curr_null_col_2314].fillna(0.0) / 100.0
            base_pnull_2314 = base_idx_2314[base_null_col_2314].fillna(0.0) / 100.0

            # üí°üí° Two-bucket distribution: {null, non-null} ‚Äî valid PSI & KS using null vs non-null mix
            eps_2314 = 1e-6
            curr_pnull_2314_clipped = curr_pnull_2314.clip(eps_2314, 1 - eps_2314)
            base_pnull_2314_clipped = base_pnull_2314.clip(eps_2314, 1 - eps_2314)

            curr_pnon_2314 = 1.0 - curr_pnull_2314_clipped
            base_pnon_2314 = 1.0 - base_pnull_2314_clipped

            # üí°üí° PSI over null/non-null buckets per feature ‚Äî textbook PSI formula
            psi_null_2314 = (curr_pnull_2314_clipped - base_pnull_2314_clipped) * np.log(
                curr_pnull_2314_clipped / base_pnull_2314_clipped
            )
            psi_non_2314 = (curr_pnon_2314 - base_pnon_2314) * np.log(
                curr_pnon_2314 / base_pnon_2314
            )
            psi_total_2314 = psi_null_2314 + psi_non_2314

            # üí°üí° KS statistic for 2-bucket case: max diff in CDFs = |p_null_current - p_null_baseline|
            ks_stat_2314 = (curr_pnull_2314_clipped - base_pnull_2314_clipped).abs()

            # üí°üí° Mean/std deltas if present (hook to 2.3.4 numeric_metrics_enhanced)
            delta_mean_2314 = None
            delta_std_2314 = None
            if "mean" in curr_idx_2314.columns and "mean" in base_idx_2314.columns:
                delta_mean_2314 = curr_idx_2314["mean"] - base_idx_2314["mean"]
            if "std" in curr_idx_2314.columns and "std" in base_idx_2314.columns:
                delta_std_2314 = curr_idx_2314["std"] - base_idx_2314["std"]

            delta_null_pct_2314 = (curr_pnull_2314 - base_pnull_2314) * 100.0

            # üí°üí° Drift thresholds from CONFIG ‚Äî connects to DRIFT.PSI_WARN/FAIL & DRIFT.KS_WARN/FAIL
            if "C" in globals() and callable(C):
                psi_warn_2314 = float(C("DRIFT.PSI_WARN", 0.10))
                psi_fail_2314 = float(C("DRIFT.PSI_FAIL", 0.25))
                ks_warn_2314 = float(C("DRIFT.KS_WARN", 0.10))
                ks_fail_2314 = float(C("DRIFT.KS_FAIL", 0.20))
            else:
                psi_warn_2314 = 0.10
                psi_fail_2314 = 0.25
                ks_warn_2314 = 0.10
                ks_fail_2314 = 0.20

            # üí°üí° Assemble per-feature drift frame
            data_drift_df_2314 = pd.DataFrame(
                {
                    "feature": monitored_features_2314,
                    "psi": psi_total_2314.values,
                    "ks_stat": ks_stat_2314.values,
                    "delta_null_pct": delta_null_pct_2314.values,
                }
            )

            if delta_mean_2314 is not None:
                data_drift_df_2314["delta_mean"] = delta_mean_2314.values
            else:
                data_drift_df_2314["delta_mean"] = np.nan

            if delta_std_2314 is not None:
                data_drift_df_2314["delta_std"] = delta_std_2314.values
            else:
                data_drift_df_2314["delta_std"] = np.nan

            # üí°üí° Drift severity buckets ‚Äî standard "none/low/medium/high" using PSI + KS thresholds
            def _assign_severity_row_2314(row):
                psi_val = row["psi"]
                ks_val = row["ks_stat"]
                if np.isnan(psi_val) or np.isnan(ks_val):
                    return "none"
                if psi_val >= psi_fail_2314 or ks_val >= ks_fail_2314:
                    return "high"
                if psi_val >= psi_warn_2314 or ks_val >= ks_warn_2314:
                    return "medium"
                if psi_val > 0:
                    return "low"
                return "none"

            data_drift_df_2314["drift_severity"] = data_drift_df_2314.apply(_assign_severity_row_2314, axis=1)

# üí°üí° Integrate drift with model readiness (2.3.8) to flag high-drift / low-readiness risks
if not data_drift_df_2314.empty and model_readiness_path_2314.exists():
    model_ready_df_2314 = pd.read_csv(model_readiness_path_2314)

    if "feature" not in model_ready_df_2314.columns and "column" in model_ready_df_2314.columns:
        model_ready_df_2314["feature"] = model_ready_df_2314["column"].astype("string")

    data_drift_df_2314 = data_drift_df_2314.merge(
        model_ready_df_2314[["feature", "readiness_score"]] if "readiness_score" in model_ready_df_2314.columns else model_ready_df_2314[["feature"]],
        on="feature",
        how="left",
    )

    if "readiness_score" not in data_drift_df_2314.columns:
        data_drift_df_2314["readiness_score"] = np.nan
else:
    if not data_drift_df_2314.empty:
        data_drift_df_2314["readiness_score"] = np.nan

# üí°üí° high_drift_low_readiness flag ‚Äî core signal used in Data Contracts (DRIFT + READINESS combo)
if not data_drift_df_2314.empty:
    data_drift_df_2314["high_drift_low_readiness"] = (
        data_drift_df_2314["drift_severity"].isin(["medium", "high"])
        & (data_drift_df_2314["readiness_score"] < 0.7)
    )

# üí°üí° Write data_drift_metrics.csv (primary drift artifact for 2.3.14 & contracts)
data_drift_path_2314 = NUMERIC_DIR / "data_drift_metrics.csv"
tmp_2314 = data_drift_path_2314.with_suffix(".tmp.csv")
data_drift_df_2314.to_csv(tmp_2314, index=False)
os.replace(tmp_2314, data_drift_path_2314)
print(f"üíæ Wrote data drift metrics ‚Üí {data_drift_path_2314}")

print("\nüìä 2.3.14 data_drift_metrics (head):")
if not data_drift_df_2314.empty:
    display(data_drift_df_2314.head(20))
else:
    print("   (no drift metrics computed)")

# üí°üí° Build dashboard_updates.json ‚Äî compact payload for BI/alerting hooks
dashboard_updates_path_2314 = NUMERIC_DIR / "dashboard_updates.json"
dashboard_summary_2314 = {}

if not data_drift_df_2314.empty:
    severity_counts_2314 = data_drift_df_2314["drift_severity"].value_counts().to_dict()
    max_psi_2314 = float(data_drift_df_2314["psi"].max())
    max_ks_2314 = float(data_drift_df_2314["ks_stat"].max())
    n_features_mon_2314 = int(data_drift_df_2314.shape[0])

    # top drifted features by PSI
    top_n_2314 = min(10, n_features_mon_2314)
    top_drift_2314 = (
        data_drift_df_2314.sort_values("psi", ascending=False)
        .head(top_n_2314)[["feature", "psi", "ks_stat", "drift_severity", "readiness_score", "high_drift_low_readiness"]]
        .to_dict(orient="records")
    )

    # optional: pull run metadata from numeric_audit_metadata.json if present (2.3.10)
    numeric_audit_path_2314 = NUMERIC_DIR / "numeric_audit_metadata.json"
    run_meta_2314 = {}
    if numeric_audit_path_2314.exists():
        try:
            with open(numeric_audit_path_2314, "r", encoding="utf-8") as f:
                run_meta_2314 = json.load(f)
        except Exception:
            run_meta_2314 = {}

    dashboard_summary_2314 = {
        "timestamp_utc": pd.Timestamp.utcnow().isoformat(),
        "run_metadata": run_meta_2314,
        "counts_by_severity": severity_counts_2314,
        "n_features_monitored": n_features_mon_2314,
        "max_psi": max_psi_2314,
        "max_ks_stat": max_ks_2314,
        "top_drift_features": top_drift_2314,
    }

    with open(dashboard_updates_path_2314, "w", encoding="utf-8") as f:
        json.dump(dashboard_summary_2314, f, indent=2, default=str)
    print(f"üíæ Wrote dashboard drift payload ‚Üí {dashboard_updates_path_2314}")
else:
    # still emit a tiny stub so downstream systems don't break on missing file
    stub_payload_2314 = {
        "timestamp_utc": pd.Timestamp.utcnow().isoformat(),
        "message": "No drift metrics computed for this run.",
        "counts_by_severity": {},
    }
    with open(dashboard_updates_path_2314, "w", encoding="utf-8") as f:
        json.dump(stub_payload_2314, f, indent=2)
    print(f"‚ÑπÔ∏è Wrote stub dashboard drift payload ‚Üí {dashboard_updates_path_2314}")

# üí°üí° Unified diagnostics row for 2.3.14 ‚Äî appended to Section 2 master report
if not data_drift_df_2314.empty:
    n_features_monitored_2314 = int(data_drift_df_2314.shape[0])
    n_drift_medium_2314 = int((data_drift_df_2314["drift_severity"] == "medium").sum())
    n_drift_high_2314 = int((data_drift_df_2314["drift_severity"] == "high").sum())
    max_psi_val_2314 = float(data_drift_df_2314["psi"].max())
else:
    n_features_monitored_2314 = 0
    n_drift_medium_2314 = 0
    n_drift_high_2314 = 0
    max_psi_val_2314 = 0.0

# derive status based on config thresholds
if "C" in globals() and callable(C):
    psi_warn_cfg_2314 = float(C("DRIFT.PSI_WARN", 0.10))
    psi_fail_cfg_2314 = float(C("DRIFT.PSI_FAIL", 0.25))
else:
    psi_warn_cfg_2314 = 0.10
    psi_fail_cfg_2314 = 0.25

if n_drift_high_2314 == 0 and max_psi_val_2314 < psi_warn_cfg_2314:
    status_2314 = "OK"
elif max_psi_val_2314 >= psi_fail_cfg_2314 or n_drift_high_2314 > 0:
    status_2314 = "FAIL"
elif n_drift_medium_2314 > 0 or max_psi_val_2314 >= psi_warn_cfg_2314:
    status_2314 = "WARN"
else:
    status_2314 = "OK"

summary_2316 = pd.DataFrame([{
    "section":             "2.3.16",
    "section_name":        "Data drift & monitoring hooks",
    "check":               "Compare numeric distributions vs baseline & emit monitoring artifacts",
    "level":               "info",
    "status":              status_2314,
    "n_features_monitored": int(n_features_monitored_2314),
    "n_drift_medium":      int(n_drift_medium_2314),
    "n_drift_high":        int(n_drift_high_2314),
    "max_psi":             float(max_psi_val_2314),
    "detail":              "data_drift_metrics.csv; dashboard_updates.json",
    "timestamp":           pd.Timestamp.utcnow(),
}])

append_sec2(summary_2316 , SECTION2_REPORT_PATH)
display(summary_2316)
```

># SOLUTION:

You‚Äôve basically already got a mini ‚Äúgovernance engine‚Äù here ‚Äî it just needs a tiny bit of hardening and tidying to feel truly production-grade.

I‚Äôll show you **what to improve conceptually** and then give you a **refined version of your cell** that stays very close to what you have now but is more robust and professional.

---

## 1. Make the CONTRACTS config a first-class, documented schema

Right now, `CONTRACTS` is ‚Äúwhatever fits‚Äù. I‚Äôd lock this down:

**Recommended shape (YAML-ish):**

```yaml
CONTRACTS:
  - name: "no_high_null_for_key_features"
    scope: "numeric_profile"
    severity: "hard"        # "hard" or "soft"
    target: "null_pct"
    op: "<="
    threshold: 5.0          # % null
    where:
      is_key_feature: True  # optional filters

  - name: "non_negative_readiness"
    scope: "readiness"
    severity: "soft"
    target: "readiness_score"
    op: ">="
    threshold: 0.0

  - name: "limit_high_drift_low_readiness"
    scope: "drift"
    severity: "hard"
    target: "high_drift_low_readiness"
    op: "fraction_eq"
    value: True
    max_fraction: 0.05       # at most 5% of monitored features

  - name: "no_hard_contract_failures"
    scope: "contracts"
    severity: "hard"
    target: "hard_contract_failures"
    op: "=="
    value: 0
```

üí°üí° **Implementation detail:** add a small **config validation step** before evaluation:

* Ensure `scope` is one of `{"numeric_integrity","numeric_profile","readiness","drift","contracts"}`.
* Ensure `op` is in a known set.
* Ensure required keys per `op` are present (`threshold`, `value`, `max_fraction`, etc.).
* If misconfigured: mark the contract as `SKIP` with a `reason` column instead of silently doing weird things.

---

## 2. Normalize & centralize small helpers

You already wrote `_apply_where_filter_2316`. I‚Äôd add:

* A **`_normalize_contracts_cfg`** helper (turn dict/None into list, unfold `rules` key).
* A **`_evaluate_single_contract`** helper that returns a dict with:

  * `n_subjects`, `n_violations`, `pct_violations`,
  * `contract_status`, and
  * a `reason` string when status is `SKIP`.

These helpers keep your big cell readable and will make debugging SO much easier.

---

## 3. Add a `reason` column for SKIP/misconfig cases

Right now, `contract_status = "SKIP"` has no explanation. Add:

* `reason` column, for example:

  * `"scope_frame_empty"`
  * `"target_column_missing"`
  * `"unsupported_op"`
  * `"no_subject_rows"`
  * `"config_invalid"`

You‚Äôll thank yourself when reading `data_contract_violations.csv` later.

---

## 4. A polished, robust version of your cell

Here‚Äôs a refactor that keeps your semantics but tightens everything up:

```python
# 2.3.16 | Data Contracts & Threshold Enforcement
print("\n2.3.16 üìú Data contracts & threshold enforcement")

# -------------------------------------------------------------------
# 0) Config resolution & normalization
# -------------------------------------------------------------------
def _normalize_contracts_cfg_2316(raw_cfg):
    """Normalize CONTRACTS config into a list of dict contracts."""
    if raw_cfg is None:
        return []
    if isinstance(raw_cfg, dict):
        # allow {"rules": [...]} or dict-of-rules
        if "rules" in raw_cfg and isinstance(raw_cfg["rules"], list):
            return raw_cfg["rules"]
        return list(raw_cfg.values())
    if isinstance(raw_cfg, list):
        return raw_cfg
    print("‚ö†Ô∏è CONTRACTS config is not a list/dict ‚Äî treating as no contracts.")
    return []

if "C" in globals() and callable(C):
    raw_contracts_cfg_2316 = C("CONTRACTS", [])
elif "CONFIG" in globals():
    raw_contracts_cfg_2316 = CONFIG.get("CONTRACTS", [])
else:
    raw_contracts_cfg_2316 = []

contracts_cfg_2316 = _normalize_contracts_cfg_2316(raw_contracts_cfg_2316)

if not contracts_cfg_2316:
    print("‚ÑπÔ∏è No contracts configured (CONTRACTS empty) ‚Äî 2.3.16 will emit stub artifacts.")

# -------------------------------------------------------------------
# 1) Load core artifacts from earlier sections
# -------------------------------------------------------------------
numeric_integrity_path_2316 = NUMERIC_DIR / "numeric_integrity_report.csv"
numeric_profile_path_2316   = NUMERIC_DIR / "numeric_profile_df.csv"
readiness_path_2316         = NUMERIC_DIR / "model_readiness_report.csv"
drift_path_2316             = NUMERIC_DIR / "data_drift_metrics.csv"

frames_2316 = {}

def _safe_read_csv_2316(path, label):
    if not path.exists():
        print(f"‚ö†Ô∏è {path} missing ‚Äî {label} contracts may be skipped.")
        return pd.DataFrame()
    try:
        if path.stat().st_size == 0:
            print(f"‚ö†Ô∏è {path} is empty ‚Äî {label} contracts will be skipped.")
            return pd.DataFrame()
        return pd.read_csv(path)
    except pd.errors.EmptyDataError:
        print(f"‚ö†Ô∏è {path} contained no parsable data ‚Äî {label} contracts will be skipped.")
        return pd.DataFrame()

frames_2316["numeric_integrity"] = _safe_read_csv_2316(numeric_integrity_path_2316, "numeric_integrity")
frames_2316["numeric_profile"]   = _safe_read_csv_2316(numeric_profile_path_2316, "numeric_profile")
frames_2316["readiness"]         = _safe_read_csv_2316(readiness_path_2316, "readiness")
frames_2316["drift"]             = _safe_read_csv_2316(drift_path_2316, "drift")

# normalize feature/column keys
for scope_name_2316, df_2316 in frames_2316.items():
    if not df_2316.empty:
        if "column" not in df_2316.columns and "feature" in df_2316.columns:
            df_2316["column"] = df_2316["feature"].astype("string")
        if "feature" not in df_2316.columns and "column" in df_2316.columns:
            df_2316["feature"] = df_2316["column"].astype("string")
    frames_2316[scope_name_2316] = df_2316

def _apply_where_filter_2316(df, where_dict):
    if not where_dict:
        return df
    subset = df.copy()
    for k, v in (where_dict or {}).items():
        if k in subset.columns:
            subset = subset[subset[k] == v]
    return subset

# -------------------------------------------------------------------
# 2) Evaluate non-meta contracts (scope != "contracts")
# -------------------------------------------------------------------
SUPPORTED_SCOPES_2316 = {"numeric_integrity", "numeric_profile", "readiness", "drift", "contracts"}
SUPPORTED_OPS_2316 = {
    "<", "<=", ">", ">=",
    "==", "!=",
    "fraction_eq", "fraction_ge", "fraction_lt",
    "not_any_in",
}

contract_rows_2316 = []

def _eval_single_contract_2316(contract, frames):
    """Return metrics dict for a single (non-meta) contract."""
    scope = contract.get("scope")
    name = contract.get("name", "")
    severity = str(contract.get("severity", "hard")).lower()
    target = contract.get("target")
    op = contract.get("op")
    where = contract.get("where", {})
    threshold = contract.get("threshold", None)
    value = contract.get("value", None)
    values = contract.get("values", None)
    min_fraction = contract.get("min_fraction", None)
    max_fraction = contract.get("max_fraction", None)

    # default result
    result = {
        "name": name,
        "scope": scope,
        "severity": severity,
        "target": target,
        "op": op,
        "threshold": threshold,
        "value": value,
        "min_fraction": min_fraction,
        "max_fraction": max_fraction,
        "n_subjects": 0,
        "n_violations": 0,
        "pct_violations": 0.0,
        "contract_status": "SKIP",
        "reason": "",
    }

    # basic config validation
    if scope not in SUPPORTED_SCOPES_2316 or scope == "contracts":
        result["reason"] = "unsupported_scope_or_meta_scope"
        return result
    if op not in SUPPORTED_OPS_2316:
        result["reason"] = "unsupported_op"
        return result

    df_scope = frames.get(scope, pd.DataFrame())
    if df_scope.empty:
        result["reason"] = "scope_frame_empty"
        return result

    df_filtered = _apply_where_filter_2316(df_scope, where)
    n_subj = int(df_filtered.shape[0])
    result["n_subjects"] = n_subj
    if n_subj == 0:
        result["reason"] = "no_subject_rows_after_where"
        return result

    if target not in df_filtered.columns:
        result["reason"] = "target_column_missing"
        return result

    series = df_filtered[target]
    # start evaluation
    n_viol = 0
    pct_viol = 0.0
    status = "SKIP"

    # relational ops
    if op in ("<", "<=", ">", ">=") and threshold is not None:
        if op == "<":
            cond_ok = series < threshold
        elif op == "<=":
            cond_ok = series <= threshold
        elif op == ">":
            cond_ok = series > threshold
        else:
            cond_ok = series >= threshold
        n_viol = int((~cond_ok).sum())
        pct_viol = float(n_viol / max(1, n_subj) * 100.0)
        status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    # equality / inequality
    elif op in ("==", "!=") and value is not None:
        if op == "==":
            n_viol = int((series != value).sum())
        else:
            n_viol = int((series == value).sum())
        pct_viol = float(n_viol / max(1, n_subj) * 100.0)
        status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    # fraction_eq: fraction equal to value must be <= max_fraction
    elif op == "fraction_eq" and value is not None and max_fraction is not None:
        cond_match = series == value
        n_match = int(cond_match.sum())
        frac_match = n_match / max(1, n_subj)
        if frac_match <= max_fraction:
            n_viol = 0
            pct_viol = 0.0
            status = "OK"
        else:
            n_viol = n_match
            pct_viol = float(frac_match * 100.0)
            status = "FAIL" if severity == "hard" else "WARN"

    # fraction_ge: fraction >= threshold must be >= or <= bound
    elif op == "fraction_ge" and threshold is not None:
        cond_ge = series >= threshold
        n_ge = int(cond_ge.sum())
        frac_ge = n_ge / max(1, n_subj)
        if min_fraction is not None:
            ok = (frac_ge >= min_fraction)
            if ok:
                n_viol = 0
                pct_viol = 0.0
                status = "OK"
            else:
                n_viol = int(n_subj - n_ge)
                pct_viol = float(n_viol / max(1, n_subj) * 100.0)
                status = "FAIL" if severity == "hard" else "WARN"
        elif max_fraction is not None:
            ok = (frac_ge <= max_fraction)
            if ok:
                n_viol = 0
                pct_viol = 0.0
                status = "OK"
            else:
                n_viol = n_ge
                pct_viol = float(frac_ge * 100.0)
                status = "FAIL" if severity == "hard" else "WARN"
        else:
            # all must satisfy
            n_viol = int((~cond_ge).sum())
            pct_viol = float(n_viol / max(1, n_subj) * 100.0)
            status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    # fraction_lt: fraction < threshold must be <= max_fraction
    elif op == "fraction_lt" and threshold is not None and max_fraction is not None:
        cond_lt = series < threshold
        n_lt = int(cond_lt.sum())
        frac_lt = n_lt / max(1, n_subj)
        if frac_lt <= max_fraction:
            n_viol = 0
            pct_viol = 0.0
            status = "OK"
        else:
            n_viol = n_lt
            pct_viol = float(frac_lt * 100.0)
            status = "FAIL" if severity == "hard" else "WARN"

    # not_any_in: no value should be in values
    elif op == "not_any_in" and values is not None:
        values_set = set(values)
        cond_bad = series.isin(values_set)
        n_viol = int(cond_bad.sum())
        pct_viol = float(n_viol / max(1, n_subj) * 100.0)
        status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    else:
        result["reason"] = "unsupported_or_incomplete_config"
        return result

    result["n_violations"] = int(n_viol)
    result["pct_violations"] = float(pct_viol)
    result["contract_status"] = status
    return result

# evaluate non-meta contracts
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") == "contracts":
        continue
    contract_rows_2316.append(_eval_single_contract_2316(contract_2316, frames_2316))

contracts_df_2316 = pd.DataFrame(contract_rows_2316)

# -------------------------------------------------------------------
# 3) Meta contracts (scope == "contracts")
# -------------------------------------------------------------------
if not contracts_df_2316.empty:
    hard_fail_mask_2316 = (contracts_df_2316["severity"] == "hard") & (contracts_df_2316["contract_status"] == "FAIL")
    soft_warn_mask_2316 = (contracts_df_2316["severity"] == "soft") & (contracts_df_2316["contract_status"].isin(["WARN", "FAIL"]))

    hard_contract_failures_2316 = int(hard_fail_mask_2316.sum())
    soft_contract_non_ok_2316 = int(soft_warn_mask_2316.sum())
else:
    hard_contract_failures_2316 = 0
    soft_contract_non_ok_2316 = 0

meta_rows_2316 = []
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") != "contracts":
        continue

    name_2316 = contract_2316.get("name", "")
    severity_2316 = str(contract_2316.get("severity", "hard")).lower()
    target_col_2316 = contract_2316.get("target")
    op_2316 = contract_2316.get("op")
    value_2316 = contract_2316.get("value", None)

    meta_metric_value_2316 = None
    if target_col_2316 == "hard_contract_failures":
        meta_metric_value_2316 = hard_contract_failures_2316

    n_subj_2316 = 1
    if meta_metric_value_2316 is None:
        n_viol_2316 = 0
        pct_viol_2316 = 0.0
        contract_status_2316 = "SKIP"
        reason_2316 = "unknown_meta_target"
    else:
        if op_2316 == "==" and value_2316 is not None:
            ok_2316 = (meta_metric_value_2316 == value_2316)
        elif op_2316 == "!=" and value_2316 is not None:
            ok_2316 = (meta_metric_value_2316 != value_2316)
        else:
            ok_2316 = True
        if op_2316 not in ("==", "!="):
            contract_status_2316 = "SKIP"
            n_viol_2316 = 0
            pct_viol_2316 = 0.0
            reason_2316 = "unsupported_meta_op"
        else:
            if ok_2316:
                contract_status_2316 = "OK"
                n_viol_2316 = 0
                pct_viol_2316 = 0.0
                reason_2316 = ""
            else:
                contract_status_2316 = "FAIL" if severity_2316 == "hard" else "WARN"
                n_viol_2316 = 1
                pct_viol_2316 = 100.0
                reason_2316 = ""

    meta_rows_2316.append(
        {
            "name": name_2316,
            "scope": "contracts",
            "severity": severity_2316,
            "target": target_col_2316,
            "op": op_2316,
            "threshold": None,
            "value": value_2316,
            "min_fraction": None,
            "max_fraction": None,
            "n_subjects": int(n_subj_2316),
            "n_violations": int(n_viol_2316),
            "pct_violations": float(pct_viol_2316),
            "contract_status": contract_status_2316,
            "reason": reason_2316,
        }
    )

if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)

# -------------------------------------------------------------------
# 4) Overall run status + artifacts
# -------------------------------------------------------------------
if not contracts_df_2316.empty:
    hard_fail_mask_2316 = (contracts_df_2316["severity"] == "hard") & (contracts_df_2316["contract_status"] == "FAIL")
    soft_warn_mask_2316 = (contracts_df_2316["severity"] == "soft") & (contracts_df_2316["contract_status"].isin(["WARN", "FAIL"]))

    n_hard_fail_2316 = int(hard_fail_mask_2316.sum())
    n_soft_warn_2316 = int(soft_warn_mask_2316.sum())

    if n_hard_fail_2316 > 0:
        overall_status_2316 = "FAIL"
    elif n_soft_warn_2316 > 0:
        overall_status_2316 = "WARN"
    else:
        overall_status_2316 = "OK"
else:
    overall_status_2316 = "OK"
    n_hard_fail_2316 = 0
    n_soft_warn_2316 = 0

contracts_json_path_2316 = NUMERIC_DIR / "data_contract_violations.json"
contracts_csv_path_2316  = NUMERIC_DIR / "data_contract_violations.csv"

run_meta_2316 = CONFIG.get("META", {}) if "CONFIG" in globals() else {}
violations_payload_2316 = {
    "run_id": run_meta_2316.get("VERSION"),
    "timestamp": pd.Timestamp.utcnow().isoformat(),
    "snapshot_id": run_meta_2316.get("SNAPSHOT_ID"),
    "overall_status": overall_status_2316,
    "hard_contract_failures": int(n_hard_fail_2316),
    "soft_contract_non_ok": int(n_soft_warn_2316),
    "n_contracts": int(len(contracts_cfg_2316)),
    "contracts": contracts_df_2316.to_dict(orient="records") if not contracts_df_2316.empty else [],
}

with open(contracts_json_path_2316, "w", encoding="utf-8") as f:
    json.dump(violations_payload_2316, f, indent=2, default=str)
print(f"üíæ Wrote contract violations JSON ‚Üí {contracts_json_path_2316}")

tmp_csv_2316 = contracts_csv_path_2316.with_suffix(".tmp.csv")
contracts_df_2316.to_csv(tmp_csv_2316, index=False)
os.replace(tmp_csv_2316, contracts_csv_path_2316)
print(f"üíæ Wrote contract violations CSV ‚Üí {contracts_csv_path_2316}")

print("\nüìä 2.3.16 data_contract_violations (head):")
if not contracts_df_2316.empty:
    display(contracts_df_2316.head(20))
else:
    print("   (no contracts evaluated)")

summary_2316 = pd.DataFrame([{
    "section":               "2.3.16",
    "section_name":          "Data contracts & threshold enforcement",
    "check":                 "Evaluate configured data contracts against numeric artifacts",
    "level":                 "info",
    "status":                overall_status_2316,
    "n_contracts":           int(len(contracts_cfg_2316)),
    "n_contracts_fail_hard": int(n_hard_fail_2316),
    "n_contracts_warn_soft": int(n_soft_warn_2316),
    "detail":                "data_contract_violations.json",
    "timestamp":             pd.Timestamp.utcnow(),
}])

append_sec2(summary_2316, SECTION2_REPORT_PATH)
display(summary_2316)
```

> This does a few ‚Äúprofessional‚Äù things:

* Normalizes and validates config.
* Encapsulates single-contract logic in one place.
* Gives you **reasons** for SKIP.
* Guards all the CSV reads.
* Uses the same summary pattern you‚Äôre using everywhere else.

> If you want, next step is: we can design 3‚Äì5 **canonical contract examples** for this Telco dataset (nulls, drift, readiness) so the CONTRACTS config file becomes almost self-documenting.

># How can we make this better?

>## Handle the FutureWarning

Nice, this is looking *really* legit now. üòé
That `FutureWarning` is basically pandas saying: ‚Äúhey, concatenating with an empty frame like this is going to change behavior later.‚Äù

## What the warning is about

You‚Äôre doing:

```python
contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

when `contracts_df_2316` might be empty (no non-meta contracts) and `meta_df_2316` has real data. Pandas is warning that the way it infers dtypes when some entries are empty/all-NA is changing in a future version.

## Easiest professional fix: avoid concat with an empty base

Just treat ‚Äúno base df yet‚Äù as ‚Äúset it equal to the meta df‚Äù.

># Replace this block:

```python
if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

with:

```python
if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    if contracts_df_2316.empty:
        # First contracts ‚Üí just use meta_df directly
        contracts_df_2316 = meta_df_2316
    else:
        # Append meta contracts
        contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

That:

* Removes the warning.
* Keeps dtypes clean and intuitive.
* Reads clearly: ‚Äúif we have nothing yet, start with meta; otherwise, append.‚Äù

---

### A couple of small polish ideas (optional)

These are ‚Äúnice to have‚Äù, not required:

1. **Freeze the supported sets at module level**

   Right now:

   ```python
   SUPPORTED_SCOPES_2316 = {...}
   SUPPORTED_OPS_2316 = {...}
   ```

   That‚Äôs good; if you ever split this into a module, you can move those to the top and reuse them elsewhere (e.g., config validation phase).

2. **Expose `reason` in the JSON payload**

   You‚Äôre already writing `reason` in the CSV. Since `violations_payload_2316["contracts"]` is coming from `contracts_df_2316.to_dict(orient="records")`, that‚Äôs already included ‚Äî which is great. I‚Äôd keep that; it makes the JSON self-explanatory.

3. **Tiny docstring tweak**

   For `_eval_single_contract_2316`, you might change the docstring to:

   ```python
   """Return a metrics dict for a single non-meta contract (row in data_contract_violations)."""
   ```

   so future-you instantly knows this maps 1:1 to a row.

### TL;DR

* Yes, it‚Äôs worth handling the `FutureWarning`.
* The best fix is to **special-case the empty `contracts_df_2316` before concatenating** (snippet above).
* The rest of your implementation is already very ‚Äúprofessional‚Äù ‚Äî config-normalized, reason-coded, and artifacted. This is solid.


---
?
---

># Why is dataframe empty? what should be refactored to handle properly?

The key thing: the warning is about a **possible** empty frame in the concat, not that your final contracts table is empty.

walk through where the ‚Äúempty frame‚Äù can come from in your code.

---

### Where the empty DataFrame appears

You build `contracts_df_2316` like this:

```python
contract_rows_2316 = []

# evaluate non-meta contracts
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") == "contracts":
        continue
    contract_rows_2316.append(_eval_single_contract_2316(contract_2316, frames_2316))

contracts_df_2316 = pd.DataFrame(contract_rows_2316)
```

There are legit scenarios where `contract_rows_2316` ends up **empty**:

1. **Your config has only meta contracts**
   i.e. all entries in `CONTRACTS` have `scope: "contracts"`, so the loop always hits:

   ```python
   if contract_2316.get("scope") == "contracts":
       continue
   ```

   ‚Üí nothing ever gets appended
   ‚Üí `contract_rows_2316 == []`
   ‚Üí `contracts_df_2316 = pd.DataFrame([])` ‚Üí **empty frame**.

2. **Your config list is empty**
   (you might still add meta contracts later or not at all).
   Same result: `contract_rows_2316` stays `[]`, `contracts_df_2316` is empty.

Later, you do:

```python
if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

So pandas sees a concat like:

```python
pd.concat([EmptyDataFrame, NonEmptyDataFrame], ignore_index=True)
```

and that pattern is exactly what the `FutureWarning` is about:

> ‚ÄúDataFrame concatenation with empty or all-NA entries is deprecated...‚Äù

It‚Äôs complaining about the *inputs* to concat (one of them is empty), not saying your final `contracts_df_2316` is empty. In your current run, you *do* have non-meta contracts, so the base frame isn‚Äôt empty at that point ‚Äî but the warning is triggered by the general pattern and pandas being conservative about future behaviour.

---

### Why the fix I suggested helps

By changing:

```python
if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

to:

```python
if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    if contracts_df_2316.empty:
        contracts_df_2316 = meta_df_2316
    else:
        contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

you avoid ever calling `concat` with an empty base frame:

* If there were **no non-meta contracts**, you just set `contracts_df_2316 = meta_df_2316`.
* If there **were** non-meta contracts (your usual case), you still concatenate as before.

So:

* The warning goes away.
* The code explicitly reflects the two possible shapes of the pipeline:

  * ‚Äúonly meta contracts‚Äù
  * ‚Äúnon-meta + meta contracts‚Äù

---

So the short answer:

> **It has an empty frame whenever you have zero non-meta contracts (or other config paths that never append rows)**. Pandas doesn‚Äôt love `concat([empty_df, df])` in future versions, hence the warning, and we handle that by special-casing the empty case instead of always concatenating.

># Q: What should be refactored upstream?

Good instincts to ask this instead of just slapping conditionals on the warning. üôÉ

The cleanest ‚Äúupstream‚Äù refactor is:

> **Don‚Äôt build a DataFrame, then later try to concat more rows into it.
> Build *one list of rows*, then make the DataFrame once at the end.**

That removes the empty-frame case entirely and makes the flow easier to reason about.

---

## 1. Change the data flow: rows ‚Üí one DataFrame

Right now, the flow is:

1. Build `contract_rows_2316` for non-meta contracts.
2. Turn that into `contracts_df_2316 = pd.DataFrame(contract_rows_2316)`.
3. Later build `meta_rows_2316`.
4. Turn those into `meta_df_2316`.
5. `pd.concat([contracts_df_2316, meta_df_2316])` ‚Üí warning when base is empty.

Refactor that to:

1. Maintain a **single list** `all_rows_2316`.
2. Append non-meta contract results to that list.
3. Append meta contract results to the same list.
4. At the end, do **one** `pd.DataFrame(all_rows_2316)`.

### What to change upstream

Replace this pattern:

```python
contract_rows_2316 = []

# non-meta contracts
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") == "contracts":
        continue
    contract_rows_2316.append(_eval_single_contract_2316(contract_2316, frames_2316))

contracts_df_2316 = pd.DataFrame(contract_rows_2316)

# ... later ...

meta_rows_2316 = []
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") != "contracts":
        continue
    # build meta row dict, append to meta_rows_2316

if meta_rows_2316:
    meta_df_2316 = pd.DataFrame(meta_rows_2316)
    contracts_df_2316 = pd.concat([contracts_df_2316, meta_df_2316], ignore_index=True)
```

with something like:

```python
all_rows_2316 = []

# 2a) non-meta contracts
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") == "contracts":
        continue
    row = _eval_single_contract_2316(contract_2316, frames_2316)
    all_rows_2316.append(row)

# compute hard_contract_failures / soft_contract_non_ok from non-meta rows
tmp_df_2316 = pd.DataFrame(all_rows_2316) if all_rows_2316 else pd.DataFrame()
if not tmp_df_2316.empty:
    hard_fail_mask_2316 = (tmp_df_2316["severity"] == "hard") & (tmp_df_2316["contract_status"] == "FAIL")
    soft_warn_mask_2316 = (tmp_df_2316["severity"] == "soft") & (tmp_df_2316["contract_status"].isin(["WARN", "FAIL"]))
    hard_contract_failures_2316 = int(hard_fail_mask_2316.sum())
    soft_contract_non_ok_2316 = int(soft_warn_mask_2316.sum())
else:
    hard_contract_failures_2316 = 0
    soft_contract_non_ok_2316 = 0

# 3) meta contracts (scope == "contracts")
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") != "contracts":
        continue
    # build meta row dict with hard_contract_failures_2316, etc.
    meta_row = {
        "name": ...,
        "scope": "contracts",
        # ...
    }
    all_rows_2316.append(meta_row)

# 4) final contracts DataFrame
if all_rows_2316:
    contracts_df_2316 = pd.DataFrame(all_rows_2316)
else:
    contracts_df_2316 = pd.DataFrame(columns=[
        "name", "scope", "severity", "target", "op",
        "threshold", "value", "min_fraction", "max_fraction",
        "n_subjects", "n_violations", "pct_violations",
        "contract_status", "reason",
    ])
```

Now:

* There‚Äôs **no concat with empty frames** at all ‚Üí warning disappears naturally.
* The control flow is simpler:

  * ‚ÄúWe always just accumulate row dicts, then DataFrame once.‚Äù
* You still keep the nice `reason` column, `hard_contract_failures`, etc.

---

## 2. Bonus: config-aware early exit

If you want to be *extra* clean, you can also short-circuit early:

```python
if not contracts_cfg_2316:
    # write an explicit stub: no contracts, OK status
    contracts_df_2316 = pd.DataFrame(columns=[...])
    overall_status_2316 = "OK"
    n_hard_fail_2316 = 0
    n_soft_warn_2316 = 0
    # write JSON/CSV stub and your summary row
    # then return / end cell
```

That makes it crystal clear in logs and artifacts that ‚Äúno contracts were configured‚Äù is a *different* state from ‚Äúcontracts configured and all passed‚Äù.

---

### TL;DR

* The ‚Äúproper‚Äù upstream refactor is to **avoid building a base empty DataFrame and then concatenating into it**.
* Instead, **accumulate all contract results (non-meta + meta) in a list of row dicts**, and build `contracts_df_2316` once at the end.
* Optionally, early-exit when `CONTRACTS` is empty and emit a stub artifact, so the behavior and logs are explicit.

># Q Futurewarning NOT FIXED!!! Explain why

Yeah, pandas is being petty here üòÖ ‚Äî your guard fixed the literal empty-DataFrame case, but `concat` is *still* seeing something it considers ‚Äúempty or all-NA entries‚Äù (like a frame where some columns are entirely NaN), so it keeps nagging you.

Best move: **stop concatenating DataFrames at all** and just build one list of rows, then turn that into a single DataFrame at the end. That‚Äôs cleaner and kills the warning.

Below is a refactor of your 2.3.18 cell that:

* Uses **one list** of row dicts (`all_rows_2316`)
* Computes `hard_contract_failures` from **non-meta contracts only**
* Adds meta contracts as additional rows
* Builds `contracts_df_2316` **once** (no `concat`, no FutureWarning)
* Fixes the log message that still said ‚Äú2.3.16‚Äù

You can drop this in place of your current 2.3.18 cell.

```python
# 2.3.18 | Data Contracts & Threshold Enforcement
print("\n2.3.18 üìú Data contracts & threshold enforcement")

# -------------------------------------------------------------------
# 0) Config resolution & normalization
# -------------------------------------------------------------------
def _normalize_contracts_cfg_2316(raw_cfg):
    """Normalize CONTRACTS config into a list of dict contracts."""
    if raw_cfg is None:
        return []
    if isinstance(raw_cfg, dict):
        # allow {"rules": [...]} or dict-of-rules
        if "rules" in raw_cfg and isinstance(raw_cfg["rules"], list):
            return raw_cfg["rules"]
        return list(raw_cfg.values())
    if isinstance(raw_cfg, list):
        return raw_cfg
    print("‚ö†Ô∏è CONTRACTS config is not a list/dict ‚Äî treating as no contracts.")
    return []

if "C" in globals() and callable(C):
    raw_contracts_cfg_2316 = C("CONTRACTS", [])
elif "CONFIG" in globals():
    raw_contracts_cfg_2316 = CONFIG.get("CONTRACTS", [])
else:
    raw_contracts_cfg_2316 = []

contracts_cfg_2316 = _normalize_contracts_cfg_2316(raw_contracts_cfg_2316)

if not contracts_cfg_2316:
    print("‚ÑπÔ∏è No contracts configured (CONTRACTS empty) ‚Äî 2.3.18 will emit stub artifacts.")

# -------------------------------------------------------------------
# 1) Load core artifacts from earlier sections
# -------------------------------------------------------------------
numeric_integrity_path_2316 = NUMERIC_DIR / "numeric_integrity_report.csv"
numeric_profile_path_2316   = NUMERIC_DIR / "numeric_profile_df.csv"
readiness_path_2316         = NUMERIC_DIR / "model_readiness_report.csv"
drift_path_2316             = NUMERIC_DIR / "data_drift_metrics.csv"

frames_2316 = {}

def _safe_read_csv_2316(path, label):
    if not path.exists():
        print(f"‚ö†Ô∏è {path} missing ‚Äî {label} contracts may be skipped.")
        return pd.DataFrame()
    try:
        if path.stat().st_size == 0:
            print(f"‚ö†Ô∏è {path} is empty ‚Äî {label} contracts will be skipped.")
            return pd.DataFrame()
        return pd.read_csv(path)
    except pd.errors.EmptyDataError:
        print(f"‚ö†Ô∏è {path} contained no parsable data ‚Äî {label} contracts will be skipped.")
        return pd.DataFrame()

frames_2316["numeric_integrity"] = _safe_read_csv_2316(numeric_integrity_path_2316, "numeric_integrity")
frames_2316["numeric_profile"]   = _safe_read_csv_2316(numeric_profile_path_2316, "numeric_profile")
frames_2316["readiness"]         = _safe_read_csv_2316(readiness_path_2316, "readiness")
frames_2316["drift"]             = _safe_read_csv_2316(drift_path_2316, "drift")

# normalize feature/column keys
for scope_name_2316, df_2316 in frames_2316.items():
    if not df_2316.empty:
        if "column" not in df_2316.columns and "feature" in df_2316.columns:
            df_2316["column"] = df_2316["feature"].astype("string")
        if "feature" not in df_2316.columns and "column" in df_2316.columns:
            df_2316["feature"] = df_2316["column"].astype("string")
    frames_2316[scope_name_2316] = df_2316

def _apply_where_filter_2316(df, where_dict):
    if not where_dict:
        return df
    subset = df.copy()
    for k, v in (where_dict or {}).items():
        if k in subset.columns:
            subset = subset[subset[k] == v]
    return subset

# -------------------------------------------------------------------
# 2) Evaluate non-meta contracts (scope != "contracts")
# -------------------------------------------------------------------
SUPPORTED_SCOPES_2316 = {"numeric_integrity", "numeric_profile", "readiness", "drift", "contracts"}
SUPPORTED_OPS_2316 = {
    "<", "<=", ">", ">=",
    "==", "!=",
    "fraction_eq", "fraction_ge", "fraction_lt",
    "not_any_in",
}

def _eval_single_contract_2316(contract, frames):
    """Return a metrics dict for a single non-meta contract (row in data_contract_violations)."""
    scope = contract.get("scope")
    name = contract.get("name", "")
    severity = str(contract.get("severity", "hard")).lower()
    target = contract.get("target")
    op = contract.get("op")
    where = contract.get("where", {})
    threshold = contract.get("threshold", None)
    value = contract.get("value", None)
    values = contract.get("values", None)
    min_fraction = contract.get("min_fraction", None)
    max_fraction = contract.get("max_fraction", None)

    # default result
    result = {
        "name": name,
        "scope": scope,
        "severity": severity,
        "target": target,
        "op": op,
        "threshold": threshold,
        "value": value,
        "min_fraction": min_fraction,
        "max_fraction": max_fraction,
        "n_subjects": 0,
        "n_violations": 0,
        "pct_violations": 0.0,
        "contract_status": "SKIP",
        "reason": "",
    }

    # basic config validation
    if scope not in SUPPORTED_SCOPES_2316 or scope == "contracts":
        result["reason"] = "unsupported_scope_or_meta_scope"
        return result
    if op not in SUPPORTED_OPS_2316:
        result["reason"] = "unsupported_op"
        return result

    df_scope = frames.get(scope, pd.DataFrame())
    if df_scope.empty:
        result["reason"] = "scope_frame_empty"
        return result

    df_filtered = _apply_where_filter_2316(df_scope, where)
    n_subj = int(df_filtered.shape[0])
    result["n_subjects"] = n_subj
    if n_subj == 0:
        result["reason"] = "no_subject_rows_after_where"
        return result

    if target not in df_filtered.columns:
        result["reason"] = "target_column_missing"
        return result

    series = df_filtered[target]
    n_viol = 0
    pct_viol = 0.0
    status = "SKIP"

    # relational ops
    if op in ("<", "<=", ">", ">=") and threshold is not None:
        if op == "<":
            cond_ok = series < threshold
        elif op == "<=":
            cond_ok = series <= threshold
        elif op == ">":
            cond_ok = series > threshold
        else:
            cond_ok = series >= threshold
        n_viol = int((~cond_ok).sum())
        pct_viol = float(n_viol / max(1, n_subj) * 100.0)
        status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    # equality / inequality
    elif op in ("==", "!=") and value is not None:
        if op == "==":
            n_viol = int((series != value).sum())
        else:
            n_viol = int((series == value).sum())
        pct_viol = float(n_viol / max(1, n_subj) * 100.0)
        status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    # fraction_eq: fraction equal to value must be <= max_fraction
    elif op == "fraction_eq" and value is not None and max_fraction is not None:
        cond_match = series == value
        n_match = int(cond_match.sum())
        frac_match = n_match / max(1, n_subj)
        if frac_match <= max_fraction:
            n_viol = 0
            pct_viol = 0.0
            status = "OK"
        else:
            n_viol = n_match
            pct_viol = float(frac_match * 100.0)
            status = "FAIL" if severity == "hard" else "WARN"

    # fraction_ge: fraction >= threshold must be >= or <= bound
    elif op == "fraction_ge" and threshold is not None:
        cond_ge = series >= threshold
        n_ge = int(cond_ge.sum())
        frac_ge = n_ge / max(1, n_subj)
        if min_fraction is not None:
            ok = (frac_ge >= min_fraction)
            if ok:
                n_viol = 0
                pct_viol = 0.0
                status = "OK"
            else:
                n_viol = int(n_subj - n_ge)
                pct_viol = float(n_viol / max(1, n_subj) * 100.0)
                status = "FAIL" if severity == "hard" else "WARN"
        elif max_fraction is not None:
            ok = (frac_ge <= max_fraction)
            if ok:
                n_viol = 0
                pct_viol = 0.0
                status = "OK"
            else:
                n_viol = n_ge
                pct_viol = float(frac_ge * 100.0)
                status = "FAIL" if severity == "hard" else "WARN"
        else:
            # all must satisfy
            n_viol = int((~cond_ge).sum())
            pct_viol = float(n_viol / max(1, n_subj) * 100.0)
            status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    # fraction_lt: fraction < threshold must be <= max_fraction
    elif op == "fraction_lt" and threshold is not None and max_fraction is not None:
        cond_lt = series < threshold
        n_lt = int(cond_lt.sum())
        frac_lt = n_lt / max(1, n_subj)
        if frac_lt <= max_fraction:
            n_viol = 0
            pct_viol = 0.0
            status = "OK"
        else:
            n_viol = n_lt
            pct_viol = float(frac_lt * 100.0)
            status = "FAIL" if severity == "hard" else "WARN"

    # not_any_in: no value should be in values
    elif op == "not_any_in" and values is not None:
        values_set = set(values)
        cond_bad = series.isin(values_set)
        n_viol = int(cond_bad.sum())
        pct_viol = float(n_viol / max(1, n_subj) * 100.0)
        status = "OK" if n_viol == 0 else ("FAIL" if severity == "hard" else "WARN")

    else:
        result["reason"] = "unsupported_or_incomplete_config"
        return result

    result["n_violations"] = int(n_viol)
    result["pct_violations"] = float(pct_viol)
    result["contract_status"] = status
    return result

# collect non-meta contract rows
nonmeta_rows_2316 = []
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") == "contracts":
        continue
    nonmeta_rows_2316.append(_eval_single_contract_2316(contract_2316, frames_2316))

# compute meta metrics (hard/soft failures) from non-meta only
if nonmeta_rows_2316:
    nonmeta_df_2316 = pd.DataFrame(nonmeta_rows_2316)
    hard_fail_mask_2316 = (nonmeta_df_2316["severity"] == "hard") & (nonmeta_df_2316["contract_status"] == "FAIL")
    soft_warn_mask_2316 = (nonmeta_df_2316["severity"] == "soft") & (nonmeta_df_2316["contract_status"].isin(["WARN", "FAIL"]))
    hard_contract_failures_2316 = int(hard_fail_mask_2316.sum())
    soft_contract_non_ok_2316 = int(soft_warn_mask_2316.sum())
else:
    nonmeta_df_2316 = pd.DataFrame()
    hard_contract_failures_2316 = 0
    soft_contract_non_ok_2316 = 0

# -------------------------------------------------------------------
# 3) Meta contracts (scope == "contracts")
# -------------------------------------------------------------------
meta_rows_2316 = []
for contract_2316 in contracts_cfg_2316:
    if contract_2316.get("scope") != "contracts":
        continue

    name_2316 = contract_2316.get("name", "")
    severity_2316 = str(contract_2316.get("severity", "hard")).lower()
    target_col_2316 = contract_2316.get("target")
    op_2316 = contract_2316.get("op")
    value_2316 = contract_2316.get("value", None)

    meta_metric_value_2316 = None
    if target_col_2316 == "hard_contract_failures":
        meta_metric_value_2316 = hard_contract_failures_2316

    n_subj_2316 = 1
    if meta_metric_value_2316 is None:
        n_viol_2316 = 0
        pct_viol_2316 = 0.0
        contract_status_2316 = "SKIP"
        reason_2316 = "unknown_meta_target"
    else:
        if op_2316 == "==" and value_2316 is not None:
            ok_2316 = (meta_metric_value_2316 == value_2316)
        elif op_2316 == "!=" and value_2316 is not None:
            ok_2316 = (meta_metric_value_2316 != value_2316)
        else:
            ok_2316 = True
        if op_2316 not in ("==", "!="):
            contract_status_2316 = "SKIP"
            n_viol_2316 = 0
            pct_viol_2316 = 0.0
            reason_2316 = "unsupported_meta_op"
        else:
            if ok_2316:
                contract_status_2316 = "OK"
                n_viol_2316 = 0
                pct_viol_2316 = 0.0
                reason_2316 = ""
            else:
                contract_status_2316 = "FAIL" if severity_2316 == "hard" else "WARN"
                n_viol_2316 = 1
                pct_viol_2316 = 100.0
                reason_2316 = ""

    meta_rows_2316.append(
        {
            "name": name_2316,
            "scope": "contracts",
            "severity": severity_2316,
            "target": target_col_2316,
            "op": op_2316,
            "threshold": None,
            "value": value_2316,
            "min_fraction": None,
            "max_fraction": None,
            "n_subjects": int(n_subj_2316),
            "n_violations": int(n_viol_2316),
            "pct_violations": float(pct_viol_2316),
            "contract_status": contract_status_2316,
            "reason": reason_2316,
        }
    )

# -------------------------------------------------------------------
# 4) Build final contracts DataFrame (non-meta + meta)
# -------------------------------------------------------------------
all_rows_2316 = []
all_rows_2316.extend(nonmeta_rows_2316)
all_rows_2316.extend(meta_rows_2316)

if all_rows_2316:
    contracts_df_2316 = pd.DataFrame(all_rows_2316)
else:
    contracts_df_2316 = pd.DataFrame(
        columns=[
            "name", "scope", "severity", "target", "op",
            "threshold", "value", "min_fraction", "max_fraction",
            "n_subjects", "n_violations", "pct_violations",
            "contract_status", "reason",
        ]
    )

# -------------------------------------------------------------------
# 5) Overall run status + artifacts
# -------------------------------------------------------------------
if not contracts_df_2316.empty:
    hard_fail_mask_all_2316 = (contracts_df_2316["severity"] == "hard") & (contracts_df_2316["contract_status"] == "FAIL")
    soft_warn_mask_all_2316 = (contracts_df_2316["severity"] == "soft") & (contracts_df_2316["contract_status"].isin(["WARN", "FAIL"]))

    n_hard_fail_2316 = int(hard_fail_mask_all_2316.sum())
    n_soft_warn_2316 = int(soft_warn_mask_all_2316.sum())

    if n_hard_fail_2316 > 0:
        overall_status_2316 = "FAIL"
    elif n_soft_warn_2316 > 0:
        overall_status_2316 = "WARN"
    else:
        overall_status_2316 = "OK"
else:
    overall_status_2316 = "OK"
    n_hard_fail_2316 = 0
    n_soft_warn_2316 = 0

contracts_json_path_2316 = NUMERIC_DIR / "data_contract_violations.json"
contracts_csv_path_2316  = NUMERIC_DIR / "data_contract_violations.csv"

run_meta_2316 = CONFIG.get("META", {}) if "CONFIG" in globals() else {}
violations_payload_2316 = {
    "run_id": run_meta_2316.get("VERSION"),
    "timestamp": pd.Timestamp.utcnow().isoformat(),
    "snapshot_id": run_meta_2316.get("SNAPSHOT_ID"),
    "overall_status": overall_status_2316,
    "hard_contract_failures": int(hard_contract_failures_2316),
    "soft_contract_non_ok": int(soft_contract_non_ok_2316),
    "n_contracts": int(len(contracts_cfg_2316)),
    "contracts": contracts_df_2316.to_dict(orient="records") if not contracts_df_2316.empty else [],
}

with open(contracts_json_path_2316, "w", encoding="utf-8") as f:
    json.dump(violations_payload_2316, f, indent=2, default=str)
print(f"üíæ Wrote contract violations JSON ‚Üí {contracts_json_path_2316}")

tmp_csv_2316 = contracts_csv_path_2316.with_suffix(".tmp.csv")
contracts_df_2316.to_csv(tmp_csv_2316, index=False)
os.replace(tmp_csv_2316, contracts_csv_path_2316)
print(f"üíæ Wrote contract violations CSV ‚Üí {contracts_csv_path_2316}")

print("\nüìä Data_contract_violations:")
if not contracts_df_2316.empty:
    display(contracts_df_2316.head(20))
else:
    print("   (no contracts evaluated)")

summary_2316 = pd.DataFrame([{
    "section":               "2.3.18",
    "section_name":          "Data contracts & threshold enforcement",
    "check":                 "Evaluate configured data contracts against numeric artifacts",
    "level":                 "info",
    "status":                overall_status_2316,
    "n_contracts":           int(len(contracts_cfg_2316)),
    "n_contracts_fail_hard": int(n_hard_fail_2316),
    "n_contracts_warn_soft": int(n_soft_warn_2316),
    "detail":                "data_contract_violations.json",
    "timestamp":             pd.Timestamp.utcnow(),
}])

append_sec2(summary_2316, SECTION2_REPORT_PATH)
display(summary_2316)
```

This should:
* Remove the `FutureWarning` completely (no `concat` with empty/all-NA inputs).
* Keep your behavior identical.
* Make the data flow ‚Äúrows ‚Üí one DataFrame‚Äù instead of ‚Äúframe ‚Üí concat ‚Üí frame‚Äù, which is easier to reason about long-term.