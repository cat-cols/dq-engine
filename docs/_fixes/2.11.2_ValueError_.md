```py
# 2.11.2 | Hierarchical Correlation Clustering
print("2.11.2 ▶ Hierarchical correlation clustering")

default_corr_cluster_cfg_2112 = {
    "ENABLED": True,
    "DISTANCE_METRIC": "1_minus_abs_corr",
    "LINKAGE": "average",
    "MAX_CLUSTERS": 20,
    "OUTPUT_CLUSTER_FILE": "correlation_clusters.csv",
    "OUTPUT_DENDROGRAM_FILE": "corr_dendrogram.png",
}
corr_cluster_cfg_2112 = _get_cfg_210("CORR_CLUSTERING", default_corr_cluster_cfg_2112)

corr_cluster_enabled_2112 = bool(corr_cluster_cfg_2112.get("ENABLED", True))
corr_cluster_max_clusters_2112 = int(corr_cluster_cfg_2112.get("MAX_CLUSTERS", 20))
corr_cluster_output_file_2112 = str(corr_cluster_cfg_2112.get("OUTPUT_CLUSTER_FILE", "correlation_clusters.csv"))
corr_cluster_dendro_file_2112 = str(corr_cluster_cfg_2112.get("OUTPUT_DENDROGRAM_FILE", "corr_dendrogram.png"))

corr_cluster_path_2112 = section2_reports_dir_211 / corr_cluster_output_file_2112
corr_dendro_path_2112 = figures_root_211 / corr_cluster_dendro_file_2112

corr_cluster_df_2112 = pd.DataFrame()
n_clusters_2112 = 0
avg_cluster_size_2112 = 0.0

# SciPy for hierarchical clustering (optional)
try:
    from scipy.cluster.hierarchy import linkage as _linkage_2112, dendrogram as _dendrogram_2112, fcluster as _fcluster_2112
    from scipy.spatial.distance import squareform as _squareform_2112
    _HAS_SCIPY_CLUSTER_2112 = True
except Exception:
    _HAS_SCIPY_CLUSTER_2112 = False

if corr_cluster_enabled_2112 and _HAS_SCIPY_CLUSTER_2112 and corr_pearson_2111 is not None:
    # Use absolute Pearson correlation to build distance matrix
    abs_corr = corr_pearson_2111.abs()
    np.fill_diagonal(abs_corr.values, 1.0)
    dist_matrix = 1.0 - abs_corr.values

    # Convert to condensed distance for linkage
    condensed = _squareform_2112(dist_matrix, checks=False)

    Z = _linkage_2112(condensed, method=str(corr_cluster_cfg_2112.get("LINKAGE", "average")))

    # Cluster assignment based on max number of clusters
    cluster_labels = _fcluster_2112(Z, t=corr_cluster_max_clusters_2112, criterion="maxclust")

    features = list(corr_pearson_2111.columns)
    cluster_series = pd.Series(cluster_labels, index=features, name="cluster_id")

    rows_2112 = []
    for cluster_id in sorted(cluster_series.unique()):
        members = cluster_series[cluster_series == cluster_id].index.tolist()
        cluster_size = len(members)
        if cluster_size >= 2:
            sub_corr = abs_corr.loc[members, members].values
            # exclude diagonal when computing mean
            mask = ~np.eye(cluster_size, dtype=bool)
            intra_mean = float(sub_corr[mask].mean()) if mask.sum() > 0 else np.nan
        else:
            intra_mean = 0.0

        for feat in members:
            rows_2112.append(
                {
                    "feature": feat,
                    "cluster_id": int(cluster_id),
                    "cluster_size": int(cluster_size),
                    "intra_cluster_mean_corr": intra_mean,
                }
            )

    corr_cluster_df_2112 = pd.DataFrame(rows_2112)
    n_clusters_2112 = int(corr_cluster_df_2112["cluster_id"].nunique())
    avg_cluster_size_2112 = float(corr_cluster_df_2112["cluster_size"].mean())

    tmp_2112 = corr_cluster_path_2112.with_suffix(".tmp.csv")
    corr_cluster_df_2112.to_csv(tmp_2112, index=False)
    os.replace(tmp_2112, corr_cluster_path_2112)

    # Dendrogram
    fig, ax = plt.subplots(figsize=(8, 5))
    _dendrogram_2112(Z, labels=features, leaf_rotation=90)
    ax.set_title("Hierarchical correlation clustering (numeric)")
    fig.tight_layout()
    corr_dendro_path_2112.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(corr_dendro_path_2112)
    plt.close(fig)

elif corr_cluster_enabled_2112 and not _HAS_SCIPY_CLUSTER_2112:
    print("⚠️ SciPy not available; 2.11.2 clustering skipped.")
elif corr_cluster_enabled_2112 and corr_pearson_2111 is None:
    print("⚠️ Pearson correlation matrix not available; 2.11.2 clustering skipped.")

if n_clusters_2112 == 0:
    status_2112 = "WARN"
else:
    status_2112 = "OK"

sec2_chunk_2112 = pd.DataFrame(
    {
        "section": ["2.11.2"],
        "section_name": ["Hierarchical correlation clustering"],
        "check": [
            "Cluster numeric features using 1−|corr| distance and record cluster assignments"
        ],
        "level": ["info"],
        "n_clusters": [n_clusters_2112],
        "avg_cluster_size": [avg_cluster_size_2112],
        "status": [status_2112],
        "detail": [str(corr_cluster_path_2112)],
    }
)

if "_append_sec2" in globals() and callable(_append_sec2):
    _append_sec2(sec2_chunk_2112)
else:
    print("ℹ️ _append_sec2 not available; 2.11.2 diagnostics not appended to Section 2 report.")

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[207], line 43
     40 # Convert to condensed distance for linkage`
     41 condensed = _squareform_2112(dist_matrix, checks=False)
--> 43 Z = _linkage_2112(condensed, method=str(corr_cluster_cfg_2112.get("LINKAGE", "average")))
     45 # Cluster assignment based on max number of clusters
     46 cluster_labels = _fcluster_2112(Z, t=corr_cluster_max_clusters_2112, criterion="maxclust")

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/scipy/cluster/hierarchy.py:1046, in linkage(y, method, metric, optimal_ordering)
   1043     raise ValueError("`y` must be 1 or 2 dimensional.")
   1045 if not lazy and not xp.all(xp.isfinite(y)):
-> 1046     raise ValueError("The condensed distance matrix must contain only "
   1047                      "finite values.")
   1049 n = distance.num_obs_y(y)
   1050 method_code = _LINKAGE_METHODS[method]

ValueError: The condensed distance matrix must contain only finite values.
```

---

># FIX:

---

># condensed = _squareform_2112(dist_matrix, checks=False)

---

Nice catch — this is a classic “NaNs in the distance matrix” trap.

### What’s going wrong

This bit:

```python
abs_corr = corr_pearson_2111.abs()
np.fill_diagonal(abs_corr.values, 1.0)
dist_matrix = 1.0 - abs_corr.values
condensed = _squareform_2112(dist_matrix, checks=False)
Z = _linkage_2112(condensed, method=...)
```

If any entry in `corr_pearson_2111` is `NaN` (which happens whenever a pair of features can’t have correlation computed — constant columns, all-NaN overlaps, etc.), then:

* `abs_corr` has `NaN`,
* `dist_matrix = 1 - NaN` → `NaN`,
* `condensed` contains `NaN`,
* `linkage` raises:
  **“The condensed distance matrix must contain only finite values.”**

The diagonal fill only fixes the diagonal, not off-diagonal NaNs.

---

### Minimal, robust fix

Right after you build `abs_corr`, sanitize NaNs/infs **before** building the distance matrix.

I’d change the core block to this:

```python
if corr_cluster_enabled_2112 and _HAS_SCIPY_CLUSTER_2112 and corr_pearson_2111 is not None:
    # Use absolute Pearson correlation to build distance matrix
    abs_corr = corr_pearson_2111.abs().copy()

    # Replace non-finite correlations (NaN/inf) with 0 → max distance (1.0)
    abs_corr = abs_corr.replace([np.inf, -np.inf], np.nan).fillna(0.0)

    # Enforce self-correlation = 1
    np.fill_diagonal(abs_corr.values, 1.0)

    # Distance = 1 - |corr|
    dist_matrix = 1.0 - abs_corr.values

    # Also make sure no non-finite values sneak through
    dist_matrix = np.nan_to_num(dist_matrix, nan=1.0, posinf=1.0, neginf=1.0)

    # If fewer than 2 features, bail early
    if dist_matrix.shape[0] < 2:
        print("⚠️ Not enough numeric features for clustering; 2.11.2 skipped.")
    else:
        # Convert to condensed distance for linkage
        condensed = _squareform_2112(dist_matrix, checks=False)

        Z = _linkage_2112(
            condensed,
            method=str(corr_cluster_cfg_2112.get("LINKAGE", "average"))
        )

        # Cluster assignment based on max number of clusters
        cluster_labels = _fcluster_2112(Z, t=corr_cluster_max_clusters_2112, criterion="maxclust")

        features = list(corr_pearson_2111.columns)
        cluster_series = pd.Series(cluster_labels, index=features, name="cluster_id")

        rows_2112 = []
        for cluster_id in sorted(cluster_series.unique()):
            members = cluster_series[cluster_series == cluster_id].index.tolist()
            cluster_size = len(members)
            if cluster_size >= 2:
                sub_corr = abs_corr.loc[members, members].values
                # exclude diagonal when computing mean
                mask = ~np.eye(cluster_size, dtype=bool)
                intra_mean = float(sub_corr[mask].mean()) if mask.sum() > 0 else np.nan
            else:
                intra_mean = 0.0

            for feat in members:
                rows_2112.append(
                    {
                        "feature": feat,
                        "cluster_id": int(cluster_id),
                        "cluster_size": int(cluster_size),
                        "intra_cluster_mean_corr": intra_mean,
                    }
                )

        corr_cluster_df_2112 = pd.DataFrame(rows_2112)
        n_clusters_2112 = int(corr_cluster_df_2112["cluster_id"].nunique())
        avg_cluster_size_2112 = float(corr_cluster_df_2112["cluster_size"].mean())

        tmp_2112 = corr_cluster_path_2112.with_suffix(".tmp.csv")
        corr_cluster_df_2112.to_csv(tmp_2112, index=False)
        os.replace(tmp_2112, corr_cluster_path_2112)

        # Dendrogram
        fig, ax = plt.subplots(figsize=(8, 5))
        _dendrogram_2112(Z, labels=features, leaf_rotation=90)
        ax.set_title("Hierarchical correlation clustering (numeric)")
        fig.tight_layout()
        corr_dendro_path_2112.parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(corr_dendro_path_2112)
        plt.close(fig)
```

Key changes:

* `abs_corr = abs_corr.replace(...).fillna(0.0)`
  → treat unknown correlations as 0 (i.e., max distance 1).
* `dist_matrix = np.nan_to_num(...)`
  → safety net in case anything weird sneaks through.
* Guard for `< 2` features to avoid degenerate clustering.

---

If you’d like, I can also help you add a tiny **sanity print** just before `linkage` to log how many NaNs you had to patch, so you can see when your dataset is especially sparse or constant-heavy.
