
```py
# 2.1.9 üß© Column Role Classification & Feature Group Registration | def(reporting) no C()
print("\n2.1.9 üß© Column Role Classification & Feature Group Registration")

assert "df" in globals(), "‚ùå df not found."
assert "CONFIG" in globals(), "‚ùå CONFIG missing."
assert "REPORTS_DIR" in globals(), "‚ùå REPORTS_DIR missing."
assert "ARTIFACTS_DIR" in globals(), "‚ùå ARTIFACTS_DIR missing."
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing."

#
SEC2_REPORTS_DIR   = REPORTS_DIR   / "section2"
SEC2_ARTIFACTS_DIR = ARTIFACTS_DIR / "section2"
SEC2_REPORTS_DIR.mkdir(parents=True, exist_ok=True)
SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# Target config
target_block = CONFIG.get("TARGET") or {}
encoded_target_col = target_block.get("COLUMN") or "Churn_flag"
raw_target_col = target_block.get("RAW_COLUMN") or "Churn"

# ID columns (from memory or config)
if "id_cols" in globals():
    id_cols_local = list(id_cols)
else:
    id_cols_local = CONFIG.get("ID_COLUMNS") or ["customerID"]

# Special numeric flags (from memory or CSV)
special_numeric_map_local = {}
if "special_numeric_map" in globals() and isinstance(special_numeric_map, dict):
    special_numeric_map_local = dict(special_numeric_map)

special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
if special_flags_path.exists():
    try:
        _sf = pd.read_csv(special_flags_path)
        if not _sf.empty and "column" in _sf.columns:
            for _, row in _sf.iterrows():
                col_name = row.get("column")
                role_val = row.get("role", "special_flag")
                if isinstance(col_name, str):
                    special_numeric_map_local[col_name] = role_val
    except Exception:
        pass

# Protected columns (from memory or artifacts)
protected_columns_local = set()
if "protected_columns" in globals():
    try:
        protected_columns_local.update(list(protected_columns))
    except Exception:
        pass

section2_artifacts_dir = ARTIFACTS_DIR / "section2"
section2_artifacts_dir.mkdir(parents=True, exist_ok=True)

for fname in [
    "protected_columns_2_1_3.json",
    "protected_columns_2_0_6.json",
]:
    p = section2_artifacts_dir / fname
    if p.exists():
        try:
            with p.open("r", encoding="utf-8") as f:
                payload = json.load(f)
            cols = payload.get("protected_columns") or []
            for c in cols:
                protected_columns_local.add(str(c))
            break
        except Exception:
            continue

# ID integrity (optional, for notes)
id_integrity_path = SEC2_REPORTS_DIR / "id_integrity_report.csv"
id_unique_ok = {}
if id_integrity_path.exists():
    try:
        _id_df = pd.read_csv(id_integrity_path)
        if "id_column" in _id_df.columns and "unique_ok" in _id_df.columns:
            for _, row in _id_df.iterrows():
                id_col_name = row.get("id_column")
                unique_flag = bool(row.get("unique_ok"))
                if isinstance(id_col_name, str):
                    id_unique_ok[id_col_name] = unique_flag
    except Exception:
        pass

numeric_detected = df.select_dtypes(include="number").columns.tolist()
categorical_detected = df.select_dtypes(include=["object", "category"]).columns.tolist()
bool_detected = df.select_dtypes(include=["bool", "boolean"]).columns.tolist()
datetime_detected = df.select_dtypes(include=["datetime"]).columns.tolist()

role_rows = []

for col in df.columns:
    s = df[col]
    dtype_str = str(s.dtype)
    n_unique = int(s.nunique(dropna=True))

    # Infer feature_group
    feature_group = "other"
    role = "feature"
    notes = []

    if col in id_cols_local:
        role = "id"
        feature_group = "id"
        if col in id_unique_ok and not bool(id_unique_ok[col]):
            notes.append("id_not_unique")
    elif col == encoded_target_col:
        role = "target"
        feature_group = "target"
    elif col == raw_target_col:
        role = "target_aux"
        feature_group = "target_aux"
    elif col in special_numeric_map_local or col in bool_detected:
        role = "feature"
        feature_group = "numeric_flag"
        if col in special_numeric_map_local:
            notes.append(f"special_numeric_flag:{special_numeric_map_local[col]}")
    elif col in numeric_detected:
        role = "feature"
        feature_group = "numeric_continuous"
    elif col in categorical_detected:
        # Low vs high card threshold
        low_card_threshold = 20
        if n_unique <= low_card_threshold:
            feature_group = "categorical_low_card"
        else:
            feature_group = "categorical_high_card"
    elif col in datetime_detected:
        feature_group = "datetime"
    else:
        feature_group = "other"

    is_protected = col in protected_columns_local

    if is_protected:
        notes.append("protected")

    role_rows.append(
        {
            "column": col,
            "role": role,
            "feature_group": feature_group,
            "dtype": dtype_str,
            "n_unique": n_unique,
            "is_protected": bool(is_protected),
            "notes": "; ".join(notes),
        }
    )

feature_roles_df = pd.DataFrame(
    role_rows,
    columns=[
        "column",
        "role",
        "feature_group",
        "dtype",
        "n_unique",
        "is_protected",
        "notes",
    ],
).sort_values(["feature_group", "role", "column"])

feature_roles_path = SEC2_REPORTS_DIR / "feature_roles.csv"
tmp = feature_roles_path.with_suffix(".tmp.csv")
feature_roles_df.to_csv(tmp, index=False)
os.replace(tmp, feature_roles_path)

print(f"üßæ Wrote feature roles ‚Üí {feature_roles_path}")

# Build feature groups payload for YAML
groups_dict = {}
for grp in sorted(feature_roles_df["feature_group"].dropna().unique()):
    cols_grp = (
        feature_roles_df.loc[feature_roles_df["feature_group"] == grp, "column"]
        .astype("string")
        .tolist()
    )
    groups_dict[grp] = cols_grp

feature_groups_payload = {
    "generated_at_utc": pd.Timestamp.utcnow().isoformat(timespec="seconds") + "Z",
    "protected_columns": sorted(list(protected_columns_local)),
    "groups": groups_dict,
    "source_sections": ["2.1.1", "2.1.2", "2.1.3", "2.1.4", "2.1.5", "2.1.6", "2.1.7"],
}

feature_groups_path = section2_artifacts_dir / "feature_groups_2_1_7.yaml"
tmp = feature_groups_path.with_suffix(".tmp.yaml")
with tmp.open("w", encoding="utf-8") as f:
    yaml.safe_dump(feature_groups_payload, f, sort_keys=False)
os.replace(tmp, feature_groups_path)

print(f"üßæ Wrote feature groups YAML ‚Üí {feature_groups_path}")

n_columns_217 = int(feature_roles_df.shape[0])
n_feature_groups_217 = int(feature_roles_df["feature_group"].nunique())
n_protected_217 = int(feature_roles_df["is_protected"].sum())
n_unassigned_217 = int((feature_roles_df["feature_group"] == "other").sum())

status_219 = "OK" if n_unassigned_217 == 0 else "WARN"

sec2_chunk_219 = pd.DataFrame(
    {
        "section": ["2.1.9"],
        "section_name": ["Column role classification & feature group registration"],
        "check": ["Feature roles & groups catalog"],
        "level": ["info"],
        "status": [status_217],
        "n_columns": [n_columns_217],
        "n_feature_groups": [n_feature_groups_217],
        "n_protected": [n_protected_217],
        "n_unassigned": [n_unassigned_217],
        "detail": [
            f"Roles: {feature_roles_path.name}; Groups YAML: {feature_groups_path.name}"
        ],
        "timestamp": [pd.Timestamp.now()],
    }
)

#
display(sec2_chunk_219)

append_sec2(sec2_chunk_219, SECTION2_REPORT_PATH)
SECTION2_APPEND_SECTIONS.add("2.1.7")

#
print(
    f"‚úÖ [2.1.9] Column roles & feature groups | "
    f"status={status_219} | "
    f"cols={n_columns_219}, groups={n_feature_groups_219}, "
    f"protected={n_protected_219}, unassigned={n_unassigned_219}"
)
```

># ANSWER:

Nice, this cell is *almost* perfect ‚Äî it‚Äôs just got the same copy-paste ghosts as 2.1.10. Functionally it works, but all the metadata says **2.1.7** instead of **2.1.9**, and the YAML filename is still `feature_groups_2_1_7.yaml`.

Here‚Äôs exactly what I‚Äôd change.

---

## 1. Fix the YAML metadata + timestamp

### Replace this block:

```python
feature_groups_payload = {
    "generated_at_utc": pd.Timestamp.utcnow().isoformat(timespec="seconds") + "Z",
    "protected_columns": sorted(list(protected_columns_local)),
    "groups": groups_dict,
    "source_sections": ["2.1.1", "2.1.2", "2.1.3", "2.1.4", "2.1.5", "2.1.6", "2.1.7"],
}

feature_groups_path = section2_artifacts_dir / "feature_groups_2_1_7.yaml"
```

with:

```python
from datetime import timezone  # at top of notebook if not already imported

feature_groups_payload = {
    "generated_at_utc": pd.Timestamp.now(timezone.utc).isoformat(timespec="seconds"),
    "protected_columns": sorted(list(protected_columns_local)),
    "groups": groups_dict,
    "source_sections": [
        "2.1.1", "2.1.2", "2.1.3", "2.1.4", "2.1.5", "2.1.6", "2.1.9"
    ],
}

feature_groups_path = section2_artifacts_dir / "feature_groups_2_1_9.yaml"
```

So:

* **Section list** now includes `"2.1.9"` instead of `"2.1.7"`.
* **Filename** matches the section: `feature_groups_2_1_9.yaml`.
* Timestamp is timezone-aware (`now(timezone.utc)`).

The rest of the YAML-writing block stays the same.

---

## 2. Fix the diagnostics section ID / status block

### Replace this whole stats + diagnostics chunk:

```python
n_columns_217 = int(feature_roles_df.shape[0])
n_feature_groups_217 = int(feature_roles_df["feature_group"].nunique())
n_protected_217 = int(feature_roles_df["is_protected"].sum())
n_unassigned_217 = int((feature_roles_df["feature_group"] == "other").sum())

status_217 = "OK" if n_unassigned_217 == 0 else "WARN"

sec2_chunk_217 = pd.DataFrame(
    {
        "section": ["2.1.7"],
        "section_name": ["Column role classification & feature group registration"],
        "check": ["Feature roles & groups catalog"],
        "level": ["info"],
        "status": [status_217],
        "n_columns": [n_columns_217],
        "n_feature_groups": [n_feature_groups_217],
        "n_protected": [n_protected_217],
        "n_unassigned": [n_unassigned_217],
        "detail": [
            f"Roles: {feature_roles_path.name}; Groups YAML: {feature_groups_path.name}"
        ],
        "timestamp": [pd.Timestamp.now()],
    }
)

#
display(sec2_chunk_217)

append_sec2(sec2_chunk_217, SECTION2_REPORT_PATH)
SECTION2_APPEND_SECTIONS.add("2.1.7")

#
print(
    f"‚úÖ [2.1.9] Column roles & feature groups | "
    f"status={status_217} | "
    f"cols={n_columns_217}, groups={n_feature_groups_217}, "
    f"protected={n_protected_217}, unassigned={n_unassigned_217}"
)
```

with a clean **2.1.9** version:

```python
n_columns_219 = int(feature_roles_df.shape[0])
n_feature_groups_219 = int(feature_roles_df["feature_group"].nunique())
n_protected_219 = int(feature_roles_df["is_protected"].sum())
n_unassigned_219 = int((feature_roles_df["feature_group"] == "other").sum())

status_219 = "OK" if n_unassigned_219 == 0 else "WARN"

sec2_chunk_219 = pd.DataFrame(
    {
        "section": ["2.1.9"],
        "section_name": ["Column role classification & feature group registration"],
        "check": ["Feature roles & groups catalog"],
        "level": ["info"],
        "status": [status_219],
        "n_columns": [n_columns_219],
        "n_feature_groups": [n_feature_groups_219],
        "n_protected": [n_protected_219],
        "n_unassigned": [n_unassigned_219],
        "detail": [
            f"Roles: {feature_roles_path.name}; Groups YAML: {feature_groups_path.name}"
        ],
        "timestamp": [pd.Timestamp.now(timezone.utc)],
    }
)

display(sec2_chunk_219)

append_sec2(sec2_chunk_219, SECTION2_REPORT_PATH)
SECTION2_APPEND_SECTIONS.add("2.1.9")

print(
    f"‚úÖ [2.1.9] Column roles & feature groups | "
    f"status={status_219} | "
    f"cols={n_columns_219}, groups={n_feature_groups_219}, "
    f"protected={n_protected_219}, unassigned={n_unassigned_219}"
)
```

Key fixes:

* `section` is `"2.1.9"` (not `"2.1.7"`).
* `SECTION2_APPEND_SECTIONS.add("2.1.9")` matches the section.
* Local variables use `_219` instead of `_217` to avoid confusion.
* Diagnostics timestamp uses `pd.Timestamp.now(timezone.utc)` to be timezone-aware like the other updated cells.

---

## 3. Sanity check after you patch

After you make those edits and re-run 2.1.9:

* `feature_roles.csv` path stays the same.
* New YAML at:
  `Level_3/resources/artifacts/section2/feature_groups_2_1_9.yaml`
* In your `section2_data_quality_*.csv`, you should now see a row with:

  * `section = "2.1.9"`
  * `section_name = "Column role classification & feature group registration"`

If you want, next step we can wire **2.2.x** pieces (like type detection and ID/protected registry) to *read* these feature groups instead of re-deriving anything.
