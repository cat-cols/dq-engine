
[1](https://www.geeksforgeeks.org/python/read-multiple-csv-files-into-separate-dataframes-in-python/)
[2](https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe)
[3](https://mungingdata.com/pandas/read-multiple-csv-pandas-dataframe/)
[4](https://www.reddit.com/r/rstats/comments/1b0idff/how_can_i_more_efficiently_use_read_csv_with/)
[5](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
[6](https://sparkbyexamples.com/pandas/pandas-read-multiple-csv-files/)
```python
# 2.3.7.4 | Rule Confidence Scores
print("\n2.3.7.4 ‚è±Ô∏è Rule confidence scores")

rule_rows_2374 = []

range_path = NUMERIC_DIR / "range_violation_report.csv"
outlier_path = NUMERIC_DIR / "outlier_report_iqr_z.csv"
ts_outliers_path = globals().get("time_series_outliers_path", NUMERIC_DIR / "time_series_outliers.csv")
corr_anom_path = NUMERIC_DIR / "correlation_anomalies.csv"

try:
    hard_types_cfg_2374 = C("NUMERIC.RULES.HARD_TYPES", ["range"]) or ["range"]
except Exception:
    hard_types_cfg_2374 = ["range"]

# --- Range rules -------------------------------------------------------
if range_path.exists():
    range_df_2374 = pd.read_csv(range_path)
else:
    range_df_2374 = pd.DataFrame()

for _, r in range_df_2374.iterrows():
    has_range_rule = bool(r.get("has_range_rule", False))
    if not has_range_rule:
        continue

    col = r.get("column")

    n_below_raw = r.get("n_below_min", 0)
    n_above_raw = r.get("n_above_max", 0)
    n_in_raw    = r.get("n_in_range", 0)

    n_below = float(0 if pd.isna(n_below_raw) else n_below_raw)
    n_above = float(0 if pd.isna(n_above_raw) else n_above_raw)
    n_in    = float(0 if pd.isna(n_in_raw)    else n_in_raw)

    total = n_below + n_above + n_in
    if pd.isna(total) or total <= 0:
        total = 1.0

    viol_rate = (n_below + n_above) / total

    if total >= 1000:
        size_factor = 1.0
    elif total >= 100:
        size_factor = 0.8
    else:
        size_factor = 0.6

    viol_factor = max(0.2, 1.0 - viol_rate * 4.0)
    confidence = float(min(1.0, size_factor * viol_factor))

    total_display = int(round(total))

    rule_rows_2374.append(
        {
            "feature":          col,
            "rule_type":        "range",
            "rule_id":          "range_minmax",
            "confidence_score": round(confidence, 3),
            "hard_vs_soft":     "hard" if "range" in hard_types_cfg_2374 else "soft",
            "notes":            f"viol_rate={round(viol_rate,4)}, total={total_display}",
        }
    )

# --- Outlier rules (IQR/Z) ---------------------------------------------
if outlier_path.exists():
    out_df_2374 = pd.read_csv(outlier_path)
else:
    out_df_2374 = pd.DataFrame()

for _, r in out_df_2374.iterrows():
    col = r.get("column")
    pct_iqr = float(r.get("pct_outliers_iqr", 0) or 0)
    pct_z   = float(r.get("pct_outliers_z", 0) or 0)

    max_pct = max(pct_iqr, pct_z)
    if max_pct < 1.0:
        sev_factor = 1.0
    elif max_pct < 5.0:
        sev_factor = 0.8
    else:
        sev_factor = 0.6

    confidence = float(sev_factor)

    rule_rows_2374.append(
        {
            "feature":          col,
            "rule_type":        "outlier_iqr_z",
            "rule_id":          "outlier_iqr_z",
            "confidence_score": round(confidence, 3),
            "hard_vs_soft":     "soft",
            "notes":            f"max_pct_outliers={round(max_pct,3)}",
        }
    )

# --- Temporal time-series rules ----------------------------------------
ts_df_2374 = pd.DataFrame()
if isinstance(ts_outliers_path, Path) and ts_outliers_path.exists() and ts_outliers_path.stat().st_size > 0:
    try:
        ts_df_2374 = pd.read_csv(ts_outliers_path)
    except pd.errors.EmptyDataError:
        ts_df_2374 = pd.DataFrame()

if not ts_df_2374.empty:
    if ts_df_2374["is_outlier"].dtype != bool:
        ts_df_2374["is_outlier"] = ts_df_2374["is_outlier"].astype(bool)

    total_buckets = ts_df_2374["time_bucket"].nunique()
    if total_buckets <= 0:
        total_buckets = 1

    for feat, g in ts_df_2374.groupby("feature"):
        n_out_feat = int(g[g["is_outlier"]].shape[0])
        rate_feat  = n_out_feat / total_buckets

        if rate_feat == 0:
            conf = 0.9
        elif rate_feat < 0.2:
            conf = 0.8
        else:
            conf = 0.6

        rule_rows_2374.append(
            {
                "feature":          feat,
                "rule_type":        "temporal_ts_outlier",
                "rule_id":          "ts_zscore",
                "confidence_score": round(float(conf), 3),
                "hard_vs_soft":     "soft",
                "notes":            f"outlier_bucket_rate={round(rate_feat,4)}",
            }
        )

# --- Correlation anomaly rules -----------------------------------------
corr_df_2374 = pd.DataFrame()
if corr_anom_path.exists() and corr_anom_path.stat().st_size > 0:
    try:
        corr_df_2374 = pd.read_csv(corr_anom_path)
    except pd.errors.EmptyDataError:
        corr_df_2374 = pd.DataFrame()

for _, r in corr_df_2374.iterrows():
    feat_i = r.get("feature_i")
    feat_j = r.get("feature_j")
    abs_delta = float(r.get("abs_delta", 0) or 0)

    if abs_delta < corr_delta_threshold_237:
        conf = 0.7
    elif abs_delta < 2 * corr_delta_threshold_237:
        conf = 0.8
    else:
        conf = 0.9

    rule_rows_2374.append(
        {
            "feature":          f"{feat_i}__{feat_j}",
            "rule_type":        "correlation",
            "rule_id":          r.get("time_window", ""),
            "confidence_score": round(float(conf), 3),
            "hard_vs_soft":     "soft",
            "notes":            f"abs_delta={round(abs_delta,4)}",
        }
    )

rule_conf_df_2374 = pd.DataFrame(rule_rows_2374)

rule_conf_path_2374 = NUMERIC_DIR / "rule_confidence_scores.csv"
tmp_2374 = rule_conf_path_2374.with_suffix(".tmp.csv")
rule_conf_df_2374.to_csv(tmp_2374, index=False)
os.replace(tmp_2374, rule_conf_path_2374)

print(f"üíæ Wrote rule confidence scores ‚Üí {rule_conf_path_2374}")
if not rule_conf_df_2374.empty:
    print("\nüìä 2.3.7.4 rule confidence scores (head):")
    display(rule_conf_df_2374.head(30))

n_rules_2374 = int(rule_conf_df_2374.shape[0])
n_hard_rules_2374 = int((rule_conf_df_2374["hard_vs_soft"] == "hard").sum()) if n_rules_2374 else 0
n_soft_rules_2374 = int((rule_conf_df_2374["hard_vs_soft"] == "soft").sum()) if n_rules_2374 else 0

status_2374 = "SKIP" if n_rules_2374 == 0 else "OK"

summary_2374 = pd.DataFrame([{
    "section":       "2.3.7.4",
    "section_name":  "Rule confidence scores",
    "check":         "Assign confidence & hardness to numeric rules",
    "level":         "info",
    "status":        status_2374,
    "n_rules":       n_rules_2374,
    "n_hard_rules":  n_hard_rules_2374,
    "n_soft_rules":  n_soft_rules_2374,
    "detail":        f"rule_confidence_scores.csv under {NUMERIC_DIR.name}",
    "timestamp":     pd.Timestamp.utcnow(),
}])

append_sec2(summary_2374, SECTION2_REPORT_PATH)
display(summary_2374)

# 2.3.8 üìö DQ rule catalog (joined with numeric profile)
print("\n2.3.8 üìö DQ rule catalog (joined with numeric profile)")

# 1) Load rule confidence artifact (safe)
rule_conf_path = NUMERIC_DIR / "rule_confidence_scores.csv"

# Safe load
try:
    if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:
        rule_conf_df = pd.read_csv(rule_conf_path)
    else:
        rule_conf_df = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {rule_conf_path} is empty or has no columns. Treating as no rules.")
    rule_conf_df = pd.DataFrame()

# --- 2) Load numeric profile (safe)
numeric_profile_path = NUMERIC_DIR / "numeric_profile.csv"

try:
    if numeric_profile_path.exists() and numeric_profile_path.stat().st_size > 0:
        numeric_profile_df = pd.read_csv(numeric_profile_path)
    else:
        numeric_profile_df = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {numeric_profile_path} is empty or has no columns. Skipping join.")
    numeric_profile_df = pd.DataFrame()

# Canonicalize rule_conf_df key
if not rule_conf_df.empty:
    rule_conf_df["feature"] = rule_conf_df["feature"].astype("string").str.strip()

# Canonicalize numeric_profile_df key
if not numeric_profile_df.empty:
    if "feature" in numeric_profile_df.columns:
        numeric_profile_df["feature"] = numeric_profile_df["feature"].astype("string").str.strip()
    elif "column" in numeric_profile_df.columns:
        numeric_profile_df["feature"] = numeric_profile_df["column"].astype("string").str.strip()
    else:
        numeric_profile_df["feature"] = pd.NA


# --- 3) Build DQ rule catalog ---------------------------------------------
if not rule_conf_df.empty and not numeric_profile_df.empty:
    if "column" in numeric_profile_df.columns:
        dq_rule_catalog_df = (
            numeric_profile_df
            .merge(rule_conf_df, on="feature", how="left")
            .sort_values(["feature", "rule_type", "rule_id"], na_position="last")
            .reset_index(drop=True)
        )
    else:
        print("‚ö†Ô∏è numeric_profile_df missing 'column' col; using rule_conf_df only.")
        dq_rule_catalog_df = rule_conf_df.copy()
else:
    dq_rule_catalog_df = rule_conf_df.copy()

dq_rule_catalog_path = NUMERIC_DIR / "dq_rule_catalog.csv"
tmp_238 = dq_rule_catalog_path.with_suffix(".tmp.csv")
dq_rule_catalog_df.to_csv(tmp_238, index=False)
os.replace(tmp_238, dq_rule_catalog_path)

print(f"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}")

if not dq_rule_catalog_df.empty:
    print("\nüìä Data Quality Rule Catalog (head):")
    cols_preview = [
        "feature",
        "role" if "role" in dq_rule_catalog_df.columns else "feature",
        "rule_type",
        "rule_id",
        "confidence_score",
        "hard_vs_soft",
    ]
    cols_preview = [c for c in cols_preview if c in dq_rule_catalog_df.columns]
    display(dq_rule_catalog_df[cols_preview].head(30))
else:
    print("   (no rules to catalog)")

# --- 4) ‚ÄúDQ rules‚Äù tab in your report (aggregated view)
dq_rules_path = NUMERIC_DIR / "dq_rule_catalog.csv"

try:
    if dq_rules_path.exists() and dq_rules_path.stat().st_size > 0:
        dq_rules_df = pd.read_csv(dq_rules_path)
    else:
        dq_rules_df = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {dq_rules_path} is empty or has no columns. Skipping aggregation.")
    dq_rules_df = pd.DataFrame()

if (
    not dq_rules_df.empty
    and {"feature", "rule_id", "confidence_score", "hard_vs_soft"}.issubset(dq_rules_df.columns)
):
    agg_rules_df = (
        dq_rules_df
        .groupby("feature", as_index=False)
        .agg(
            n_rules=("rule_id", "nunique"),
            max_hard_conf=(
                "confidence_score",
                lambda s: s[dq_rules_df.loc[s.index, "hard_vs_soft"] == "hard"].max()
            ),
            max_soft_conf=(
                "confidence_score",
                lambda s: s[dq_rules_df.loc[s.index, "hard_vs_soft"] == "soft"].max()
            ),
    )
        )
    print("\nüìä Aggregated DQ rules per feature (head):")
    display(agg_rules_df.head(20))
else:
    print("‚ö†Ô∏è Not enough columns / data to build aggregated DQ rules view.")

summary_2374 = pd.DataFrame([{
    "feature": "summary",
    "role": "summary",
    "n_rules": agg_rules_df["n_rules"].sum(),
    "max_hard_conf": agg_rules_df["max_hard_conf"].max(),
    "max_soft_conf": agg_rules_df["max_soft_conf"].max(),
}])
append_sec2(summary_2374, SECTION2_REPORT_PATH)

display(summary_2374)

2.3.8 üìö DQ rule catalog (joined with numeric profile)
üíæ Wrote DQ rule catalog ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/numeric_integrity/dq_rule_catalog.csv

üìä Data Quality Rule Catalog (head):
# 2.3.9 üßÆ Model readiness impact summary
print("\n2.3.9 üßÆ Model readiness impact summary")

# 1) Load artifacts [{(with guards (EmptyDataError-safe))}]

# Paths
numeric_profile_path_239 = NUMERIC_DIR / "numeric_profile_df.csv"
range_path_239          = NUMERIC_DIR / "range_violation_report.csv"
outlier_path_239        = NUMERIC_DIR / "outlier_report_iqr_z.csv"
time_series_outliers_path_239 = globals().get("time_series_outliers_path", NUMERIC_DIR / "time_series_outliers.csv")
corr_anom_path_239   = NUMERIC_DIR / "correlation_anomalies.csv"
integrity_path_239      = NUMERIC_DIR / "numeric_integrity_report.csv"  # may or may not exist
model_readiness_path_239 = NUMERIC_DIR / "model_readiness_report.csv"

# Rule confidence path( must match 2.3.7.4)
rule_conf_path_239 = NUMERIC_DIR / "rule_confidence_scores.csv"

# Check if rule_conf_path_239 exists
if not rule_conf_path_239.exists():
    print(f"‚ùå rule_confidence_scores.csv not found at: {rule_conf_path_239}")
    print("   Likely: NUMERIC_DIR differs between 2.3.7.4 and 2.3.9, or 2.3.7.4 did not run.")
    # Optional: try a fallback if you have multiple numeric dirs
    fallback = (REPORTS_DIR / "section2" / "numeric_integrity" / "rule_confidence_scores.csv")
    if fallback.exists():
        print(f"   ‚úÖ Found fallback: {fallback}")
        rule_conf_path_239 = fallback
    else:
        print(f"   ‚ö†Ô∏è No fallback found. Rule confidence will be empty.")

# numeric_profile_df
try:
    if numeric_profile_df.exists() and numeric_profile_path.stat().st_size > 0:
        numeric_profile_df = pd.read_csv(numeric_profile_path)
    else:
        print(f"‚ö†Ô∏è {numeric_profile_path} missing/empty ‚Äî using empty numeric_profile_df for 2.3.9.")
        numeric_profile_df = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {numeric_profile_path} is empty or has no columns. Using empty numeric_profile_df for 2.3.9.")
    numeric_profile_df = pd.DataFrame()

# range_df_239
try:
    if range_path_239.exists() and range_path_239.stat().st_size > 0:
        range_df_239 = pd.read_csv(range_path_239)
    else:
        print(f"‚ö†Ô∏è {range_path_239} missing/empty ‚Äî no range info for 2.3.9.")
        range_df_239 = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {range_path_239} is empty or has no columns. No range info for 2.3.9.")
    range_df_239 = pd.DataFrame()

# outlier_df_239
try:
    if outlier_path_239.exists() and outlier_path_239.stat().st_size > 0:
        outlier_df_239 = pd.read_csv(outlier_path_239)
    else:
        print(f"‚ö†Ô∏è {outlier_path_239} missing/empty ‚Äî no outlier info for 2.3.9.")
        outlier_df_239 = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {outlier_path_239} is empty or has no columns. No outlier info for 2.3.9.")
    outlier_df_239 = pd.DataFrame()

# # rule_conf_df_239
# try:
#     if rule_conf_path_2374.exists() and rule_conf_path_2374.stat().st_size > 0:
#         rule_conf_df_239 = pd.read_csv(rule_conf_path_2374)
#     else:
#         print(f"‚ö†Ô∏è {rule_conf_path_2374} missing/empty ‚Äî no rule confidence info for 2.3.9.")
#         rule_conf_df_239 = pd.DataFrame()
# except EmptyDataError:
#     print(f"‚ö†Ô∏è {rule_conf_path_2374} is empty or has no columns. No rule confidence info for 2.3.9.")
#     rule_conf_df_239 = pd.DataFrame()

# integrity_df_239
try:
    if integrity_path_239.exists() and integrity_path_239.stat().st_size > 0:
        integrity_df_239 = pd.read_csv(integrity_path_239)
    else:
        integrity_df_239 = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {integrity_path_239} is empty or has no columns. Using empty integrity_df_239.")
    integrity_df_239 = pd.DataFrame()



else:
    print(f"‚ö†Ô∏è {rule_conf_path} missing ‚Äî no rule confidence info for 2.3.9.")
    # Create an empty DataFrame
    rule_conf_df_239 = pd.DataFrame()
if integrity_path_239.exists():
    integrity_df_239 = pd.read_csv(integrity_path_239)
else:
    integrity_df_239 = pd.DataFrame()

# 2) Normalize each DF to have a 'feature' column where possible
def _ensure_feature_col(df):
    if df.empty:
        return df
    cols = df.columns.tolist()
    if "feature" in cols:
        df["feature"] = df["feature"].astype("string")
    elif "column" in cols:
        df["feature"] = df["column"].astype("string")
    return df

numeric_profile_df = _ensure_feature_col(numeric_profile_df)
range_df_239          = _ensure_feature_col(range_df_239)
outlier_df_239        = _ensure_feature_col(outlier_df_239)
rule_conf_df_239      = _ensure_feature_col(rule_conf_df_239)
integrity_df_239      = _ensure_feature_col(integrity_df_239)

# 3) Build a unified base_239 indexed by 'feature'
feature_series_list_239 = []

for df_tmp in [numeric_profile_df, range_df_239, outlier_df_239, rule_conf_df_239, integrity_df_239]:
    if (not df_tmp.empty) and ("feature" in df_tmp.columns):
        feature_series_list_239.append(df_tmp["feature"].astype("string"))

if feature_series_list_239:
    all_features_239 = (
        pd.concat(feature_series_list_239, ignore_index=True)
        .dropna()
        .astype("string")
        .unique()
        .tolist()
    )
    all_features_239 = sorted(all_features_239)
    base_239 = pd.DataFrame({"feature": all_features_239})
else:
    base_239 = pd.DataFrame(columns=["feature"])

# 4) Attach core profile info (role, feature_group, null_pct, etc.)
if (not numeric_profile_df.empty) and ("feature" in numeric_profile_df.columns):
    keep_cols_np_239 = [
        c for c in [
            "feature",
            "column",
            "role",
            "feature_group",
            "null_pct",
            "numeric_integrity_status",
        ] if c in numeric_profile_df.columns
    ]
    numeric_core_239 = numeric_profile_df[keep_cols_np_239].drop_duplicates(subset=["feature"])
    base_239 = base_239.merge(numeric_core_239, on="feature", how="left")

# If integrity report has extra status, prefer it
if (not integrity_df_239.empty) and ("feature" in integrity_df_239.columns):
    if "numeric_integrity_status" in integrity_df_239.columns:
        integ_core_239 = integrity_df_239[["feature", "numeric_integrity_status"]].drop_duplicates("feature")
        base_239 = base_239.merge(integ_core_239, on="feature", how="left", suffixes=("", "_from_integrity"))
        if "numeric_integrity_status_from_integrity" in base_239.columns:
            base_239["numeric_integrity_status"] = base_239["numeric_integrity_status_from_integrity"].combine_first(
                base_239.get("numeric_integrity_status")
            )
            base_239.drop(columns=["numeric_integrity_status_from_integrity"], inplace=True)
else:
    if "numeric_integrity_status" not in base_239.columns:
        base_239["numeric_integrity_status"] = None

# 5) Attach range & outlier diagnostics

# Range info
if (not range_df_239.empty) and ("feature" in range_df_239.columns):
    keep_cols_range_239 = [c for c in ["feature", "total_violation_pct", "range_status"] if c in range_df_239.columns]
    range_core_239 = range_df_239[keep_cols_range_239].drop_duplicates(subset=["feature"])
    base_239 = base_239.merge(range_core_239, on="feature", how="left")
else:
    base_239["total_violation_pct"] = None
    base_239["range_status"] = None

# Outlier info
if (not outlier_df_239.empty) and ("feature" in outlier_df_239.columns):
    for col_name in ["pct_outliers_iqr", "pct_outliers_z"]:
        if col_name not in outlier_df_239.columns:
            outlier_df_239[col_name] = 0.0
    outlier_core_239 = outlier_df_239[["feature", "pct_outliers_iqr", "pct_outliers_z"]].drop_duplicates("feature")
    base_239 = base_239.merge(outlier_core_239, on="feature", how="left")
else:
    base_239["pct_outliers_iqr"] = None
    base_239["pct_outliers_z"] = None

# 6) Aggregate rule confidence per feature
if (not rule_conf_df_239.empty) and ("feature" in rule_conf_df_239.columns):
    agg_rule_conf_239 = (
        rule_conf_df_239
        .groupby("feature", dropna=False)
        .agg(
            avg_confidence=("confidence_score", "mean"),
            n_rules=("rule_type", "count"),
            n_hard_rules=("hard_vs_soft", lambda s: (s == "hard").sum()),
            n_soft_rules=("hard_vs_soft", lambda s: (s == "soft").sum()),
        )
        .reset_index()
    )
    base_239 = base_239.merge(agg_rule_conf_239, on="feature", how="left")
else:
    base_239["avg_confidence"] = None
    base_239["n_rules"] = 0
    base_239["n_hard_rules"] = 0
    base_239["n_soft_rules"] = 0

# 7) Compute pct_rows_impacted & readiness_score
if "null_pct" in base_239.columns:
    null_pct_239 = base_239["null_pct"].fillna(0.0)
else:
    null_pct_239 = pd.Series(0.0, index=base_239.index)

range_violation_pct_239 = base_239["total_violation_pct"].fillna(0.0)
out_iqr_239 = base_239["pct_outliers_iqr"].fillna(0.0)
out_z_239 = base_239["pct_outliers_z"].fillna(0.0)

max_out_pct_239 = out_iqr_239.combine(out_z_239, func=lambda a, b: max(a, b))

pct_rows_impacted_239 = null_pct_239.combine(range_violation_pct_239, max)
pct_rows_impacted_239 = pct_rows_impacted_239.combine(max_out_pct_239, max)
base_239["pct_rows_impacted"] = pct_rows_impacted_239

# avg_conf_239 = base_239["avg_confidence"].fillna(0.8)
# replaced with:
base_239["avg_confidence"] = pd.to_numeric(base_239.get("avg_confidence"), errors="coerce")
avg_conf_239 = base_239["avg_confidence"].astype("Float64").fillna(0.8)

#
base_239["total_violation_pct"] = pd.to_numeric(base_239.get("total_violation_pct"), errors="coerce")
base_239["pct_outliers_iqr"] = pd.to_numeric(base_239.get("pct_outliers_iqr"), errors="coerce")
base_239["pct_outliers_z"] = pd.to_numeric(base_239.get("pct_outliers_z"), errors="coerce")
base_239["null_pct"] = pd.to_numeric(base_239.get("null_pct"), errors="coerce")

#
n_hard_239 = base_239["n_hard_rules"].fillna(0)

#
readiness_raw_239 = (
    1.0
    - (pct_rows_impacted_239 / 100.0) * 0.7
    - (n_hard_239 > 0).astype(float) * 0.05
    - (avg_conf_239 < 0.7).astype(float) * 0.05
)
base_239["readiness_score"] = readiness_raw_239.clip(0.0, 1.0)

base_239["hard_rule_violations"] = (
    (range_violation_pct_239 > 0.0) & (n_hard_239 > 0)
).astype(bool)

# 8) Final column ordering + write artifact
model_readiness_cols = [
    col for col in [
        "feature",
        "column" if "column" in base_239.columns else None,
        "role" if "role" in base_239.columns else None,
        "feature_group" if "feature_group" in base_239.columns else None,
        "numeric_integrity_status" if "numeric_integrity_status" in base_239.columns else None,
        "pct_rows_impacted",
        "readiness_score",
        "n_rules",
        "n_hard_rules",
        "n_soft_rules",
        "avg_confidence",
        "hard_rule_violations",
    ] if col is not None
]

model_readiness_df_239 = base_239[model_readiness_cols].copy()

tmp_239 = model_readiness_path_239.with_suffix(".tmp.csv")
model_readiness_df_239.to_csv(tmp_239, index=False)
os.replace(tmp_239, model_readiness_path_239)

print(f"üíæ Wrote model readiness report ‚Üí {model_readiness_path_239}")
if not model_readiness_df_239.empty:
    print("\nüìä 2.3.9 model readiness report (head):")
    display(model_readiness_df_239.head(20))
else:
    print("   (no features found for model readiness)")

# 9) Section 2.3.9 summary row
n_features_239 = int(model_readiness_df_239.shape[0])
avg_readiness_239 = float(model_readiness_df_239["readiness_score"].mean()) if n_features_239 else None
n_low_readiness_239 = int((model_readiness_df_239["readiness_score"] < 0.6).sum()) if n_features_239 else 0

if n_features_239 == 0:
    status_239 = "SKIP"
else:
    frac_low = n_low_readiness_239 / max(1, n_features_239)
    status_239 = "OK" if frac_low <= 0.3 else "WARN"

summary_239 = pd.DataFrame([{
    "section":          "2.3.9",
    "section_name":     "Model readiness impact summary",
    "check":            "Per-feature readiness scores based on numeric quality",
    "level":            "info",
    "status":           status_239,
    "n_features":       int(n_features_239),
    "avg_readiness":    float(avg_readiness_239) if avg_readiness_239 is not None else None,
    "n_low_readiness":  int(n_low_readiness_239),
    "detail":           "model_readiness_report.csv",
    "timestamp":        pd.Timestamp.utcnow(),
}])

append_sec2(summary_239, SECTION2_REPORT_PATH)
display(summary_239)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/dn/wtcqyjbd1hn63s40771sdddm0000gn/T/ipykernel_6727/3793643747.py in ?()
     33         numeric_profile_df = pd.read_csv(numeric_profile_path)
     34     else:
     35         print(f"‚ö†Ô∏è {numeric_profile_path} missing/empty ‚Äî using empty numeric_profile_df for 2.3.9.")
     36         numeric_profile_df = pd.DataFrame()
---> 37 except EmptyDataError:
     38     print(f"‚ö†Ô∏è {numeric_profile_path} is empty or has no columns. Using empty numeric_profile_df for 2.3.9.")
     39     numeric_profile_df = pd.DataFrame()
     40 

~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/generic.py in ?(self, name)
   6317             and name not in self._accessors
   6318             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   6319         ):
   6320             return self[name]
-> 6321         return object.__getattribute__(self, name)

AttributeError: 'DataFrame' object has no attribute 'exists'
```

The error comes from a small typo in the 2.3.9 loading block: you are calling `.exists()` on a DataFrame instead of on the path object.  

## Exact line to fix

In 2.3.9 you currently have:

```python
# numeric_profile_df
try:
    if numeric_profile_df.exists() and numeric_profile_path.stat().st_size > 0:
        numeric_profile_df = pd.read_csv(numeric_profile_path)
    else:
        print(f"‚ö†Ô∏è {numeric_profile_path} missing/empty ‚Äî using empty numeric_profile_df for 2.3.9.")
        numeric_profile_df = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {numeric_profile_path} is empty or has no columns. Using empty numeric_profile_df for 2.3.9.")
    numeric_profile_df = pd.DataFrame()
```

`numeric_profile_df` is a DataFrame, so `numeric_profile_df.exists()` raises `AttributeError`. It should be the **path** object that you defined above: `numeric_profile_path_239`.  

## Corrected block

Change that whole block to use `numeric_profile_path_239` consistently:

```python
# numeric_profile_df_239
try:
    if numeric_profile_path_239.exists() and numeric_profile_path_239.stat().st_size > 0:
        numeric_profile_df = pd.read_csv(numeric_profile_path_239)
    else:
        print(f"‚ö†Ô∏è {numeric_profile_path_239} missing/empty ‚Äî using empty numeric_profile_df for 2.3.9.")
        numeric_profile_df = pd.DataFrame()
except EmptyDataError:
    print(f"‚ö†Ô∏è {numeric_profile_path_239} is empty or has no columns. Using empty numeric_profile_df for 2.3.9.")
    numeric_profile_df = pd.DataFrame()
```

Key points:  
- Use `numeric_profile_path_239.exists()` (a Path) instead of `numeric_profile_df.exists()`.  
- Use the same `_239`-suffixed variable for the size check and log messages so the path is clear and consistent.  

After this change, 2.3.9 should run without the `AttributeError` and will properly load `numeric_profile_df` if the CSV is present.