># Q: missing ANOMALY_CONTEXT_PATH

># 2.5 | ğŸ“ŠğŸ“ˆ Logic Scoring & Health Layer

```python
# PART D | 2.5.12â€“2.5.15 | ğŸ“ŠğŸ“ˆ Logic Scoring & Health Layer
print("PART D | 2.5.12â€“2.5.15 | ğŸ“ŠğŸ“ˆ Logic Scoring & Health Layer")

# Assumes:
#   - df is loaded
#   - pandas as pd, numpy as np, os, Path, json imported
#   - CONFIG and/or C() available
#   - _append_sec2 available OR fallback summary-append logic
#   - SECTION2_REPORT_PATH and sec2_reports_dir resolved similarly to 2.5.7â€“2.5.11

# Safety: df present (even though these sections mostly use artifacts)
if "df" not in globals():
    raise RuntimeError("âŒ df not found in globals(); cannot run 2.5.12â€“2.5.15")

# 2) Load anomaly context (from 2.5.11)
if "ANOMALY_CONTEXT_PATH" not in globals():
    raise RuntimeError("âŒ Run strap_paths() first (missing ANOMALY_CONTEXT_PATH).")

if ANOMALY_CONTEXT_PATH.exists():
    try:
        anomaly_df_2512 = pd.read_parquet(ANOMALY_CONTEXT_PATH)
        print(f"   âœ… Loaded anomaly context from {ANOMALY_CONTEXT_PATH} | rows={len(anomaly_df_2512)}")
    except Exception as e:
        print(f"   âš ï¸ Could not read anomaly context parquet: {e}")
        anomaly_df_2512 = pd.DataFrame()
else:
    print(f"   â„¹ï¸ Anomaly context parquet not found at {ANOMALY_CONTEXT_PATH}")
    anomaly_df_2512 = pd.DataFrame()


n_rows_25D = int(df.shape[0])

# 2.5.12 | Row-level anomaly aggregation & scoring | v1 no setup block
print("\n2.5.12 ğŸ“Š Row-level anomaly aggregation & scoring")

# 2) Load anomaly context (from 2.5.11)
if ANOMALY_CONTEXT_PATH.exists():
    try:
        anomaly_df_2512 = pd.read_parquet(ANOMALY_CONTEXT_PATH)
        print(f"   âœ… Loaded anomaly context from {ANOMALY_CONTEXT_PATH} | rows={len(anomaly_df_2512)}")
    except Exception as e:
        print(f"   âš ï¸ Could not read logic_anomaly_context.parquet: {e}")
        anomaly_df_2512 = pd.DataFrame()
else:
    print(f"   â„¹ï¸ logic_anomaly_context.parquet not found at {ANOMALY_CONTEXT_PATH}")
    anomaly_df_2512 = pd.DataFrame()

# 3) Resolve scoring config
severity_weights_2512 = {"info": 0.0, "ok": 0.0, "warn": 1.0, "fail": 3.0}
type_weights_2512 = {}
default_severity_weight_2512 = 0.5
default_type_weight_2512 = 1.0
max_score_cap_2512 = None

anomaly_scores_cfg_2512 = {}
if "CONFIG" in globals() and isinstance(CONFIG, dict):
    _ascfg_2512 = CONFIG.get("ANOMALY_SCORES", {})
    if isinstance(_ascfg_2512, dict):
        anomaly_scores_cfg_2512 = _ascfg_2512

if anomaly_scores_cfg_2512:
    print("   ğŸ”§ ANOMALY_SCORES config resolved.")
    _sev_cfg = anomaly_scores_cfg_2512.get("SEVERITY_WEIGHTS", {})
    if isinstance(_sev_cfg, dict) and _sev_cfg:
        severity_weights_2512 = {}
        for _k, _v in _sev_cfg.items():
            try:
                severity_weights_2512[str(_k).lower()] = float(_v)
            except Exception:
                continue

    _type_cfg = anomaly_scores_cfg_2512.get("TYPE_WEIGHTS", {})
    if isinstance(_type_cfg, dict) and _type_cfg:
        type_weights_2512 = {}
        for _k, _v in _type_cfg.items():
            try:
                type_weights_2512[str(_k)] = float(_v)
            except Exception:
                continue

    if "DEFAULT_SEVERITY_WEIGHT" in anomaly_scores_cfg_2512:
        try:
            default_severity_weight_2512 = float(anomaly_scores_cfg_2512["DEFAULT_SEVERITY_WEIGHT"])
        except Exception:
            pass

    if "DEFAULT_TYPE_WEIGHT" in anomaly_scores_cfg_2512:
        try:
            default_type_weight_2512 = float(anomaly_scores_cfg_2512["DEFAULT_TYPE_WEIGHT"])
        except Exception:
            pass

    if "MAX_SCORE_CAP" in anomaly_scores_cfg_2512:
        try:
            max_score_cap_2512 = float(anomaly_scores_cfg_2512["MAX_SCORE_CAP"])
        except Exception:
            max_score_cap_2512 = None

print(f"   â€¢ Severity weights: {severity_weights_2512}")
print(f"   â€¢ Default severity weight: {default_severity_weight_2512}")
print(f"   â€¢ Default type weight: {default_type_weight_2512}")
print(f"   â€¢ Type weights (if any): {type_weights_2512}")
print(f"   â€¢ Score cap: {max_score_cap_2512 if max_score_cap_2512 is not None else '[no cap]'}")

# 4) Compute per-row scores
row_scores_df_2512 = pd.DataFrame()
n_rows_scored_2512 = 0

if not anomaly_df_2512.empty:
    # Ensure expected columns exist
    for _col in ["row_key", "severity", "anomaly_type"]:
        if _col not in anomaly_df_2512.columns:
            anomaly_df_2512[_col] = np.nan

    # Normalize severity + type
    _sev_series_2512 = anomaly_df_2512["severity"].astype("string").str.lower().fillna("info")
    _atype_series_2512 = anomaly_df_2512["anomaly_type"].astype("string").fillna("")

    # Map to weights
    _sev_weight_2512 = _sev_series_2512.map(severity_weights_2512).fillna(default_severity_weight_2512)

    def _map_type_weight_inline_2512(_atype_val):
        _atype_key = str(_atype_val)
        if _atype_key in type_weights_2512:
            return type_weights_2512[_atype_key]
        return default_type_weight_2512

    _type_weight_2512 = _atype_series_2512.apply(_map_type_weight_inline_2512)

    anomaly_df_2512 = anomaly_df_2512.copy()
    anomaly_df_2512["severity_weight"] = _sev_weight_2512
    anomaly_df_2512["type_weight"] = _type_weight_2512
    anomaly_df_2512["row_score_contribution"] = anomaly_df_2512["severity_weight"] * anomaly_df_2512["type_weight"]

    # Rank severities for max_severity computation
    sev_rank_2512 = {"info": 0, "ok": 0, "warn": 1, "fail": 2}
    anomaly_df_2512["severity_rank"] = _sev_series_2512.map(sev_rank_2512).fillna(0).astype(int)

    # Aggregate per row_key
    if "row_key" in anomaly_df_2512.columns:
        _grp_2512 = anomaly_df_2512.groupby("row_key", dropna=False)

        row_scores_df_2512 = _grp_2512.agg(
            n_anomalies=("row_score_contribution", "size"),
            n_warn=("severity", lambda x: (x.astype("string").str.lower() == "warn").sum()),
            n_fail=("severity", lambda x: (x.astype("string").str.lower() == "fail").sum()),
            max_severity_rank=("severity_rank", "max"),
            total_score=("row_score_contribution", "sum"),
        ).reset_index()

        # Derive max_severity label back from rank
        _rank_to_label_2512 = {v: k for k, v in sev_rank_2512.items()}
        row_scores_df_2512["max_severity"] = row_scores_df_2512["max_severity_rank"].map(_rank_to_label_2512).fillna("info")

        # Cap score if configured
        if max_score_cap_2512 is not None:
            row_scores_df_2512["total_score_capped"] = row_scores_df_2512["total_score"].clip(upper=max_score_cap_2512)
        else:
            row_scores_df_2512["total_score_capped"] = row_scores_df_2512["total_score"]

        # Simple normalized score [0,1] based on global max (safe if all zero)
        max_score_observed_2512 = float(row_scores_df_2512["total_score_capped"].max()) if not row_scores_df_2512.empty else 0.0
        if max_score_observed_2512 > 0.0:
            row_scores_df_2512["score_normalized"] = row_scores_df_2512["total_score_capped"] / max_score_observed_2512
        else:
            row_scores_df_2512["score_normalized"] = 0.0

        n_rows_scored_2512 = int(len(row_scores_df_2512))
    else:
        print("   â„¹ï¸ 'row_key' column missing in anomaly context; cannot aggregate row-level scores.")

# 5) Persist row_anomaly_scores.parquet / row_anomaly_scores.csv
row_scores_parquet_path_2512 = ANOMALY_CONTEXT_PATH / "row_anomaly_scores.parquet"
row_scores_csv_path_2512 = ANOMALY_CONTEXT_PATH / "row_anomaly_scores.csv"
row_scores_csv_tmp_2512 = row_scores_csv_path_2512.with_suffix(".tmp.csv")

if not row_scores_df_2512.empty:
    try:
        row_scores_df_2512.to_parquet(row_scores_parquet_path_2512, index=False)
    except Exception as e:
        print(f"   âš ï¸ Could not write row_anomaly_scores.parquet: {e}")

    try:
        row_scores_df_2512.to_csv(row_scores_csv_tmp_2512, index=False)
        os.replace(row_scores_csv_tmp_2512, row_scores_csv_path_2512)
    except Exception:
        if row_scores_csv_tmp_2512.exists():
            row_scores_csv_tmp_2512.unlink()
else:
    # Write empty parquet to document that index is empty
    try:
        row_scores_df_2512.to_parquet(row_scores_parquet_path_2512, index=False)
    except Exception:
        pass

# 6) Summary row
status_2512 = "INFO"
if not anomaly_df_2512.empty and n_rows_scored_2512 > 0:
    status_2512 = "OK"

summary_2512 = pd.DataFrame([{
    "section": "2.5.12",
    "section_name": "Row-level anomaly aggregation & scoring",
    "check": "Aggregate anomaly context rows into per-row scores for downstream explainability",
    "level": "info",
    "status": status_2512,
    "n_anomaly_rows": int(len(anomaly_df_2512)),
    "n_rows_scored": int(n_rows_scored_2512),
    "detail": "row_anomaly_scores.parquet, row_anomaly_scores.csv",
    "timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_2512, SECTION2_REPORT_PATH)
# 7) Console UX
print(f"ğŸ’¾ 2.5.12 row_anomaly_scores.parquet â†’ {row_scores_parquet_path_2512}")
print(f"ğŸ’¾ 2.5.12 row_anomaly_scores.csv â†’ {row_scores_csv_path_2512}")
print(f"   Anomaly rows in context: {int(len(anomaly_df_2512))}")
print(f"   Distinct row_keys scored: {n_rows_scored_2512}")

if not row_scores_df_2512.empty:
    print("   ğŸ“‹ Row score preview (top 10):")
    display(
        row_scores_df_2512.loc[
            :,
            [
                "row_key", "n_anomalies", "n_warn", "n_fail",
                "max_severity", "total_score", "total_score_capped", "score_normalized",
            ]
        ].head(10)
    )
else:
    print("   â„¹ï¸ No row-level scores computed (empty anomaly context or missing row_key).")

display(summary_2512)

# 2.5.13 | Column-level anomaly density & impact profile | v1 no setup block
print("\n2.5.13 ğŸ“Š Column-level anomaly density & impact profile")

if "row_scores_df_2512" in globals():
    # We can reuse anomaly_df_2512 if desired, but for robustness we reload from disk if needed
    anomaly_df_2513 = anomaly_df_2512.copy()
else:
    if anomaly_path_2513.exists():
        try:
            anomaly_df_2513 = pd.read_parquet(anomaly_path_2513)
            print(f"   âœ… Loaded anomaly context from {anomaly_path_2513} | rows={len(anomaly_df_2513)}")
        except Exception as e:
            print(f"   âš ï¸ Could not read logic_anomaly_context.parquet: {e}")
            anomaly_df_2513 = pd.DataFrame()
    else:
        print(f"   â„¹ï¸ logic_anomaly_context.parquet not found at {anomaly_path_2513}")
        anomaly_df_2513 = pd.DataFrame()

# 2) Column-level aggregation
col_profile_df_2513 = pd.DataFrame()
n_columns_with_anomalies_2513 = 0

if not anomaly_df_2513.empty:
    # Ensure required columns
    for _col in ["row_key", "severity", "anomaly_type", "feature_names"]:
        if _col not in anomaly_df_2513.columns:
            anomaly_df_2513[_col] = np.nan

    # Expand feature_names (comma-separated) into long form
    _rows_2513 = []
    for _idx_2513, _row_2513 in anomaly_df_2513.iterrows():
        _features_str = str(_row_2513.get("feature_names", "") or "")
        if not _features_str:
            continue
        _features_list = [f.strip() for f in _features_str.split(",") if f.strip()]
        if not _features_list:
            continue
        _row_key_2513 = _row_2513.get("row_key", _idx_2513)
        _severity_2513 = str(_row_2513.get("severity", "info")).lower()
        _atype_2513 = str(_row_2513.get("anomaly_type", "") or "")
        _mag_2513 = _row_2513.get("magnitude", np.nan)

        for _feat_2513 in _features_list:
            _rows_2513.append(
                {
                    "column_name": _feat_2513,
                    "row_key": _row_key_2513,
                    "severity": _severity_2513,
                    "anomaly_type": _atype_2513,
                    "magnitude": _mag_2513,
                }
            )

    col_long_df_2513 = pd.DataFrame(_rows_2513)

    if not col_long_df_2513.empty:
        # Severity ranking reused
        sev_rank_2513 = {"info": 0, "ok": 0, "warn": 1, "fail": 2}
        col_long_df_2513["severity_rank"] = col_long_df_2513["severity"].map(sev_rank_2513).fillna(0).astype(int)

        _grp_col_2513 = col_long_df_2513.groupby("column_name", dropna=False)

        col_profile_df_2513 = _grp_col_2513.agg(
            n_anomalies=("row_key", "size"),
            n_rows_touched=("row_key", "nunique"),
            n_warn=("severity", lambda x: (x == "warn").sum()),
            n_fail=("severity", lambda x: (x == "fail").sum()),
            max_severity_rank=("severity_rank", "max"),
            mean_magnitude=("magnitude", "mean"),
        ).reset_index()

        # Convert max severity label back
        _rank_to_label_2513 = {v: k for k, v in sev_rank_2513.items()}
        col_profile_df_2513["max_severity"] = col_profile_df_2513["max_severity_rank"].map(_rank_to_label_2513).fillna("info")

        # Density = anomalies per distinct row
        col_profile_df_2513["anomaly_density_per_row"] = np.where(
            col_profile_df_2513["n_rows_touched"] > 0,
            col_profile_df_2513["n_anomalies"] / col_profile_df_2513["n_rows_touched"],
            np.nan,
        )

        # Simple column risk score: combine density + severity
        col_profile_df_2513["risk_score"] = (
            col_profile_df_2513["anomaly_density_per_row"].fillna(0.0)
            * (1.0 + col_profile_df_2513["max_severity_rank"])
        )

        n_columns_with_anomalies_2513 = int(len(col_profile_df_2513))

# 3) Persist column_anomaly_profile.csv
col_profile_path_2513 = section2_reports_dir_2513 / "column_anomaly_profile.csv"
col_profile_tmp_2513 = col_profile_path_2513.with_suffix(".tmp.csv")

if not col_profile_df_2513.empty:
    try:
        col_profile_df_2513.to_csv(col_profile_tmp_2513, index=False)
        os.replace(col_profile_tmp_2513, col_profile_path_2513)
    except Exception:
        if col_profile_tmp_2513.exists():
            col_profile_tmp_2513.unlink()

# 4) Summary row
status_2513 = "INFO"
if n_columns_with_anomalies_2513 > 0:
    status_2513 = "OK"

summary_2513 = pd.DataFrame([{
    "section": "2.5.13",
    "section_name": "Column-level anomaly density & impact profile",
    "check": "Summarize logic anomalies per column to identify fragile features",
    "level": "info",
    "status": status_2513,
    "n_anomaly_rows": int(len(anomaly_df_2513)),
    "n_columns_with_anomalies": int(n_columns_with_anomalies_2513),
    "detail": "column_anomaly_profile.csv",
    "timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_2513, SECTION2_REPORT_PATH)

# 5) Console UX
print(f"ğŸ’¾ 2.5.13 column_anomaly_profile.csv â†’ {col_profile_path_2513}")
print(f"   Anomaly rows in context: {int(len(anomaly_df_2513))}")
print(f"   Columns with any anomalies: {n_columns_with_anomalies_2513}")

if not col_profile_df_2513.empty:
    print("   ğŸ“‹ Column anomaly profile preview (top 15 by risk_score):")
    display(
        col_profile_df_2513.sort_values("risk_score", ascending=False).loc[
            :,
            [
                "column_name",
                "n_anomalies",
                "n_rows_touched",
                "n_warn",
                "n_fail",
                "max_severity",
                "anomaly_density_per_row",
                "risk_score",
            ],
        ].head(15)
    )
else:
    print("   â„¹ï¸ No column-level anomaly profile computed (no feature_names in anomaly context).")

display(col_profile_df_2513)
display(summary_2513)
```

># A:

Yep â€” youâ€™re super close. The crash and a couple downstream weirdnesses are coming from **path semantics** + **variable naming drift**.

## Why you got `missing ANOMALY_CONTEXT_PATH`

Your earlier cell assumes a bootstrap exported `ANOMALY_CONTEXT_PATH` into `globals()`. It didnâ€™t, so the guard tripped.

### Fix pattern (do this once at top of Part D)

Instead of hard-failing, **resolve it from the same canonical roots you already use**:

* If you have `SEC2_REPORT_DIRS["2.5"]` or `sec2_reports_dir`, use that
* Else fall back to `REPORTS_DIR / "section2"` (your established convention)

Hereâ€™s a robust â€œno-excusesâ€ resolver:

```python
from pathlib import Path

# --- Part D Path Resolver (robust, no strap dependency) ---
def _resolve_anomaly_context_path():
    # 1) Best: explicit global
    if "ANOMALY_CONTEXT_PATH" in globals() and globals()["ANOMALY_CONTEXT_PATH"]:
        return Path(globals()["ANOMALY_CONTEXT_PATH"]).resolve()

    # 2) If your Section 2 dirs dict exists
    if "SEC2_REPORT_DIRS" in globals() and isinstance(SEC2_REPORT_DIRS, dict):
        # Preferred: section 2.5 subdir where youâ€™ve been putting logic outputs
        base = Path(SEC2_REPORT_DIRS.get("2.5", "")).resolve()
        if str(base) != ".":
            return (base / "logic_anomaly_context.parquet").resolve()

    # 3) If you use a generic section2 reports root
    if "sec2_reports_dir" in globals() and globals()["sec2_reports_dir"]:
        base = Path(globals()["sec2_reports_dir"]).resolve()
        return (base / "logic_anomaly_context.parquet").resolve()

    if "REPORTS_DIR" in globals() and globals()["REPORTS_DIR"]:
        base = (Path(globals()["REPORTS_DIR"]).resolve() / "section2")
        return (base / "logic_anomaly_context.parquet").resolve()

    # 4) Final fallback
    return Path("section2_reports/logic_anomaly_context.parquet").resolve()

ANOMALY_CONTEXT_PATH = _resolve_anomaly_context_path()
print("âœ… ANOMALY_CONTEXT_PATH =", ANOMALY_CONTEXT_PATH)
```

**Key point:** `ANOMALY_CONTEXT_PATH` should be the **file path** to the parquet, not a directory.

---

## Two big bugs in your current code

### 1) Youâ€™re treating a *file path* as a *directory*

You do:

```python
row_scores_parquet_path_2512 = ANOMALY_CONTEXT_PATH / "row_anomaly_scores.parquet"
```

If `ANOMALY_CONTEXT_PATH` is `.../logic_anomaly_context.parquet`, then that becomes:
`.../logic_anomaly_context.parquet/row_anomaly_scores.parquet` â† invalid

âœ… Fix: use `ANOMALY_CONTEXT_PATH.parent` (the directory).

```python
ANOMALY_DIR = ANOMALY_CONTEXT_PATH.parent
ANOMALY_DIR.mkdir(parents=True, exist_ok=True)

row_scores_parquet_path_2512 = (ANOMALY_DIR / "row_anomaly_scores.parquet").resolve()
row_scores_csv_path_2512     = (ANOMALY_DIR / "row_anomaly_scores.csv").resolve()
```

### 2) You reference `anomaly_path_2513` but never define it (in the v1)

Your 2.5.13 fallback does:

```python
if anomaly_path_2513.exists():
```

â€¦but `anomaly_path_2513` isnâ€™t guaranteed to exist.

âœ… Fix: use `ANOMALY_CONTEXT_PATH` consistently.

```python
anomaly_path_2513 = ANOMALY_CONTEXT_PATH
```

---

## Drop-in corrected skeleton for 2.5.12â€“2.5.13 (keeps your logic)

This keeps your structure, but fixes the path + missing-var issues.

```python
import os, json
import numpy as np
import pandas as pd
from pathlib import Path

print("PART D | 2.5.12â€“2.5.15 | ğŸ“ŠğŸ“ˆ Logic Scoring & Health Layer")

# Safety: df present
if "df" not in globals():
    raise RuntimeError("âŒ df not found in globals(); cannot run 2.5.12â€“2.5.15")

# --- Resolve anomaly context file path robustly ---
def _resolve_anomaly_context_path():
    if "ANOMALY_CONTEXT_PATH" in globals() and globals()["ANOMALY_CONTEXT_PATH"]:
        return Path(globals()["ANOMALY_CONTEXT_PATH"]).resolve()

    if "SEC2_REPORT_DIRS" in globals() and isinstance(SEC2_REPORT_DIRS, dict):
        base = Path(SEC2_REPORT_DIRS.get("2.5", "")).resolve()
        if str(base) != ".":
            return (base / "logic_anomaly_context.parquet").resolve()

    if "sec2_reports_dir" in globals() and globals()["sec2_reports_dir"]:
        base = Path(globals()["sec2_reports_dir"]).resolve()
        return (base / "logic_anomaly_context.parquet").resolve()

    if "REPORTS_DIR" in globals() and globals()["REPORTS_DIR"]:
        base = (Path(globals()["REPORTS_DIR"]).resolve() / "section2")
        return (base / "logic_anomaly_context.parquet").resolve()

    return Path("section2_reports/logic_anomaly_context.parquet").resolve()

ANOMALY_CONTEXT_PATH = _resolve_anomaly_context_path()
ANOMALY_DIR = ANOMALY_CONTEXT_PATH.parent
ANOMALY_DIR.mkdir(parents=True, exist_ok=True)

print("âœ… ANOMALY_CONTEXT_PATH =", ANOMALY_CONTEXT_PATH)

n_rows_25D = int(df.shape[0])

# -------------------------
# 2.5.12 | Row-level scoring
# -------------------------
print("\n2.5.12 ğŸ“Š Row-level anomaly aggregation & scoring")

if ANOMALY_CONTEXT_PATH.exists():
    try:
        anomaly_df_2512 = pd.read_parquet(ANOMALY_CONTEXT_PATH)
        print(f"   âœ… Loaded anomaly context | rows={len(anomaly_df_2512)}")
    except Exception as e:
        print(f"   âš ï¸ Could not read anomaly context parquet: {e}")
        anomaly_df_2512 = pd.DataFrame()
else:
    print(f"   â„¹ï¸ Anomaly context parquet not found at {ANOMALY_CONTEXT_PATH}")
    anomaly_df_2512 = pd.DataFrame()

# scoring config (your current)
severity_weights_2512 = {"info": 0.0, "ok": 0.0, "warn": 1.0, "fail": 3.0}
type_weights_2512 = {}
default_severity_weight_2512 = 0.5
default_type_weight_2512 = 1.0
max_score_cap_2512 = None

# Optional config hook
anomaly_scores_cfg_2512 = {}
if "CONFIG" in globals() and isinstance(CONFIG, dict):
    _ascfg = CONFIG.get("ANOMALY_SCORES", {})
    if isinstance(_ascfg, dict):
        anomaly_scores_cfg_2512 = _ascfg

if anomaly_scores_cfg_2512:
    _sev_cfg = anomaly_scores_cfg_2512.get("SEVERITY_WEIGHTS", {})
    if isinstance(_sev_cfg, dict) and _sev_cfg:
        severity_weights_2512 = {str(k).lower(): float(v) for k, v in _sev_cfg.items() if v is not None}

    _type_cfg = anomaly_scores_cfg_2512.get("TYPE_WEIGHTS", {})
    if isinstance(_type_cfg, dict) and _type_cfg:
        type_weights_2512 = {str(k): float(v) for k, v in _type_cfg.items() if v is not None}

    if "DEFAULT_SEVERITY_WEIGHT" in anomaly_scores_cfg_2512:
        default_severity_weight_2512 = float(anomaly_scores_cfg_2512["DEFAULT_SEVERITY_WEIGHT"])
    if "DEFAULT_TYPE_WEIGHT" in anomaly_scores_cfg_2512:
        default_type_weight_2512 = float(anomaly_scores_cfg_2512["DEFAULT_TYPE_WEIGHT"])
    if "MAX_SCORE_CAP" in anomaly_scores_cfg_2512:
        try:
            max_score_cap_2512 = float(anomaly_scores_cfg_2512["MAX_SCORE_CAP"])
        except Exception:
            max_score_cap_2512 = None

row_scores_df_2512 = pd.DataFrame()
n_rows_scored_2512 = 0

if not anomaly_df_2512.empty:
    for _col in ["row_key", "severity", "anomaly_type"]:
        if _col not in anomaly_df_2512.columns:
            anomaly_df_2512[_col] = np.nan

    sev = anomaly_df_2512["severity"].astype("string").str.lower().fillna("info")
    atype = anomaly_df_2512["anomaly_type"].astype("string").fillna("")

    sev_w = sev.map(severity_weights_2512).fillna(default_severity_weight_2512)
    type_w = atype.apply(lambda x: type_weights_2512.get(str(x), default_type_weight_2512))

    tmp = anomaly_df_2512.copy()
    tmp["severity_rank"] = sev.map({"info": 0, "ok": 0, "warn": 1, "fail": 2}).fillna(0).astype(int)
    tmp["row_score_contribution"] = sev_w * type_w

    grp = tmp.groupby("row_key", dropna=False)
    row_scores_df_2512 = grp.agg(
        n_anomalies=("row_score_contribution", "size"),
        n_warn=("severity", lambda x: (x.astype("string").str.lower() == "warn").sum()),
        n_fail=("severity", lambda x: (x.astype("string").str.lower() == "fail").sum()),
        max_severity_rank=("severity_rank", "max"),
        total_score=("row_score_contribution", "sum"),
    ).reset_index()

    row_scores_df_2512["max_severity"] = row_scores_df_2512["max_severity_rank"].map({0:"info",1:"warn",2:"fail"}).fillna("info")

    if max_score_cap_2512 is not None:
        row_scores_df_2512["total_score_capped"] = row_scores_df_2512["total_score"].clip(upper=max_score_cap_2512)
    else:
        row_scores_df_2512["total_score_capped"] = row_scores_df_2512["total_score"]

    m = float(row_scores_df_2512["total_score_capped"].max()) if len(row_scores_df_2512) else 0.0
    row_scores_df_2512["score_normalized"] = (row_scores_df_2512["total_score_capped"] / m) if m > 0 else 0.0

    n_rows_scored_2512 = int(len(row_scores_df_2512))

# Persist (fixed paths)
row_scores_parquet_path_2512 = (ANOMALY_DIR / "row_anomaly_scores.parquet").resolve()
row_scores_csv_path_2512 = (ANOMALY_DIR / "row_anomaly_scores.csv").resolve()
row_scores_csv_tmp_2512 = row_scores_csv_path_2512.with_suffix(".tmp.csv")

if not row_scores_df_2512.empty:
    row_scores_df_2512.to_parquet(row_scores_parquet_path_2512, index=False)
    row_scores_df_2512.to_csv(row_scores_csv_tmp_2512, index=False)
    os.replace(row_scores_csv_tmp_2512, row_scores_csv_path_2512)

print(f"ğŸ’¾ 2.5.12 row_anomaly_scores.parquet â†’ {row_scores_parquet_path_2512}")
print(f"ğŸ’¾ 2.5.12 row_anomaly_scores.csv â†’ {row_scores_csv_path_2512}")
print(f"   Anomaly rows in context: {int(len(anomaly_df_2512))}")
print(f"   Distinct row_keys scored: {n_rows_scored_2512}")

# -------------------------
# 2.5.13 | Column anomaly profile
# -------------------------
print("\n2.5.13 ğŸ“Š Column-level anomaly density & impact profile")

anomaly_path_2513 = ANOMALY_CONTEXT_PATH

if "anomaly_df_2512" in globals() and isinstance(anomaly_df_2512, pd.DataFrame):
    anomaly_df_2513 = anomaly_df_2512.copy()
else:
    if anomaly_path_2513.exists():
        anomaly_df_2513 = pd.read_parquet(anomaly_path_2513)
    else:
        anomaly_df_2513 = pd.DataFrame()

# ...keep your existing 2.5.13 expansion/aggregation...
```

---

## ğŸ’¡ğŸ’¡ Two â€œTHE NEW NEWâ€ alignment suggestions (small but important)

### ğŸ’¡ğŸ’¡ 1) Rename `ANOMALY_SCORES` â†’ `LOGIC_IMPACT`

Your spec calls it `LOGIC_IMPACT` (and itâ€™s conceptually better). Right now youâ€™re reading `CONFIG["ANOMALY_SCORES"]`.

Best move: support both (backward compatible):

```python
impact_cfg = {}
if isinstance(CONFIG, dict):
    impact_cfg = CONFIG.get("LOGIC_IMPACT") or CONFIG.get("ANOMALY_SCORES") or {}
```

### ğŸ’¡ğŸ’¡ 2) Your 2.5.12/2.5.13 are â€œscores & densityâ€ â€” but your spec wants **readiness**

Right now youâ€™re producing:

* row_anomaly_scores.*
* column_anomaly_profile.csv

Thatâ€™s great, but **spec 2.5.12 output is `logic_readiness_report.csv`** and includes:

* `logic_pct_rows_touched`
* `logic_max_severity`
* `logic_readiness_score`
* `logic_readiness_label`
  â€¦and also row survivability metrics (`pct_rows_logic_clean`).

You can keep your two artifacts, but Iâ€™d position them as:

* **2.5.11A/2.5.11B** style supporting artifacts, or
* sub-artifacts of 2.5.12 (fine) **as long as you also emit the readiness report**.

---

If you paste me your current `strap_paths()` (or the part that sets `SEC2_REPORT_DIRS` / `sec2_reports_dir`), Iâ€™ll snap this into your exact directory conventions so Part D never needs to guess paths again.

># X: NO FUNCTIONS VERSION

Got you. No functions yet. Hereâ€™s the **Level 3â†’4** way to do it: **one small â€œsetup shimâ€ block** (inline) + then **pure inline code** for 2.5.12 and 2.5.13.

Below is a **drop-in replacement** that fixes your two core issues:

1. `ANOMALY_CONTEXT_PATH` missing â†’ resolve it inline (no strap required)
2. treating the parquet file path like a directory â†’ always use `ANOMALY_CONTEXT_PATH.parent`

---

## 0) Inline setup shim (place once at top of Part D)

```python
print("PART D | 2.5.12â€“2.5.15 | ğŸ“ŠğŸ“ˆ Logic Scoring & Health Layer")

# Assumes df exists
if "df" not in globals():
    raise RuntimeError("âŒ df not found in globals(); cannot run 2.5.12â€“2.5.15")

# --- Resolve ANOMALY_CONTEXT_PATH (FILE path) inline, no functions ---
if "ANOMALY_CONTEXT_PATH" in globals() and globals().get("ANOMALY_CONTEXT_PATH") is not None:
    ANOMALY_CONTEXT_PATH = Path(globals()["ANOMALY_CONTEXT_PATH"]).resolve()
else:
    # Prefer section-specific 2.5 dir if available
    if "SEC2_REPORT_DIRS" in globals() and isinstance(SEC2_REPORT_DIRS, dict) and SEC2_REPORT_DIRS.get("2.5"):
        ANOMALY_CONTEXT_PATH = (Path(SEC2_REPORT_DIRS["2.5"]).resolve() / "logic_anomaly_context.parquet").resolve()
    elif "sec2_reports_dir" in globals() and globals().get("sec2_reports_dir"):
        ANOMALY_CONTEXT_PATH = (Path(globals()["sec2_reports_dir"]).resolve() / "logic_anomaly_context.parquet").resolve()
    elif "REPORTS_DIR" in globals() and globals().get("REPORTS_DIR"):
        ANOMALY_CONTEXT_PATH = (Path(globals()["REPORTS_DIR"]).resolve() / "section2" / "logic_anomaly_context.parquet").resolve()
    else:
        ANOMALY_CONTEXT_PATH = Path("section2_reports/logic_anomaly_context.parquet").resolve()

ANOMALY_DIR = ANOMALY_CONTEXT_PATH.parent
ANOMALY_DIR.mkdir(parents=True, exist_ok=True)

print("âœ… ANOMALY_CONTEXT_PATH =", ANOMALY_CONTEXT_PATH)
print("âœ… ANOMALY_DIR         =", ANOMALY_DIR)

n_rows_25D = int(df.shape[0])
```

---

## 2.5.12 Inline (fixed paths + your scoring logic)

```python
print("\n2.5.12 ğŸ“Š Row-level anomaly aggregation & scoring")

# Load anomaly context
if ANOMALY_CONTEXT_PATH.exists():
    try:
        anomaly_df_2512 = pd.read_parquet(ANOMALY_CONTEXT_PATH)
        print(f"   âœ… Loaded anomaly context | rows={len(anomaly_df_2512)}")
    except Exception as e:
        print(f"   âš ï¸ Could not read logic_anomaly_context.parquet: {e}")
        anomaly_df_2512 = pd.DataFrame()
else:
    print(f"   â„¹ï¸ logic_anomaly_context.parquet not found at {ANOMALY_CONTEXT_PATH}")
    anomaly_df_2512 = pd.DataFrame()

# Resolve scoring config (inline; optional CONFIG hook)
severity_weights_2512 = {"info": 0.0, "ok": 0.0, "warn": 1.0, "fail": 3.0}
type_weights_2512 = {}
default_severity_weight_2512 = 0.5
default_type_weight_2512 = 1.0
max_score_cap_2512 = None

anomaly_scores_cfg_2512 = {}
if "CONFIG" in globals() and isinstance(CONFIG, dict):
    _tmp_cfg = CONFIG.get("ANOMALY_SCORES", {})  # (Level 3â†’4: keep simple; can migrate to LOGIC_IMPACT later)
    if isinstance(_tmp_cfg, dict):
        anomaly_scores_cfg_2512 = _tmp_cfg

if anomaly_scores_cfg_2512:
    print("   ğŸ”§ ANOMALY_SCORES config resolved.")

    _sev_cfg = anomaly_scores_cfg_2512.get("SEVERITY_WEIGHTS", {})
    if isinstance(_sev_cfg, dict) and _sev_cfg:
        severity_weights_2512 = {}
        for _k, _v in _sev_cfg.items():
            try:
                severity_weights_2512[str(_k).lower()] = float(_v)
            except Exception:
                continue

    _type_cfg = anomaly_scores_cfg_2512.get("TYPE_WEIGHTS", {})
    if isinstance(_type_cfg, dict) and _type_cfg:
        type_weights_2512 = {}
        for _k, _v in _type_cfg.items():
            try:
                type_weights_2512[str(_k)] = float(_v)
            except Exception:
                continue

    if "DEFAULT_SEVERITY_WEIGHT" in anomaly_scores_cfg_2512:
        try:
            default_severity_weight_2512 = float(anomaly_scores_cfg_2512["DEFAULT_SEVERITY_WEIGHT"])
        except Exception:
            pass

    if "DEFAULT_TYPE_WEIGHT" in anomaly_scores_cfg_2512:
        try:
            default_type_weight_2512 = float(anomaly_scores_cfg_2512["DEFAULT_TYPE_WEIGHT"])
        except Exception:
            pass

    if "MAX_SCORE_CAP" in anomaly_scores_cfg_2512:
        try:
            max_score_cap_2512 = float(anomaly_scores_cfg_2512["MAX_SCORE_CAP"])
        except Exception:
            max_score_cap_2512 = None

print(f"   â€¢ Severity weights: {severity_weights_2512}")
print(f"   â€¢ Default severity weight: {default_severity_weight_2512}")
print(f"   â€¢ Default type weight: {default_type_weight_2512}")
print(f"   â€¢ Type weights (if any): {type_weights_2512}")
print(f"   â€¢ Score cap: {max_score_cap_2512 if max_score_cap_2512 is not None else '[no cap]'}")

# Compute per-row scores
row_scores_df_2512 = pd.DataFrame()
n_rows_scored_2512 = 0

if not anomaly_df_2512.empty:
    for _col in ["row_key", "severity", "anomaly_type"]:
        if _col not in anomaly_df_2512.columns:
            anomaly_df_2512[_col] = np.nan

    _sev_series_2512 = anomaly_df_2512["severity"].astype("string").str.lower().fillna("info")
    _atype_series_2512 = anomaly_df_2512["anomaly_type"].astype("string").fillna("")

    _sev_weight_2512 = _sev_series_2512.map(severity_weights_2512).fillna(default_severity_weight_2512)

    def _map_type_weight_inline_2512(_atype_val):
        _atype_key = str(_atype_val)
        if _atype_key in type_weights_2512:
            return type_weights_2512[_atype_key]
        return default_type_weight_2512

    _type_weight_2512 = _atype_series_2512.apply(_map_type_weight_inline_2512)

    anomaly_df_2512 = anomaly_df_2512.copy()
    anomaly_df_2512["severity_weight"] = _sev_weight_2512
    anomaly_df_2512["type_weight"] = _type_weight_2512
    anomaly_df_2512["row_score_contribution"] = anomaly_df_2512["severity_weight"] * anomaly_df_2512["type_weight"]

    sev_rank_2512 = {"info": 0, "ok": 0, "warn": 1, "fail": 2}
    anomaly_df_2512["severity_rank"] = _sev_series_2512.map(sev_rank_2512).fillna(0).astype(int)

    _grp_2512 = anomaly_df_2512.groupby("row_key", dropna=False)

    row_scores_df_2512 = _grp_2512.agg(
        n_anomalies=("row_score_contribution", "size"),
        n_warn=("severity", lambda x: (x.astype("string").str.lower() == "warn").sum()),
        n_fail=("severity", lambda x: (x.astype("string").str.lower() == "fail").sum()),
        max_severity_rank=("severity_rank", "max"),
        total_score=("row_score_contribution", "sum"),
    ).reset_index()

    _rank_to_label_2512 = {0: "info", 1: "warn", 2: "fail"}
    row_scores_df_2512["max_severity"] = row_scores_df_2512["max_severity_rank"].map(_rank_to_label_2512).fillna("info")

    if max_score_cap_2512 is not None:
        row_scores_df_2512["total_score_capped"] = row_scores_df_2512["total_score"].clip(upper=max_score_cap_2512)
    else:
        row_scores_df_2512["total_score_capped"] = row_scores_df_2512["total_score"]

    max_score_observed_2512 = float(row_scores_df_2512["total_score_capped"].max()) if not row_scores_df_2512.empty else 0.0
    if max_score_observed_2512 > 0.0:
        row_scores_df_2512["score_normalized"] = row_scores_df_2512["total_score_capped"] / max_score_observed_2512
    else:
        row_scores_df_2512["score_normalized"] = 0.0

    n_rows_scored_2512 = int(len(row_scores_df_2512))

# Persist row_anomaly_scores (FIXED: write into ANOMALY_DIR)
row_scores_parquet_path_2512 = (ANOMALY_DIR / "row_anomaly_scores.parquet").resolve()
row_scores_csv_path_2512 = (ANOMALY_DIR / "row_anomaly_scores.csv").resolve()
row_scores_csv_tmp_2512 = row_scores_csv_path_2512.with_suffix(".tmp.csv")

if not row_scores_df_2512.empty:
    try:
        row_scores_df_2512.to_parquet(row_scores_parquet_path_2512, index=False)
    except Exception as e:
        print(f"   âš ï¸ Could not write row_anomaly_scores.parquet: {e}")

    try:
        row_scores_df_2512.to_csv(row_scores_csv_tmp_2512, index=False)
        os.replace(row_scores_csv_tmp_2512, row_scores_csv_path_2512)
    except Exception as e:
        print(f"   âš ï¸ Could not write row_anomaly_scores.csv: {e}")
        if row_scores_csv_tmp_2512.exists():
            row_scores_csv_tmp_2512.unlink()

# Summary row
status_2512 = "INFO"
if not anomaly_df_2512.empty and n_rows_scored_2512 > 0:
    status_2512 = "OK"

summary_2512 = pd.DataFrame([{
    "section": "2.5.12",
    "section_name": "Row-level anomaly aggregation & scoring",
    "check": "Aggregate anomaly context rows into per-row scores for downstream explainability",
    "level": "info",
    "status": status_2512,
    "n_anomaly_rows": int(len(anomaly_df_2512)),
    "n_rows_scored": int(n_rows_scored_2512),
    "detail": "row_anomaly_scores.parquet, row_anomaly_scores.csv",
    "timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_2512, SECTION2_REPORT_PATH)

print(f"ğŸ’¾ 2.5.12 row_anomaly_scores.parquet â†’ {row_scores_parquet_path_2512}")
print(f"ğŸ’¾ 2.5.12 row_anomaly_scores.csv     â†’ {row_scores_csv_path_2512}")
print(f"   Anomaly rows in context: {int(len(anomaly_df_2512))}")
print(f"   Distinct row_keys scored: {n_rows_scored_2512}")

if not row_scores_df_2512.empty:
    display(
        row_scores_df_2512.loc[:, [
            "row_key", "n_anomalies", "n_warn", "n_fail",
            "max_severity", "total_score", "total_score_capped", "score_normalized",
        ]].head(10)
    )

display(summary_2512)
```

---

## 2.5.13 Inline (fixes missing `anomaly_path_2513` + keeps your approach)

```python
print("\n2.5.13 ğŸ“Š Column-level anomaly density & impact profile")

# Robust reuse: prefer already-loaded anomaly_df_2512
if "anomaly_df_2512" in globals() and isinstance(anomaly_df_2512, pd.DataFrame):
    anomaly_df_2513 = anomaly_df_2512.copy()
else:
    if ANOMALY_CONTEXT_PATH.exists():
        try:
            anomaly_df_2513 = pd.read_parquet(ANOMALY_CONTEXT_PATH)
            print(f"   âœ… Loaded anomaly context | rows={len(anomaly_df_2513)}")
        except Exception as e:
            print(f"   âš ï¸ Could not read logic_anomaly_context.parquet: {e}")
            anomaly_df_2513 = pd.DataFrame()
    else:
        print(f"   â„¹ï¸ logic_anomaly_context.parquet not found at {ANOMALY_CONTEXT_PATH}")
        anomaly_df_2513 = pd.DataFrame()

col_profile_df_2513 = pd.DataFrame()
n_columns_with_anomalies_2513 = 0

if not anomaly_df_2513.empty:
    for _col in ["row_key", "severity", "anomaly_type", "feature_names"]:
        if _col not in anomaly_df_2513.columns:
            anomaly_df_2513[_col] = np.nan

    # Expand feature_names (comma-separated) into long form
    _rows_2513 = []
    for _idx_2513, _row_2513 in anomaly_df_2513.iterrows():
        _features_str = str(_row_2513.get("feature_names", "") or "")
        if not _features_str:
            continue
        _features_list = [f.strip() for f in _features_str.split(",") if f.strip()]
        if not _features_list:
            continue

        _row_key_2513 = _row_2513.get("row_key", _idx_2513)
        _severity_2513 = str(_row_2513.get("severity", "info")).lower()
        _atype_2513 = str(_row_2513.get("anomaly_type", "") or "")
        _mag_2513 = _row_2513.get("magnitude", np.nan)

        for _feat_2513 in _features_list:
            _rows_2513.append({
                "column_name": _feat_2513,
                "row_key": _row_key_2513,
                "severity": _severity_2513,
                "anomaly_type": _atype_2513,
                "magnitude": _mag_2513,
            })

    col_long_df_2513 = pd.DataFrame(_rows_2513)

    if not col_long_df_2513.empty:
        sev_rank_2513 = {"info": 0, "ok": 0, "warn": 1, "fail": 2}
        col_long_df_2513["severity_rank"] = col_long_df_2513["severity"].map(sev_rank_2513).fillna(0).astype(int)

        _grp_col_2513 = col_long_df_2513.groupby("column_name", dropna=False)

        col_profile_df_2513 = _grp_col_2513.agg(
            n_anomalies=("row_key", "size"),
            n_rows_touched=("row_key", "nunique"),
            n_warn=("severity", lambda x: (x == "warn").sum()),
            n_fail=("severity", lambda x: (x == "fail").sum()),
            max_severity_rank=("severity_rank", "max"),
            mean_magnitude=("magnitude", "mean"),
        ).reset_index()

        col_profile_df_2513["max_severity"] = col_profile_df_2513["max_severity_rank"].map({0:"info",1:"warn",2:"fail"}).fillna("info")

        col_profile_df_2513["anomaly_density_per_row"] = np.where(
            col_profile_df_2513["n_rows_touched"] > 0,
            col_profile_df_2513["n_anomalies"] / col_profile_df_2513["n_rows_touched"],
            np.nan,
        )

        col_profile_df_2513["risk_score"] = (
            col_profile_df_2513["anomaly_density_per_row"].fillna(0.0)
            * (1.0 + col_profile_df_2513["max_severity_rank"])
        )

        n_columns_with_anomalies_2513 = int(len(col_profile_df_2513))

# Persist (use your same reports dir pattern â€” inline)
if "section2_reports_dir_2513" in globals():
    out_dir_2513 = Path(section2_reports_dir_2513).resolve()
elif "sec2_reports_dir" in globals() and globals().get("sec2_reports_dir"):
    out_dir_2513 = Path(globals()["sec2_reports_dir"]).resolve()
elif "REPORTS_DIR" in globals() and globals().get("REPORTS_DIR"):
    out_dir_2513 = (Path(globals()["REPORTS_DIR"]).resolve() / "section2")
else:
    out_dir_2513 = Path("section2_reports").resolve()

out_dir_2513.mkdir(parents=True, exist_ok=True)

col_profile_path_2513 = (out_dir_2513 / "column_anomaly_profile.csv").resolve()
col_profile_tmp_2513 = col_profile_path_2513.with_suffix(".tmp.csv")

if not col_profile_df_2513.empty:
    try:
        col_profile_df_2513.to_csv(col_profile_tmp_2513, index=False)
        os.replace(col_profile_tmp_2513, col_profile_path_2513)
    except Exception as e:
        print(f"   âš ï¸ Could not write column_anomaly_profile.csv: {e}")
        if col_profile_tmp_2513.exists():
            col_profile_tmp_2513.unlink()

status_2513 = "INFO"
if n_columns_with_anomalies_2513 > 0:
    status_2513 = "OK"

summary_2513 = pd.DataFrame([{
    "section": "2.5.13",
    "section_name": "Column-level anomaly density & impact profile",
    "check": "Summarize logic anomalies per column to identify fragile features",
    "level": "info",
    "status": status_2513,
    "n_anomaly_rows": int(len(anomaly_df_2513)),
    "n_columns_with_anomalies": int(n_columns_with_anomalies_2513),
    "detail": "column_anomaly_profile.csv",
    "timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_2513, SECTION2_REPORT_PATH)

print(f"ğŸ’¾ 2.5.13 column_anomaly_profile.csv â†’ {col_profile_path_2513}")
print(f"   Anomaly rows in context: {int(len(anomaly_df_2513))}")
print(f"   Columns with any anomalies: {n_columns_with_anomalies_2513}")

if not col_profile_df_2513.empty:
    display(
        col_profile_df_2513.sort_values("risk_score", ascending=False).loc[:, [
            "column_name", "n_anomalies", "n_rows_touched", "n_warn", "n_fail",
            "max_severity", "anomaly_density_per_row", "risk_score",
        ]].head(15)
    )

display(summary_2513)
```

---

### ğŸ’¡ğŸ’¡ Level 3â†’4 â€œkeep it simpleâ€ guardrails (what I intentionally did)

* No helper functions (except your existing `append_sec2`, which you already treat as a shared utility)
* No class structures, no modules
* Only **one inline setup shim**, then straightforward blocks
* Fix only whatâ€™s necessary: path semantics + missing variables

If you paste the **exact value** of `SEC2_REPORT_DIRS["2.5"]` or `sec2_reports_dir` (just print them), I can make the output location **100% consistent** with the rest of 2.5.1â€“2.5.11 (still no functions).

---
># X:
---

Short answer: **No.**
`ANOMALY_CONTEXT_PATH` is **not** your `project_config.yaml`.

But they are **related** in a clean, intentional way.

---

## What `ANOMALY_CONTEXT_PATH` actually is

**`ANOMALY_CONTEXT_PATH` is a runtime *artifact path*** â€” a concrete file location on disk.

In your case:

```text
resources/
â””â”€â”€ reports/
    â””â”€â”€ section2/
        â””â”€â”€ 2.5/
            â””â”€â”€ logic_anomaly_context.parquet   â† this
```

So:

* âœ… **File path**
* âœ… Produced by **2.5.11**
* âœ… Consumed by **2.5.12+**
* âŒ Not configuration
* âŒ Not YAML
* âŒ Not user-edited

---

## What `project_config.yaml` is

`project_config.yaml` is **declarative intent**.

It answers:

* *What rules exist?*
* *What severity means what?*
* *What thresholds matter?*
* *What features are governed?*

Example from your spec:

```yaml
ANOMALY_CONTEXT:
  INCLUDE_ROW_KEY: true
  INCLUDE_FEATURE_NAMES: true
  MAX_ROWS_PER_RULE: 50000
```

This config **controls how** the anomaly context is built â€”
but **does not store the anomalies themselves**.

---

## The correct mental model (this is important)

### Think in three layers

```
CONFIG (project_config.yaml)
    â†“ governs
CODE (2.5.11 logic)
    â†“ produces
ARTIFACT (logic_anomaly_context.parquet)
```

So:

* `project_config.yaml` â†’ **rules & knobs**
* `ANOMALY_CONTEXT_PATH` â†’ **where the result is written/read**
* `logic_anomaly_context.parquet` â†’ **what actually happened**

---

## Why defining `ANOMALY_CONTEXT_PATH` in 2.0 is correct

You are not â€œconfiguring logicâ€ in 2.0 â€” youâ€™re declaring **where artifacts live**.

Thatâ€™s infrastructure, not logic.

This line in bootstrap is totally appropriate:

```python
ANOMALY_CONTEXT_PATH = SEC2_25_DIR / "logic_anomaly_context.parquet"
```

It does **not** duplicate config.
It gives every later section a **stable contract**.

---

## What you should NOT do

âŒ Donâ€™t put full paths in `project_config.yaml`
âŒ Donâ€™t make later sections â€œdiscoverâ€ files
âŒ Donâ€™t recompute paths ad hoc per section

Thatâ€™s how pipelines become brittle.

---

## Optional (advanced, later-level) bridge â€” *not for Level 3â†’4 yet*

At higher levels, you *might* add something like this to config:

```yaml
ARTIFACT_NAMES:
  LOGIC_ANOMALY_CONTEXT: "logic_anomaly_context.parquet"
```

â€¦but youâ€™re **correct not to do this yet**.

---

## Final answer (one sentence)

**`ANOMALY_CONTEXT_PATH` points to a runtime artifact on disk, while `project_config.yaml` only controls how that artifact is produced â€” they are complementary, not the same thing.**

If you want, next I can:

* sanity-check your **2.5.11 write logic** against this mental model, or
* help you draft a **one-paragraph â€œArchitecture Overviewâ€** that explains this cleanly to a hiring manager ğŸ’¡ğŸ’¡
