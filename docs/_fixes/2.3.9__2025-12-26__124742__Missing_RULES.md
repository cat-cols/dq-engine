># Q: Starting from the  2.3.7.4 rule confidence scores, where does the disconnect happen in 2.3.9?

[1](https://www.youtube.com/watch?v=LApzseb0OBY)
[2](https://stataiml.com/posts/83_calculate_ci_pandas_py/)
[3](https://www.tjansson.dk/2023/12/enhancing-pandas-groupby-with-bootstrapped-confidence-intervals/)
[4](https://www.geeksforgeeks.org/python/how-to-calculate-confidence-intervals-in-python/)
[5](https://stackoverflow.com/questions/53519823/confidence-interval-in-python-dataframe)
[6](https://blog.ando.ai/posts/applied-stats/)
[7](https://www.yourdatateacher.com/2021/11/08/how-to-calculate-confidence-intervals-in-python/)
[8](https://www.kaggle.com/code/haseebwar07/explore-confidence-intervals-in-python)
[9](https://discuss.python.org/t/need-advice-direction-help-with-confidence-scoring-or-a-weighted-scoring/58701)

```python
# 2.3.7.4 | Rule Confidence Scores
print("\n2.3.7.4 ‚è±Ô∏è Rule confidence scores")

rule_rows_2374 = []

range_path_2374 = NUMERIC_DIR / "range_violation_report.csv"
outlier_path_2374 = NUMERIC_DIR / "outlier_report_iqr_z.csv"
ts_outliers_path_2374 = globals().get("time_series_outliers_path", NUMERIC_DIR / "time_series_outliers.csv")
corr_anom_path_2374 = NUMERIC_DIR / "correlation_anomalies.csv"

try:
hard_types_cfg_2374 = C("NUMERIC.RULES.HARD_TYPES", ["range"]) or ["range"]
except Exception:
hard_types_cfg_2374 = ["range"]

# --- Range rules -------------------------------------------------------
if range_path_2374.exists():
range_df_2374 = pd.read_csv(range_path_2374)
else:
range_df_2374 = pd.DataFrame()

for _, r in range_df_2374.iterrows():
has_range_rule = bool(r.get("has_range_rule", False))
if not has_range_rule:
continue

col = r.get("column")

n_below_raw = r.get("n_below_min", 0)
n_above_raw = r.get("n_above_max", 0)
n_in_raw = r.get("n_in_range", 0)

n_below = float(0 if pd.isna(n_below_raw) else n_below_raw)
n_above = float(0 if pd.isna(n_above_raw) else n_above_raw)
n_in = float(0 if pd.isna(n_in_raw) else n_in_raw)

total = n_below + n_above + n_in
if pd.isna(total) or total <= 0:
total = 1.0

viol_rate = (n_below + n_above) / total

if total >= 1000:
size_factor = 1.0
elif total >= 100:
size_factor = 0.8
else:
size_factor = 0.6

viol_factor = max(0.2, 1.0 - viol_rate * 4.0)
confidence = float(min(1.0, size_factor * viol_factor))

total_display = int(round(total))

rule_rows_2374.append(
{
"feature": col,
"rule_type": "range",
"rule_id": "range_minmax",
"confidence_score": round(confidence, 3),
"hard_vs_soft": "hard" if "range" in hard_types_cfg_2374 else "soft",
"notes": f"viol_rate={round(viol_rate,4)}, total={total_display}",
}
)

# --- Outlier rules (IQR/Z) ---------------------------------------------
if outlier_path_2374.exists():
out_df_2374 = pd.read_csv(outlier_path_2374)
else:
out_df_2374 = pd.DataFrame()

for _, r in out_df_2374.iterrows():
col = r.get("column")
pct_iqr = float(r.get("pct_outliers_iqr", 0) or 0)
pct_z = float(r.get("pct_outliers_z", 0) or 0)

max_pct = max(pct_iqr, pct_z)
if max_pct < 1.0:
sev_factor = 1.0
elif max_pct < 5.0:
sev_factor = 0.8
else:
sev_factor = 0.6

confidence = float(sev_factor)

rule_rows_2374.append(
{
"feature": col,
"rule_type": "outlier_iqr_z",
"rule_id": "outlier_iqr_z",
"confidence_score": round(confidence, 3),
"hard_vs_soft": "soft",
"notes": f"max_pct_outliers={round(max_pct,3)}",
}
)

# --- Temporal time-series rules ----------------------------------------
ts_df_2374 = pd.DataFrame()
if isinstance(ts_outliers_path_2374, Path) and ts_outliers_path_2374.exists() and ts_outliers_path_2374.stat().st_size > 0:
try:
ts_df_2374 = pd.read_csv(ts_outliers_path_2374)
except pd.errors.EmptyDataError:
ts_df_2374 = pd.DataFrame()

if not ts_df_2374.empty:
if ts_df_2374["is_outlier"].dtype != bool:
ts_df_2374["is_outlier"] = ts_df_2374["is_outlier"].astype(bool)

total_buckets = ts_df_2374["time_bucket"].nunique()
if total_buckets <= 0:
total_buckets = 1

for feat, g in ts_df_2374.groupby("feature"):
n_out_feat = int(g[g["is_outlier"]].shape[0])
rate_feat = n_out_feat / total_buckets

if rate_feat == 0:
conf = 0.9
elif rate_feat < 0.2:
conf = 0.8
else:
conf = 0.6

rule_rows_2374.append(
{
"feature": feat,
"rule_type": "temporal_ts_outlier",
"rule_id": "ts_zscore",
"confidence_score": round(float(conf), 3),
"hard_vs_soft": "soft",
"notes": f"outlier_bucket_rate={round(rate_feat,4)}",
}
)

# --- Correlation anomaly rules -----------------------------------------
corr_df_2374 = pd.DataFrame()
if corr_anom_path_2374.exists() and corr_anom_path_2374.stat().st_size > 0:
try:
corr_df_2374 = pd.read_csv(corr_anom_path_2374)
except pd.errors.EmptyDataError:
corr_df_2374 = pd.DataFrame()

for _, r in corr_df_2374.iterrows():
feat_i = r.get("feature_i")
feat_j = r.get("feature_j")
abs_delta = float(r.get("abs_delta", 0) or 0)

if abs_delta < corr_delta_threshold_237:
conf = 0.7
elif abs_delta < 2 * corr_delta_threshold_237:
conf = 0.8
else:
conf = 0.9

rule_rows_2374.append(
{
"feature": f"{feat_i}__{feat_j}",
"rule_type": "correlation",
"rule_id": r.get("time_window", ""),
"confidence_score": round(float(conf), 3),
"hard_vs_soft": "soft",
"notes": f"abs_delta={round(abs_delta,4)}",
}
)

rule_conf_df_2374 = pd.DataFrame(rule_rows_2374)

rule_conf_path_2374 = NUMERIC_DIR / "rule_confidence_scores.csv"
tmp_2374 = rule_conf_path_2374.with_suffix(".tmp.csv")
rule_conf_df_2374.to_csv(tmp_2374, index=False)
os.replace(tmp_2374, rule_conf_path_2374)

print(f"üíæ Wrote rule confidence scores ‚Üí {rule_conf_path_2374}")
if not rule_conf_df_2374.empty:
print("\nüìä 2.3.7.4 rule confidence scores (head):")
display(rule_conf_df_2374.head(30))

n_rules_2374 = int(rule_conf_df_2374.shape[0])
n_hard_rules_2374 = int((rule_conf_df_2374["hard_vs_soft"] == "hard").sum()) if n_rules_2374 else 0
n_soft_rules_2374 = int((rule_conf_df_2374["hard_vs_soft"] == "soft").sum()) if n_rules_2374 else 0

status_2374 = "SKIP" if n_rules_2374 == 0 else "OK"

summary_2374 = pd.DataFrame([{
"section": "2.3.7.4",
"section_name": "Rule confidence scores",
"check": "Assign confidence & hardness to numeric rules",
"level": "info",
"status": status_2374,
"n_rules": n_rules_2374,
"n_hard_rules": n_hard_rules_2374,
"n_soft_rules": n_soft_rules_2374,
"detail": f"rule_confidence_scores.csv under {NUMERIC_DIR.name}",
"timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_2374, SECTION2_REPORT_PATH)
display(summary_2374)

2.3.7.4 ‚è±Ô∏è Rule confidence scores
üíæ Wrote rule confidence scores ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/numeric_integrity/rule_confidence_scores.csv

üìä 2.3.7.4 rule confidence scores (head):
<div>
<style scoped>
.dataframe tbody tr th:only-of-type {
vertical-align: middle;
}

.dataframe tbody tr th {
vertical-align: top;
}

.dataframe thead th {
text-align: right;
}
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>feature</th>
<th>rule_type</th>
<th>rule_id</th>
<th>confidence_score</th>
<th>hard_vs_soft</th>
<th>notes</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>SeniorCitizen</td>
<td>outlier_iqr_z</td>
<td>outlier_iqr_z</td>
<td>0.6</td>
<td>soft</td>
<td>max_pct_outliers=16.215</td>
</tr>
<tr>
<th>1</th>
<td>Churn_flag</td>
<td>outlier_iqr_z</td>
<td>outlier_iqr_z</td>
<td>1.0</td>
<td>soft</td>
<td>max_pct_outliers=0.0</td>
</tr>
<tr>
<th>2</th>
<td>MonthlyCharges</td>
<td>outlier_iqr_z</td>
<td>outlier_iqr_z</td>
<td>1.0</td>
<td>soft</td>
<td>max_pct_outliers=0.0</td>
</tr>
<tr>
<th>3</th>
<td>TotalCharges</td>
<td>outlier_iqr_z</td>
<td>outlier_iqr_z</td>
<td>1.0</td>
<td>soft</td>
<td>max_pct_outliers=0.0</td>
</tr>
<tr>
<th>4</th>
<td>tenure</td>
<td>outlier_iqr_z</td>
<td>outlier_iqr_z</td>
<td>1.0</td>
<td>soft</td>
<td>max_pct_outliers=0.0</td>
</tr>
</tbody>
</table>
</div>

<div>
<style scoped>
.dataframe tbody tr th:only-of-type {
vertical-align: middle;
}

.dataframe tbody tr th {
vertical-align: top;
}

.dataframe thead th {
text-align: right;
}
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>section</th>
<th>section_name</th>
<th>check</th>
<th>level</th>
<th>status</th>
<th>n_rules</th>
<th>n_hard_rules</th>
<th>n_soft_rules</th>
<th>detail</th>
<th>timestamp</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>2.3.7.4</td>
<td>Rule confidence scores</td>
<td>Assign confidence &amp; hardness to numeric rules</td>
<td>info</td>
<td>OK</td>
<td>5</td>
<td>0</td>
<td>5</td>
<td>rule_confidence_scores.csv under numeric_integ...</td>
<td>2025-12-26 20:37:15.961934+00:00</td>
</tr>
</tbody>
</table>
</div>

# PART C | 2.3.8‚Äì2.3.14 üßÆ Model Readiness & Operational Hooks
print("\n2.3.8‚Äì2.3.14 üßÆ Model readiness & operational hooks")

# Assumes:
# - df, NUMERIC_DIR, REPORTS_DIR, SECTION2_REPORT_PATH exist
# - prior numeric artifacts already written by 2.3.x & 2.3.7.x
# - CONFIG may exist as a dict (optional)

# ---------------------------------------------------------------------------
# Utility: safe loader for CSV ‚Üí DataFrame with a 'feature' column
# (still inline, no def)
# ---------------------------------------------------------------------------
# (We just repeat a tiny pattern; no functions)
# 2.3.8 üìö DQ rule catalog (joined with numeric profile)
print("\n2.3.8 üìö DQ rule catalog (joined with numeric profile)")

# --- 1) Load rule confidence artifact (safe) -------------------------------
rule_conf_path = NUMERIC_DIR / "rule_confidence_scores.csv"

try:
if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:
rule_conf_df = pd.read_csv(rule_conf_path)
else:
rule_conf_df = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {rule_conf_path} is empty or has no columns. Treating as no rules.")
rule_conf_df = pd.DataFrame()

# --- 2) Load numeric profile (safe) ---------------------------------------
numeric_profile_path = NUMERIC_DIR / "numeric_profile_df.csv"

try:
if numeric_profile_path.exists() and numeric_profile_path.stat().st_size > 0:
numeric_profile_df_238 = pd.read_csv(numeric_profile_path)
else:
numeric_profile_df_238 = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {numeric_profile_path} is empty or has no columns. Skipping join.")
numeric_profile_df_238 = pd.DataFrame()

# Canonicalize rule_conf_df key
if not rule_conf_df.empty:
rule_conf_df["feature"] = rule_conf_df["feature"].astype("string").str.strip()

# Canonicalize numeric_profile_df_238 key
if not numeric_profile_df_238.empty:
if "feature" in numeric_profile_df_238.columns:
numeric_profile_df_238["feature"] = numeric_profile_df_238["feature"].astype("string").str.strip()
elif "column" in numeric_profile_df_238.columns:
numeric_profile_df_238["feature"] = numeric_profile_df_238["column"].astype("string").str.strip()
else:
numeric_profile_df_238["feature"] = pd.NA


# --- 3) Build DQ rule catalog ---------------------------------------------
if not rule_conf_df.empty and not numeric_profile_df_238.empty:
if "column" in numeric_profile_df_238.columns:
dq_rule_catalog_df = (
numeric_profile_df_238
.merge(rule_conf_df, on="feature", how="left")
.sort_values(["feature", "rule_type", "rule_id"], na_position="last")
.reset_index(drop=True)
)
else:
print("‚ö†Ô∏è numeric_profile_df_238 missing 'column' col; using rule_conf_df only.")
dq_rule_catalog_df = rule_conf_df.copy()
else:
dq_rule_catalog_df = rule_conf_df.copy()

dq_rule_catalog_path = NUMERIC_DIR / "dq_rule_catalog.csv"
tmp_238 = dq_rule_catalog_path.with_suffix(".tmp.csv")
dq_rule_catalog_df.to_csv(tmp_238, index=False)
os.replace(tmp_238, dq_rule_catalog_path)

print(f"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}")

if not dq_rule_catalog_df.empty:
print("\nüìä Data Quality Rule Catalog (head):")
cols_preview = [
"feature",
"role" if "role" in dq_rule_catalog_df.columns else "feature",
"rule_type",
"rule_id",
"confidence_score",
"hard_vs_soft",
]
cols_preview = [c for c in cols_preview if c in dq_rule_catalog_df.columns]
display(dq_rule_catalog_df[cols_preview].head(30))
else:
print(" (no rules to catalog)")

# --- 4) ‚ÄúDQ rules‚Äù tab in your report (aggregated view) -------------------
dq_rules_path = NUMERIC_DIR / "dq_rule_catalog.csv"

try:
if dq_rules_path.exists() and dq_rules_path.stat().st_size > 0:
dq_rules_df = pd.read_csv(dq_rules_path)
else:
dq_rules_df = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {dq_rules_path} is empty or has no columns. Skipping aggregation.")
dq_rules_df = pd.DataFrame()

if (
not dq_rules_df.empty
and {"feature", "rule_id", "confidence_score", "hard_vs_soft"}.issubset(dq_rules_df.columns)
):
agg_rules_df = (
dq_rules_df
.groupby("feature", as_index=False)
.agg(
n_rules=("rule_id", "nunique"),
max_hard_conf=(
"confidence_score",
lambda s: s[dq_rules_df.loc[s.index, "hard_vs_soft"] == "hard"].max()
),
max_soft_conf=(
"confidence_score",
lambda s: s[dq_rules_df.loc[s.index, "hard_vs_soft"] == "soft"].max()
),
)
)
print("\nüìä Aggregated DQ rules per feature (head):")
display(agg_rules_df.head(20))
else:
print("‚ö†Ô∏è Not enough columns / data to build aggregated DQ rules view.")

# TODO: add display and append?
print("‚úÖ base catalog complete")

# 2.3.9 üßÆ Model readiness impact summary
print("\n2.3.9 üßÆ Model readiness impact summary")

# 1) Load artifacts [{(with guards (EmptyDataError-safe))}]

# Paths
numeric_profile_path_239 = NUMERIC_DIR / "numeric_profile_df.csv"
range_path_239 = NUMERIC_DIR / "range_violation_report.csv"
outlier_path_239 = NUMERIC_DIR / "outlier_report_iqr_z.csv"
time_series_outliers_path_239 = globals().get("time_series_outliers_path", NUMERIC_DIR / "time_series_outliers.csv")
corr_anom_path_239 = NUMERIC_DIR / "correlation_anomalies.csv"
integrity_path_239 = NUMERIC_DIR / "numeric_integrity_report.csv" # may or may not exist
model_readiness_path_239 = NUMERIC_DIR / "model_readiness_report.csv"

# Rule confidence path( must match 2.3.7.4)
rule_conf_path_239 = NUMERIC_DIR / "rule_confidence_scores.csv"

# Check if rule_conf_path_239 exists
if not rule_conf_path_239.exists():
print(f"‚ùå rule_confidence_scores.csv not found at: {rule_conf_path_239}")
print(" Likely: NUMERIC_DIR differs between 2.3.7.4 and 2.3.9, or 2.3.7.4 did not run.")
# Optional: try a fallback if you have multiple numeric dirs
fallback = (REPORTS_DIR / "section2" / "numeric_integrity" / "rule_confidence_scores.csv")
if fallback.exists():
print(f" ‚úÖ Found fallback: {fallback}")
rule_conf_path_239 = fallback
else:
print(f" ‚ö†Ô∏è No fallback found. Rule confidence will be empty.")

# numeric_profile_df_239
try:
if numeric_profile_path_239.exists() and numeric_profile_path_239.stat().st_size > 0:
numeric_profile_df_239 = pd.read_csv(numeric_profile_path_239)
else:
print(f"‚ö†Ô∏è {numeric_profile_path_239} missing/empty ‚Äî using empty numeric_profile_df for 2.3.9.")
numeric_profile_df_239 = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {numeric_profile_path_239} is empty or has no columns. Using empty numeric_profile_df for 2.3.9.")
numeric_profile_df_239 = pd.DataFrame()

# range_df_239
try:
if range_path_239.exists() and range_path_239.stat().st_size > 0:
range_df_239 = pd.read_csv(range_path_239)
else:
print(f"‚ö†Ô∏è {range_path_239} missing/empty ‚Äî no range info for 2.3.9.")
range_df_239 = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {range_path_239} is empty or has no columns. No range info for 2.3.9.")
range_df_239 = pd.DataFrame()

# outlier_df_239
try:
if outlier_path_239.exists() and outlier_path_239.stat().st_size > 0:
outlier_df_239 = pd.read_csv(outlier_path_239)
else:
print(f"‚ö†Ô∏è {outlier_path_239} missing/empty ‚Äî no outlier info for 2.3.9.")
outlier_df_239 = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {outlier_path_239} is empty or has no columns. No outlier info for 2.3.9.")
outlier_df_239 = pd.DataFrame()

# # rule_conf_df_239
# try:
# if rule_conf_path_2374.exists() and rule_conf_path_2374.stat().st_size > 0:
# rule_conf_df_239 = pd.read_csv(rule_conf_path_2374)
# else:
# print(f"‚ö†Ô∏è {rule_conf_path_2374} missing/empty ‚Äî no rule confidence info for 2.3.9.")
# rule_conf_df_239 = pd.DataFrame()
# except EmptyDataError:
# print(f"‚ö†Ô∏è {rule_conf_path_2374} is empty or has no columns. No rule confidence info for 2.3.9.")
# rule_conf_df_239 = pd.DataFrame()

# integrity_df_239
try:
if integrity_path_239.exists() and integrity_path_239.stat().st_size > 0:
integrity_df_239 = pd.read_csv(integrity_path_239)
else:
integrity_df_239 = pd.DataFrame()
except EmptyDataError:
print(f"‚ö†Ô∏è {integrity_path_239} is empty or has no columns. Using empty integrity_df_239.")
integrity_df_239 = pd.DataFrame()



else:
print(f"‚ö†Ô∏è {rule_conf_path_239} missing ‚Äî no rule confidence info for 2.3.9.")
# Create an empty DataFrame
rule_conf_df_239 = pd.DataFrame()
if integrity_path_239.exists():
integrity_df_239 = pd.read_csv(integrity_path_239)
else:
integrity_df_239 = pd.DataFrame()

# 2) Normalize each DF to have a 'feature' column where possible
def _ensure_feature_col(df):
if df.empty:
return df
cols = df.columns.tolist()
if "feature" in cols:
df["feature"] = df["feature"].astype("string")
elif "column" in cols:
df["feature"] = df["column"].astype("string")
return df

numeric_profile_df_239 = _ensure_feature_col(numeric_profile_df_239)
range_df_239 = _ensure_feature_col(range_df_239)
outlier_df_239 = _ensure_feature_col(outlier_df_239)
rule_conf_df_239 = _ensure_feature_col(rule_conf_df_239)
integrity_df_239 = _ensure_feature_col(integrity_df_239)

# 3) Build a unified base_239 indexed by 'feature'
feature_series_list_239 = []

for df_tmp in [numeric_profile_df_239, range_df_239, outlier_df_239, rule_conf_df_239, integrity_df_239]:
if (not df_tmp.empty) and ("feature" in df_tmp.columns):
feature_series_list_239.append(df_tmp["feature"].astype("string"))

if feature_series_list_239:
all_features_239 = (
pd.concat(feature_series_list_239, ignore_index=True)
.dropna()
.astype("string")
.unique()
.tolist()
)
all_features_239 = sorted(all_features_239)
base_239 = pd.DataFrame({"feature": all_features_239})
else:
base_239 = pd.DataFrame(columns=["feature"])

# 4) Attach core profile info (role, feature_group, null_pct, etc.)
if (not numeric_profile_df_239.empty) and ("feature" in numeric_profile_df_239.columns):
keep_cols_np_239 = [
c for c in [
"feature",
"column",
"role",
"feature_group",
"null_pct",
"numeric_integrity_status",
] if c in numeric_profile_df_239.columns
]
numeric_core_239 = numeric_profile_df_239[keep_cols_np_239].drop_duplicates(subset=["feature"])
base_239 = base_239.merge(numeric_core_239, on="feature", how="left")

# If integrity report has extra status, prefer it
if (not integrity_df_239.empty) and ("feature" in integrity_df_239.columns):
if "numeric_integrity_status" in integrity_df_239.columns:
integ_core_239 = integrity_df_239[["feature", "numeric_integrity_status"]].drop_duplicates("feature")
base_239 = base_239.merge(integ_core_239, on="feature", how="left", suffixes=("", "_from_integrity"))
if "numeric_integrity_status_from_integrity" in base_239.columns:
base_239["numeric_integrity_status"] = base_239["numeric_integrity_status_from_integrity"].combine_first(
base_239.get("numeric_integrity_status")
)
base_239.drop(columns=["numeric_integrity_status_from_integrity"], inplace=True)
else:
if "numeric_integrity_status" not in base_239.columns:
base_239["numeric_integrity_status"] = None

# 5) Attach range & outlier diagnostics

# Range info
if (not range_df_239.empty) and ("feature" in range_df_239.columns):
keep_cols_range_239 = [c for c in ["feature", "total_violation_pct", "range_status"] if c in range_df_239.columns]
range_core_239 = range_df_239[keep_cols_range_239].drop_duplicates(subset=["feature"])
base_239 = base_239.merge(range_core_239, on="feature", how="left")
else:
base_239["total_violation_pct"] = None
base_239["range_status"] = None

# Outlier info
if (not outlier_df_239.empty) and ("feature" in outlier_df_239.columns):
for col_name in ["pct_outliers_iqr", "pct_outliers_z"]:
if col_name not in outlier_df_239.columns:
outlier_df_239[col_name] = 0.0
outlier_core_239 = outlier_df_239[["feature", "pct_outliers_iqr", "pct_outliers_z"]].drop_duplicates("feature")
base_239 = base_239.merge(outlier_core_239, on="feature", how="left")
else:
base_239["pct_outliers_iqr"] = None
base_239["pct_outliers_z"] = None

# 6) Aggregate rule confidence per feature
if (not rule_conf_df_239.empty) and ("feature" in rule_conf_df_239.columns):
agg_rule_conf_239 = (
rule_conf_df_239
.groupby("feature", dropna=False)
.agg(
avg_confidence=("confidence_score", "mean"),
n_rules=("rule_type", "count"),
n_hard_rules=("hard_vs_soft", lambda s: (s == "hard").sum()),
n_soft_rules=("hard_vs_soft", lambda s: (s == "soft").sum()),
)
.reset_index()
)
base_239 = base_239.merge(agg_rule_conf_239, on="feature", how="left")
else:
base_239["avg_confidence"] = None
base_239["n_rules"] = 0
base_239["n_hard_rules"] = 0
base_239["n_soft_rules"] = 0

# 7) Compute pct_rows_impacted & readiness_score
if "null_pct" in base_239.columns:
null_pct_239 = base_239["null_pct"].fillna(0.0)
else:
null_pct_239 = pd.Series(0.0, index=base_239.index)

range_violation_pct_239 = base_239["total_violation_pct"].fillna(0.0)
out_iqr_239 = base_239["pct_outliers_iqr"].fillna(0.0)
out_z_239 = base_239["pct_outliers_z"].fillna(0.0)

max_out_pct_239 = out_iqr_239.combine(out_z_239, func=lambda a, b: max(a, b))

pct_rows_impacted_239 = null_pct_239.combine(range_violation_pct_239, max)
pct_rows_impacted_239 = pct_rows_impacted_239.combine(max_out_pct_239, max)
base_239["pct_rows_impacted"] = pct_rows_impacted_239

# avg_conf_239 = base_239["avg_confidence"].fillna(0.8)
# replaced with:
base_239["avg_confidence"] = pd.to_numeric(base_239.get("avg_confidence"), errors="coerce")
avg_conf_239 = base_239["avg_confidence"].astype("Float64").fillna(0.8)

base_239["total_violation_pct"] = pd.to_numeric(base_239.get("total_violation_pct"), errors="coerce")
base_239["pct_outliers_iqr"] = pd.to_numeric(base_239.get("pct_outliers_iqr"), errors="coerce")
base_239["pct_outliers_z"] = pd.to_numeric(base_239.get("pct_outliers_z"), errors="coerce")
base_239["null_pct"] = pd.to_numeric(base_239.get("null_pct"), errors="coerce")
#

#
n_hard_239 = base_239["n_hard_rules"].fillna(0)

readiness_raw_239 = (
1.0
- (pct_rows_impacted_239 / 100.0) * 0.7
- (n_hard_239 > 0).astype(float) * 0.05
- (avg_conf_239 < 0.7).astype(float) * 0.05
)
base_239["readiness_score"] = readiness_raw_239.clip(0.0, 1.0)

base_239["hard_rule_violations"] = (
(range_violation_pct_239 > 0.0) & (n_hard_239 > 0)
).astype(bool)

# 8) Final column ordering + write artifact
model_readiness_cols = [
col for col in [
"feature",
"column" if "column" in base_239.columns else None,
"role" if "role" in base_239.columns else None,
"feature_group" if "feature_group" in base_239.columns else None,
"numeric_integrity_status" if "numeric_integrity_status" in base_239.columns else None,
"pct_rows_impacted",
"readiness_score",
"n_rules",
"n_hard_rules",
"n_soft_rules",
"avg_confidence",
"hard_rule_violations",
] if col is not None
]

model_readiness_df_239 = base_239[model_readiness_cols].copy()

tmp_239 = model_readiness_path_239.with_suffix(".tmp.csv")
model_readiness_df_239.to_csv(tmp_239, index=False)
os.replace(tmp_239, model_readiness_path_239)

print(f"üíæ Wrote model readiness report ‚Üí {model_readiness_path_239}")
if not model_readiness_df_239.empty:
print("\nüìä 2.3.9 model readiness report (head):")
display(model_readiness_df_239.head(20))
else:
print(" (no features found for model readiness)")

# 9) Section 2.3.9 summary row
n_features_239 = int(model_readiness_df_239.shape[0])
avg_readiness_239 = float(model_readiness_df_239["readiness_score"].mean()) if n_features_239 else None
n_low_readiness_239 = int((model_readiness_df_239["readiness_score"] < 0.6).sum()) if n_features_239 else 0

if n_features_239 == 0:
status_239 = "SKIP"
else:
frac_low = n_low_readiness_239 / max(1, n_features_239)
status_239 = "OK" if frac_low <= 0.3 else "WARN"

summary_239 = pd.DataFrame([{
"section": "2.3.9",
"section_name": "Model readiness impact summary",
"check": "Per-feature readiness scores based on numeric quality",
"level": "info",
"status": status_239,
"n_features": int(n_features_239),
"avg_readiness": float(avg_readiness_239) if avg_readiness_239 is not None else None,
"n_low_readiness": int(n_low_readiness_239),
"detail": "model_readiness_report.csv",
"timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_239, SECTION2_REPORT_PATH)
display(summary_239)
```

```text
2.3.9 üßÆ Model readiness impact summary
‚ö†Ô∏è /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/numeric_integrity/rule_confidence_scores.csv missing ‚Äî no rule confidence info for 2.3.9.
üíæ Wrote model readiness report ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/numeric_integrity/model_readiness_report.csv

üìä 2.3.9 model readiness report (head):
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature</th>
      <th>column</th>
      <th>role</th>
      <th>feature_group</th>
      <th>numeric_integrity_status</th>
      <th>pct_rows_impacted</th>
      <th>readiness_score</th>
      <th>n_rules</th>
      <th>n_hard_rules</th>
      <th>n_soft_rules</th>
      <th>avg_confidence</th>
      <th>hard_rule_violations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Churn_flag</td>
      <td>Churn_flag</td>
      <td>target</td>
      <td>target</td>
      <td>ok</td>
      <td>0.000</td>
      <td>1.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>MonthlyCharges</td>
      <td>MonthlyCharges</td>
      <td>feature</td>
      <td>numeric_continuous</td>
      <td>ok</td>
      <td>0.000</td>
      <td>1.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SeniorCitizen</td>
      <td>SeniorCitizen</td>
      <td>feature</td>
      <td>binary</td>
      <td>warn</td>
      <td>16.215</td>
      <td>0.886495</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>TotalCharges</td>
      <td>TotalCharges</td>
      <td>feature</td>
      <td>numeric_continuous</td>
      <td>warn</td>
      <td>0.156</td>
      <td>0.998908</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>tenure</td>
      <td>tenure</td>
      <td>feature</td>
      <td>numeric_continuous</td>
      <td>ok</td>
      <td>0.000</td>
      <td>1.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

># A:
The disconnect is in how 2.3.9 loads rule confidence: 2.3.7.4 successfully writes `rule_confidence_scores.csv`, but the branch that actually populates `rule_conf_df_239` in 2.3.9 never runs, so 2.3.9 behaves as if there were no rules.  

## Where 2.3.7.4 ends

- 2.3.7.4 builds `rule_conf_df_2374` from range, outlier, temporal, and correlation artifacts, then writes it to `NUMERIC_DIR / "rule_confidence_scores.csv"`.  
- Your screenshot confirms that this file exists and has 5 rows, all with `hard_vs_soft == "soft"` and valid `confidence_score` values, so the artifact itself is fine.  

## Where 2.3.9 starts to diverge

- In 2.3.9, you construct `rule_conf_path_239 = NUMERIC_DIR / "rule_confidence_scores.csv"` and immediately check `if not rule_conf_path_239.exists():`.  
- In the log you pasted, 2.3.9 prints that this file is missing and falls into the `else` branch that sets `rule_conf_df_239 = pd.DataFrame()`, so the later aggregation sees zero rules and leaves `n_rules`, `n_hard_rules`, `n_soft_rules`, and `avg_confidence` empty or zero.
- That is why your model-readiness table shows `n_rules == 0` and `avg_confidence` as `NaN` for all features, even though 2.3.7.4 clearly wrote rules.

## Why this happens in practice

- The most likely cause is that **`NUMERIC_DIR` is not the same object** at the moment 2.3.9 runs as it was when 2.3.7.4 wrote the file.
  - For example, if 2.3.7.4 runs with `NUMERIC_DIR = SEC2_REPORTS_DIR / "numeric_integrity"` but later another cell or section redefines `NUMERIC_DIR` (or `SEC2_REPORTS_DIR`) to a different path, 2.3.9 will look in the wrong directory and not see the file.
  - Your 2.3.9 log even prints a warning that the file is missing and then tries a fallback path under `REPORTS_DIR / section2 / numeric_integrity`, which indicates it is already anticipating possible path mismatches.

## What to check and fix

- Verify that the value of **`NUMERIC_DIR` is identical** in both sections:
  - Right after 2.3.7.4 (before the write) and right before 2.3.9, print `NUMERIC_DIR` and ensure they point to the same directory.
  - If they differ, ensure that you set `NUMERIC_DIR` only once for the whole numeric pipeline or that both 2.3.7.4 and 2.3.9 derive it from a common, unchanged base.
- Also confirm that both are using the same base `REPORTS_DIR` and `SEC2_REPORTS_DIR`, so that `NUMERIC_DIR / "rule_confidence_scores.csv"` in 2.3.9 is the path where 2.3.7.4 actually wrote the file.

Once `NUMERIC_DIR` is consistent, the `if not rule_conf_path_239.exists():` check will pass, `rule_conf_df_239` will be loaded with your 5 rules, and the `avg_confidence`, `n_rules`, and related fields will be populated in the 2.3.9 model readiness report.
