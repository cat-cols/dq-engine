># Q: Why are the mentioned anomaly sources not configured and how do i fix it without using functions?

```python
# 2.5.11 üßæ Anomaly context index
print("\n2.5.11 üßæ Anomaly context index")
print("   üîç Debug 2.5.11 ‚Äî ANOMALY_CONTEXT presence")

# --- 0) Resolve a canonical base dir for relative anomaly source paths
# Prefer your existing canonical section2 reports dir if present; else fall back to SEC2_REPORT_DIRS.
if "sec2_reports_dir" in globals() and sec2_reports_dir:
    section2_reports_dir_2511 = Path(sec2_reports_dir).resolve()
elif "REPORTS_DIR" in globals() and REPORTS_DIR:
    section2_reports_dir_2511 = (Path(REPORTS_DIR) / "section2").resolve()
elif "SEC2_REPORT_DIRS" in globals() and isinstance(SEC2_REPORT_DIRS, dict) and "2.5" in SEC2_REPORT_DIRS:
    section2_reports_dir_2511 = Path(SEC2_REPORT_DIRS["2.5"]).resolve()
else:
    section2_reports_dir_2511 = Path.cwd().resolve()

section2_reports_dir_2511.mkdir(parents=True, exist_ok=True)

# --- 1) Pull config (Type-3 style)
anomaly_cfg_2511 = {}
if "CONFIG" in globals() and isinstance(CONFIG, dict):
    _ac_2511 = CONFIG.get("ANOMALY_CONTEXT", {})
    if isinstance(_ac_2511, dict):
        anomaly_cfg_2511 = _ac_2511

sources_2511 = anomaly_cfg_2511.get("SOURCES", {}) if isinstance(anomaly_cfg_2511, dict) else {}

severity_filter_2511 = set()
if isinstance(anomaly_cfg_2511, dict):
    _sev = anomaly_cfg_2511.get("INCLUDE_SEVERITIES", None)
    if isinstance(_sev, (list, tuple, set)):
        severity_filter_2511 = set(str(s).lower() for s in _sev)

max_rows_2511 = anomaly_cfg_2511.get("MAX_ROWS", None)
try:
    max_rows_2511 = int(max_rows_2511) if max_rows_2511 is not None else None
except Exception:
    max_rows_2511 = None

run_id_2511 = anomaly_cfg_2511.get("RUN_ID", None)
if not run_id_2511:
    run_id_2511 = f"sec2_{pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%SZ')}"

# --- UX: show configured sources BEFORE reading files
print("   üîß ANOMALY_CONTEXT sources resolved:")
print(f"      ‚Ä¢ BASE_DIR (for relative paths): {section2_reports_dir_2511}")
print(f"      ‚Ä¢ INCLUDE_SEVERITIES: {sorted(severity_filter_2511) if severity_filter_2511 else '[none ‚Üí all severities]'}")
print(f"      ‚Ä¢ MAX_ROWS: {max_rows_2511 if max_rows_2511 is not None else '[no cap]'}")

if sources_2511 and isinstance(sources_2511, dict):
    for _src_name, _src_cfg in sources_2511.items():
        if not isinstance(_src_cfg, dict):
            continue
        print(
            f"      ‚Ä¢ {_src_name}: path={_src_cfg.get('path','')} | fmt={_src_cfg.get('format','csv')} "
            f"| section={_src_cfg.get('section_ref','')} | type={_src_cfg.get('anomaly_type', _src_name)}"
        )
else:
    print("      (no SOURCES configured)")

# --- 2) Collect anomalies
anomaly_rows_2511 = []

n_sources_cfg_2511 = len(sources_2511) if isinstance(sources_2511, dict) else 0
n_sources_with_file_2511 = 0
n_sources_nonempty_2511 = 0
n_rows_raw_2511 = 0

# 3) Load + normalize anomalies from each source
if isinstance(sources_2511, dict):
    for src_name_2511, src_cfg_2511 in sources_2511.items():
        if not isinstance(src_cfg_2511, dict):
            continue

        rel_path = str(src_cfg_2511.get("path", "") or "").strip()
        if not rel_path:
            continue

        fmt = str(src_cfg_2511.get("format", "csv") or "csv").lower()

        # determine full path; if not absolute, resolve relative to section2_reports_dir_2511
        src_path = Path(rel_path)
        if not src_path.is_absolute():
            src_path = (section2_reports_dir_2511 / src_path).resolve()
        else:
            src_path = src_path.resolve()

        if not src_path.exists():
            print(f"   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: {src_name_2511} ‚Üí {src_path}")
            continue

        n_sources_with_file_2511 += 1

        # Column mapping
        row_key_col      = str(src_cfg_2511.get("row_key_col", "") or "").strip()
        rule_id_col      = str(src_cfg_2511.get("rule_id_col", "rule_id") or "rule_id").strip()
        anomaly_type_val = src_cfg_2511.get("anomaly_type", src_name_2511)
        feature_cols     = src_cfg_2511.get("feature_cols", [])
        severity_col     = str(src_cfg_2511.get("severity_col", "severity") or "severity").strip()
        magnitude_col    = str(src_cfg_2511.get("magnitude_col", "") or "").strip()
        section_ref_val  = str(src_cfg_2511.get("section_ref", "") or "").strip()

        if not isinstance(feature_cols, (list, tuple)):
            feature_cols = []

        # Load
        try:
            if fmt == "parquet":
                _df_src_2511 = pd.read_parquet(src_path)
            else:
                _df_src_2511 = pd.read_csv(src_path)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Could not read anomaly source {src_name_2511} ({src_path}): {e}")
            continue
        #
        n_rows_src_2511 = int(len(_df_src_2511))
        n_rows_raw_2511 += n_rows_src_2511
        #
        if _df_src_2511.empty:
            print(f"   ‚úÖ Source '{src_name_2511}': rows=0 | anomalies_kept=0")
            continue
        #
        n_sources_nonempty_2511 += 1
        #
        anomalies_kept_this_source = 0

        # UX
        n_rows_src_2511 = len(_df_src_2511)
        print(
            f"   ‚úÖ Source '{src_name_2511}': "
            f"rows={n_rows_src_2511} | "
            f"anomalies_kept={sum(1 for r in anomaly_rows_2511 if r['source_name'] == src_name_2511)}"
        )

        # Normalize per-row anomalies
        for _idx_2511, _row_2511 in _df_src_2511.iterrows():
            # Row key
            if row_key_col and row_key_col in _df_src_2511.columns:
                _row_key = _row_2511[row_key_col]
            elif "customerID" in _df_src_2511.columns:
                _row_key = _row_2511["customerID"]
            elif "id" in _df_src_2511.columns:
                _row_key = _row_2511["id"]
            else:
                _row_key = _idx_2511

            # Rule & type
            _rule_id = _row_2511[rule_id_col] if rule_id_col in _df_src_2511.columns else f"{src_name_2511}"
            _anom_type = anomaly_type_val

            # Severity
            _sev = str(_row_2511[severity_col]).lower() if severity_col in _df_src_2511.columns else "info"
            if severity_filter_2511 and _sev not in severity_filter_2511:
                continue

            # Magnitude
            if magnitude_col and magnitude_col in _df_src_2511.columns:
                try:
                    _mag = float(_row_2511[magnitude_col])
                except Exception:
                    _mag = float("nan")
            else:
                _mag = float("nan")

            # Feature names joined
            _feat_names = []
            for _fc in feature_cols:
                if _fc in _df_src_2511.columns:
                    _val = _row_2511[_fc]
                    if isinstance(_val, str) and _val.strip():
                        _feat_names.append(_val.strip())
                    else:
                        _feat_names.append(str(_fc))
            _feat_names = [f for f in _feat_names if f]

            # Extra context (avoid huge stuff)
            extra_keys = [c for c in _df_src_2511.columns if c not in {row_key_col, rule_id_col, severity_col, magnitude_col}]
            extra_ctx = {}
            for _ck in extra_keys:
                _cv = _row_2511[_ck]
                extra_ctx[_ck] = str(_cv) if isinstance(_cv, (list, dict)) else _cv

            anomaly_rows_2511.append(
                {
                    "run_id": run_id_2511,
                    "row_key": _row_key,
                    "rule_id": _rule_id,
                    "section_ref": section_ref_val,
                    "anomaly_type": _anom_type,
                    "feature_names": ", ".join(_feat_names) if _feat_names else "",
                    "severity": _sev,
                    "magnitude": _mag,
                    "source_name": src_name_2511,
                    "created_at_utc": pd.Timestamp.utcnow(),
                    "extra_context_json": json.dumps(extra_ctx, default=str),
                }
            )
            anomalies_kept_this_source += 1

        print(f"   ‚úÖ Source '{src_name_2511}': rows={n_rows_src_2511} | anomalies_kept={anomalies_kept_this_source}")

# 4) Build DataFrame + sampling
anomaly_df_2511 = pd.DataFrame(anomaly_rows_2511)

if max_rows_2511 is not None and not anomaly_df_2511.empty and len(anomaly_df_2511) > max_rows_2511:
    _non_info_mask = anomaly_df_2511["severity"].isin(["warn", "fail"])
    _non_info = anomaly_df_2511[_non_info_mask]
    _info = anomaly_df_2511[~_non_info_mask]
    remaining = max_rows_2511 - len(_non_info)
    if remaining > 0:
        _info_sampled = _info.sample(n=min(remaining, len(_info)), random_state=42)
        anomaly_df_2511 = pd.concat([_non_info, _info_sampled], ignore_index=True)
    else:
        anomaly_df_2511 = _non_info.copy()

# 5) Persist
anomaly_path_2511 = (section2_reports_dir_2511 / "logic_anomaly_context.parquet").resolve()
try:
    anomaly_df_2511.to_parquet(anomaly_path_2511, index=False)
except Exception as e:
    print(f"   ‚ö†Ô∏è Could not write logic_anomaly_context.parquet: {e}")

# 6) Diagnostics summary + status
n_anomalies_2511 = int(len(anomaly_df_2511)) if not anomaly_df_2511.empty else 0
n_rules_covered_2511 = int(anomaly_df_2511["rule_id"].nunique()) if not anomaly_df_2511.empty else 0

# status logic (tune as you like)
if n_sources_cfg_2511 == 0:
    status_2511 = "INFO"
elif n_sources_with_file_2511 == 0:
    status_2511 = "INFO"
elif n_rows_raw_2511 == 0:
    status_2511 = "INFO"
elif n_anomalies_2511 == 0:
    status_2511 = "WARN" if severity_filter_2511 else "INFO"
else:
    status_2511 = "OK"

summary_2511 = pd.DataFrame([{
    "section": "2.5.11",
    "section_name": "Anomaly context index",
    "check": "Assemble row-level anomaly index from logic checks for explainability integration",
    "level": "info",
    "status": status_2511,
    "n_sources_configured": int(n_sources_cfg_2511),
    "n_sources_with_file": int(n_sources_with_file_2511),
    "n_sources_nonempty": int(n_sources_nonempty_2511),
    "n_rows_raw": int(n_rows_raw_2511),
    "n_anomalies": int(n_anomalies_2511),
    "n_rules_covered": int(n_rules_covered_2511),
    "detail": str(anomaly_path_2511.name),
    "timestamp": pd.Timestamp.utcnow(),
}])

if "append_sec2" in globals() and callable(append_sec2) and "SECTION2_REPORT_PATH" in globals() and SECTION2_REPORT_PATH:
    append_sec2(summary_2511, SECTION2_REPORT_PATH)
else:
    print("‚ÑπÔ∏è append_sec2/SECTION2_REPORT_PATH not available; skipped unified append.")

append_sec2(summary_2511, SECTION2_REPORT_PATH)

# 7) Console UX
print(f"üíæ 2.5.11 logic_anomaly_context.parquet ‚Üí {anomaly_path_2511}")
print(f"   Sources configured: {n_sources_cfg_2511}")
print(f"   Sources with existing files: {n_sources_with_file_2511}")
print(f"   Sources with non-empty data: {n_sources_nonempty_2511}")
print(f"   Raw anomaly rows before severity filter: {n_rows_raw_2511}")
print(f"   Anomalies recorded after severity/filter/sampling: {n_anomalies_2511}")
print(f"   Rules covered in context index: {n_rules_covered_2511}")

if not anomaly_df_2511.empty:
    print("   üìã Anomaly preview (top 10):")
    display(
        anomaly_df_2511.loc[:, [
            "row_key","rule_id","section_ref","anomaly_type","feature_names",
            "severity","magnitude","source_name"
        ]].head(10)
    )
    _sev_counts_2511 = anomaly_df_2511["severity"].value_counts(dropna=False).to_dict()
    print(f"   Severity breakdown: {_sev_counts_2511}")
else:
    print("   ‚ÑπÔ∏è No anomalies recorded (after filters).")

display(anomaly_df_2511)
display(summary_2511)

2.5.11 üßæ Anomaly context index
   üîç Debug 2.5.11 ‚Äî ANOMALY_CONTEXT presence
   üîß ANOMALY_CONTEXT sources resolved:
      ‚Ä¢ BASE_DIR (for relative paths): /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2
      ‚Ä¢ INCLUDE_SEVERITIES: ['fail', 'warn']
      ‚Ä¢ MAX_ROWS: 5000
      ‚Ä¢ dependency_rule_violation: path=dependency_violations.csv | fmt=csv | section=2.5.4 | type=dependency_violation
      ‚Ä¢ dependency_row_violation: path=dependency_row_violations.csv | fmt=csv | section=2.5.test | type=dependency_violation
      ‚Ä¢ mutual_exclusion_violation: path=mutual_exclusion_report.csv | fmt=csv | section=2.5.3 | type=mutual_exclusion
      ‚Ä¢ ratio_check_violation: path=ratio_consistency_report.csv | fmt=csv | section=2.5.5 | type=ratio_violation
      ‚Ä¢ temporal_violation: path=temporal_logic_report.csv | fmt=csv | section=2.5.6 | type=temporal_violation
      ‚Ä¢ onehot_integrity_violation: path=onehot_integrity_report.csv | fmt=csv | section=2.5.8 | type=onehot_violation
      ‚Ä¢ drift_numeric_psi: path=numeric_drift_psi.csv | fmt=csv | section=3.2 | type=numeric_drift
      ‚Ä¢ drift_categorical_psi: path=categorical_drift_psi.csv | fmt=csv | section=3.3 | type=categorical_drift
      ‚Ä¢ drift_logic_dependency: path=logic_drift_dependency.csv | fmt=csv | section=3.4 | type=logic_drift_dependency
      ‚Ä¢ churn_logic_rules: path=logic_anomalies/churn_rules.csv | fmt=csv | section=2.5.?? | type=logic_rule_violation
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: dependency_rule_violation ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/dependency_violations.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: dependency_row_violation ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/dependency_row_violations.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: mutual_exclusion_violation ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/mutual_exclusion_report.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: ratio_check_violation ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/ratio_consistency_report.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: temporal_violation ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/temporal_logic_report.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: onehot_integrity_violation ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/onehot_integrity_report.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: drift_numeric_psi ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/numeric_drift_psi.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: drift_categorical_psi ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/categorical_drift_psi.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: drift_logic_dependency ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/logic_drift_dependency.csv
   ‚ÑπÔ∏è Anomaly source missing for 2.5.11: churn_logic_rules ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/logic_anomalies/churn_rules.csv
üßæ Appended diagnostics ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/section2_data_quality_20251222_151057.csv
üßæ Appended diagnostics ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/section2_data_quality_20251222_151057.csv
üíæ 2.5.11 logic_anomaly_context.parquet ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/resources/reports/section2/logic_anomaly_context.parquet
   Sources configured: 10
   Sources with existing files: 0
   Sources with non-empty data: 0
   Raw anomaly rows before severity filter: 0
   Anomalies recorded after severity/filter/sampling: 0
   Rules covered in context index: 0
   ‚ÑπÔ∏è No anomalies recorded (after filters).

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>section</th>
      <th>section_name</th>
      <th>check</th>
      <th>level</th>
      <th>status</th>
      <th>n_sources_configured</th>
      <th>n_sources_with_file</th>
      <th>n_sources_nonempty</th>
      <th>n_rows_raw</th>
      <th>n_anomalies</th>
      <th>n_rules_covered</th>
      <th>detail</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.5.11</td>
      <td>Anomaly context index</td>
      <td>Assemble row-level anomaly index from logic ch...</td>
      <td>info</td>
      <td>INFO</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>logic_anomaly_context.parquet</td>
      <td>2025-12-23 00:04:21.578532+00:00</td>
    </tr>
  </tbody>
</table>
</div>
```

># A:

They **are configured** ‚Äî your log proves it (‚ÄúSources configured: 10‚Äù).
They‚Äôre just **not found on disk**, because you‚Äôre resolving relative paths against the **wrong base folder**.

### What‚Äôs happening

You set:

```python
section2_reports_dir_2511 = ... /resources/reports/section2
```

Then for each source like `dependency_violations.csv` you do:

```python
src_path = (section2_reports_dir_2511 / "dependency_violations.csv")
```

But your actual files are almost certainly being written into the **2.5 report subfolder**, something like:

```text
.../resources/reports/section2/2_5/dependency_violations.csv
# or .../section2/2.5/... (depending on your naming)
```

That‚Äôs why every ‚Äúmissing‚Äù path is:

```text
.../resources/reports/section2/dependency_violations.csv   ‚ùå
```

instead of:

```text
.../resources/reports/section2/2_5/dependency_violations.csv ‚úÖ
```

---

## Fix (no functions): point BASE_DIR at the 2.5 report dir

Replace your ‚ÄúBASE_DIR‚Äù resolution block with this:

```python
# --- 0) Resolve canonical BASE_DIR for relative anomaly source paths
# Prefer the 2.5 report dir (because your 2.5.x outputs live there)
if "SEC2_REPORT_DIRS" in globals() and isinstance(SEC2_REPORT_DIRS, dict) and SEC2_REPORT_DIRS.get("2.5"):
    base_dir_2511 = Path(SEC2_REPORT_DIRS["2.5"]).resolve()
elif "SEC2_REPORTS_DIR" in globals() and SEC2_REPORTS_DIR:
    # fallback: try section2 root + "2_5"
    base_dir_2511 = (Path(SEC2_REPORTS_DIR).resolve() / "2_5").resolve()
elif "REPORTS_DIR" in globals() and REPORTS_DIR:
    base_dir_2511 = (Path(REPORTS_DIR).resolve() / "section2" / "2_5").resolve()
else:
    base_dir_2511 = Path.cwd().resolve()

base_dir_2511.mkdir(parents=True, exist_ok=True)

print(f"      ‚Ä¢ BASE_DIR (for relative paths): {base_dir_2511}")
```

Then, later where you resolve paths, change:

```python
src_path = (section2_reports_dir_2511 / src_path).resolve()
```

to:

```python
src_path = (base_dir_2511 / src_path).resolve()
```

That‚Äôs the core fix.

---

## Also: you are writing the parquet to the wrong place

You currently persist here:

```python
anomaly_path_2511 = (section2_reports_dir_2511 / "logic_anomaly_context.parquet")
```

But you already have a canonical artifact path:

‚úÖ `ANOMALY_CONTEXT_PATH` (from bootstrap) under **artifacts/section2/2_5** (recommended)

So swap your persist block to this (still no functions):

```python
# 5) Persist to canonical artifact path (preferred)
if "ANOMALY_CONTEXT_PATH" in globals() and ANOMALY_CONTEXT_PATH:
    anomaly_path_2511 = Path(ANOMALY_CONTEXT_PATH).resolve()
else:
    anomaly_path_2511 = (base_dir_2511 / "logic_anomaly_context.parquet").resolve()

try:
    anomaly_df_2511.to_parquet(anomaly_path_2511, index=False)
except Exception as e:
    print(f"   ‚ö†Ô∏è Could not write logic_anomaly_context.parquet: {e}")
```

---

## Extra bug: you append twice

You have:

```python
if "append_sec2" ...:
    append_sec2(...)
else:
    ...
append_sec2(summary_2511, SECTION2_REPORT_PATH)
```

Delete the last line. Keep only one append.

---

## Quick ‚Äúno-function‚Äù debug: list the directory you *think* contains the files

Drop this right after you set `base_dir_2511`:

```python
print("   üîç Listing BASE_DIR (top-level):")
try:
    print([p.name for p in base_dir_2511.iterdir()][:40])
except Exception as e:
    print("   ‚ö†Ô∏è Could not list BASE_DIR:", e)
```

If you don‚Äôt see `dependency_violations.csv` etc., then your reports are being written somewhere else ‚Äî but 99% of the time it‚Äôs just the **2.5 subfolder** mismatch.

---

## Summary

They *are* configured. They‚Äôre ‚Äúmissing‚Äù because you resolve relative paths against `.../section2/` instead of `.../section2/2_5/` (your 2.5 report directory). Switch the BASE_DIR to `SEC2_REPORT_DIRS["2.5"]`, and write the parquet to `ANOMALY_CONTEXT_PATH`.

If you paste what `SEC2_REPORT_DIRS["2.5"]` prints and the actual filenames you see in that folder, I‚Äôll adjust the base-dir logic to match your exact naming (e.g., `2_5` vs `2.5`).
