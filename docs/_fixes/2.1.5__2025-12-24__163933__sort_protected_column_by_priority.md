How do i make it sort the summary_215 column 'protected' by what is most important first (i.e. True)

```python
# 2.1.5 üß¨ Feature Group Registration | Column-level Feature Catalog
print("\n2.1.5 üß¨ Feature Group Registration | Column-level Feature Catalog")

# REQUIREMENTS: special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
# TODO: fix numbering issues in 2.1.5 & PART B

# Guards
assert "df" in globals(), "‚ùå df not found. Run Section 2.0.0 first."
assert "CONFIG" in globals(), "‚ùå CONFIG not found. Run 2.0.0 first."
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing. Run 2.0.1 first."
assert "SEC2_REPORTS_DIR" in globals(), "‚ùå SEC2_REPORTS_DIR missing. Run 2.0.0/2.0.1 first."
assert "SEC2_ARTIFACTS_DIR" in globals(), "‚ùå SEC2_ARTIFACTS_DIR missing. Run 2.0.0 first."
assert "id_cols" in globals(), "‚ùå id_cols not found. Run 2.1.2 first."

SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# 1) Config + thresholds
fg_cfg = CONFIG.get("FEATURE_GROUPING", {}) or {}
low_card_threshold = int(fg_cfg.get("LOW_CARDINALITY_THRESHOLD", 20))
free_text_min_avg_len = int(fg_cfg.get("FREE_TEXT_MIN_AVG_LEN", 30))

ordinal_cfg = CONFIG.get("ORDINAL_COLUMNS", []) or []
if isinstance(ordinal_cfg, (str, bytes)):
    ordinal_cols = [ordinal_cfg]
else:
    ordinal_cols = list(ordinal_cfg)

protected_cfg = CONFIG.get("PROTECTED_COLUMNS", []) or []
if isinstance(protected_cfg, (str, bytes)):
    protected_from_config = {protected_cfg}
else:
    protected_from_config = set(protected_cfg)

# 2) Resolve target columns (prefer CONFIG, fallback to previously-resolved globals)
raw_target_col = None
encoded_target_col = None

_target_block = CONFIG.get("TARGET", {}) or {}
_raw_from_cfg = _target_block.get("RAW_COLUMN")
_enc_from_cfg = _target_block.get("COLUMN")

# Raw target (e.g., "Churn")
if _raw_from_cfg and _raw_from_cfg in df.columns:
    raw_target_col = _raw_from_cfg
else:
    _prev_raw = globals().get("raw_target_col")
    if isinstance(_prev_raw, str) and _prev_raw in df.columns:
        raw_target_col = _prev_raw

# Encoded target (e.g., "Churn_flag")
if _enc_from_cfg and _enc_from_cfg in df.columns:
    encoded_target_col = _enc_from_cfg
else:
    _prev_enc = globals().get("encoded_target_col")
    if isinstance(_prev_enc, str) and _prev_enc in df.columns:
        encoded_target_col = _prev_enc

print("üéØ raw_target_col:", raw_target_col, "| encoded_target_col:", encoded_target_col)

# 3) Load special numeric flags (from 2.1.3)
special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
special_flag_cols = set()

if special_flags_path.exists() and special_flags_path.stat().st_size > 0:
    try:
        _special_flags_df = pd.read_csv(special_flags_path)
    except Exception:
        _special_flags_df = pd.DataFrame()
    if "column" in _special_flags_df.columns:
        special_flag_cols = set(
            _special_flags_df["column"].dropna().astype("string")
        )
else:
    _special_flags_df = pd.DataFrame(columns=["column", "role"])

# 4) Build protected-columns set
protected_cols = set()

# IDs
protected_cols.update(id_cols)

# Targets
if raw_target_col is not None:
    protected_cols.add(raw_target_col)
if encoded_target_col is not None:
    protected_cols.add(encoded_target_col)

# Special numeric flags
protected_cols.update(special_flag_cols)

# Config-driven protected list
protected_cols.update(protected_from_config)

# Ensure only columns that actually exist in df are considered protected
# Only keep those that actually exist in df
protected_cols = {c for c in protected_cols if c in df.columns}

# --- 2.1.5.5 Feature grouping logic (per-column catalog) --------------------
feature_group_rows = []

for col in df.columns:
    s = df[col]
    dtype_str = str(s.dtype)
    n_unique = int(s.nunique(dropna=True))
    is_protected = col in protected_cols

    # Base notes str we can enrich
    notes = []

    # Target / target_aux
    if encoded_target_col is not None and col == encoded_target_col:
        feature_group = "target"
        notes.append("binary target (encoded)")
    elif raw_target_col is not None and col == raw_target_col:
        feature_group = "target_aux"
        notes.append("raw target label")
    # ID / primary key
    elif col in id_cols:
        feature_group = "id"
        notes.append("ID / key candidate")
    # Explicit ordinal from config
    elif col in ordinal_cols:
        feature_group = "ordinal"
        notes.append("ordinal from CONFIG.ORDINAL_COLUMNS")
    # Special-case numeric flags from 2.1.3
    elif col in special_flag_cols:
        feature_group = "numeric_flag"
        notes.append("special numeric flag (2.1.3)")
    else:
        # Type-based rules
        if pd.api.types.is_datetime64_any_dtype(s):
            feature_group = "datetime"
            notes.append("datetime-like dtype")
        elif pd.api.types.is_bool_dtype(s):
            feature_group = "numeric_flag"
            notes.append("bool ‚Üí treated as flag")
        elif pd.api.types.is_numeric_dtype(s):
            # numeric: detect potential flag-like or discrete small-card
            if n_unique <= low_card_threshold and n_unique <= 10:
                feature_group = "numeric_flag"
                notes.append(
                    f"numeric small-card (n_unique={n_unique} ‚â§ {low_card_threshold})"
                )
            else:
                feature_group = "numeric_continuous"
                notes.append("numeric continuous / high-card")
        else:
            # object / string-like / category
            # quick heuristic for free text: high card + long strings
            try:
                avg_len = float(
                    s.dropna()
                    .astype("string")
                    .str.len()
                    .mean()
                )
            except Exception:
                avg_len = 0.0

            if n_unique <= low_card_threshold:
                feature_group = "categorical_low_card"
                notes.append(
                    f"low-card categorical (n_unique={n_unique} ‚â§ {low_card_threshold})"
                )
            else:
                if avg_len >= free_text_min_avg_len:
                    feature_group = "free_text"
                    notes.append(
                        f"free text (avg_len‚âà{avg_len:.1f} ‚â• {free_text_min_avg_len})"
                    )
                else:
                    feature_group = "categorical_high_card"
                    notes.append(
                        f"high-card categorical (n_unique={n_unique} > {low_card_threshold})"
                    )

    feature_group_rows.append(
        {
            "column": col,
            "dtype": dtype_str,
            "feature_group": feature_group,
            "n_unique": n_unique,
            "protected": bool(is_protected),
            "notes": "; ".join(notes),
        }
    )

#
feature_groups_df = pd.DataFrame(feature_group_rows)

# Just in case, ensure feature_group is not missing
feature_groups_df["feature_group"] = feature_groups_df["feature_group"].fillna("other")

# metrics?
n_features = int(len(feature_groups_df))
n_protected = int(feature_groups_df["protected"].sum())
n_unassigned = int((feature_groups_df["feature_group"] == "other").sum())

# --- 2.1.5.6 Persist CSV artifact under SEC2_REPORTS_DIR --------------------
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups.csv"
fg_tmp_csv = fg_csv_path.with_suffix(".tmp.csv")

feature_groups_df.to_csv(fg_tmp_csv, index=False)
os.replace(fg_tmp_csv, fg_csv_path)

# TODO: turn checkmark ‚úÖ into a function
print(f"2.1.5 feature groups CSV written ‚Üí {fg_csv_path} ‚úÖ")
display(feature_groups_df.head(30))

# Evidence helpers for notes/debug
_detected_id_rows = feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].tolist()

# CREATE ISSUES LIST
issues_215 = []

_detected_id_cols = set(
    feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].astype("string")
)
_config_id_cols = set(id_cols)  # canonical IDs from 2.1.2

missing_from_protection = sorted(_detected_id_cols - _config_id_cols)
if missing_from_protection:
    issues_215.append(f"id-like cols not in id_cols: {missing_from_protection}")

# Unassigned
if n_unassigned > 0:
    issues_215.append(f"{n_unassigned} column(s) unassigned ‚Üí feature_group='other'")

# Targets
if raw_target_col is None:
    issues_215.append("raw target not resolved (CONFIG.TARGET.RAW_COLUMN missing/invalid and no global fallback)")
if encoded_target_col is None:
    issues_215.append("encoded target not resolved (CONFIG.TARGET.COLUMN missing/invalid and no global fallback)")

# IDs (strong signal if empty) with evidence
if not id_cols:
    issues_215.append(
        "id_cols is empty (2.1.2 may not have detected IDs; ID protection may be incomplete). "
        f"Columns currently grouped as id: {sorted(_detected_id_rows)}"
    )

# FIXME: integrate into 2.1.6: 2.1.5.6 Persist YAML / JSON artifact under SEC2_ARTIFACTS_DIR
# Build group ‚Üí columns mapping (+ protected info) for YAML/JSON
group_map = {}
for grp, sub_df in feature_groups_df.groupby("feature_group"):
    group_map[str(grp)] = sorted(sub_df["column"].astype("string").tolist())

protected_list = sorted(feature_groups_df.loc[feature_groups_df["protected"], "column"].astype("string").tolist())
meta = {
    "section": "2.1.5",
    "description": "Feature group catalog at end of Section 2.1",
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

fg_struct = {
    "groups": group_map,
    "meta": meta,
}

# Create JSON (latest + snapshot)
SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

fg_json_path = SEC2_ARTIFACTS_DIR / "feature_groups.json"

# timestamped snapshot (UTC)
ts_snap = pd.Timestamp.utcnow().strftime("%Y-%m-%d__%H%M%S") + "Z"
fg_json_snapshot = SEC2_ARTIFACTS_DIR / f"feature_groups__{ts_snap}.json"

# write latest atomically
fg_tmp_json = fg_json_path.with_suffix(".tmp.json")
with open(fg_tmp_json, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_json, fg_json_path)

# write snapshot (also atomically)
fg_tmp_snap = fg_json_snapshot.with_suffix(".tmp.json")
with open(fg_tmp_snap, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_snap, fg_json_snapshot)

print(f"üíæ feature groups JSON written ‚Üí {fg_json_path}")
print(f"üìå snapshot JSON written ‚Üí {fg_json_snapshot}")

# -- YAML (optional)
fg_yaml_path = SEC2_ARTIFACTS_DIR / "feature_groups.yaml"
if yaml is not None:
    fg_tmp_yaml = fg_yaml_path.with_suffix(".tmp.yaml")
    with open(fg_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(fg_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fg_tmp_yaml, fg_yaml_path)
    print(f"üíæ feature groups YAML written ‚Üí {fg_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature groups.")

# -- 2.1.5.7 Summary metrics + unified diagnostics row
group_counts = (
    feature_groups_df["feature_group"]
    .value_counts()
    .sort_index()
    .to_dict()
)

# Determine status (severity ladder)
status_215 = "OK"

# Hard failures (ERROR)
if not id_cols:
    status_215 = "ERROR"
# Medium failures (WARN) if not already ERROR
elif missing_from_protection:
    status_215 = "WARN"
elif raw_target_col is None or encoded_target_col is None:
    status_215 = "WARN"
elif n_unassigned > 0:
    status_215 = "WARN"


# Level mapping
level_215 = "info"
if status_215 == "WARN":
    level_215 = "warn"
elif status_215 == "ERROR":
    level_215 = "error"

notes_215 = "; ".join(issues_215) if issues_215 else None

# use scalar values ([{}])
summary_215 = pd.DataFrame([{
    "section":             "2.1.5",
    "section_name":        "Feature group registration",
    "check":               "Feature group catalog (column-level)",
    "level":               level_215,
    "status":              status_215,
    "n_features":          n_features,
    "n_protected":         n_protected,
    "n_unassigned":        n_unassigned,
    "group_counts_json":   json.dumps(group_counts, sort_keys=True),
    "feature_groups_csv":  fg_csv_path.name,
    "feature_groups_json": fg_json_path.name,
    "feature_groups_yaml": fg_yaml_path.name if yaml is not None else None,
    "timestamp": pd.Timestamp.utcnow(),
    "detail": (
        f"Feature groups registered for {n_features} column(s); "
        f"catalog written to {fg_csv_path.name}."),
    "notes":                notes_215,
}])

append_sec2(summary_215, SECTION2_REPORT_PATH)
display(summary_215)
```

># SOLUTION:

If you want **`summary_215`** to show ‚Äúprotected first‚Äù, you‚Äôre really talking about **sorting the *details* table (`feature_groups_df`)** (because `summary_215` is one row and doesn‚Äôt have a `protected` column list to sort).

Do this right before your `display(...)` (and/or before writing the CSV) by sorting with **protected True first**, then by feature group and column name:

```python
# Sort: protected=True first, then feature_group, then column
feature_groups_df = (
    feature_groups_df
    .sort_values(
        by=["protected", "feature_group", "column"],
        ascending=[False, True, True],
        kind="mergesort"  # stable sort (nice when you re-run)
    )
    .reset_index(drop=True)
)

display(feature_groups_df.head(30))
```

If you also want the CSV to be saved in that order, put the sort **before** your `to_csv(...)`.

üí°üí° If you want ‚Äúmost important protected‚Äù even *within* protected=True (targets first, then ids, then flags, etc.), define an explicit priority map and sort by it:

```python
# Priority (smaller = higher priority)
grp_priority = {
    "target": 0,
    "target_aux": 1,
    "id": 2,
    "numeric_flag": 3,
    "datetime": 4,
    "ordinal": 5,
    "numeric_continuous": 6,
    "categorical_low_card": 7,
    "categorical_high_card": 8,
    "free_text": 9,
    "other": 99,
}

feature_groups_df["_grp_priority"] = (
    feature_groups_df["feature_group"].map(grp_priority).fillna(999).astype(int)
)

feature_groups_df = (
    feature_groups_df
    .sort_values(
        by=["protected", "_grp_priority", "column"],
        ascending=[False, True, True],
        kind="mergesort"
    )
    .drop(columns=["_grp_priority"])
    .reset_index(drop=True)
)
```