{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301d1fe0",
   "metadata": {},
   "source": [
    "```\n",
    "OUTLINE:\n",
    "\n",
    "PROBLEM #1:\n",
    "```\n",
    "\n",
    "```\n",
    "# PROBLEM 1:\n",
    "\n",
    "# Import what we need\n",
    "from data.loader import DataLoader\n",
    "from data.preprocessor import clean_telco_data\n",
    "from analysis.statistics import perform_statistical_analysis\n",
    "\n",
    "# Load configuration\n",
    "import yaml\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Project: {config['project']['name']}\")\n",
    "print(f\"Data path: {config['data']['filepath']}\")\n",
    "\n",
    "output:\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[9], line 8\n",
    "      6 # Import what we need\n",
    "      7 from data.loader import DataLoader\n",
    "----> 8 from data.preprocessor import clean_telco_data\n",
    "      9 from analysis.statistics import perform_statistical_analysis\n",
    "     11 # Load configuration\n",
    "\n",
    "File ~/DATA/PROJECTS/Telco/Level_3/src/data/preprocessor.py:128\n",
    "    125     logger.info(f\"Train churn rate: {y_train.mean():.2%}, Test churn rate: {y_test.mean():.2%}\")\n",
    "    127     return X_train, X_test, y_train, y_test\n",
    "--> 128 src/telco_analysis/analysis/statistics.py\n",
    "    129 \"\"\"\n",
    "    130 Statistical Analysis Module - What Separates Data Analysts from Data Scientists\n",
    "    131 \n",
    "   (...)    140 - \"I report effect sizes alongside significance\"\n",
    "    141 \"\"\"\n",
    "    143 import pandas as pd\n",
    "\n",
    "NameError: name 'src' is not defined\n",
    "```\n",
    "\n",
    "# #1 SOLUTION:\n",
    "```\n",
    "I see whatâ€™s happening here. That **`NameError: name 'src' is not defined`** isnâ€™t actually about Python not knowing `src` â€” itâ€™s about an *extra line of text* that accidentally got left inside your `preprocessor.py` file.\n",
    "\n",
    "Take a look at the error trace:\n",
    "\n",
    "```python\n",
    "File ~/DATA/PROJECTS/Telco/Level_3/src/data/preprocessor.py:128\n",
    "...\n",
    "--> 128 src/telco_analysis/analysis/statistics.py\n",
    "```\n",
    "\n",
    "That line (`src/telco_analysis/analysis/statistics.py`) looks like a **stray reference** (maybe from a copy-paste, docstring, or bad merge). Python is trying to *execute it* as code, which triggers the `NameError`.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… How to Fix\n",
    "\n",
    "1. Open `src/data/preprocessor.py`.\n",
    "\n",
    "2. Scroll to around **line 128**.\n",
    "\n",
    "3. Remove or comment out the stray line:\n",
    "\n",
    "   ```python\n",
    "   # src/telco_analysis/analysis/statistics.py\n",
    "   ```\n",
    "\n",
    "   It should not be there.\n",
    "\n",
    "4. Save the file and rerun your script.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Why it happened\n",
    "\n",
    "It looks like you meant to **document** or **reference** another module (`analysis/statistics.py`), but instead of a comment or import, you pasted the raw path.\n",
    "\n",
    "* If your goal was to *import it*, you should have:\n",
    "\n",
    "  ```python\n",
    "  from analysis.statistics import perform_statistical_analysis\n",
    "  ```\n",
    "* If your goal was just to *leave a note*, you should use a comment:\n",
    "\n",
    "  ```python\n",
    "  # src/telco_analysis/analysis/statistics.py\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "âš¡ Once you delete that stray line, the import error should go away.\n",
    "\n",
    "Do you want me to also help you **restructure your imports** so that modules under `src/` are always found (using `__init__.py` and `sys.path` setup), instead of hitting these issues repeatedly?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    ">### PROBLEM 1: \"ModuleNotFoundError: No module named 'src'\"\n",
    "#### input:\n",
    "```py\n",
    "# notebooks/analysis.ipynb\n",
    "\n",
    "from src.data.loader import DataLoader\n",
    "from src.config import load_config\n",
    "\n",
    "config = load_config('config/config.yaml')\n",
    "loader = DataLoader(config)\n",
    "df = loader.load_telco_data('../resources/data/raw/Telco-Customer-Churn.csv')\n",
    "```\n",
    "\n",
    "#### output:\n",
    "```py\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[5], line 2\n",
    "      1 # notebooks/analysis.ipynb\n",
    "----> 2 from src.data.loader import DataLoader\n",
    "      3 from src.config import load_config\n",
    "      5 config = load_config('config/config.yaml')\n",
    "\n",
    "ModuleNotFoundError: No module named 'src'\n",
    "```\n",
    "\n",
    ">## FIX:\n",
    "Youâ€™re hitting that error because of how Python resolves imports. Right now your notebook is trying to import:\n",
    "\n",
    "```python\n",
    "from notebooks.src.data.loader import DataLoader\n",
    "```\n",
    "\n",
    "But your actual file is here:\n",
    "\n",
    "```\n",
    "/Users/b/DATA/PROJECTS/Telco/Level_3/src/data/loader.py\n",
    "```\n",
    "\n",
    "There is no `notebooks.src` package â€” so Python canâ€™t find it.\n",
    "\n",
    "---\n",
    "\n",
    "### Fix Options\n",
    "\n",
    "#### 1. Import directly from `src`\n",
    "\n",
    "If your project structure looks like this:\n",
    "\n",
    "```\n",
    "Level_3/\n",
    "â”œâ”€â”€ notebooks/\n",
    "â”‚   â””â”€â”€ analysis.ipynb\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â””â”€â”€ data/\n",
    "â”‚       â””â”€â”€ loader.py\n",
    "```\n",
    "\n",
    "Then in `analysis.ipynb` you should import like:\n",
    "\n",
    "```python\n",
    "from src.data.loader import DataLoader\n",
    "```\n",
    "\n",
    "But for that to work, your notebook needs to know about the `Level_3` folder as the project root.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Add project root to `sys.path`\n",
    "\n",
    "At the top of your notebook:\n",
    "\n",
    "```python\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n",
    "\n",
    "from src.data.loader import DataLoader\n",
    "```\n",
    "\n",
    "If your notebook is at `Level_3/notebooks/analysis.ipynb`, then `..` takes you back to `Level_3`, so `src/` is visible.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Use a `.env` or Jupyter kernel setup\n",
    "\n",
    "A cleaner approach for ongoing work is to configure `PYTHONPATH` so `src` is always discoverable:\n",
    "\n",
    "```bash\n",
    "export PYTHONPATH=/Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "```\n",
    "\n",
    "Or in VS Code add this to `.vscode/settings.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"python.analysis.extraPaths\": [\"./src\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "âœ… Recommended: Start with option 2 (quick fix), then move to option 3 for a permanent setup.\n",
    "\n",
    "Do you want me to show you how to make `src` always discoverable automatically in Jupyter (so you donâ€™t need to hack `sys.path` in every notebook)?\n",
    "\n",
    "\n",
    "># PROBLEM #2:\n",
    "> TypeError: 'NoneType' object is not subscriptable.\n",
    "# Practical Example\n",
    "## Using Our Modules in Practice\n",
    "\n",
    "# Import what we need\n",
    "from data.loader import DataLoader\n",
    "from data.preprocessor import clean_telco_data\n",
    "from analysis.statistics import perform_statistical_analysis\n",
    "\n",
    "# Load configuration\n",
    "import yaml\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Project: {config['project']['name']}\")\n",
    "print(f\"Data path: {config['data']['filepath']}\")\n",
    "\n",
    "output:\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "Cell In[12], line 17\n",
    "     14     config = yaml.safe_load(f)\n",
    "     16 print(\"Configuration loaded!\")\n",
    "---> 17 print(f\"Project: {config['project']['name']}\")\n",
    "     18 print(f\"Data path: {config['data']['filepath']}\")\n",
    "\n",
    "TypeError: 'NoneType' object is not subscriptable\n",
    "\n",
    ">## PROBLEM #2 FIX:\n",
    "`yaml.safe_load` returned **None**, which means your `../config/config.yaml` parsed as an **empty YAML document** (e.g., file is empty, only comments, or just `---` with nothing else). Thatâ€™s why `config['project']['name']` blows up: youâ€™re subscripting `None`.\n",
    "## Quick checks\n",
    "1. Open the file and confirm it has real keys/values:\n",
    "\n",
    "   ```yaml\n",
    "   project:\n",
    "     name: Telco Churn\n",
    "   data:\n",
    "     filepath: data/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "   ```\n",
    "2. Sanity-print what you loaded:\n",
    "   ```python\n",
    "   import yaml, pprint\n",
    "   with open('../config/config.yaml', 'r') as f:\n",
    "       config = yaml.safe_load(f)\n",
    "   print(\"Raw config:\", config)\n",
    "   ```\n",
    "## Make the loader robust\n",
    "Add a small guard so you get a helpful error instead of a cryptic traceback:\n",
    "```python\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "cfg_path = Path('../config/config.yaml')\n",
    "if not cfg_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found: {cfg_path.resolve()}\")\n",
    "\n",
    "with cfg_path.open('r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "if not isinstance(config, dict) or not config:\n",
    "    raise ValueError(\n",
    "        f\"Config file {cfg_path} is empty or invalid YAML. \"\n",
    "        \"Expected a mapping with keys like 'project' and 'data'.\"\n",
    "    )\n",
    "\n",
    "required = [('project', 'name'), ('data', 'filepath')]\n",
    "for section, key in required:\n",
    "    if section not in config or key not in config[section]:\n",
    "        raise KeyError(f\"Missing config key: {section}.{key}\")\n",
    "```\n",
    "## Minimal working `config.yaml`\n",
    "Copy this in to unblock:\n",
    "\n",
    "```yaml\n",
    "project:\n",
    "  name: Telco Churn\n",
    "data:\n",
    "  filepath: data/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "```\n",
    "\n",
    "## Why this happens\n",
    "\n",
    "* **Empty file / only comments** â†’ `safe_load` returns `None`.\n",
    "* **Only `---`** (YAML doc start) with no content â†’ `None`.\n",
    "* **Multiple docs** (using `---` and `...`) but the first is empty and you used `safe_load` instead of `safe_load_all`. If you truly have multiple documents, do:\n",
    "\n",
    "  ```python\n",
    "  docs = list(yaml.safe_load_all(open('../config/config.yaml')))\n",
    "  print(docs)  # pick the doc you need\n",
    "  ```\n",
    "\n",
    "Once the file has valid YAML with those keys, your prints will work:\n",
    "\n",
    "```text\n",
    "Configuration loaded!\n",
    "Project: Telco Churn\n",
    "Data path: data/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "```\n",
    "># PROBLEM # 3: \n",
    "```py\n",
    "# Cell 1: Statistical Analysis Deep Dive\n",
    "\"\"\"\n",
    "# Statistical Analysis with Business Context\n",
    "\n",
    "This notebook demonstrates how to apply statistical thinking to business problems.\n",
    "\"\"\"\n",
    "\n",
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Our modules\n",
    "from data.loader import DataLoader\n",
    "from data.preprocessor import clean_telco_data\n",
    "from src.analysis.statistics import (\n",
    "    test_numerical_vs_churn,\n",
    "    test_categorical_vs_churn,\n",
    "    identify_risk_segments\n",
    ")\n",
    "\n",
    "output:\n",
    "---------------------------------------------------------------------------\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[7], line 22\n",
    "     20 from data.loader import DataLoader\n",
    "     21 from data.preprocessor import clean_telco_data\n",
    "---> 22 from src.analysis.statistics import (\n",
    "     23     test_numerical_vs_churn,\n",
    "     24     test_categorical_vs_churn,\n",
    "     25     identify_risk_segments\n",
    "     26 )\n",
    "\n",
    "ModuleNotFoundError: No module named 'src'\n",
    "```\n",
    "># SOLUTION # 3: \n",
    "```\n",
    "the error is because youâ€™re importing from src.analysis... while your sys.path already points inside src. So Python is looking for a top-level package named src (which it canâ€™t find).\n",
    "\n",
    "Since you did:\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))  # => adds .../Level_3/src to sys.path\n",
    "\n",
    "\n",
    "you should import packages directly under src/ like this:\n",
    "\n",
    "# correct imports when .../Level_3/src is on sys.path\n",
    "from data.loader import DataLoader\n",
    "from data.preprocessor import clean_telco_data\n",
    "from analysis.statistics import (\n",
    "    test_numerical_vs_churn,\n",
    "    test_categorical_vs_churn,\n",
    "    identify_risk_segments\n",
    ")\n",
    "\n",
    "Quick checklist\n",
    "\n",
    "Ensure your layout is:\n",
    "\n",
    "Level_3/\n",
    "â”œâ”€ src/\n",
    "â”‚  â”œâ”€ data/\n",
    "â”‚  â”‚  â”œâ”€ __init__.py\n",
    "â”‚  â”‚  â””â”€ loader.py\n",
    "â”‚  â”œâ”€ analysis/\n",
    "â”‚  â”‚  â”œâ”€ __init__.py\n",
    "â”‚  â”‚  â””â”€ statistics.py\n",
    "â”‚  â””â”€ data/\n",
    "â””â”€ notebooks/\n",
    "\n",
    "\n",
    "Make sure __init__.py exists in data/ and analysis/ (helps avoid namespace weirdness).\n",
    "\n",
    "Keep sys.path pointing to .../Level_3/src (as you did), and do not prefix imports with src.\n",
    "\n",
    "Alternative (if you insist on from src.analysis ...)\n",
    "\n",
    "Insert the project root instead, and treat src as the package:\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))  # adds .../Level_3\n",
    "from src.analysis.statistics import ...\n",
    "\n",
    "\n",
    "In that case, add src/__init__.py (or rely on namespace packages). Generally not recommendedâ€”using the project name as the top-level package (e.g., telco.analysis) is cleaner.\n",
    "```\n",
    "\n",
    "># PROBLEM # 4: \n",
    "```py\n",
    "\n",
    "# Cell 2: Load and Prepare Data\n",
    "\"\"\"\n",
    "## Load and Prepare Data\n",
    "\n",
    "Using our modular functions makes this clean and reproducible\n",
    "\"\"\"\n",
    "\n",
    "# Load configuration\n",
    "import yaml\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load data\n",
    "loader = DataLoader(config)\n",
    "df_raw, load_report = loader.load_data(config['data']['filepath'])\n",
    "\n",
    "print(\"Data Load Report:\")\n",
    "for key, value in load_report.items():\n",
    "    if key != 'dtypes':  # Skip dtypes for brevity\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean data\n",
    "df_clean = clean_telco_data(df_raw)\n",
    "print(f\"\\nCleaned data shape: {df_clean.shape}\")\n",
    "\n",
    "output:\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[10], line 15\n",
    "     13 # Load data\n",
    "     14 loader = DataLoader(config)\n",
    "---> 15 df_raw, load_report = loader.load_data(config['data']['filepath'])\n",
    "     17 print(\"Data Load Report:\")\n",
    "     18 for key, value in load_report.items():\n",
    "\n",
    "KeyError: 'filepath'\n",
    "```\n",
    "># SOLUTION # 4: \n",
    "```\n",
    "The traceback makes perfect sense:\n",
    "\n",
    "Your config currently looks like this (from what you shared earlier):\n",
    "\n",
    "data:\n",
    "  raw_path: \"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"data/processed/telco_clean.csv\"\n",
    "\n",
    "\n",
    "But your code is trying to do:\n",
    "\n",
    "config['data']['filepath']\n",
    "\n",
    "\n",
    "ðŸ‘‰ There is no filepath key, so you get a KeyError.\n",
    "\n",
    "âœ… Two Ways to Fix\n",
    "Option 1: Update Python code to use the keys you actually have\n",
    "# Load data\n",
    "df_raw, load_report = loader.load_data(config['data']['raw_path'])\n",
    "\n",
    "\n",
    "If later you want to load the processed version:\n",
    "\n",
    "df_clean, load_report = loader.load_data(config['data']['processed_path'])\n",
    "\n",
    "Option 2: Change your YAML to match your code\n",
    "\n",
    "If you prefer your Python to stay with filepath, update your config:\n",
    "\n",
    "data:\n",
    "  filepath: \"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "ðŸ”‘ Best Practice\n",
    "\n",
    "Iâ€™d recommend Option 1 (keep raw_path and processed_path in YAML) â€” itâ€™s clearer as your project grows. Your loader can even accept either key:\n",
    "\n",
    "raw_file = config['data'].get('raw_path') or config['data'].get('filepath')\n",
    "df_raw, load_report = loader.load_data(raw_file)\n",
    "```\n",
    "\n",
    "> # PROBLEM # 5: \n",
    "```\n",
    "My raw data actually lives here because I am doing multiple projects with the same data: /Users/b/DATA/PROJECTS/Telco/resources/data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "ChatGPT said:\n",
    "\n",
    "Got it ðŸ‘ â€” thanks for clarifying. Since youâ€™re re-using the same raw dataset across multiple projects, itâ€™s best to make that path explicit and keep your project configs consistent.\n",
    "```\n",
    "\n",
    "> # SOLUTION # 5: \n",
    "Since youâ€™re re-using the same raw dataset across multiple projects, itâ€™s best to make that path explicit and keep your project configs consistent.\n",
    "âœ… Update your config file\n",
    "\n",
    "Instead of this:\n",
    "\n",
    "```py\n",
    "data:\n",
    "  raw_path: \"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"data/processed/telco_clean.csv\"\n",
    "```\n",
    "\n",
    "use the absolute path for the shared raw data, and keep the processed data relative to each project:\n",
    "```py\n",
    "data:\n",
    "  raw_path: \"/Users/b/DATA/PROJECTS/Telco/resources/data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"data/processed/telco_clean.csv\"\n",
    "```\n",
    "\n",
    "That way:\n",
    "\n",
    "raw_path â†’ always points to your central shared dataset\n",
    "\n",
    "processed_path â†’ lives inside each projectâ€™s working directory (Level_3/data/processed/â€¦)\n",
    "```py\n",
    "âœ… Update your notebook code\n",
    "# Load data\n",
    "df_raw, load_report = loader.load_data(config['data']['raw_path'])\n",
    "\n",
    "print(\"Data Load Report:\")\n",
    "for key, value in load_report.items():\n",
    "    if key != 'dtypes':  # Skip dtypes for brevity\n",
    "        print(f\"  {key}: {value}\")\n",
    "```\n",
    "```\n",
    "ðŸ”‘ Best Practice (Flexible paths)\n",
    "\n",
    "If youâ€™d like portability (e.g., move between machines), you can parameterize a base data directory in your config and build paths dynamically:\n",
    "```\n",
    "\n",
    "```py\n",
    "paths:\n",
    "  base_data: \"/Users/b/DATA/PROJECTS/Telco/resources/data\"\n",
    "\n",
    "data:\n",
    "  raw_filename: \"raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"processed/telco_clean.csv\"\n",
    "```\n",
    "\n",
    "Then in code:\n",
    "```py\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(config['paths']['base_data'])\n",
    "raw_file = base / config['data']['raw_filename']\n",
    "processed_file = Path(\"data\") / config['data']['processed_path']\n",
    "```\n",
    "\n",
    "This way, only base_data changes if you move your datasets.\n",
    "\n",
    "># PROBLEM # 6: \n",
    "Should I add code to save the cleaned data?\n",
    "\n",
    "># SOLUTION # 6:\n",
    "```\n",
    "Yes âœ… â€” saving the cleaned dataset is a really good idea. It gives you a reproducible checkpoint so you donâ€™t have to re-run cleaning every time, and downstream steps (EDA, modeling) can just read the cleaned file.\n",
    "\n",
    "ðŸ”‘ Why save cleaned data\n",
    "\n",
    "Reproducibility â†’ same input for everyone using the project.\n",
    "\n",
    "Speed â†’ skip cleaning on every run.\n",
    "\n",
    "Versioning â†’ keep track of raw vs processed.\n",
    "\n",
    "Flexibility â†’ you can reload the cleaned version when experimenting.\n",
    "\n",
    "âœ… How to do it\n",
    "1. Update your config.yaml\n",
    "\n",
    "You already have:\n",
    "\n",
    "data:\n",
    "  raw_path: \"/Users/b/DATA/PROJECTS/Telco/resources/data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"data/processed/telco_clean.csv\"\n",
    "\n",
    "\n",
    "Make sure the processed_path is there.\n",
    "\n",
    "2. Add saving code to your notebook\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the directory exists\n",
    "processed_path = Path(config['data']['processed_path'])\n",
    "processed_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "print(f\"Cleaned data saved to {processed_path}\")\n",
    "\n",
    "3. Optional improvements\n",
    "\n",
    "Save as Parquet (faster I/O, preserves types):\n",
    "\n",
    "df_clean.to_parquet(processed_path.with_suffix('.parquet'), index=False)\n",
    "\n",
    "\n",
    "Add a timestamped version if you want to keep history:\n",
    "\n",
    "import datetime\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_file = processed_path.with_name(f\"telco_clean_{ts}.csv\")\n",
    "df_clean.to_csv(versioned_file, index=False)\n",
    "\n",
    "\n",
    "Wrap this into a helper function in your utils/io_ops.py (e.g., save_dataset(df, path)).\n",
    "```\n",
    "\n",
    "># PROBLEM # 7: \n",
    ">Why does this not clean the 11 entries with blanks in the TotalCharges?\n",
    "```\n",
    "\"\"\"\n",
    "## Load and Prepare Data\n",
    "\n",
    "Using our modular functions makes this clean and reproducible\n",
    "\"\"\"\n",
    "\n",
    "# Load configuration\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load data\n",
    "df_raw, load_report = loader.load_data(config['data']['raw_path'])\n",
    "\n",
    "print(\"Data Load Report:\")\n",
    "for key, value in load_report.items():\n",
    "    if key != 'dtypes':  # Skip dtypes for brevity\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean data\n",
    "df_clean = clean_telco_data(df_raw)\n",
    "print(f\"\\nCleaned data shape: {df_clean.shape}\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "processed_path = Path(config['data']['processed_path'])\n",
    "processed_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "print(f\"Cleaned data saved to {processed_path}\")\n",
    "\n",
    "# Optional improvements\n",
    "\n",
    "# 1. Save as Parquet (faster I/O, preserves types):\n",
    "# df_clean.to_parquet(processed_path.with_suffix('.parquet'), index=False)\n",
    "\n",
    "# 2. Add a timestamped version if you want to keep history:\n",
    "\n",
    "# import datetime\n",
    "# ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# versioned_file = processed_path.with_name(f\"telco_clean_{ts}.csv\")\n",
    "# df_clean.to_csv(versioned_file, index=False)\n",
    "```\n",
    "># SOLUTION # 7A:\n",
    "```\n",
    "Short answer: those 11 â€œblanksâ€ in TotalCharges are actually space-only strings (\" \") in the CSV. If your loader/cleaner doesnâ€™t explicitly convert space-only strings â†’ NaN â†’ numeric, theyâ€™ll sneak through as strings and never get fixed.\n",
    "```\n",
    "Hereâ€™s a bullet-proof way to handle it:\n",
    "```py\n",
    "1) Make sure you load TotalCharges as text first (so you can sanitize)\n",
    "\n",
    "If your DataLoader sets dtypes, donâ€™t force TotalCharges to numeric on read. Either let pandas infer, or explicitly read it as str and clean later.\n",
    "\n",
    "# inside DataLoader.load_data(...)\n",
    "df = pd.read_csv(config['data']['raw_path'], dtype={\"TotalCharges\": \"string\"})\n",
    "\n",
    "2) In clean_telco_data, normalize whitespace â†’ NaN â†’ numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_telco_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) Trim whitespace in object/string cols\n",
    "    str_cols = out.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "    out[str_cols] = out[str_cols].apply(lambda s: s.str.strip())\n",
    "\n",
    "    # 2) Convert space-only TotalCharges to NaN, then numeric\n",
    "    out[\"TotalCharges\"] = (\n",
    "        out[\"TotalCharges\"]\n",
    "        .replace(r\"^\\s*$\", np.nan, regex=True)     # empty/space-only â†’ NaN\n",
    "        .replace(\"\", np.nan)                       # double-safety\n",
    "    )\n",
    "    out[\"TotalCharges\"] = pd.to_numeric(out[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "    # 3) Business rule: when tenure == 0, set TotalCharges = 0 (the classic 11 rows)\n",
    "    mask_tenure0 = out[\"tenure\"].fillna(0).astype(float).eq(0)\n",
    "    out.loc[mask_tenure0, \"TotalCharges\"] = out.loc[mask_tenure0, \"TotalCharges\"].fillna(0.0)\n",
    "\n",
    "    # 4) Sanity check: after fix, there should be no NaN left in TotalCharges\n",
    "    #    If any remain, surface them loudly (these would be true data issues).\n",
    "    remaining = out[\"TotalCharges\"].isna().sum()\n",
    "    if remaining:\n",
    "        raise ValueError(f\"TotalCharges still has {remaining} NaN after cleaning (non-tenure==0 rows).\")\n",
    "\n",
    "    # (Optional) enforce types you expect downstream\n",
    "    out[\"TotalCharges\"] = out[\"TotalCharges\"].astype(float)\n",
    "    out[\"MonthlyCharges\"] = pd.to_numeric(out[\"MonthlyCharges\"], errors=\"coerce\")\n",
    "\n",
    "    return out\n",
    "\n",
    "3) Quick diagnostic you can run before/after cleaning\n",
    "# BEFORE\n",
    "n_blank = df_raw[\"TotalCharges\"].astype(str).str.fullmatch(r\"\\s*\").sum()\n",
    "print(\"Space-only TotalCharges rows (raw):\", n_blank)\n",
    "\n",
    "# AFTER\n",
    "print(\"Any NaN in TotalCharges after cleaning?\", df_clean[\"TotalCharges\"].isna().any())\n",
    "print(\"dtype:\", df_clean[\"TotalCharges\"].dtype)\n",
    "\n",
    "Common gotchas that cause your symptom\n",
    "\n",
    "Cleaning uses fillna(0) without first converting \" \" to NaN â†’ blanks remain strings.\n",
    "\n",
    "Converted to numeric with errors=\"ignore\" (or not at all) â†’ strings persist.\n",
    "\n",
    "Read CSV with skipinitialspace=True and assume it fixes everything â€” it doesnâ€™t remove entirely space-only fields.\n",
    "\n",
    "If you paste this clean_telco_data in place, those 11 rows (tenure==0) will have TotalCharges=0.0, TotalCharges will be float, and no blanks remain.\n",
    "```\n",
    "># SOLUTION # 7B: \n",
    "Short answer: your cleaning function isnâ€™t catching the **11 rows where `TotalCharges` is a blank string on rows with `tenure == 0`**. Common causes:\n",
    "\n",
    "1. `TotalCharges` is read as **object/string** and the blanks are `\" \"` (spaces), not true NaN.\n",
    "2. You convert to numeric **without stripping**, so `\" \"` stays non-NaN (or raises).\n",
    "3. You fill NaNs but **donâ€™t gate by `tenure == 0`**.\n",
    "\n",
    "Hereâ€™s a robust, drop-in fix you can add inside `clean_telco_data` (or run once to verify):\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fix_total_charges_blank_for_zero_tenure(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure tenure is numeric\n",
    "    df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "\n",
    "    # Normalize TotalCharges: strip spaces, set empty -> NaN, then to numeric\n",
    "    tc = (df['TotalCharges']\n",
    "            .astype('string')          # safe string dtype\n",
    "            .str.strip()               # remove whitespace\n",
    "            .replace('', np.nan))      # blank -> NaN\n",
    "    df['TotalCharges'] = pd.to_numeric(tc, errors='coerce')\n",
    "\n",
    "    # Only set to 0 when tenure == 0 and TotalCharges is NaN (the 11 special rows)\n",
    "    mask = (df['tenure'].fillna(0) == 0) & (df['TotalCharges'].isna())\n",
    "    df.loc[mask, 'TotalCharges'] = 0.0\n",
    "\n",
    "    # Optional: report to confirm we hit 11 rows\n",
    "    print(f\"Filled TotalCharges=0 for {mask.sum()} rows where tenure==0 and TotalCharges blank.\")\n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "If you prefer to do it at **load** time, make your loader more NA-aware:\n",
    "\n",
    "```python\n",
    "pd.read_csv(\n",
    "    path,\n",
    "    dtype={'TotalCharges': 'string'},      # keep as string first\n",
    "    na_values=['', ' ', 'NA', 'NaN'],      # treat blanks as NaN\n",
    "    keep_default_na=True,\n",
    "    skipinitialspace=True\n",
    ")\n",
    "```\n",
    "\n",
    "Then call the fix:\n",
    "\n",
    "```python\n",
    "df_raw = loader.load_data(config['data']['raw_path'])[0]\n",
    "df_clean = clean_telco_data(df_raw)\n",
    "df_clean = fix_total_charges_blank_for_zero_tenure(df_clean)\n",
    "```\n",
    "\n",
    "Finally, assert the dataset is in the expected state:\n",
    "\n",
    "```python\n",
    "# Expect exactly 11 rows corrected\n",
    "assert ((df_clean['tenure'] == 0) & (df_clean['TotalCharges'] == 0)).sum() >= 11\n",
    "\n",
    "# Guard against any remaining NaNs\n",
    "assert df_clean['TotalCharges'].notna().all()\n",
    "```\n",
    "\n",
    "If this still doesnâ€™t print â€œ11,â€ check for **column name mismatches** (e.g., `total_charges` vs `TotalCharges`) or upstream transformations that renamed/typed the columns.\n",
    "\n",
    "># WHERE DO I PUT IT?\n",
    "\n",
    "Good clarifier ðŸ‘\n",
    "\n",
    "You **have two good options** for where this fix can live:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Option 1: Inside `loader.py`\n",
    "\n",
    "If you want every load of the Telco dataset to *always* normalize `TotalCharges`, you can fold it into your `DataLoader`.\n",
    "Example (`loader.py`):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self, path: str):\n",
    "        p = Path(path)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(p)\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            dtype={'TotalCharges': 'string'},\n",
    "            na_values=['', ' ', 'NA', 'NaN'],\n",
    "            keep_default_na=True,\n",
    "            skipinitialspace=True\n",
    "        )\n",
    "\n",
    "        # Fix TotalCharges blanks for tenure==0\n",
    "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "        mask = (df['tenure'] == 0) & (df['TotalCharges'].isna())\n",
    "        df.loc[mask, 'TotalCharges'] = 0.0\n",
    "\n",
    "        report = {\n",
    "            \"rows\": len(df),\n",
    "            \"cols\": df.shape[1],\n",
    "            \"nulls\": df.isna().sum().to_dict(),\n",
    "            \"dtypes\": df.dtypes.to_dict()\n",
    "        }\n",
    "        return df, report\n",
    "```\n",
    "\n",
    "Benefit: every time you use the loader, the fix happens automatically.\n",
    "Downside: it mixes **raw ingestion** with **domain-specific cleaning**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Option 2: Inside `preprocessor.py` (clean\\_telco\\_data)\n",
    "\n",
    "If you want your loader to stay â€œpureâ€ (just bring the raw CSV into memory), keep the fix in `clean_telco_data`.\n",
    "Example (`preprocessor.py`):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_telco_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize TotalCharges\n",
    "    tc = (df['TotalCharges']\n",
    "            .astype('string')\n",
    "            .str.strip()\n",
    "            .replace('', np.nan))\n",
    "    df['TotalCharges'] = pd.to_numeric(tc, errors='coerce')\n",
    "\n",
    "    # Fix blanks where tenure == 0\n",
    "    mask = (df['tenure'] == 0) & (df['TotalCharges'].isna())\n",
    "    df.loc[mask, 'TotalCharges'] = 0.0\n",
    "\n",
    "    # (other cleaning steps here...)\n",
    "    return df\n",
    "```\n",
    "\n",
    ">**Benefit:** clean separation â€” loader does **I/O**, preprocessor does **data fixes**.\n",
    ">**Downside:** you must remember to always run the preprocessor after loading.\n",
    "\n",
    "># âœ… Recommendation for your project\n",
    "\n",
    "Since this `TotalCharges` issue is **domain-specific to Telco**, it belongs in `preprocessor.py` (`clean_telco_data`), not `loader.py`.\n",
    "\n",
    "That way:\n",
    "\n",
    "* `loader.py` stays reusable across other datasets.\n",
    "* `preprocessor.py` documents your Telco-specific cleaning rules in one place.\n",
    "\n",
    "---\n",
    "># PROBLEM # 9: AssertionError\n",
    "```py\n",
    "# Verify data cleaning of TotalCharges\n",
    "# Count tenure==0 rows\n",
    "n_tenure0 = (df_clean['tenure'] == 0).sum()\n",
    "print(\"Rows with tenure==0:\", n_tenure0)\n",
    "\n",
    "# How many had TotalCharges fixed to 0?\n",
    "n_fixed = ((df_clean['tenure'] == 0) & (df_clean['TotalCharges'] == 0)).sum()\n",
    "print(\"Rows with tenure==0 AND TotalCharges==0:\", n_fixed)\n",
    "\n",
    "# Sanity: any NaNs left in TotalCharges?\n",
    "print(\"Remaining NaNs in TotalCharges:\", df_clean['TotalCharges'].isna().sum())\n",
    "\n",
    "# Optional hard assertions (will raise if unexpected)\n",
    "assert n_fixed == 11, f\"Expected 11 fixed rows, found {n_fixed}\"\n",
    "assert df_clean['TotalCharges'].notna().all(), \"TotalCharges still has NaNs!\"\n",
    "\n",
    "output:\n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "Cell In[20], line 14\n",
    "     11 print(\"Remaining NaNs in TotalCharges:\", df_clean['TotalCharges'].isna().sum())\n",
    "     13 # Optional hard assertions (will raise if unexpected)\n",
    "---> 14 assert n_fixed == 11, f\"Expected 11 fixed rows, found {n_fixed}\"\n",
    "     15 assert df_clean['TotalCharges'].notna().all(), \"TotalCharges still has NaNs!\"\n",
    "\n",
    "AssertionError: Expected 11 fixed rows, found 0\n",
    "```\n",
    "\n",
    "># SOLUTION # 9:\n",
    "\n",
    "># PROBLEM # 10:\n",
    "```py \n",
    "import  pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Our modules\n",
    "from utils.loader import DataLoader\n",
    "from utils.preprocessor import clean_telco_data\n",
    "from utils.statistics import (\n",
    "    test_numerical_vs_churn,\n",
    "    test_categorical_vs_churn,\n",
    "    identify_risk_segments\n",
    ")\n",
    "```\n",
    "># SOLUTION # 10:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dc6dc",
   "metadata": {},
   "source": [
    ">### PROBLEM 1: \"ModuleNotFoundError: No module named 'src'\"\n",
    "#### input:\n",
    "```py\n",
    "# notebooks/analysis.ipynb\n",
    "\n",
    "from src.data.loader import DataLoader\n",
    "from src.config import load_config\n",
    "\n",
    "config = load_config('config/config.yaml')\n",
    "loader = DataLoader(config)\n",
    "df = loader.load_telco_data('../resources/data/raw/Telco-Customer-Churn.csv')\n",
    "```\n",
    "\n",
    "#### output:\n",
    "```py\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[5], line 2\n",
    "      1 # notebooks/analysis.ipynb\n",
    "----> 2 from src.data.loader import DataLoader\n",
    "      3 from src.config import load_config\n",
    "      5 config = load_config('config/config.yaml')\n",
    "\n",
    "ModuleNotFoundError: No module named 'src'\n",
    "```\n",
    "\n",
    ">## FIX: Youâ€™re hitting that error because of how Python resolves imports. Right now your notebook is trying to import:\n",
    "\n",
    "```python\n",
    "from notebooks.src.data.loader import DataLoader\n",
    "```\n",
    "\n",
    "But your actual file is here:\n",
    "\n",
    "```\n",
    "/Users/b/DATA/PROJECTS/Telco/Level_3/src/data/loader.py\n",
    "```\n",
    "\n",
    "There is no `notebooks.src` package â€” so Python canâ€™t find it.\n",
    "\n",
    "---\n",
    "\n",
    "### Fix Options\n",
    "\n",
    "#### 1. Import directly from `src`\n",
    "\n",
    "If your project structure looks like this:\n",
    "\n",
    "```\n",
    "Level_3/\n",
    "â”œâ”€â”€ notebooks/\n",
    "â”‚   â””â”€â”€ analysis.ipynb\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â””â”€â”€ data/\n",
    "â”‚       â””â”€â”€ loader.py\n",
    "```\n",
    "\n",
    "Then in `analysis.ipynb` you should import like:\n",
    "\n",
    "```python\n",
    "from src.data.loader import DataLoader\n",
    "```\n",
    "\n",
    "But for that to work, your notebook needs to know about the `Level_3` folder as the project root.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Add project root to `sys.path`\n",
    "\n",
    "At the top of your notebook:\n",
    "\n",
    "```python\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n",
    "\n",
    "from src.data.loader import DataLoader\n",
    "```\n",
    "\n",
    "If your notebook is at `Level_3/notebooks/analysis.ipynb`, then `..` takes you back to `Level_3`, so `src/` is visible.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Use a `.env` or Jupyter kernel setup\n",
    "\n",
    "A cleaner approach for ongoing work is to configure `PYTHONPATH` so `src` is always discoverable:\n",
    "\n",
    "```bash\n",
    "export PYTHONPATH=/Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "```\n",
    "\n",
    "Or in VS Code add this to `.vscode/settings.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"python.analysis.extraPaths\": [\"./src\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "âœ… Recommended: Start with option 2 (quick fix), then move to option 3 for a permanent setup.\n",
    "\n",
    "Do you want me to show you how to make `src` always discoverable automatically in Jupyter (so you donâ€™t need to hack `sys.path` in every notebook)?\n",
    "\n",
    "```\n",
    "Great question. Letâ€™s unpack exactly where Python is looking when you do:\n",
    "\n",
    "```python\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ How Relative Paths Work in Python\n",
    "\n",
    "* Relative paths like `../config/config.yaml` are resolved **relative to the processâ€™s current working directory (CWD)** â€” *not* relative to the Python fileâ€™s location.\n",
    "* The CWD is usually the directory from which you launched Python or Jupyter.\n",
    "\n",
    "You can always check what that is:\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.getcwd())\n",
    "```\n",
    "\n",
    "If `os.getcwd()` prints, say:\n",
    "\n",
    "```\n",
    "/Users/b/DATA/PROJECTS/Telco/Level_3/src\n",
    "```\n",
    "\n",
    "then Python looks for:\n",
    "\n",
    "```\n",
    "/Users/b/DATA/PROJECTS/Telco/Level_3/config/config.yaml\n",
    "```\n",
    "\n",
    "(because `../` goes up one level from `src/` to `Level_3/`).\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Common Scenarios\n",
    "\n",
    "1. **Running from inside `src/`**\n",
    "\n",
    "   * CWD = `.../Level_3/src`\n",
    "   * `../config/config.yaml` â†’ `.../Level_3/config/config.yaml`\n",
    "\n",
    "2. **Running from project root (`Level_3/`)**\n",
    "\n",
    "   * CWD = `.../Level_3/`\n",
    "   * `../config/config.yaml` â†’ `.../config/config.yaml` (probably wrong, too far up).\n",
    "\n",
    "3. **Running in Jupyter Notebook**\n",
    "\n",
    "   * CWD is often the folder where the notebook lives (but can be the project root depending on how Jupyter launched).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ How to Make it Robust\n",
    "\n",
    "If you want the path **relative to the scriptâ€™s file**, not the CWD:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "# Path of the current script\n",
    "BASE_DIR = Path(__file__).resolve().parent.parent  # go up from src/ to project root\n",
    "cfg_path = BASE_DIR / \"config\" / \"config.yaml\"\n",
    "\n",
    "with cfg_path.open('r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "```\n",
    "\n",
    "This way it always resolves relative to your projectâ€™s folder structure, no matter where you run Python from.\n",
    "\n",
    "---\n",
    "```\n",
    "Q: would it be better to have my config folder and file at Level_3/config/config.yaml or Level_3/src/config/config.yaml\n",
    "```\n",
    "\n",
    "```\n",
    "Short answer: put it at the project root â€” Level_3/config/config.yaml, not under src/.\n",
    "\n",
    "Why root is better\n",
    "\n",
    "Separation of concerns: code lives in src/, configuration lives in config/. (12-factor friendly)\n",
    "\n",
    "Packaging: src/ is often your importable package; keeping config out means you donâ€™t accidentally ship environment-specific files.\n",
    "\n",
    "Reuse: notebooks, scripts, and tests outside src/ can all read config/ without fiddly paths.\n",
    "\n",
    "Overrides: easy to keep config/local.yaml in .gitignore while committing config/base.yaml.\n",
    "\n",
    "When src/config/ might make sense\n",
    "\n",
    "Youâ€™re distributing a library that needs default, versioned package_data bundled (and youâ€™ll access it via importlib.resources). For app projects, this is rare.\n",
    "```\n",
    "\n",
    "Recommended layout\n",
    "```\n",
    "Level_3/\n",
    "â”œâ”€ src/\n",
    "â”‚  â””â”€ your_package/...\n",
    "â”œâ”€ config/\n",
    "â”‚  â”œâ”€ base.yaml\n",
    "â”‚  â”œâ”€ dev.yaml\n",
    "â”‚  â”œâ”€ prod.yaml\n",
    "â”‚  â””â”€ local.yaml   # .gitignore\n",
    "â”œâ”€ data/\n",
    "â”œâ”€ notebooks/\n",
    "â”œâ”€ tests/\n",
    "â””â”€ run_analysis.py\n",
    "```\n",
    "Robust loading (works from anywhere)\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# project root = directory that contains the \"config\" folder\n",
    "ROOT = Path(__file__).resolve().parents[1]  # adjust if file is elsewhere\n",
    "CFG = ROOT / \"config\" / \"base.yaml\"\n",
    "\n",
    "with CFG.open() as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "Optional niceties\n",
    "\n",
    "Env var override:\n",
    "\n",
    "import os\n",
    "cfg_path = Path(os.getenv(\"APP_CONFIG\", ROOT / \"config\" / \"base.yaml\"))\n",
    "\n",
    "\n",
    "Layered configs (base + env-specific + local):\n",
    "\n",
    "def load_yaml(p): return yaml.safe_load(open(p)) or {}\n",
    "cfg = load_yaml(ROOT/\"config/base.yaml\")\n",
    "cfg |= load_yaml(ROOT/\"config/dev.yaml\")\n",
    "cfg |= load_yaml(ROOT/\"config/local.yaml\")\n",
    "\n",
    "\n",
    "If you want, I can drop in a tiny paths.py helper for centralizing ROOT, CONFIG_DIR, DATA_DIR, etc., so every module uses the same paths.\n",
    "```\n",
    "You said:\n",
    "what about my utils folder?\n",
    "ChatGPT said:\n",
    "Thought for 4 seconds\n",
    "\n",
    "Great question. Put your utils inside your package under src/ so theyâ€™re importable as part of your codeâ€”not at the project root.\n",
    "```\n",
    "âœ… Recommended placement\n",
    "Level_3/\n",
    "â”œâ”€ src/\n",
    "â”‚  â””â”€ telco/                # your package name\n",
    "â”‚     â”œâ”€ __init__.py\n",
    "â”‚     â”œâ”€ utils/             # utils lives here\n",
    "â”‚     â”‚  â”œâ”€ __init__.py\n",
    "â”‚     â”‚  â”œâ”€ io_ops.py       # file I/O, paths\n",
    "â”‚     â”‚  â”œâ”€ validate.py     # schema checks, assertions\n",
    "â”‚     â”‚  â”œâ”€ transform.py    # cleaning & feature funcs\n",
    "â”‚     â”‚  â”œâ”€ viz.py          # plotting helpers\n",
    "â”‚     â”‚  â””â”€ model.py        # baseline model helpers (optional)\n",
    "â”‚     â”œâ”€ data/              # domain modules, not raw data files\n",
    "â”‚     â””â”€ analysis/\n",
    "â”œâ”€ config/\n",
    "â”‚  â””â”€ base.yaml\n",
    "â”œâ”€ notebooks/\n",
    "â”œâ”€ tests/\n",
    "â””â”€ run_analysis.py\n",
    "```\n",
    "\n",
    "Why:\n",
    "\n",
    "Importable: from telco.utils.validate import run_checks works everywhere.\n",
    "\n",
    "Separation: keeps app code in src/ and non-code (configs, data, notebooks) outside.\n",
    "\n",
    "Refactor-friendly: easier to split utils by domain (I/O, validation, viz) instead of one junk drawer.\n",
    "\n",
    "ðŸ”§ Import patterns (clean & stable)\n",
    "\n",
    "In your scripts/notebooks:\n",
    "```py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure src on sys.path for notebooks/scripts run from project root\n",
    "sys.path.append(str(Path(__file__).resolve().parents[0] / \"src\"))  # or set in your notebook\n",
    "from telco.utils.io_ops import load_dataset\n",
    "from telco.utils.validate import run_checks\n",
    "```\n",
    "Inside package modules (prefer absolute imports):\n",
    "```py\n",
    "from telco.utils.io_ops import load_dataset\n",
    "```\n",
    "\n",
    "Avoid from utils import * and deep relative imports like from ..utils import x.\n",
    "\n",
    "ðŸ§© Make utils discoverable (optional)\n",
    "\n",
    "Expose a friendly top-level API in telco/utils/__init__.py:\n",
    "```py\n",
    "from .io_ops import load_dataset, save_table\n",
    "from .validate import run_checks\n",
    "from .transform import clean_and_feature_engineer\n",
    "\n",
    "__all__ = [\"load_dataset\", \"save_table\", \"run_checks\", \"clean_and_feature_engineer\"]\n",
    "```\n",
    "Now you can do:\n",
    "```py\n",
    "from telco import utils\n",
    "df = utils.load_dataset(path)\n",
    "```\n",
    "ðŸ—‚ï¸ What goes in utils vs domain modules?\n",
    "\n",
    ">utils: generic, reusable helpers (I/O wrappers, logging setup, plotting wrappers, schema checks)\n",
    "\n",
    ">domain modules (telco/data, telco/analysis): business-specific logic (Telco cleaning rules, churn-specific features)\n",
    "\n",
    ">If a function mentions Telco-specific columns or business rules, it belongs in a domain module, not utils.\n",
    "\n",
    "ðŸ§ª Testing layout\n",
    "```\n",
    "tests/\n",
    "â”œâ”€ test_utils_io_ops.py\n",
    "â”œâ”€ test_utils_validate.py\n",
    "â””â”€ test_transform_telco.py\n",
    "```\n",
    "ðŸ“ Small starter for io_ops.py\n",
    "```py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    if p.suffix == \".csv\":\n",
    "        return pd.read_csv(p)\n",
    "    if p.suffix == \".parquet\":\n",
    "        return pd.read_parquet(p)\n",
    "    raise ValueError(f\"Unsupported file type: {p.suffix}\")\n",
    "\n",
    "def ensure_dir(path: str | Path) -> Path:\n",
    "    p = Path(path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "```\n",
    "ðŸ§­ One place for common paths (nice-to-have)\n",
    "\n",
    "Create telco/paths.py:\n",
    "```py\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[2]   # up to Level_3/\n",
    "CONFIG = ROOT / \"config\"\n",
    "DATA = ROOT / \"data\"\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "```\n",
    "Then use:\n",
    "```py\n",
    "from telco.paths import CONFIG\n",
    "```\n",
    ">Bottom line: src/<your_package>/utils/ is the sweet spot. Keep it modular (io/validate/transform/viz), keep domain rules outside utils, and expose a clean import surface.\n",
    "\n",
    "\n",
    "># PROBLEM # 3:\n",
    "```\n",
    "Should I change my working directory? \n",
    "\n",
    "this is the current: /Users/b/DATA/PROJECTS/Telco/Level_3/notebooks\n",
    "\n",
    "ChatGPT said: Thatâ€™s a smart thing to consider ðŸ‘Œ\n",
    "```\n",
    "```\n",
    "Right now your working directory (CWD) is:\n",
    "\n",
    "/Users/b/DATA/PROJECTS/Telco/Level_3/notebooks\n",
    "\n",
    "\n",
    "That means any relative path (like ../config/config.yaml) is being resolved from inside notebooks/, not from your project root. Thatâ€™s why youâ€™ve been juggling ../ and hitting issues.\n",
    "```\n",
    "```\n",
    "ðŸŽ¯ Best Practices for CWD in a Project\n",
    "\n",
    "Project root as CWD\n",
    "\n",
    "If you treat Level_3/ as your project root, running everything from there makes paths predictable:\n",
    "\n",
    "Level_3/\n",
    "â”œâ”€ src/\n",
    "â”œâ”€ config/\n",
    "â”œâ”€ data/\n",
    "â”œâ”€ notebooks/\n",
    "â””â”€ run_analysis.py\n",
    "\n",
    "\n",
    "Then your code can load config simply as config/config.yaml without ../.\n",
    "\n",
    "Donâ€™t rely on â€œwhatever CWD isâ€\n",
    "\n",
    "Instead, compute paths relative to your package or project root:\n",
    "```\n",
    "```py\n",
    "from pathlib import Path\n",
    "ROOT = Path(__file__).resolve().parents[1]  # points to Level_3/\n",
    "CONFIG = ROOT / \"config\" / \"config.yaml\"\n",
    "```\n",
    "That way it doesnâ€™t matter if you launch code from notebooks/, src/, or even VS Code.\n",
    "\n",
    "For Jupyter Notebooks\n",
    "\n",
    "They often start in the notebooks/ folder, which is convenient for saving .ipynb files but awkward for relative imports.\n",
    "\n",
    "Two fixes:\n",
    "```py\n",
    "1. Add project root to sys.path:\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parents[0] / \"src\"))\n",
    "```\n",
    "\n",
    "**OR**\n",
    "\n",
    "```py \n",
    "2. launch Jupyter \n",
    "\n",
    "with project root as CWD:\n",
    "\n",
    "cd /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "jupyter notebook notebooks/\n",
    "```\n",
    "\n",
    "```\n",
    "âœ… My Recommendation for You\n",
    "- Keep your CWD as the project root (Level_3/) when running scripts or notebooks.\n",
    "- Put configs in Level_3/config/ and data in Level_3/data/.\n",
    "- Use path utilities (like a paths.py) so your code doesnâ€™t depend on where itâ€™s run.\n",
    "```\n",
    "---\n",
    "> # PROBLEM # 4:\n",
    "```\n",
    "from pathlib import Path\n",
    "ROOT = Path(__file__).resolve().parents[1]  # points to Level_3/\n",
    "CONFIG = ROOT / \"config\" / \"config.yaml\"\n",
    "\n",
    "output: \n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[7], line 2\n",
    "      1 from pathlib import Path\n",
    "----> 2 ROOT = Path(__file__).resolve().parents[1]  # points to Level_3/\n",
    "      3 CONFIG = ROOT / \"config\" / \"config.yaml\"\n",
    "\n",
    "NameError: name '__file__' is not defined\n",
    "```\n",
    "> # SOLUTION # 4:\n",
    "```\n",
    "that error is happening because youâ€™re running this inside a Jupyter notebook (or IPython).\n",
    "\n",
    "ðŸ‘‰ In a notebook there is no __file__, because __file__ only exists for actual Python files.\n",
    "```\n",
    "âœ… How to fix\n",
    "```py\n",
    "1. Use Path.cwd() in notebooks\n",
    "\n",
    "If you launch Jupyter from your project root (Level_3/):\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()              # current working directory\n",
    "CONFIG = ROOT / \"config\" / \"config.yaml\"\n",
    "print(CONFIG.resolve())\n",
    "```\n",
    "```py\n",
    "2. If your notebook lives in notebooks/ but project root is one level up\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[0]   # go up from notebooks/ â†’ Level_3/\n",
    "CONFIG = ROOT / \"config\" / \"config.yaml\"\n",
    "print(CONFIG.resolve())\n",
    "```\n",
    "```py\n",
    "3. Portable solution: detect notebook vs script\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Works in scripts\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    # Works in notebooks\n",
    "    ROOT = Path.cwd().parents[0]  # assumes youâ€™re in notebooks/\n",
    "    \n",
    "CONFIG = ROOT / \"config\" / \"config.yaml\"\n",
    "print(CONFIG.resolve())\n",
    "```\n",
    "```\n",
    "ðŸ”‘ Best Practice\n",
    "\n",
    "Always treat Level_3/ as your project root.\n",
    "\n",
    "In scripts â†’ use __file__.\n",
    "\n",
    "In notebooks â†’ use Path.cwd().parents[0].\n",
    "\n",
    "If you want to make this bulletproof, put it in a paths.py module under src/ so you import it consistently instead of repeating logic.\n",
    "```\n",
    "\n",
    "># PROBLEM 5\n",
    "```\n",
    "How do I set my cwd to /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "ChatGPT said:\n",
    "\n",
    "Youâ€™ve got a couple of clean ways to do this depending on where youâ€™re working.\n",
    "```\n",
    "```\n",
    "ðŸ–¥ï¸ In Bash / Zsh before running Python or Jupyter\n",
    "cd /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "pwd   # confirms your cwd is set\n",
    "\n",
    "\n",
    "Anything you start after that (Python REPL, Jupyter, VS Code debug session) will inherit this as its current working directory.\n",
    "```\n",
    "ðŸ Inside a Python script or REPL\n",
    "```py\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/b/DATA/PROJECTS/Telco/Level_3\")\n",
    "print(os.getcwd())   # confirm\n",
    "```\n",
    "\n",
    "ðŸ““ Inside a Jupyter notebook\n",
    "\n",
    "Two easy options:\n",
    "\n",
    "Option A â€” Python\n",
    "```py\n",
    "import os\n",
    "os.chdir(\"/Users/b/DATA/PROJECTS/Telco/Level_3\")\n",
    "print(os.getcwd())\n",
    "```\n",
    "\n",
    "Option B â€” Jupyter magic\n",
    "```py\n",
    "%cd /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "```\n",
    "âš¡ Best Practice:\n",
    "- If Level_3/ is your project root, itâ€™s often best to always launch Jupyter or scripts from there. That way all relative paths like config/config.yaml or data/ â€œjust work.â€\n",
    "- If you want Jupyter to always start in Level_3/ by default, you can configure it in Jupyterâ€™s settings (c.NotebookApp.notebook_dir in jupyter_notebook_config.py).\n",
    "\n",
    "># PROBLEM 6:\n",
    "```\n",
    "Thanks for sharing your actual config.yaml ðŸ™Œ â€” now itâ€™s clear why youâ€™re getting a NoneType error.\n",
    "\n",
    "Your file does not have a project section.\n",
    "But your Python code was trying to do:\n",
    "\n",
    "print(f\"Project: {config['project']['name']}\")\n",
    "\n",
    "Since project doesnâ€™t exist in the YAML, config['project'] raises a KeyError. But because you earlier hit the â€œempty/Noneâ€ state, my guess is you accidentally saved the file differently once. Now that you have a valid YAML mapping, the current problem is just a wrong expectation about keys.\n",
    "\n",
    "âœ… Two ways to fix\n",
    "Option 1: Update your Python code to match your config\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Raw data: {config['data']['raw_path']}\")\n",
    "print(f\"Processed data: {config['data']['processed_path']}\")\n",
    "print(f\"Optimize memory? {config['options']['optimize_memory']}\")\n",
    "\n",
    "Option 2: Add a project section to your YAML\n",
    "\n",
    "If you want to keep the Python code as-is:\n",
    "\n",
    "project:\n",
    "  name: Telco Churn\n",
    "\n",
    "data:\n",
    "  raw_path: \"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"data/processed/telco_clean.csv\"\n",
    "\n",
    "ðŸ”‘ Best Practice\n",
    "\n",
    "Decide on a schema for your config file (e.g., always have project, data, options, etc.).\n",
    "\n",
    "Keep your YAML + Python expectations in sync â€” otherwise youâ€™ll keep hitting KeyError.\n",
    "\n",
    "ðŸ‘‰ Do you want me to draft a recommended config.yaml schema for your Telco project (with project, data, preprocessing, analysis, visualization) so your code and config stay aligned as you grow this into Levels 4â€“6?\n",
    "```\n",
    "\n",
    "># PROBLEM 7:\n",
    "```\n",
    "# Import what we need\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "ROOT = Path.cwd()  # /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "CFG = ROOT / \"config\" / \"config.yaml\"\n",
    "\n",
    "with CFG.open('r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Project: {config['project']['name']}\")\n",
    "print(f\"Data path: {config['data']['filepath']}\")\n",
    "\n",
    "output:\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[18], line 18\n",
    "     16 print(\"Configuration loaded!\")\n",
    "     17 print(f\"Project: {config['project']['name']}\")\n",
    "---> 18 print(f\"Data path: {config['data']['filepath']}\")\n",
    "\n",
    "KeyError: 'filepath'\n",
    "```\n",
    "```\n",
    "Now itâ€™s clear ðŸŽ¯\n",
    "\n",
    "Your YAML file (from the message you shared) looks like this:\n",
    "\n",
    "data:\n",
    "  raw_path: \"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "  processed_path: \"data/processed/telco_clean.csv\"\n",
    "\n",
    "\n",
    "But your Python is expecting:\n",
    "\n",
    "config['data']['filepath']\n",
    "\n",
    "\n",
    "Thereâ€™s no filepath key in your YAML â†’ hence the KeyError.\n",
    "\n",
    "â›³ï¸ Two ways to fix \n",
    "\n",
    "âœ… Option 1: Update your Python to match YAML\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Raw data: {config['data']['raw_path']}\")\n",
    "print(f\"Processed data: {config['data']['processed_path']}\")\n",
    "\n",
    "âŒ Option 2: Update your YAML to match Python\n",
    "\n",
    "If you **REALLY** want a single filepath (instead of raw/processed), change your YAML:\n",
    "\n",
    "data:\n",
    "  filepath: \"data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "ðŸ”‘ Best Practice\n",
    "\n",
    "Be consistent: either define raw_path + processed_path in YAML and use them in Python, or stick to a simple single filepath.\n",
    "\n",
    "For a growing project, Iâ€™d recommend keeping both paths (raw_path, processed_path) â€” more flexible as you add cleaning steps.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
