# 2.4.3 | Encoding / Case / Whitespace Hygiene
print("\n2.4.3 ğŸ§¼ Encoding / case / whitespace hygiene")

hygiene_rows_243 = []

# ---------------------------------------------------------
# Filter cat_cols to only columns that still exist in df
# ---------------------------------------------------------
all_cat_cols_243 = list(cat_cols)Â  # keep original for reference
valid_cat_cols_243 = [c for c in all_cat_cols_243 if c in df.columns]
missing_cat_cols_243 = [c for c in all_cat_cols_243 if c not in df.columns]

if missing_cat_cols_243:
Â  Â  print(f"Â  Â âš ï¸ 2.4.3: Skipping missing categorical columns (not in df): {missing_cat_cols_243}")

# Make sure n_rows_24 is defined
n_rows_24 = df.shape[0]

for col in valid_cat_cols_243:
Â  Â  s_243 = df[col].astype("string")
Â  Â  role_243 = role_map_24.get(col, "feature")
Â  Â  fgroup_243 = feature_group_map_24.get(col, "unknown")

Â  Â  value_counts_243 = s_243.value_counts(dropna=False)
Â  Â  uniques_243 = value_counts_243.index.tolist()

Â  Â  norm_to_raws_243 = {}

Â  Â  for val in uniques_243:
Â  Â  Â  Â  if pd.isna(val):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  raw_243 = str(val)

Â  Â  Â  Â  norm_243 = raw_243.strip()
Â  Â  Â  Â  if case_mode_243 == "lower":
Â  Â  Â  Â  Â  Â  norm_243 = norm_243.lower()
Â  Â  Â  Â  elif case_mode_243 == "upper":
Â  Â  Â  Â  Â  Â  norm_243 = norm_243.upper()
Â  Â  Â  Â  elif case_mode_243 == "title":
Â  Â  Â  Â  Â  Â  norm_243 = norm_243.title()

Â  Â  Â  Â  if unicode_norm_243:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  norm_243 = unicodedata.normalize(str(unicode_norm_243), norm_243)
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  if norm_243 not in norm_to_raws_243:
Â  Â  Â  Â  Â  Â  norm_to_raws_243[norm_243] = []
Â  Â  Â  Â  norm_to_raws_243[norm_243].append(raw_243)

Â  Â  for norm_val_243, raw_list_243 in norm_to_raws_243.items():
Â  Â  Â  Â  raw_set_243 = sorted(set(raw_list_243))
Â  Â  Â  Â  if len(raw_set_243) <= 1:
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  for raw_243 in raw_set_243:
Â  Â  Â  Â  Â  Â  if raw_243 == norm_val_243:
Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  count_243 = int(value_counts_243.get(raw_243, 0))
Â  Â  Â  Â  Â  Â  pct_243 = float(count_243 / n_rows_24 * 100.0) if n_rows_24 else 0.0

Â  Â  Â  Â  Â  Â  if raw_243.strip() != raw_243:
Â  Â  Â  Â  Â  Â  Â  Â  issue_type_243 = "whitespace"
Â  Â  Â  Â  Â  Â  elif case_mode_243 and (
Â  Â  Â  Â  Â  Â  Â  Â  (case_mode_243 == "lower" and raw_243.lower() == norm_val_243)
Â  Â  Â  Â  Â  Â  Â  Â  or (case_mode_243 == "upper" and raw_243.upper() == norm_val_243)
Â  Â  Â  Â  Â  Â  Â  Â  or (case_mode_243 == "title" and raw_243.title() == norm_val_243)
Â  Â  Â  Â  Â  Â  ):
Â  Â  Â  Â  Â  Â  Â  Â  issue_type_243 = "case_mismatch"
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  issue_type_243 = "encoding"

Â  Â  Â  Â  Â  Â  if col in target_cols_24 or fgroup_243 == "model_feature":
Â  Â  Â  Â  Â  Â  Â  Â  severity_243 = "critical"
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  severity_243 = "warn"

Â  Â  Â  Â  Â  Â  hygiene_rows_243.append(
Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "column": col,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "raw_value": raw_243,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "normalized_value": norm_val_243,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "count": count_243,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "pct": round(pct_243, 5),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "issue_type": issue_type_243,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "severity": severity_243,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "role": role_243,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "feature_group": fgroup_243,
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  )

hygiene_df_243 = pd.DataFrame(hygiene_rows_243)

hygiene_path_243 = sec24_reports_dir / "hygiene_report.csv"
tmp_243 = hygiene_path_243.with_suffix(".tmp.csv")
hygiene_df_243.to_csv(tmp_243, index=False)
os.replace(tmp_243, hygiene_path_243)

print(f"ğŸ’¾ hygiene_report.csv â†’ {hygiene_path_243}")
print("\nğŸ“Š hygiene_report (head):")
if not hygiene_df_243.empty:
Â  Â  display(hygiene_df_243.head(20))
else:
Â  Â  print("Â  Â (no hygiene issues detected)")

#
n_cols_with_hygiene_243 = len(set(hygiene_df_243["column"])) if not hygiene_df_243.empty else 0
n_issue_pairs_243 = int(hygiene_df_243.shape[0]) if not hygiene_df_243.empty else 0
has_critical_243 = bool(not hygiene_df_243.empty and (hygiene_df_243["severity"] == "critical").any())

if n_cols_with_hygiene_243 == 0:
Â  Â  status_243 = "OK"
elif has_critical_243:
Â  Â  status_243 = "FAIL"
else:
Â  Â  status_243 = "WARN"

summary_243 = pd.DataFrame([{
Â  Â  "section": "2.4.3",
Â  Â  "section_name": "Encoding / case / whitespace hygiene",
Â  Â  "check": "Detect near-duplicate categories caused by encoding/case/whitespace",
Â  Â  "level": "info",
Â  Â  "status": status_243,
Â  Â  "n_columns_with_hygiene_issues": int(n_cols_with_hygiene_243),
Â  Â  "n_distinct_issue_pairs": int(n_issue_pairs_243),
Â  Â  "detail": "hygiene_report.csv",
Â  Â  "timestamp": pd.Timestamp.utcnow(),
}])
append_sec2(summary_243, SECTION2_REPORT_PATH)
display(summary_243)

# 2.4.4 | Domain Frequency Audit
print("\n2.4.4 ğŸ“Š Domain frequency audit")

freq_rows_244 = []

# Ensure domain frequency thresholds exist


# ---------------------------------------------------------
# Filter cat_cols to only columns that still exist in df
# ---------------------------------------------------------
all_cat_cols_244 = list(cat_cols)Â  # original list
valid_cat_cols_244 = [c for c in all_cat_cols_244 if c in df.columns]
missing_cat_cols_244 = [c for c in all_cat_cols_244 if c not in df.columns]

if missing_cat_cols_244:
Â  Â  print(f"Â  Â âš ï¸ 2.4.4: Skipping missing categorical columns (not in df): {missing_cat_cols_244}")

n_rows_24 = df.shape[0]

for col in valid_cat_cols_244:
Â  Â  s_244 = df[col]
Â  Â  role_244 = role_map_24.get(col, "feature")
Â  Â  fgroup_244 = feature_group_map_24.get(col, "unknown")

Â  Â  n_rows_col_244 = int(s_244.shape[0])
Â  Â  n_unique_244 = int(s_244.nunique(dropna=True))
Â  Â  pct_blank_244 = float(s_244.isna().mean() * 100.0) if n_rows_col_244 else 0.0

Â  Â  vc_244 = s_244.value_counts(dropna=True)
Â  Â  if vc_244.empty:
Â  Â  Â  Â  pct_top_244 = 0.0
Â  Â  Â  Â  entropy_244 = 0.0
Â  Â  else:
Â  Â  Â  Â  top_cnt_244 = int(vc_244.iloc[0])
Â  Â  Â  Â  pct_top_244 = float(top_cnt_244 / n_rows_col_244 * 100.0) if n_rows_col_244 else 0.0
Â  Â  Â  Â  probs_244 = (vc_244 / n_rows_col_244).astype(float)
Â  Â  Â  Â  with np.errstate(divide="ignore", invalid="ignore"):
Â  Â  Â  Â  Â  Â  ent_terms_244 = -probs_244 * np.log2(probs_244)
Â  Â  Â  Â  entropy_244 = float(
Â  Â  Â  Â  Â  Â  ent_terms_244.replace([np.inf, -np.inf], 0.0).fillna(0.0).sum()
Â  Â  Â  Â  )

Â  Â  if n_unique_244 <= 1:
Â  Â  Â  Â  domain_shape_244 = "dominant"
Â  Â  elif pct_top_244 >= dominant_top_pct_244:
Â  Â  Â  Â  domain_shape_244 = "dominant"
Â  Â  elif pct_top_244 <= fragmented_top_pct_244 and n_unique_244 > 5:
Â  Â  Â  Â  domain_shape_244 = "fragmented"
Â  Â  else:
Â  Â  Â  Â  domain_shape_244 = "balanced"

Â  Â  freq_rows_244.append(
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  "column": col,
Â  Â  Â  Â  Â  Â  "n_unique": n_unique_244,
Â  Â  Â  Â  Â  Â  "pct_blank": round(pct_blank_244, 5),
Â  Â  Â  Â  Â  Â  "pct_top_category": round(pct_top_244, 5),
Â  Â  Â  Â  Â  Â  "entropy": round(entropy_244, 5),
Â  Â  Â  Â  Â  Â  "domain_shape": domain_shape_244,
Â  Â  Â  Â  Â  Â  "role": role_244,
Â  Â  Â  Â  Â  Â  "feature_group": fgroup_244,
Â  Â  Â  Â  }
Â  Â  )

domain_freq_df_244 = (
Â  Â  pd.DataFrame(freq_rows_244)
Â  Â  .sort_values(["domain_shape", "column"])
Â  Â  .reset_index(drop=True)
)

domain_freq_path_244 = sec24_reports_dir / "domain_frequency_report.csv"
tmp_244 = domain_freq_path_244.with_suffix(".tmp.csv")
domain_freq_df_244.to_csv(tmp_244, index=False)
os.replace(tmp_244, domain_freq_path_244)

n_cols_profiled_244 = int(domain_freq_df_244.shape[0])
n_dom_244 = int((domain_freq_df_244["domain_shape"] == "dominant").sum())
n_frag_244 = int((domain_freq_df_244["domain_shape"] == "fragmented").sum())

print(f"ğŸ’¾ domain_frequency_report.csv â†’ {domain_freq_path_244}")
print("\nğŸ“Š domain_frequency_report:")
if not domain_freq_df_244.empty:
Â  Â  display(domain_freq_df_244.head(20))
else:
Â  Â  print("Â  Â (no categorical domain frequency stats â€” this would be unusual)")

#
status_244 = "OK" if n_cols_profiled_244 > 0 else "ERROR"

summary_244 = pd.DataFrame([{
Â  Â  "section": "2.4.4",
Â  Â  "section_name": "Domain frequency audit",
Â  Â  "check": "Summarize per-column domain shape and dominance",
Â  Â  "level": "info",
Â  Â  "status": status_244,
Â  Â  "n_columns_profiled": n_cols_profiled_244,
Â  Â  "n_dominant_domains": n_dom_244,
Â  Â  "n_fragmented_domains": n_frag_244,
Â  Â  "detail": "domain_frequency_report.csv",
Â  Â  "timestamp": pd.Timestamp.utcnow(),
}])
append_sec2(summary_244, SECTION2_REPORT_PATH)
display(summary_244)


# 2.4.5 | Cardinality Audit
print("\n2.4.5 ğŸ“ Cardinality audit")

# Decide which frame to use (df_clean if available)
frame_245 = df_clean if "df_clean" in globals() else df

card_rows_245 = []

# ----------------------------------------
# Ensure cardinality thresholds exist
# ----------------------------------------
if "high_card_limit_245" not in globals():
Â  Â  high_card_limit_245 = None

Â  Â  # 1) Prefer config helper, if available
Â  Â  if "C" in globals() and callable(C):
Â  Â  Â  Â  high_card_limit_245 = C("CATEGORICAL.HIGH_CARDINALITY_LIMIT", None)

Â  Â  # 2) Fallback to raw CONFIG, CATEGORICAL namespace
Â  Â  if high_card_limit_245 is None and "CONFIG" in globals():
Â  Â  Â  Â  cfg = CONFIG
Â  Â  Â  Â  for k in "CATEGORICAL.HIGH_CARDINALITY_LIMIT".split("."):
Â  Â  Â  Â  Â  Â  if isinstance(cfg, dict) and k in cfg:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = cfg[k]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = None
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if cfg is not None:
Â  Â  Â  Â  Â  Â  high_card_limit_245 = cfg

Â  Â  # 3) Fallback to DATA_QUALITY.HIGH_CARD_THRESHOLD from project_config.yaml
Â  Â  if high_card_limit_245 is None and "CONFIG" in globals():
Â  Â  Â  Â  cfg = CONFIG
Â  Â  Â  Â  for k in "DATA_QUALITY.HIGH_CARD_THRESHOLD".split("."):
Â  Â  Â  Â  Â  Â  if isinstance(cfg, dict) and k in cfg:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = cfg[k]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = None
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if cfg is not None:
Â  Â  Â  Â  Â  Â  high_card_limit_245 = cfg

Â  Â  # 4) Final hard-coded default
Â  Â  if high_card_limit_245 is None:
Â  Â  Â  Â  high_card_limit_245 = 50

Â  Â  high_card_limit_245 = int(high_card_limit_245)

if "near_unique_threshold_245" not in globals():
Â  Â  near_unique_threshold_245 = None

Â  Â  # 1) Prefer config helper
Â  Â  if "C" in globals() and callable(C):
Â  Â  Â  Â  near_unique_threshold_245 = C("CATEGORICAL.NEAR_UNIQUE_THRESHOLD", None)

Â  Â  # 2) Fallback to raw CONFIG
Â  Â  if near_unique_threshold_245 is None and "CONFIG" in globals():
Â  Â  Â  Â  cfg = CONFIG
Â  Â  Â  Â  for k in "CATEGORICAL.NEAR_UNIQUE_THRESHOLD".split("."):
Â  Â  Â  Â  Â  Â  if isinstance(cfg, dict) and k in cfg:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = cfg[k]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = None
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if cfg is not None:
Â  Â  Â  Â  Â  Â  near_unique_threshold_245 = cfg

Â  Â  # 3) Final default (90% of rows unique â‰ˆ "near-unique")
Â  Â  if near_unique_threshold_245 is None:
Â  Â  Â  Â  near_unique_threshold_245 = 0.9

Â  Â  near_unique_threshold_245 = float(near_unique_threshold_245)

# Filter cat_cols to those actually present in the frame
missing_cat_245 = [c for c in cat_cols if c not in frame_245.columns]
if missing_cat_245:
Â  Â  print("âš ï¸ 2.4.5: skipping categorical columns not in frame:", missing_cat_245)

cat_cols_245 = [c for c in cat_cols if c in frame_245.columns]

for col in cat_cols_245:
Â  Â  s_245 = frame_245[col]
Â  Â  role_245 = role_map_24.get(col, "feature")
Â  Â  fgroup_245 = feature_group_map_24.get(col, "unknown")

Â  Â  n_rows_col_245 = int(s_245.shape[0])
Â  Â  n_unique_245 = int(s_245.nunique(dropna=True))
Â  Â  card_ratio_245 = float(n_unique_245 / n_rows_col_245) if n_rows_col_245 else 0.0

Â  Â  high_cardinality_245 = bool(n_unique_245 > high_card_limit_245)
Â  Â  near_unique_245 = bool(card_ratio_245 >= near_unique_threshold_245)
Â  Â  quasi_identifier_risk_245 = bool(
Â  Â  Â  Â  near_unique_245 and (role_245 in {"id", "target"} or fgroup_245 == "model_feature")
Â  Â  )

Â  Â  card_rows_245.append(
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  "column": col,
Â  Â  Â  Â  Â  Â  "n_unique": n_unique_245,
Â  Â  Â  Â  Â  Â  "cardinality_ratio": round(card_ratio_245, 5),
Â  Â  Â  Â  Â  Â  "high_cardinality": high_cardinality_245,
Â  Â  Â  Â  Â  Â  "near_unique": near_unique_245,
Â  Â  Â  Â  Â  Â  "quasi_identifier_risk": quasi_identifier_risk_245,
Â  Â  Â  Â  Â  Â  "role": role_245,
Â  Â  Â  Â  Â  Â  "feature_group": fgroup_245,
Â  Â  Â  Â  }
Â  Â  )

card_df_245 = pd.DataFrame(card_rows_245).sort_values("n_unique", ascending=False)

card_path_245 = sec24_reports_dir / "cardinality_audit.csv"
tmp_245 = card_path_245.with_suffix(".tmp.csv")
card_df_245.to_csv(tmp_245, index=False)
os.replace(tmp_245, card_path_245)

n_high_card_245 = int(card_df_245["high_cardinality"].sum())
n_quasi_245 = int(card_df_245["quasi_identifier_risk"].sum())

if n_quasi_245 > 0 and any(
Â  Â  card_df_245.loc[card_df_245["quasi_identifier_risk"], "feature_group"] == "model_feature"
):
Â  Â  status_245 = "FAIL"
elif n_high_card_245 > 0:
Â  Â  status_245 = "WARN"
else:
Â  Â  status_245 = "OK"

summary_245 = pd.DataFrame([{
Â  Â  "section": "2.4.5",
Â  Â  "section_name": "Cardinality audit",
Â  Â  "check": "Identify high-cardinality / near-unique categorical features",
Â  Â  "level": "info",
Â  Â  "status": status_245,
Â  Â  "n_high_cardinality_columns": n_high_card_245,
Â  Â  "n_quasi_identifier_columns": n_quasi_245,
Â  Â  "detail": "cardinality_audit.csv",
Â  Â  "timestamp": pd.Timestamp.utcnow(),
}])

append_sec2(summary_245, SECTION2_REPORT_PATH)

print(f" ğŸ’¾ 2.4.5 cardinality_audit.csv â†’ {card_path_245}")
print("\nğŸ“Š cardinality_audit:")
if not card_df_245.empty:
Â  Â  display(card_df_245.head(20))
else:
Â  Â  print("Â  Â (no cardinality stats â€” this would be unusual)")

display(summary_245)
# 2.4.6 | Rare-Category Audit
print("\n2.4.6 ğŸ§¬ Rare-category audit")

rare_rows_246 = []

# Ensure rare-category threshold exists
if "rare_threshold_pct_246" not in globals():
Â  Â  rare_threshold_pct_246 = None

Â  Â  # 1) Prefer config helper, if available
Â  Â  if "C" in globals() and callable(C):
Â  Â  Â  Â  rare_threshold_pct_246 = C("CATEGORICAL.RARE_PCT_THRESHOLD", None)

Â  Â  # 2) Fallback to raw CONFIG under CATEGORICAL.RARE_PCT_THRESHOLD
Â  Â  if rare_threshold_pct_246 is None and "CONFIG" in globals():
Â  Â  Â  Â  cfg = CONFIG
Â  Â  Â  Â  for k in "CATEGORICAL.RARE_PCT_THRESHOLD".split("."):
Â  Â  Â  Â  Â  Â  if isinstance(cfg, dict) and k in cfg:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = cfg[k]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = None
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if cfg is not None:
Â  Â  Â  Â  Â  Â  rare_threshold_pct_246 = cfg

Â  Â  # 3) Fallback to DATA_QUALITY.RARE_PCT_THRESHOLD from project_config.yaml
Â  Â  if rare_threshold_pct_246 is None and "CONFIG" in globals():
Â  Â  Â  Â  cfg = CONFIG
Â  Â  Â  Â  for k in "DATA_QUALITY.RARE_PCT_THRESHOLD".split("."):
Â  Â  Â  Â  Â  Â  if isinstance(cfg, dict) and k in cfg:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = cfg[k]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  cfg = None
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if cfg is not None:
Â  Â  Â  Â  Â  Â  rare_threshold_pct_246 = cfg

Â  Â  # 4) Final default if nothing in config:
Â  Â  #Â  Â  interpret as percentage because pct_246 is already in 0â€“100 scale
Â  Â  if rare_threshold_pct_246 is None:
Â  Â  Â  Â  rare_threshold_pct_246 = 1.0Â  # treat <1% as rare by default

Â  Â  rare_threshold_pct_246 = float(rare_threshold_pct_246)

for col in cat_cols:
Â  Â  s_246 = df[col].astype("string")
Â  Â  role_246 = role_map_24.get(col, "feature")
Â  Â  fgroup_246 = feature_group_map_24.get(col, "unknown")

Â  Â  vc_246 = s_246.value_counts(dropna=False)
Â  Â  for val, cnt in vc_246.items():
Â  Â  Â  Â  if pd.isna(val):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  count_246 = int(cnt)
Â  Â  Â  Â  pct_246 = float(count_246 / n_rows_24 * 100.0) if n_rows_24 else 0.0
Â  Â  Â  Â  is_rare_246 = bool(pct_246 < rare_threshold_pct_246)
Â  Â  Â  Â  if not is_rare_246:
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  rare_rows_246.append(
Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  "column": col,
Â  Â  Â  Â  Â  Â  Â  Â  "value": str(val),
Â  Â  Â  Â  Â  Â  Â  Â  "count": count_246,
Â  Â  Â  Â  Â  Â  Â  Â  "pct": round(pct_246, 5),
Â  Â  Â  Â  Â  Â  Â  Â  "is_rare": is_rare_246,
Â  Â  Â  Â  Â  Â  Â  Â  "suggested_group": "Other",
Â  Â  Â  Â  Â  Â  Â  Â  "role": role_246,
Â  Â  Â  Â  Â  Â  Â  Â  "feature_group": fgroup_246,
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  )

rare_df_246 = pd.DataFrame(rare_rows_246)

rare_path_246 = sec24_reports_dir / "rare_category_report.csv"
tmp_246 = rare_path_246.with_suffix(".tmp.csv")
rare_df_246.to_csv(tmp_246, index=False)
os.replace(tmp_246, rare_path_246)

n_cols_with_rare_246 = len(set(rare_df_246["column"])) if not rare_df_246.empty else 0
n_rare_values_total_246 = int(rare_df_246.shape[0]) if not rare_df_246.empty else 0

print(f"ğŸ’¾ rare_category_report.csv â†’ {rare_path_246}")
print("\nğŸ“Š rare_category_report (head):")
if not rare_df_246.empty:
Â  Â  display(rare_df_246.head(20))
else:
Â  Â  print("Â  Â (no rare categories under configured threshold)")

#
summary_246 = pd.DataFrame([{
Â  Â  "section": "2.4.6",
Â  Â  "section_name": "Rare-category audit",
Â  Â  "check": "Detect rare categorical levels and suggest grouping strategies",
Â  Â  "level": "info",
Â  Â  "status": "OK",
Â  Â  "n_columns_with_rare_categories": n_cols_with_rare_246,
Â  Â  "n_rare_values_total": n_rare_values_total_246,
Â  Â  "detail": "rare_category_report.csv",
Â  Â  "timestamp": pd.Timestamp.utcnow(),
}])
append_sec2(summary_246, SECTION2_REPORT_PATH)
display(summary_246)


# 2.4.7 | Export Issue Catalog
print("\n2.4.7 ğŸ“¦ Export categorical issues catalog")

catalog_dir_247 = sec24_reports_dir / "categorical_domain_issues_catalog"
catalog_dir_247.mkdir(parents=True, exist_ok=True)

issue_files_247 = {
Â  Â  "invalid_tokens": invalid_tokens_path_241,
Â  Â  "unexpected_values": unexpected_path_242,
Â  Â  "hygiene_report": hygiene_path_243,
Â  Â  "domain_frequency_report": domain_freq_path_244,
Â  Â  "cardinality_audit": card_path_245,
Â  Â  "rare_category_report": rare_path_246,
}

index_rows_247 = []

for issue_type_247, src_path_247 in issue_files_247.items():
Â  Â  src_path_247 = Path(src_path_247)
Â  Â  if not src_path_247.exists() or src_path_247.stat().st_size == 0:
Â  Â  Â  Â  continue

Â  Â  dest_path_247 = sec24_reports_dir / src_path_247.name
Â  Â  try:
Â  Â  Â  Â  shutil.copy2(src_path_247, dest_path_247)
Â  Â  except Exception:
Â  Â  Â  Â  dest_path_247 = None

Â  Â  n_rows_issue_247 = 0
Â  Â  has_critical_247 = False
Â  Â  if dest_path_247 is not None and dest_path_247.exists():
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  df_issue_247 = pd.read_csv(dest_path_247)
Â  Â  Â  Â  Â  Â  n_rows_issue_247 = int(df_issue_247.shape[0])
Â  Â  Â  Â  Â  Â  if "severity" in df_issue_247.columns:
Â  Â  Â  Â  Â  Â  Â  Â  has_critical_247 = bool((df_issue_247["severity"] == "critical").any())
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  n_rows_issue_247 = 0
Â  Â  Â  Â  Â  Â  has_critical_247 = False

Â  Â  index_rows_247.append(
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  "issue_type": issue_type_247,
Â  Â  Â  Â  Â  Â  "artifact_path": dest_path_247.name if dest_path_247 is not None else "",
Â  Â  Â  Â  Â  Â  "n_rows": n_rows_issue_247,
Â  Â  Â  Â  Â  Â  "has_critical": has_critical_247,
Â  Â  Â  Â  }
Â  Â  )

issues_index_df_247 = pd.DataFrame(index_rows_247)
index_path_247 = sec24_reports_dir / "issues_index.csv"
tmp_247 = index_path_247.with_suffix(".tmp.csv")
issues_index_df_247.to_csv(tmp_247, index=False)
os.replace(tmp_247, index_path_247)

n_issue_files_247 = len(index_rows_247)
n_critical_types_247 = int(sum(1 for _r in index_rows_247 if _r["has_critical"]))

if n_issue_files_247 == 0:
Â  Â  status_247 = "WARN"
elif n_critical_types_247 > 0:
Â  Â  status_247 = "WARN"
else:
Â  Â  status_247 = "OK"

#
print(f"ğŸ’¾Â  categorical_domain_issues_catalog/ â†’ {sec24_reports_dir}")
print("\nğŸ“Š issues_index (head):")
if not issues_index_df_247.empty:
Â  Â  display(issues_index_df_247.head(20))
else:
Â  Â  print("Â  Â (issue catalog is empty â€” no categorical issues captured)")

summary_247 = pd.DataFrame([{
Â  Â  "section": "2.4.7",
Â  Â  "section_name": "Export categorical issues catalog",
Â  Â  "check": "Bundle all Part A outputs into a consolidated issues folder",
Â  Â  "level": "info",
Â  Â  "status": status_247,
Â  Â  "n_issue_files": n_issue_files_247,
Â  Â  "n_critical_issue_types": n_critical_types_247,
Â  Â  "detail": "categorical_domain_issues_catalog/",
Â  Â  "timestamp": pd.Timestamp.utcnow(),
}])
append_sec2(summary_247, SECTION2_REPORT_PATH)
display(summary_247)

---------------------------------------------------------------------------
NameErrorÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Traceback (most recent call last)
Cell In[23], line 179
Â  Â  177 if n_unique_244 <= 1:
Â  Â  178Â  Â  Â domain_shape_244 = "dominant"
--> 179 elif pct_top_244 >= dominant_top_pct_244:
Â  Â  180Â  Â  Â domain_shape_244 = "dominant"
Â  Â  181 elif pct_top_244 <= fragmented_top_pct_244 and n_unique_244 > 5:

NameError: name 'dominant_top_pct_244' is not defined

It looks like you are missing the threshold definitions for your **Domain Frequency Audit (Section 2.4.4)**. The error occurs because the variables `dominant_top_pct_244` and `fragmented_top_pct_244` are referenced but haven't been assigned a value in the current scope.

To fix this, you should define these thresholds right before the loop in section **2.4.4**.

### The Fix

Insert the following block into your code just after the `# Ensure domain frequency thresholds exist` comment:

```python
# 2.4.4 | Domain Frequency Audit
# ... (previous code)

# Ensure domain frequency thresholds exist
if 'dominant_top_pct_244' not in globals():
    dominant_top_pct_244 = 90.0  # Example: 90% dominance threshold

if 'fragmented_top_pct_244' not in globals():
    fragmented_top_pct_244 = 1.0  # Example: 1% fragmentation threshold

# ... (rest of the loop)

```

---

### Why this happened

The logic in your loop uses these variables to categorize the "shape" of your data:

1. **Dominant:** One category takes up more than `dominant_top_pct_244` (e.g., 90%) of the data.
2. **Fragmented:** The most frequent category takes up less than `fragmented_top_pct_244` (e.g., 1%) and there are many unique values.
3. **Balanced:** Everything in between.

### A Note on Config Patterns

I noticed in **Section 2.4.5** and **2.4.6** you use a very robust configuration-fetching pattern (checking `C()`, then `CONFIG`, then hardcoded defaults). For consistency, you might want to apply that same pattern to these variables:

```python
# Suggested robust check for 2.4.4
if "dominant_top_pct_244" not in globals():
    # Attempt to get from CONFIG or use default 95.0
    dominant_top_pct_244 = 95.0 
    if "CONFIG" in globals():
        # logic to navigate CONFIG dictionary...
        pass

if "fragmented_top_pct_244" not in globals():
    fragmented_top_pct_244 = 5.0

```

Would you like me to rewrite the **Section 2.4.4** block for you using the same robust configuration-fetching logic seen in your other sections?