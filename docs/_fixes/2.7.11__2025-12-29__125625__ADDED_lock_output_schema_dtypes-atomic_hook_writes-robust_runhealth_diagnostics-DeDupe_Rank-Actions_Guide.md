Add three things: deterministic schemas, guardrails with explicit failure modes, and action outputs that are machine-consumable.

## Production-grade upgrades for 2.7.11

### 1) Lock output schema and dtypes (stable CSV contract)

Define a fixed column order and force types. This prevents ‚Äútoday‚Äôs run wrote different columns‚Äù drift.

```python
# ---- schema lock (stable contract) ----
EFFECT_SCHEMA_COLS = [
    "source_file","test_name","test_type","outcome_col","group_col",
    "effect_type","effect_value","effect_abs","magnitude_label",
    "p_value","statistic","n_a","n_b","notes"
]

# after df_effect_2711 built
if effect_rows_2711:
    df_effect_2711 = pd.DataFrame(effect_rows_2711)

    # ensure columns exist
    for c in EFFECT_SCHEMA_COLS:
        if c not in df_effect_2711.columns:
            df_effect_2711[c] = np.nan

    # derived fields
    df_effect_2711["effect_value"] = pd.to_numeric(df_effect_2711["effect_value"], errors="coerce")
    df_effect_2711["effect_abs"]   = df_effect_2711["effect_value"].abs()

    df_effect_2711["p_value"]   = pd.to_numeric(df_effect_2711["p_value"], errors="coerce")
    df_effect_2711["statistic"] = pd.to_numeric(df_effect_2711["statistic"], errors="coerce")

    # final ordering
    df_effect_2711 = df_effect_2711[EFFECT_SCHEMA_COLS].copy()
```

üí°üí° Also write a tiny `effect_size_report.schema.json` once (per repo) so reviewers know the contract.

---

### 2) Atomic writes + ‚Äúlatest‚Äù publish hook

You already do atomic patterns elsewhere. Do it here too, and publish to `_latest/` to unblock downstream steps.

```python
effect_path = (sec2_27_dir / effect_output_file_2711).resolve()
tmp_path = effect_path.with_suffix(".tmp.csv")
df_effect_2711.to_csv(tmp_path, index=False)
os.replace(tmp_path, effect_path)

# optional latest publish
if "SEC2_LATEST_DIR" in globals() and SEC2_LATEST_DIR is not None:
    SEC2_LATEST_DIR.mkdir(parents=True, exist_ok=True)
    latest_path = (SEC2_LATEST_DIR / effect_output_file_2711).resolve()
    tmp_latest = latest_path.with_suffix(".tmp.csv")
    df_effect_2711.to_csv(tmp_latest, index=False)
    os.replace(tmp_latest, latest_path)
```

---

### 3) Add ‚Äúrun health‚Äù diagnostics and explicit statuses

Right now, ‚ÄúSKIPPED/OK/WARN‚Äù is mostly based on existence. Add hard checks:

* **WARN** if >X% effects are `unknown`
* **WARN** if `p_value` missing for most rows
* **FAIL** if output empty but upstream sources existed

```python
n_rows = int(df_effect_2711.shape[0])
pct_unknown = float((df_effect_2711["magnitude_label"].fillna("unknown") == "unknown").mean()) if n_rows else 1.0
pct_p_missing = float(df_effect_2711["p_value"].isna().mean()) if n_rows else 1.0

health_notes = []
if pct_unknown > 0.25:
    health_notes.append(f"high_unknown_labels={pct_unknown:.2%}")
if pct_p_missing > 0.50:
    health_notes.append(f"many_missing_p_values={pct_p_missing:.2%}")

if n_rows == 0:
    effect_status_2711 = "FAIL" if source_dfs_2711 else "SKIPPED"
elif pct_unknown > 0.50:
    effect_status_2711 = "WARN"
else:
    effect_status_2711 = "OK"
```

---

### 4) De-duplicate and rank consistently

You may compute multiple effect types per test. That is fine, but you want a deterministic ‚Äútop list‚Äù.

```python
# ranking key
df_effect_2711["magnitude_score"] = df_effect_2711["effect_abs"]

# stable top list
top_effects = (df_effect_2711
    .sort_values(["magnitude_score", "p_value"], ascending=[False, True], na_position="last")
    .head(10)
    [["test_name","effect_type","effect_value","magnitude_label","p_value"]]
    .round(3)
)
display(top_effects)
```

---

### 5) Add a machine-readable ‚Äúactions‚Äù artifact

Printing is human-only. Write `effect_actions_2711.csv` with columns like `priority`, `action`, `why`, `target`.

```python
# action mapping
action_guide = {
    "very large": {"priority": 1, "action": "Immediate intervention / feature engineering"},
    "large":      {"priority": 2, "action": "Prioritize modeling + segmentation"},
    "medium":     {"priority": 3, "action": "Include in baseline model; monitor"},
    "small":      {"priority": 4, "action": "Keep only if cheap; consider interactions"},
    "negligible": {"priority": 5, "action": "Deprioritize; exclude from shortlist"},
    "unknown":    {"priority": 9, "action": "Investigate inputs; missing stats"},
}

df_actions = (df_effect_2711
    .assign(mag=df_effect_2711["magnitude_label"].fillna("unknown").astype(str))
    .assign(priority=lambda d: d["mag"].map(lambda x: action_guide.get(x, action_guide["unknown"])["priority"]))
    .assign(action=lambda d: d["mag"].map(lambda x: action_guide.get(x, action_guide["unknown"])["action"]))
    .assign(why=lambda d: "high_effect_size" )
    .sort_values(["priority","effect_abs"], ascending=[True, False], na_position="last")
    [["priority","action","test_name","effect_type","effect_value","magnitude_label","p_value","source_file"]]
)

actions_path = (sec2_27_dir / "effect_actions_2711.csv").resolve()
tmp_actions = actions_path.with_suffix(".tmp.csv")
df_actions.to_csv(tmp_actions, index=False)
os.replace(tmp_actions, actions_path)
```

Now downstream dashboard code can consume actions directly.

---

## Expand ‚Äúguiding actions‚Äù so it is actually useful

Right now the actions are generic. Make them conditional on `effect_type` and whether it is **business** vs **modeling** oriented.

### Action rules by effect type

Use these as logic for `why` and `action`:

* **cohens_d**

  * large: ‚ÄúStrong separation between groups. Consider binning, interaction terms, or segment-specific policies.‚Äù
* **cramers_v / phi**

  * large: ‚ÄúHigh categorical association. Consider target encoding, grouping rare levels, or rule-based flags.‚Äù
* **r / r_squared**

  * high r¬≤: ‚ÄúStrong linear predictor. Prioritize in baseline model and check leakage.‚Äù
* **risk_difference / relative_risk / odds_ratio**

  * high RR/OR: ‚ÄúMaterial outcome uplift. Consider targeted intervention and fairness review.‚Äù

Concrete mapping:

```python
def_action = "Include in model; validate stability"

def action_from_row(row):
    mag = str(row.get("magnitude_label","unknown"))
    et  = str(row.get("effect_type",""))
    p   = row.get("p_value", np.nan)

    # base priority from magnitude
    base = action_guide.get(mag, action_guide["unknown"])
    action = base["action"]

    # specialize
    if et in ["relative_risk","odds_ratio"] and mag in ["large","very large"]:
        action = "Targeted intervention candidate; review confounding + fairness"
    elif et in ["cramers_v","phi"] and mag in ["large","very large"]:
        action = "Engineer categorical signal; consider grouping/encoding"
    elif et in ["r_squared"] and (not pd.isna(p)) and p < 0.05 and mag in ["large","very large","medium"]:
        action = "Strong predictor; prioritize baseline model; check leakage"
    elif mag == "unknown":
        action = "Investigate missing inputs; upstream test outputs incomplete"

    return action
```

If you do not want functions, inline this logic with `np.select` and boolean masks. But the key is: **actions should depend on effect_type + magnitude + p_value presence**.

---

## One more hard truth

Your section is getting ‚Äúnotebook-y‚Äù because it prints a lot. For production: print only summary counts, always write artifacts, and make dashboards consume artifacts.

If you want, I can rewrite your ‚Äúaction guidance‚Äù part as a pure-vectorized block with no function defs.

# 2.7.11 | Effect Size Computations (Cohen‚Äôs d, Œ∑¬≤, r¬≤, Œ¶/V, risk measures)
print("2.7.11 | Effect Size Computations (Cohen‚Äôs d, Œ∑¬≤, r¬≤, Œ¶/V, risk measures)")
# Handle Nans cleanly

# IPython-safe display
try:
    from IPython.display import display
except Exception:
    display = print

# üîí ROBUST GUARDS
if "sec2_27_dir" not in globals() or sec2_27_dir is None:
    raise NameError("‚ùå sec2_27_dir missing. Run the 2.7 directory bootstrap first.")
if ("df_27" not in globals()) or (df_27 is None) or (getattr(df_27, "empty", True)):
    raise NameError("‚ùå df_27 missing/None/empty. Build df_27 (or df_base) first.")

# Load CONFIG (your YAML loads here)
effect_cfg = CONFIG.get("EFFECT_SIZE", {}) if isinstance(CONFIG, dict) else {}
effect_enabled_2711 = bool(effect_cfg.get("ENABLED", True))

effect_sources_2711 = effect_cfg.get("SOURCES", [
    "t_test_results.csv",
    "anova_kruskal_results.csv",
    "chi_square_results.csv",
    "point_biserial_results.csv",
    "proportion_tests.csv",
])

# Add input validation summary
missing_sources = [s for s in effect_sources_2711 if not (sec2_27_dir/s).exists()]
if missing_sources:
    print(f"   ‚ö†Ô∏è Missing {len(missing_sources)}/{len(effect_sources_2711)} sources: {missing_sources}")

effect_metrics_2711 = effect_cfg.get("METRICS", {
    "COHENS_D": True,
    "ETA_SQUARED": True,
    "PARTIAL_ETA_SQUARED": False,
    "R_SQUARED": True,
    "PHI_CRAMER_V": True
})
effect_output_file_2711 = effect_cfg.get("OUTPUT_FILE", "effect_size_report.csv")

effect_rows_2711 = []
n_tests_covered_2711 = 0
n_large_effects_2711 = 0
effect_detail_2711 = None
effect_status_2711 = "SKIPPED"

# Magnitude helpers
def _label_magnitude_d(d_abs: float) -> str:
    if np.isnan(d_abs):
        return "unknown"
    if d_abs < 0.1:
        return "negligible"
    if d_abs < 0.3:
        return "small"
    if d_abs < 0.5:
        return "small/medium"
    if d_abs < 0.8:
        return "medium"
    if d_abs < 1.2:
        return "large"
    return "very large"

def _label_magnitude_r(r_abs: float) -> str:
    if np.isnan(r_abs):
        return "unknown"
    if r_abs < 0.1:
        return "negligible"
    if r_abs < 0.3:
        return "small"
    if r_abs < 0.5:
        return "medium"
    if r_abs < 0.7:
        return "large"
    return "very large"

def _label_magnitude_r2(r2: float) -> str:
    if np.isnan(r2):
        return "unknown"
    # Interpret via sqrt(r2)
    return _label_magnitude_r(math.sqrt(r2))

def _label_magnitude_risk_diff(diff_abs: float) -> str:
    if np.isnan(diff_abs):
        return "unknown"
    if diff_abs < 0.02:
        return "negligible"
    if diff_abs < 0.05:
        return "small"
    if diff_abs < 0.15:
        return "medium"
    if diff_abs < 0.30:
        return "large"
    return "very large"

def _label_magnitude_ratio(rr: float) -> str:
    if np.isnan(rr) or rr <= 0:
        return "unknown"
    dist = abs(math.log(rr))
    if dist < 0.1:
        return "negligible"
    if dist < 0.25:
        return "small"
    if dist < 0.5:
        return "medium"
    if dist < 0.9:
        return "large"
    return "very large"

# Main effect size computation ----------------------------------------
if not effect_enabled_2711:
    print("   ‚ö†Ô∏è 2.7.11 disabled via CONFIG.EFFECT_SIZE.ENABLED = False")
else:
    # Try to read source files if they exist
    source_dfs_2711 = {}

    for src_name in effect_sources_2711:
        src_path = sec2_27_dir / src_name
        if src_path.exists():
            try:
                source_dfs_2711[src_name] = pd.read_csv(src_path)
            except Exception as e:
                print(f"   ‚ö†Ô∏è 2.7.11: failed to read {src_path}: {e}")
        else:
            print(f"   ‚ÑπÔ∏è 2.7.11: source file not found (skip): {src_path}")

    # ---------- 1) t-test effects: Cohen's d, d_z ----------
    if effect_metrics_2711.get("COHENS_D", True) and "t_test_results.csv" in source_dfs_2711:
        df_t = source_dfs_2711["t_test_results.csv"]
        for _, row in df_t.iterrows():
            test_name = row.get("test_name", None)
            test_type = row.get("test_type", None)
            p_val = row.get("p_value", np.nan)
            t_stat = row.get("t_statistic", np.nan)

            if test_type == "independent":
                nA = row.get("n_group_A", np.nan)
                nB = row.get("n_group_B", np.nan)
                mA = row.get("mean_group_A", np.nan)
                mB = row.get("mean_group_B", np.nan)
                sA = row.get("std_group_A", np.nan)
                sB = row.get("std_group_B", np.nan)

                if any(pd.isna([nA, nB, mA, mB, sA, sB])) or (nA <= 1) or (nB <= 1):
                    continue

                try:
                    nA = float(nA)
                    nB = float(nB)
                    sA2 = float(sA) ** 2
                    sB2 = float(sB) ** 2
                    sp2 = ((nA - 1) * sA2 + (nB - 1) * sB2) / (nA + nB - 2)
                    if sp2 <= 0:
                        d_val = np.nan
                    else:
                        sp = math.sqrt(sp2)
                        d_val = (float(mA) - float(mB)) / sp
                except Exception:
                    d_val = np.nan

                d_abs = abs(d_val) if not pd.isna(d_val) else np.nan
                mag = _label_magnitude_d(d_abs)

                effect_rows_2711.append({
                    "source_file": "t_test_results.csv",
                    "test_name": test_name,
                    "test_type": test_type,
                    "outcome_col": row.get("numeric_col", None),
                    "group_col": row.get("group_col", None),
                    "effect_type": "cohens_d",
                    "effect_value": d_val,
                    "magnitude_label": mag,
                    "p_value": p_val,
                    "statistic": t_stat,
                    "notes": "Cohen's d from pooled SD (independent t-test)"
                })

            elif test_type == "paired":
                n_pairs = row.get("n_pairs", np.nan)
                if pd.isna(t_stat) or pd.isna(n_pairs) or n_pairs <= 0:
                    continue
                try:
                    n_pairs = float(n_pairs)
                    d_val = float(t_stat) / math.sqrt(n_pairs)
                except Exception:
                    d_val = np.nan

                d_abs = abs(d_val) if not pd.isna(d_val) else np.nan
                mag = _label_magnitude_d(d_abs)

                effect_rows_2711.append({
                    "source_file": "t_test_results.csv",
                    "test_name": test_name,
                    "test_type": test_type,
                    "outcome_col": None,
                    "group_col": None,
                    "effect_type": "cohens_d_z",
                    "effect_value": d_val,
                    "magnitude_label": mag,
                    "p_value": p_val,
                    "statistic": t_stat,
                    "notes": "Approximate d_z = t / sqrt(n_pairs) (paired design)"
                })

    # ---------- 2) Chi-square effects: Phi / Cram√©r‚Äôs V ----------
    if effect_metrics_2711.get("PHI_CRAMER_V", True) and "chi_square_results.csv" in source_dfs_2711:
        df_chi = source_dfs_2711["chi_square_results.csv"]
        for _, row in df_chi.iterrows():
            test_name = f"{row.get('feature_1')}__{row.get('feature_2')}"
            f1 = row.get("feature_1", None)
            f2 = row.get("feature_2", None)
            chi2 = row.get("statistic", np.nan)
            p_val = row.get("p_value", np.nan)

            if f1 not in df_27.columns or f2 not in df_27.columns or pd.isna(chi2):
                continue

            # Reconstruct contingency to get N and table shape
            sub = df_27[[f1, f2]].dropna()
            if sub.empty:
                continue
            contingency = pd.crosstab(sub[f1], sub[f2])
            r, c = contingency.shape
            N = contingency.to_numpy().sum()

            if N <= 0:
                continue

            chi2_val = float(chi2)
            phi = math.sqrt(chi2_val / N)

            if r == 2 and c == 2:
                effect_rows_2711.append({
                    "source_file": "chi_square_results.csv",
                    "test_name": test_name,
                    "test_type": "chi_square_2x2",
                    "outcome_col": f2,
                    "group_col": f1,
                    "effect_type": "phi",
                    "effect_value": phi,
                    "magnitude_label": _label_magnitude_r(abs(phi)),
                    "p_value": p_val,
                    "statistic": chi2_val,
                    "notes": "Phi coefficient for 2x2 table"
                })
            else:
                k = min(r - 1, c - 1)
                if k <= 0:
                    continue
                V = math.sqrt(chi2_val / (N * k))
                effect_rows_2711.append({
                    "source_file": "chi_square_results.csv",
                    "test_name": test_name,
                    "test_type": "chi_square",
                    "outcome_col": f2,
                    "group_col": f1,
                    "effect_type": "cramers_v",
                    "effect_value": V,
                    "magnitude_label": _label_magnitude_r(abs(V)),
                    "p_value": p_val,
                    "statistic": chi2_val,
                    "notes": f"Cram√©r's V (r={r}, c={c})"
                })

    # ---------- 3) Point-biserial: r and r¬≤ ----------
    if effect_metrics_2711.get("R_SQUARED", True) and "point_biserial_results.csv" in source_dfs_2711:
        df_pb = source_dfs_2711["point_biserial_results.csv"]
        for _, row in df_pb.iterrows():
            test_name = row.get("numeric_feature", None)
            r_val = row.get("correlation", np.nan)
            p_val = row.get("p_value", np.nan)

            if pd.isna(r_val):
                continue

            r_abs = abs(r_val)
            r2 = r_val ** 2

            effect_rows_2711.append({
                "source_file": "point_biserial_results.csv",
                "test_name": test_name,
                "test_type": "point_biserial",
                "outcome_col": None,
                "group_col": None,
                "effect_type": "r",
                "effect_value": r_val,
                "magnitude_label": _label_magnitude_r(r_abs),
                "p_value": p_val,
                "statistic": None,
                "notes": "Point-biserial correlation"
            })

            effect_rows_2711.append({
                "source_file": "point_biserial_results.csv",
                "test_name": test_name,
                "test_type": "point_biserial",
                "outcome_col": None,
                "group_col": None,
                "effect_type": "r_squared",
                "effect_value": r2,
                "magnitude_label": _label_magnitude_r2(r2),
                "p_value": p_val,
                "statistic": None,
                "notes": "Variance explained (r^2)"
            })

    # ---------- 4) Proportion tests: risk diff, RR, OR ----------
    if "proportion_tests.csv" in source_dfs_2711:
        df_prop = source_dfs_2711["proportion_tests.csv"]
        for _, row in df_prop.iterrows():
            test_name = row.get("test_name", None)
            outcome_col = row.get("outcome_col", None)
            group_col = row.get("group_col", None)
            p_val = row.get("p_value", np.nan)

            rate_A = row.get("rate_A", np.nan)
            rate_B = row.get("rate_B", np.nan)
            abs_diff = row.get("absolute_diff", np.nan)
            rr = row.get("relative_risk", np.nan)
            nA = row.get("n_A", np.nan)
            nB = row.get("n_B", np.nan)
            sA = row.get("success_A", np.nan)
            sB = row.get("success_B", np.nan)

            # Risk difference
            if not pd.isna(abs_diff):
                mag = _label_magnitude_risk_diff(abs(abs_diff))
                effect_rows_2711.append({
                    "source_file": "proportion_tests.csv",
                    "test_name": test_name,
                    "test_type": "two_proportion_z",
                    "outcome_col": outcome_col,
                    "group_col": group_col,
                    "effect_type": "risk_difference",
                    "effect_value": abs_diff,
                    "magnitude_label": mag,
                    "p_value": p_val,
                    "statistic": row.get("z_statistic", np.nan),
                    "notes": "Absolute difference in proportions (rate_A - rate_B)"
                })

            # Relative risk
            if not pd.isna(rr):
                mag_rr = _label_magnitude_ratio(rr)
                effect_rows_2711.append({
                    "source_file": "proportion_tests.csv",
                    "test_name": test_name,
                    "test_type": "two_proportion_z",
                    "outcome_col": outcome_col,
                    "group_col": group_col,
                    "effect_type": "relative_risk",
                    "effect_value": rr,
                    "magnitude_label": mag_rr,
                    "p_value": p_val,
                    "statistic": row.get("z_statistic", np.nan),
                    "notes": "Relative risk (rate_A / rate_B)"
                })

            # Odds ratio (if computable)
            try:
                if not any(pd.isna([nA, nB, sA, sB])):
                    nA = float(nA); nB = float(nB)
                    sA = float(sA); sB = float(sB)
                    fA = nA - sA
                    fB = nB - sB
                    if sA > 0 and sB > 0 and fA > 0 and fB > 0:
                        or_val = (sA / fA) / (sB / fB)
                        mag_or = _label_magnitude_ratio(or_val)
                        effect_rows_2711.append({
                            "source_file": "proportion_tests.csv",
                            "test_name": test_name,
                            "test_type": "two_proportion_z",
                            "outcome_col": outcome_col,
                            "group_col": group_col,
                            "effect_type": "odds_ratio",
                            "effect_value": or_val,
                            "magnitude_label": mag_or,
                            "p_value": p_val,
                            "statistic": row.get("z_statistic", np.nan),
                            "notes": "Odds ratio from 2x2 table"
                        })
            except Exception:
                # If OR fails, just skip
                pass

    # NOTE: ANOVA / Kruskal and nonparametric U/W tests could have
    # more nuanced effect sizes (eta^2, rank-biserial, etc.), but we
    # don‚Äôt have sums-of-squares or z-statistics logged, so we skip them
    # for now with a conservative design.

    if effect_rows_2711:
        df_effect_2711 = pd.DataFrame(effect_rows_2711)

        # derive n_tests_covered and n_large_effects
        n_tests_covered_2711 = df_effect_2711["test_name"].nunique()

        # count "large" / "very large" magnitude labels
        n_large_effects_2711 = int(
            df_effect_2711["magnitude_label"]
            .fillna("unknown")
            .isin(["large", "very large"])
            .sum()
        )

        # effect_path_2711 = sec2_27_dir / effect_output_file_2711
        # df_effect_2711.to_csv(effect_path_2711, index=False)

        # Add BEFORE your existing if effect_rows_2711 block:
        # print("\nüìä EFFECT SIZE SUMMARY:")
        # if effect_rows_2711:
        #     print(f"‚úÖ {len(effect_rows_2711)} effect sizes computed across {n_tests_covered_2711} tests")
        #     large_pct = n_large_effects_2711 / n_tests_covered_2711 * 100
        #     print(f"üî• {n_large_effects_2711}/{n_tests_covered_2711} ({large_pct:.1f}%) LARGE effects")

        #     # Show top 5
        #     top5 = df_effect_2711.nlargest(5, 'effect_value', keep='all')
        #     print("\nTOP 5 EFFECTS:")
        #     for _, row in top5.iterrows():
        #         print(f"  ‚Ä¢ {row.test_name}: {row.effect_type}={row.effect_value:.3f} ({row.magnitude_label})")
        # else:
        #     print("‚ö†Ô∏è No effects (run upstream tests first)")

        # effect_status_2711 = "OK" if effect_rows_2711 else "WARN"

        # TOP 10 Effects Table (NEW)
    #     if effect_rows_2711:
    #         print("\nüìä TOP EFFECT SIZES:")
    #         top_effects = (df_effect_2711
    #             .assign(magnitude_score=lambda df: df['effect_value'].abs())
    #             .nlargest(10, 'magnitude_score')
    #             .loc[:, ['test_name', 'effect_type', 'effect_value', 'magnitude_label', 'p_value']]
    #             .round(3)
    #         )
    #         display(top_effects)

    #         # Executive Summary
    #         print(f"\nüéØ SUMMARY: {n_tests_covered_2711} tests ‚Üí {n_large_effects_2711} large/very large effects")
    #         print(f"   üìà Largest: {df_effect_2711['effect_value'].abs().max():.3f}")

    #     print(f"   ‚úÖ 2.7.11 effect size report written to: {effect_path_2711}")
    #     effect_detail_2711 = str(effect_path_2711)
    #     effect_status_2711 = "OK"
    # else:
    #     print("   ‚ö†Ô∏è 2.7.11: no effect sizes computed (no usable test inputs).")
    #     effect_status_2711 = "FAIL"

        # üéØ NEW: ENHANCED OUTPUT WITH ACTIONABLE INSIGHTS
        if effect_rows_2711:
            df_effect_2711 = pd.DataFrame(effect_rows_2711)
            n_tests_covered_2711 = df_effect_2711["test_name"].nunique()
            n_large_effects_2711 = int(df_effect_2711["magnitude_label"].fillna("unknown").isin(["large", "very large"]).sum())

            effect_path_2711 = sec2_27_dir / effect_output_file_2711
            df_effect_2711.to_csv(effect_path_2711, index=False)
            print(f"\n‚úÖ {len(effect_rows_2711)} effects | {n_tests_covered_2711} tests | üî• {n_large_effects_2711} LARGE")

            # üìä TOP 10 TABLE
            top_effects = (df_effect_2711
                .assign(magnitude_score=df_effect_2711['effect_value'].abs())
                .nlargest(10, 'magnitude_score')
                [['test_name', 'effect_type', 'effect_value', 'magnitude_label', 'p_value']]
                .round(3))
            display(top_effects)

            # normalize labels once
            mag_counts = (
                df_effect_2711["magnitude_label"]
                .fillna("unknown")
                .astype(str)
                .value_counts()
            )

            # üöÄ ACTIONABLE RECOMMENDATIONS
            print("\nüéØ PRIORITY ACTIONS BY EFFECT SIZE:")
            action_guide = {
                "very large": "üö® CRITICAL: Immediate feature engineering / business intervention",
                "large": "üî• HIGH: Top priority for modeling / A/B testing",
                "medium": "‚ö° MEDIUM: Strong signal, include in modeling",
                "small": "üìà LOW: Monitor, may compound with interactions",
                "negligible": "‚û°Ô∏è IGNORE: Not practically meaningful",
                "check": "‚ö†Ô∏è CHECK: Missing magnitude label / bad inputs",
                "unknown": "‚ùì UNKOWN: Missing data or invalid calculation",
            }
            #
            for mag, action in action_guide.items():
                count = int(mag_counts.get(mag, 0))
                if count > 0:
                    print(f"  {action} ({count}): {mag.upper()}")

            # UNNORMALIZED:
            # for mag, action in action_guide.items():
            #     count = int(df_effect_2711["magnitude_label"] == mag).sum()
            #     if count > 0:
            #         print(f"  {action} ({count}): {mag.upper()}")

            # üíº BUSINESS TRANSLATION TABLE
            print("\nüíº BUSINESS IMPACT REFERENCE:")
            impact_table = pd.DataFrame({
                "Effect Type": ["Cohen's d > 0.8", "Relative Risk > 3.0", "Cram√©r's V > 0.3", "r¬≤ > 0.15"],
                "Meaning": ["Large group difference", "3x risk multiplier", "Strong categorical association", "15% variance explained"],
                "Action": ["Segment + target", "Risk mitigation", "Feature engineering", "Primary predictor"]
            })
            display(impact_table)

            effect_status_2711 = "OK"
        else:
            print("‚ö†Ô∏è No upstream test results. Run 2.7.8-2.7.10 first.")
            effect_status_2711 = "WARN"
            effect_detail_2711 = "No upstream test results"

summary_2711 = pd.DataFrame([{
    "section": "2.7.11",
    "section_name": "Effect size computations",
    "check": "Compute standardized effect sizes (d, Œ∑¬≤, r¬≤, Œ¶/V) for key tests",
    "level": "info",
    "n_tests_covered": n_tests_covered_2711,
    "n_large_effects": n_large_effects_2711,
    "status": effect_status_2711,
    "detail": effect_detail_2711,
    "notes": None,
    "notes2": "Effect sizes computed for t-tests, chi-square, point-biserial, and proportion tests",
    "notes3": "Cohen's d, Phi/V, r, r¬≤, risk difference, relative risk, odds ratio"
}])
append_sec2(summary_2711, SECTION2_REPORT_PATH)

display(summary_2711)