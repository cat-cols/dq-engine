># Q:Why am I still getting this err message and how do i fix it:
># X: # /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/config/project_config.yaml

```yaml
CATEGORICAL:
  VALID_DOMAINS: *EXPECTED_LEVELS
```
```py
# /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/notebooks/02_DQ_IF.ipynb
# PART A | 2.4.1‚Äì2.4.7 üö´ Categorical Integrity ‚Äì Invalid Tokens & Domain Audit
print("\nPART A | 2.4.1‚Äì2.4.7 üö´ Categorical Integrity ‚Äì Invalid Tokens & Domain Audit")

# 2.4.1 | Invalid Tokens Scan
print("\n2.4.1 üö´ Invalid tokens scan")

# key bits:
# - valid_cat_cols_241 = [c for c in cat_cols if c in df.columns]
# - Warning print for missing_cat_cols_241 so you know when helpers like
# logic_repair_applied are being skipped.

# - All downstream counts (n_columns_scanned_241) now use valid_cat_cols_241, so the summary stays honest.

# 1) Ensure exist (suspect_tokens_241, invalid_patterns_241)

suspect_tokens_241 = C(
¬† ¬† "CATEGORICAL.SUSPECT_TOKENS",
¬† ¬† ["?", "N/A", "NA", "NULL", "None", "UNK", "UNKNOWN", "-", "--"]
)

#
invalid_token_patterns_241 = C("CATEGORICAL.INVALID_TOKEN_PATTERNS", [])
invalid_patterns_241 = []
for pat in invalid_token_patterns_241:
¬† ¬† try:
¬† ¬† ¬† ¬† invalid_patterns_241.append(re.compile(pat))
¬† ¬† except re.error:
¬† ¬† ¬† ¬† pass

#
case_mode_243 = C("CATEGORICAL.CASE_NORMALIZATION", "lower")
unicode_norm_243 = C("CATEGORICAL.UNICODE_NORMALIZATION", None)

invalid_rows_241 = []

# Filter cat_cols to only those that still exist in df

all_cat_cols_241 = list(cat_cols)¬† # keep original for reference
valid_cat_cols_241 = [c for c in all_cat_cols_241 if c in df.columns]
missing_cat_cols_241 = [c for c in all_cat_cols_241 if c not in df.columns]

if missing_cat_cols_241:
¬† ¬† print(f"¬† ¬†‚ö†Ô∏è 2.4.1: Skipping missing categorical columns (not in df): {missing_cat_cols_241}")

# make sure this is defined
n_rows_24 = df.shape[0]

# ==========================================
# 5. COLUMN ROLE & GROUP MAPPING (Updated)
# ==========================================

# Initialize the missing maps
role_map_24 = {}
feature_group_map_24 = {}

# Populate role_map from global lists if they exist
for c in df.columns:
¬† ¬† if c in id_cols_24:
¬† ¬† ¬† ¬† role_map_24[c] = "id"
¬† ¬† elif c in target_cols_24:
¬† ¬† ¬† ¬† role_map_24[c] = "target"
¬† ¬† else:
¬† ¬† ¬† ¬† role_map_24[c] = "feature"

# Populate feature_group_map (defaulting to unknown or using a global if available)
# If you have a global list like 'model_features', you can map it here
model_features_list = globals().get("model_features", [])
for c in df.columns:
¬† ¬† if c in model_features_list:
¬† ¬† ¬† ¬† feature_group_map_24[c] = "model_feature"
¬† ¬† else:
¬† ¬† ¬† ¬† feature_group_map_24[c] = "unknown"

for col in valid_cat_cols_241:
¬† ¬† s = df[col].astype("string")
¬† ¬† role_241 = role_map_24.get(col, "feature")
¬† ¬† fgroup_241 = feature_group_map_24.get(col, "unknown")

¬† ¬† value_counts_241 = s.value_counts(dropna=False)
¬† ¬† uniques_241 = value_counts_241.index.tolist()

¬† ¬† for val in uniques_241:
¬† ¬† ¬† ¬† if pd.isna(val):
¬† ¬† ¬† ¬† ¬† ¬† continue
¬† ¬† ¬† ¬† val_str = str(val)

¬† ¬† ¬† ¬† is_suspect_241 = val_str in suspect_tokens_241
¬† ¬† ¬† ¬† is_pattern_241 = any(p.search(val_str) for p in invalid_patterns_241) if invalid_patterns_241 else False

¬† ¬† ¬† ¬† if not is_suspect_241 and not is_pattern_241:
¬† ¬† ¬† ¬† ¬† ¬† continue

¬† ¬† ¬† ¬† count_241 = int(value_counts_241.loc[val])
¬† ¬† ¬† ¬† pct_241 = float(count_241 / n_rows_24 * 100.0) if n_rows_24 else 0.0

¬† ¬† ¬† ¬† if is_suspect_241 and is_pattern_241:
¬† ¬† ¬† ¬† ¬† ¬† token_type_241 = "placeholder+pattern"
¬† ¬† ¬† ¬† elif is_suspect_241:
¬† ¬† ¬† ¬† ¬† ¬† token_type_241 = "placeholder"
¬† ¬† ¬† ¬† else:
¬† ¬† ¬† ¬† ¬† ¬† token_type_241 = "pattern"

¬† ¬† ¬† ¬† if col in target_cols_24 or role_241 in {"id", "target"} or fgroup_241 == "model_feature":
¬† ¬† ¬† ¬† ¬† ¬† severity_241 = "critical"
¬† ¬† ¬† ¬† else:
¬† ¬† ¬† ¬† ¬† ¬† severity_241 = "warn"

¬† ¬† ¬† ¬† invalid_rows_241.append(
¬† ¬† ¬† ¬† ¬† ¬† {
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "column": col,
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "offending_value": val_str,
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "token_type": token_type_241,
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "count": count_241,
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "pct": round(pct_241, 5),
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "severity": severity_241,
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "role": role_241,
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "feature_group": fgroup_241,
¬† ¬† ¬† ¬† ¬† ¬† }
¬† ¬† ¬† ¬† )

invalid_tokens_df_241 = pd.DataFrame(invalid_rows_241)

invalid_tokens_path_241 = sec24_reports_dir / "invalid_tokens.csv"
tmp_241 = invalid_tokens_path_241.with_suffix(".tmp.csv")
invalid_tokens_df_241.to_csv(tmp_241, index=False)
os.replace(tmp_241, invalid_tokens_path_241)

n_columns_scanned_241 = len(valid_cat_cols_241)
n_columns_with_invalid_241 = len(set(invalid_tokens_df_241["column"])) if not invalid_tokens_df_241.empty else 0
if not invalid_tokens_df_241.empty:
¬† ¬† _col_crit_241 = (invalid_tokens_df_241["severity"] == "critical").groupby(invalid_tokens_df_241["column"]).any()
¬† ¬† n_critical_cols_241 = int(_col_crit_241.sum())
else:
¬† ¬† n_critical_cols_241 = 0

if n_critical_cols_241 == 0:
¬† ¬† status_241 = "OK"
else:
¬† ¬† status_241 = "WARN"

summary_241 = pd.DataFrame([{
¬† ¬† "section": "2.4.1",
¬† ¬† "section_name": "Invalid tokens scan",
¬† ¬† "check": "Scan categorical columns for suspect placeholder / garbage tokens",
¬† ¬† "level": "info",
¬† ¬† "status": status_241,
¬† ¬† "n_columns_scanned": int(n_columns_scanned_241),
¬† ¬† "n_columns_with_invalid_tokens": int(n_columns_with_invalid_241),
¬† ¬† "n_critical_token_columns": int(n_critical_cols_241),
¬† ¬† "detail": "invalid_tokens.csv",
¬† ¬† "timestamp": pd.Timestamp.utcnow(),
}])

print(f"üíæ invalid_tokens.csv ‚Üí {invalid_tokens_path_241}")

print("\nüìä invalid_tokens")
if not invalid_tokens_df_241.empty:
¬† ¬† display(invalid_tokens_df_241.head(20))
else:
¬† ¬† print("¬† ¬†(no invalid tokens detected)")

append_sec2(summary_241 , SECTION2_REPORT_PATH)
display(summary_241)

# 2.4.2 | Unexpected Categorical Values
print("\n2.4.2 üö´ Unexpected categorical values")

valid_domains_242 = C("CATEGORICAL.VALID_DOMAINS", {})

unexpected_rows_242 = []

if not valid_domains_242:
    print("   ‚ÑπÔ∏è 2.4.2: No configured VALID_DOMAINS; skipping unexpected-value checks.")
else:
    for col_242, dom_config_242 in valid_domains_242.items():
        if col_242 not in df.columns:
            continue

        s_242 = df[col_242].astype("string")
        role_242 = role_map_24.get(col_242, "feature")
        fgroup_242 = feature_group_map_24.get(col_242, "unknown")

        values_242 = s_242.value_counts(dropna=False)
        uniques_242 = values_242.index.tolist()

        allowed_values_242 = set()
        regex_list_242 = []
        domain_name_242 = col_242

        # Accept either:
        # - list of allowed values
        # - dict {"values":[...], "regex":[...], "name":"..."}
        if isinstance(dom_config_242, dict):
            vals_242 = dom_config_242.get("values", [])
            regs_242 = dom_config_242.get("regex", [])
            for v in vals_242:
                allowed_values_242.add(str(v))
            for rg in regs_242:
                try:
                    regex_list_242.append(re.compile(rg))
                except re.error:
                    pass
            if "name" in dom_config_242:
                domain_name_242 = dom_config_242["name"]
        else:
            try:
                for v in dom_config_242:
                    allowed_values_242.add(str(v))
            except TypeError:
                pass

        for val in uniques_242:
            if pd.isna(val):
                continue

            v_str_242 = str(val)
            in_set_242 = v_str_242 in allowed_values_242
            matches_regex_242 = any(r.search(v_str_242) for r in regex_list_242) if regex_list_242 else False

            if in_set_242 or matches_regex_242:
                continue

            count_242 = int(values_242.loc[val])
            pct_242 = float(count_242 / n_rows_24 * 100.0) if n_rows_24 else 0.0

            if col_242 in target_cols_24 or role_242 in {"id", "target"} or fgroup_242 == "model_feature":
                severity_242 = "critical"
            else:
                severity_242 = "warn"

            unexpected_rows_242.append(
                {
                    "column": col_242,
                    "offending_value": v_str_242,
                    "count": count_242,
                    "pct": round(pct_242, 5),
                    "expected_domain_name": domain_name_242,
                    "severity": severity_242,
                    "role": role_242,
                    "feature_group": fgroup_242,
                }
            )

unexpected_df_242 = pd.DataFrame(unexpected_rows_242)

unexpected_path_242 = sec24_reports_dir / "unexpected_values.csv"
tmp_242 = unexpected_path_242.with_suffix(".tmp.csv")
unexpected_df_242.to_csv(tmp_242, index=False)
os.replace(tmp_242, unexpected_path_242)

n_cols_with_domains_242 = len([c for c in valid_domains_242.keys() if c in df.columns]) if valid_domains_242 else 0
n_cols_with_unexp_242 = len(set(unexpected_df_242["column"])) if not unexpected_df_242.empty else 0
n_unexp_total_242 = int(unexpected_df_242.shape[0]) if not unexpected_df_242.empty else 0

if not unexpected_df_242.empty:
    _crit_cols_242 = (unexpected_df_242["severity"] == "critical").groupby(unexpected_df_242["column"]).any()
    n_critical_unexp_242 = int(_crit_cols_242.sum())
else:
    n_critical_unexp_242 = 0

if n_cols_with_unexp_242 == 0:
    status_242 = "OK"
elif n_critical_unexp_242 > 0:
    status_242 = "FAIL"
else:
    status_242 = "WARN"

summary_242 = pd.DataFrame([{
    "section": "2.4.2",
    "section_name": "Unexpected categorical values",
    "check": "Compare observed values against configured valid domains",
    "level": "info",
    "status": status_242,
    "n_columns_with_domains": int(n_cols_with_domains_242),
    "n_columns_with_unexpected_values": int(n_cols_with_unexp_242),
    "n_unexpected_values_total": int(n_unexp_total_242),
    "detail": "unexpected_values.csv",
    "timestamp": pd.Timestamp.utcnow(),
}])

print(f"üíæ 2.4.2 unexpected_values.csv ‚Üí {unexpected_path_242}")
print("\nüìä 2.4.2 unexpected_values (head):")
if not unexpected_df_242.empty:
    display(unexpected_df_242.head(20))
else:
    print("   (no unexpected values detected)")

append_sec2(summary_242, SECTION2_REPORT_PATH)
display(summary_242)

2.4.2 üö´ Unexpected categorical values
¬† ¬†‚ÑπÔ∏è 2.4.2: No configured VALID_DOMAINS; skipping unexpected-value checks.
üíæ 2.4.2 unexpected_values.csv ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/runs/20260113_011438/reports/2_4/unexpected_values.csv

üìä 2.4.2 unexpected_values (head):
¬† ¬†(no unexpected values detected)
üßæ Appended diagnostics ‚Üí /Users/b/DATA/PROJECTS/Telco/_T2/Level_3/runs/20260113_011438/reports/section2_unified.csv
```
The reason you are seeing that message is that the Python variable `valid_domains_242` is evaluating to an empty dictionary or `None`.

In your notebook code, this happens here:

```python
valid_domains_242 = C("CATEGORICAL.VALID_DOMAINS", {})

```

Even though your `project_config.yaml` has `VALID_DOMAINS: *EXPECTED_LEVELS`, there are three likely reasons why the notebook isn't seeing it:

### 1. YAML Anchor Reference Issue

In YAML, the `*EXPECTED_LEVELS` syntax is an **alias**. For this to work, there must be a corresponding **anchor** (`&EXPECTED_LEVELS`) defined earlier in the same YAML file.

**The Fix:**
Ensure your `project_config.yaml` defines the anchor before calling it:

```yaml
# Define the anchor first
EXPECTED_LEVELS: &EXPECTED_LEVELS
  contract_type: ["Month-to-month", "One year", "Two year"]
  gender: ["Male", "Female"]

CATEGORICAL:
  # Use the asterisk to reference it
  VALID_DOMAINS: *EXPECTED_LEVELS

```

### 2. The Config "C" Helper Logic

If your `C()` function (likely a configuration getter) is not set up to handle nested YAML structures or the YAML loader isn't resolving aliases, it will return the default value `{}`.

* **Check:** Are you using `yaml.SafeLoader`? Standard PyYAML supports anchors, but some custom loaders skip them.
* **Test:** Temporarily hardcode a value in the YAML without the alias to see if the notebook picks it up:
```yaml
CATEGORICAL:
  VALID_DOMAINS:
    my_col: ["A", "B"]

```



### 3. Notebook Kernel / Config Loading

If you edited the `.yaml` file but didn't **re-run the cell that loads the config** into the notebook session, the `C()` function is still looking at the old version of the configuration.

**The Fix:**

1. Restart your Notebook Kernel.
2. Ensure the cell that initializes the `Config` object (the one that reads `project_config.yaml`) is executed before Section 2.4.2.

---

### How to Debug in the Notebook

To see exactly what the notebook thinks your config is, run this in a cell right before 2.4.2:

```python
# Debugging the config fetch
raw_val = C("CATEGORICAL.VALID_DOMAINS", "MISSING")
print(f"DEBUG: Config value retrieved: {raw_val}")
print(f"DEBUG: Type: {type(raw_val)}")

```

**Would you like me to help you write a script to verify if your YAML file is being parsed correctly by Python's `PyYAML` library?**

This is what I had for EXPECTED_LEVELS before I sent that last query:
# OLD
# EXPECTED_LEVELS:
#   gender: ["Female", "Male"]
#   SeniorCitizen: ["0", "1"]
#   Partner: ["Yes", "No"]
#   Dependents: ["Yes", "No"]
#   PhoneService: ["Yes", "No"]
#   MultipleLines: ["Yes", "No", "No phone service"]
#   InternetService: ["DSL", "Fiber optic", "No"]
#   OnlineSecurity: ["Yes", "No", "No internet service"]
#   OnlineBackup: ["Yes", "No", "No internet service"]
#   DeviceProtection: ["Yes", "No", "No internet service"]
#   TechSupport: ["Yes", "No", "No internet service"]
#   StreamingTV: ["Yes", "No", "No internet service"]
#   StreamingMovies: ["Yes", "No", "No internet service"]
#   Contract: ["Month-to-month", "One year", "Two year"]
#   PaperlessBilling: ["Yes", "No"]
#   PaymentMethod:
#     [
#       "Electronic check",
#       "Mailed check",
#       "Bank transfer (automatic)",
#       "Credit card (automatic)"
#     ]
#   Churn: ["Yes", "No"]

# NEW
EXPECTED_LEVELS: &EXPECTED_LEVELS
  gender: ["Female", "Male"]
  SeniorCitizen: ["0", "1"]
  Partner: ["Yes", "No"]
  Dependents: ["Yes", "No"]
  PhoneService: ["Yes", "No"]
  MultipleLines: ["Yes", "No", "No phone service"]
  InternetService: ["DSL", "Fiber optic", "No"]
  OnlineSecurity: ["Yes", "No", "No internet service"]
  OnlineBackup: ["Yes", "No", "No internet service"]
  DeviceProtection: ["Yes", "No", "No internet service"]
  TechSupport: ["Yes", "No", "No internet service"]
  StreamingTV: ["Yes", "No", "No internet service"]
  StreamingMovies: ["Yes", "No", "No internet service"]
  Contract: ["Month-to-month", "One year", "Two year"]
  PaperlessBilling: ["Yes", "No"]
  PaymentMethod:
    - "Electronic check"
    - "Mailed check"
    - "Bank transfer (automatic)"
    - "Credit card (automatic)"
  Churn: ["Yes", "No"]

CATEGORICAL:
  VALID_DOMAINS: *EXPECTED_LEVELS

The logic in your **NEW** YAML block is syntactically correct for standard YAML, but there is a common pitfall with how YAML parsers handle **Anchors (`&`)** and **Aliases (`*`)** depending on the specific Python loader used.

Here are the two things likely causing the `No configured VALID_DOMAINS` message:

### 1. The Mapping indentation

In your YAML, `EXPECTED_LEVELS` and `CATEGORICAL` appear to be at the same root level. If your `C()` function is looking for `CATEGORICAL.VALID_DOMAINS`, it relies on the parser successfully resolving that pointer at load time.

If your config loader uses `yaml.SafeLoader` (the standard), it should work **provided the anchor is defined before the alias**. However, if you are using a custom wrapper that reads the file keys individually, it might fail to resolve the reference.

### 2. The "PaymentMethod" Format Change

You changed `PaymentMethod` from a flow-style list `[...]` to a block-style list `-`. Both are valid, but ensure there are no hidden tab characters (YAML requires spaces).

---

### üõ†Ô∏è The Fix: Verify & Force Refresh

To get Section 2.4.2 working, follow these steps in order:

#### Step 1: Test the Parsing in a Notebook Cell

Run this code in your notebook to see if Python actually "sees" the data:

```python
import yaml

config_path = "/Users/b/DATA/PROJECTS/Telco/_T2/Level_3/config/project_config.yaml"
with open(config_path, 'r') as f:
    # Use FullLoader or SafeLoader to ensure Aliases are resolved
    test_cfg = yaml.load(f, Loader=yaml.FullLoader)

print(f"Valid Domains Found: {bool(test_cfg.get('CATEGORICAL', {}).get('VALID_DOMAINS'))}")
print(f"Sample Column (gender): {test_cfg['CATEGORICAL']['VALID_DOMAINS'].get('gender')}")

```

#### Step 2: Ensure the Config Class is Re-initialized

If you are using a singleton or a global config object (accessed via your `C()` function), **it usually caches the file content**.

* **Restart the kernel** and run the "Setup" or "Initialization" cells first.
* Changing the `.yaml` file on disk does not update the `C()` function's internal memory automatically.

#### Step 3: Check for Key Nesting

If your `C()` helper expects everything under a `CONFIG` root but your YAML starts at the top level with `CATEGORICAL`, that mismatch will return your default `{}`.

Check if your YAML should actually look like this:

```yaml
# Ensure CATEGORICAL isn't accidentally indented or missing its parent
EXPECTED_LEVELS: &EXPECTED_LEVELS
  gender: ["Female", "Male"]
  # ... (rest of your list)

CATEGORICAL:
  VALID_DOMAINS: *EXPECTED_LEVELS

```

### Why the error still appears

The message `‚ÑπÔ∏è 2.4.2: No configured VALID_DOMAINS` is triggered by this line in your notebook:

```python
if not valid_domains_242:
```

This confirms that `valid_domains_242` is currently an **empty dictionary**. This almost always means:

1. The alias `*EXPECTED_LEVELS` failed to resolve (returned `None`).
2. The path `CATEGORICAL.VALID_DOMAINS` was not found in the loaded dictionary.
3. The config file wasn't re-read after you added the "NEW" block.

**Would you like me to look at the code for your `C()` function to see how it handles nested keys?**

The issue is that you are using `yaml.safe_load(f)` in your **Section 2.0 Part C**.

Standard YAML **Anchors** (`&`) and **Aliases** (`*`) are technically part of the YAML spec, but many environments (and older versions of PyYAML) require `yaml.FullLoader` to resolve them correctly across different parts of the document. Additionally, because your `EXPECTED_LEVELS` is at the **root** of the YAML and `VALID_DOMAINS` is nested under `CATEGORICAL`, the parser needs to resolve that reference before the dictionary is handed to Python.

### The Fix

#### 1. Update the Loader in Section 2.0

In your **Part C** bootstrap code, change the loader from `safe_load` to `full_load`. This ensures the alias `*EXPECTED_LEVELS` is actually replaced by the dictionary data during the read process.

**Change this:**

```python
with CONFIG_PATH.open("r", encoding="utf-8") as f:
    config_data = yaml.safe_load(f) or {}

```

**To this:**

```python
with CONFIG_PATH.open("r", encoding="utf-8") as f:
    # FullLoader is required to resolve YAML anchors/aliases reliably
    config_data = yaml.load(f, Loader=yaml.FullLoader) or {}

```

#### 2. Verify the YAML structure

Ensure there are no tabs in your `project_config.yaml`. YAML strictly forbids tabs for indentation. Your structure should look exactly like this (using spaces):

```yaml
EXPECTED_LEVELS: &EXPECTED_LEVELS
  gender: ["Female", "Male"]
  # ... rest of levels

CATEGORICAL:
  VALID_DOMAINS: *EXPECTED_LEVELS
```

#### 3. Why `C()` is returning `{}`

In your Section 2.4 Setup, you have:

```python
valid_domains_24 = C("CATEGORICAL.VALID_DOMAINS", {})
```

Since `safe_load` likely failed to resolve the alias, `config_data['CATEGORICAL']['VALID_DOMAINS']` ended up as `None` or an empty string. The `C()` helper then sees a nullish value and returns your provided default: `{}`.

### Testing the Resolution

After changing to `FullLoader` and re-running Section 2.0, run this debug cell to confirm the data is "inflated":

```python
# Quick Debug Check
test_val = CONFIG.get("CATEGORICAL", {}).get("VALID_DOMAINS", {})
print(f"Type of VALID_DOMAINS: {type(test_val)}")
print(f"Keys found: {list(test_val.keys()) if isinstance(test_val, dict) else 'NOT A DICT'}")

```

If the keys (gender, SeniorCitizen, etc.) appear in the output, Section 2.4.2 will now find the data and run the audit.

**Would you like me to show you how to modify the `C()` helper to provide a warning if it finds a key but the value is null?**