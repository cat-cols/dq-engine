># Is this redundant?

```python
# 2.1.5 üß¨ Feature Group Registration | Column-level Feature Catalog
print("\n2.1.5 üß¨ Feature Group Registration | Column-level Feature Catalog")

# REQUIREMENTS: special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
# TODO: fix numbering issues in 2.1.5 & PART B

# Guards
assert "df" in globals(), "‚ùå df not found. Run Section 2.0.0 first."
assert "CONFIG" in globals(), "‚ùå CONFIG not found. Run 2.0.0 first."
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing. Run 2.0.1 first."
assert "SEC2_REPORTS_DIR" in globals(), "‚ùå SEC2_REPORTS_DIR missing. Run 2.0.0/2.0.1 first."
assert "SEC2_ARTIFACTS_DIR" in globals(), "‚ùå SEC2_ARTIFACTS_DIR missing. Run 2.0.0 first."
assert "id_cols" in globals(), "‚ùå id_cols not found. Run 2.1.2 first."

SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# 1) Config + thresholds
fg_cfg = CONFIG.get("FEATURE_GROUPING", {}) or {}
low_card_threshold = int(fg_cfg.get("LOW_CARDINALITY_THRESHOLD", 20))
free_text_min_avg_len = int(fg_cfg.get("FREE_TEXT_MIN_AVG_LEN", 30))

ordinal_cfg = CONFIG.get("ORDINAL_COLUMNS", []) or []
if isinstance(ordinal_cfg, (str, bytes)):
    ordinal_cols = [ordinal_cfg]
else:
    ordinal_cols = list(ordinal_cfg)

protected_cfg = CONFIG.get("PROTECTED_COLUMNS", []) or []
if isinstance(protected_cfg, (str, bytes)):
    protected_from_config = {protected_cfg}
else:
    protected_from_config = set(protected_cfg)

# 2) Resolve target columns (prefer CONFIG, fallback to previously-resolved globals)
raw_target_col = None
encoded_target_col = None

_target_block = CONFIG.get("TARGET", {}) or {}
_raw_from_cfg = _target_block.get("RAW_COLUMN")
_enc_from_cfg = _target_block.get("COLUMN")

# Raw target (e.g., "Churn")
if _raw_from_cfg and _raw_from_cfg in df.columns:
    raw_target_col = _raw_from_cfg
else:
    _prev_raw = globals().get("raw_target_col")
    if isinstance(_prev_raw, str) and _prev_raw in df.columns:
        raw_target_col = _prev_raw

# Encoded target (e.g., "Churn_flag")
if _enc_from_cfg and _enc_from_cfg in df.columns:
    encoded_target_col = _enc_from_cfg
else:
    _prev_enc = globals().get("encoded_target_col")
    if isinstance(_prev_enc, str) and _prev_enc in df.columns:
        encoded_target_col = _prev_enc

print("üéØ raw_target_col:", raw_target_col, "| encoded_target_col:", encoded_target_col)

# 3) Load special numeric flags (from 2.1.3)
special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
special_flag_cols = set()

if special_flags_path.exists() and special_flags_path.stat().st_size > 0:
    try:
        _special_flags_df = pd.read_csv(special_flags_path)
    except Exception:
        _special_flags_df = pd.DataFrame()
    if "column" in _special_flags_df.columns:
        special_flag_cols = set(
            _special_flags_df["column"].dropna().astype("string")
        )
else:
    _special_flags_df = pd.DataFrame(columns=["column", "role"])

# 4) Build protected-columns set
protected_cols = set()

# IDs
protected_cols.update(id_cols)

# Targets
if raw_target_col is not None:
    protected_cols.add(raw_target_col)
if encoded_target_col is not None:
    protected_cols.add(encoded_target_col)

# Special numeric flags
protected_cols.update(special_flag_cols)

# Config-driven protected list
protected_cols.update(protected_from_config)

# Ensure only columns that actually exist in df are considered protected
# Only keep those that actually exist in df
protected_cols = {c for c in protected_cols if c in df.columns}

# --- 2.1.5.5 Feature grouping logic (per-column catalog) --------------------
feature_group_rows = []

for col in df.columns:
    s = df[col]
    dtype_str = str(s.dtype)
    n_unique = int(s.nunique(dropna=True))
    is_protected = col in protected_cols

    # Base notes str we can enrich
    notes = []

    # Target / target_aux
    if encoded_target_col is not None and col == encoded_target_col:
        feature_group = "target"
        notes.append("binary target (encoded)")
    elif raw_target_col is not None and col == raw_target_col:
        feature_group = "target_aux"
        notes.append("raw target label")
    # ID / primary key
    elif col in id_cols:
        feature_group = "id"
        notes.append("ID / key candidate")
    # Explicit ordinal from config
    elif col in ordinal_cols:
        feature_group = "ordinal"
        notes.append("ordinal from CONFIG.ORDINAL_COLUMNS")
    # Special-case numeric flags from 2.1.3
    elif col in special_flag_cols:
        feature_group = "numeric_flag"
        notes.append("special numeric flag (2.1.3)")
    else:
        # Type-based rules
        if pd.api.types.is_datetime64_any_dtype(s):
            feature_group = "datetime"
            notes.append("datetime-like dtype")
        elif pd.api.types.is_bool_dtype(s):
            feature_group = "numeric_flag"
            notes.append("bool ‚Üí treated as flag")
        elif pd.api.types.is_numeric_dtype(s):
            # numeric: detect potential flag-like or discrete small-card
            if n_unique <= low_card_threshold and n_unique <= 10:
                feature_group = "numeric_flag"
                notes.append(
                    f"numeric small-card (n_unique={n_unique} ‚â§ {low_card_threshold})"
                )
            else:
                feature_group = "numeric_continuous"
                notes.append("numeric continuous / high-card")
        else:
            # object / string-like / category
            # quick heuristic for free text: high card + long strings
            try:
                avg_len = float(
                    s.dropna()
                    .astype("string")
                    .str.len()
                    .mean()
                )
            except Exception:
                avg_len = 0.0

            if n_unique <= low_card_threshold:
                feature_group = "categorical_low_card"
                notes.append(
                    f"low-card categorical (n_unique={n_unique} ‚â§ {low_card_threshold})"
                )
            else:
                if avg_len >= free_text_min_avg_len:
                    feature_group = "free_text"
                    notes.append(
                        f"free text (avg_len‚âà{avg_len:.1f} ‚â• {free_text_min_avg_len})"
                    )
                else:
                    feature_group = "categorical_high_card"
                    notes.append(
                        f"high-card categorical (n_unique={n_unique} > {low_card_threshold})"
                    )

    feature_group_rows.append(
        {
            "column": col,
            "dtype": dtype_str,
            "feature_group": feature_group,
            "n_unique": n_unique,
            "protected": bool(is_protected),
            "notes": "; ".join(notes),
        }
    )

#
feature_groups_df = pd.DataFrame(feature_group_rows)

# Just in case, ensure feature_group is not missing
feature_groups_df["feature_group"] = feature_groups_df["feature_group"].fillna("other")

# metrics?
n_features = int(len(feature_groups_df))
n_protected = int(feature_groups_df["protected"].sum())
n_unassigned = int((feature_groups_df["feature_group"] == "other").sum())

# --- 2.1.5.6 Persist CSV artifact under SEC2_REPORTS_DIR --------------------
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups_2_1_5.csv"
fg_tmp_csv = fg_csv_path.with_suffix(".tmp.csv")

feature_groups_df.to_csv(fg_tmp_csv, index=False)
os.replace(fg_tmp_csv, fg_csv_path)

print(f"2.1.5 feature groups CSV written ‚Üí {fg_csv_path} ‚úÖ")
display(feature_groups_df.head(30))

# Evidence helpers for notes/debug
_detected_id_rows = feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].tolist()

# CREATE ISSUES LIST
issues_215 = []

_detected_id_cols = set(
    feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].astype("string")
)
_config_id_cols = set(id_cols)  # canonical IDs from 2.1.2

missing_from_protection = sorted(_detected_id_cols - _config_id_cols)
if missing_from_protection:
    issues_215.append(f"id-like cols not in id_cols: {missing_from_protection}")

# Unassigned
if n_unassigned > 0:
    issues_215.append(f"{n_unassigned} column(s) unassigned ‚Üí feature_group='other'")

# Targets
if raw_target_col is None:
    issues_215.append("raw target not resolved (CONFIG.TARGET.RAW_COLUMN missing/invalid and no global fallback)")
if encoded_target_col is None:
    issues_215.append("encoded target not resolved (CONFIG.TARGET.COLUMN missing/invalid and no global fallback)")

# IDs (strong signal if empty) with evidence
if not id_cols:
    issues_215.append(
        "id_cols is empty (2.1.2 may not have detected IDs; ID protection may be incomplete). "
        f"Columns currently grouped as id: {sorted(_detected_id_rows)}"
    )

# FIXME: integrate into 2.1.6: 2.1.5.6 Persist YAML / JSON artifact under SEC2_ARTIFACTS_DIR
# Build group ‚Üí columns mapping (+ protected info) for YAML/JSON
group_map = {}
for grp, sub_df in feature_groups_df.groupby("feature_group"):
    group_map[str(grp)] = sorted(sub_df["column"].astype("string").tolist())

protected_list = sorted(feature_groups_df.loc[feature_groups_df["protected"], "column"].astype("string").tolist())
meta = {
    "section": "2.1.5",
    "description": "Feature group catalog at end of Section 2.1",
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

fg_struct = {
    "groups": group_map,
    "meta": meta,
}

# Create JSON (latest + snapshot)
SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

fg_json_path = SEC2_ARTIFACTS_DIR / "feature_groups.json"

# timestamped snapshot (UTC)
ts_snap = pd.Timestamp.utcnow().strftime("%Y-%m-%d__%H%M%S") + "Z"
fg_json_snapshot = SEC2_ARTIFACTS_DIR / f"feature_groups__{ts_snap}.json"

# write latest atomically
fg_tmp_json = fg_json_path.with_suffix(".tmp.json")
with open(fg_tmp_json, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_json, fg_json_path)

# write snapshot (also atomically)
fg_tmp_snap = fg_json_snapshot.with_suffix(".tmp.json")
with open(fg_tmp_snap, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_snap, fg_json_snapshot)

print(f"üíæ feature groups JSON written ‚Üí {fg_json_path}")
print(f"üìå snapshot JSON written ‚Üí {fg_json_snapshot}")

# -- YAML (optional)
fg_yaml_path = SEC2_ARTIFACTS_DIR / "feature_groups.yaml"
if yaml is not None:
    fg_tmp_yaml = fg_yaml_path.with_suffix(".tmp.yaml")
    with open(fg_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(fg_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fg_tmp_yaml, fg_yaml_path)
    print(f"üíæ feature groups YAML written ‚Üí {fg_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature groups.")

# -- 2.1.5.7 Summary metrics + unified diagnostics row
group_counts = (
    feature_groups_df["feature_group"]
    .value_counts()
    .sort_index()
    .to_dict()
)

# Determine status (severity ladder)
status_215 = "OK"

# Hard failures (ERROR)
if not id_cols:
    status_215 = "ERROR"
# Medium failures (WARN) if not already ERROR
elif missing_from_protection:
    status_215 = "WARN"
elif raw_target_col is None or encoded_target_col is None:
    status_215 = "WARN"
elif n_unassigned > 0:
    status_215 = "WARN"


# Level mapping
level_215 = "info"
if status_215 == "WARN":
    level_215 = "warn"
elif status_215 == "ERROR":
    level_215 = "error"

notes_215 = "; ".join(issues_215) if issues_215 else None

# use scalar values ([{}])
summary_215 = pd.DataFrame([{
    "section":             "2.1.5",
    "section_name":        "Feature group registration",
    "check":               "Feature group catalog (column-level)",
    "level":               level_215,
    "status":              status_215,
    "n_features":          n_features,
    "n_protected":         n_protected,
    "n_unassigned":        n_unassigned,
    "group_counts_json":   json.dumps(group_counts, sort_keys=True),
    "feature_groups_csv":  fg_csv_path.name,
    "feature_groups_json": fg_json_path.name,
    "feature_groups_yaml": fg_yaml_path.name if yaml is not None else None,
    "timestamp": pd.Timestamp.utcnow(),
    "detail": (
        f"Feature groups registered for {n_features} column(s); "
        f"catalog written to {fg_csv_path.name}."),
    "notes":                notes_215,
}])

append_sec2(summary_215, SECTION2_REPORT_PATH)
display(summary_215)

# 2.1.6 üß± Feature Space Scaffolding | Groups ‚Üí Config for Downstream
print("\n2.1.6 üß± Feature Space Scaffolding | Groups ‚Üí Config for Downstream")

# Guards
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing. Run 2.0.1 first."
assert "SEC2_REPORTS_DIR" in globals(), "‚ùå SEC2_REPORTS_DIR missing. Run 2.0.0/2.0.1 first."
assert "ARTIFACTS_DIR" in globals(), "‚ùå ARTIFACTS_DIR missing. Run 2.0.0 first."
assert "CONFIG" in globals(), "‚ùå CONFIG not found. Run 2.0.0 first."

import json
try:
    import yaml
except Exception:
    yaml = None

# --- 2.1.6.1 Reload thresholds so they are embedded in meta ------------------
fg_cfg = CONFIG.get("FEATURE_GROUPING", {}) or {}
low_card_threshold = int(fg_cfg.get("LOW_CARDINALITY_THRESHOLD", 20))
free_text_min_avg_len = int(fg_cfg.get("FREE_TEXT_MIN_AVG_LEN", 30))

# --- 2.1.6.2 Load the 2.1.5 catalog -----------------------------------------
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups.csv"
assert fg_csv_path.exists(), f"‚ùå {fg_csv_path} not found. Run 2.1.5 first."
feature_groups_df_216 = pd.read_csv(fg_csv_path)

if "feature_group" not in feature_groups_df_216.columns:
    raise KeyError("‚ùå feature_groups_2_1_5.csv missing 'feature_group' column.")

# --- 2.1.6.3 Build group ‚Üí columns mapping + protected list ------------------
group_map = {}
for grp, sub_df in feature_groups_df_216.groupby("feature_group"):
    group_map[str(grp)] = sorted(sub_df["column"].astype("string").tolist())

protected_mask = feature_groups_df_216.get("protected", pd.Series([False] * len(feature_groups_df_216)))
protected_list = sorted(feature_groups_df_216.loc[protected_mask.astype(bool), "column"].astype("string").tolist())

meta = {
    "section": "2.1.6",
    "description": "Feature space scaffolding (groups ‚Üí columns) derived from 2.1.5 catalog.",
    "source_catalog": fg_csv_path.name,
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

feature_space_struct = {
    "groups": group_map,
    "meta": meta,
}

protected_list = sorted(feature_groups_df.loc[feature_groups_df["protected"], "column"].astype("string").tolist())
meta = {
    "section": "2.1.5",
    "description": "Feature group catalog at end of Section 2.1",
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

fg_struct = {
    "groups": group_map,
    "meta": meta,
}

# JSON
fg_json_path = ARTIFACTS_DIR / "feature_groups.json"
fg_tmp_json = fg_json_path.with_suffix(".tmp.json")
with open(fg_tmp_json, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_json, fg_json_path)

print(f"üíæ feature groups JSON written ‚Üí {fg_json_path}")

# YAML (optional)
fg_yaml_path = ARTIFACTS_DIR / "feature_groups_2_1_5.yaml"
if yaml is not None:
    fg_tmp_yaml = fg_yaml_path.with_suffix(".tmp.yaml")
    with open(fg_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(fg_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fg_tmp_yaml, fg_yaml_path)
    print(f"üíæ feature groups YAML written ‚Üí {fg_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature groups.")


# --- 2.1.6.4 Persist JSON / YAML artifacts under ARTIFACTS_DIR --------------
fs_json_path = ARTIFACTS_DIR / "feature_space_2_1_6.json"
fs_tmp_json = fs_json_path.with_suffix(".tmp.json")
with open(fs_tmp_json, "w", encoding="utf-8") as f:
    json.dump(feature_space_struct, f, indent=2, ensure_ascii=False)
os.replace(fs_tmp_json, fs_json_path)

print(f"üíæ 2.1.6 feature space JSON written ‚Üí {fs_json_path}")

fs_yaml_path = ARTIFACTS_DIR / "feature_space_2_1_6.yaml"
if yaml is not None:
    fs_tmp_yaml = fs_yaml_path.with_suffix(".tmp.yaml")
    with open(fs_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(feature_space_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fs_tmp_yaml, fs_yaml_path)
    print(f"üíæ 2.1.6 feature space YAML written ‚Üí {fs_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature space scaffolding.")

# --- 2.1.6.5 Diagnostics summary + SECTION2_REPORT_PATH append ---------------
group_counts_216 = {
    grp: len(cols) for grp, cols in group_map.items()
}

n_features_216 = int(len(feature_groups_df_216))
n_protected_216 = int(len(protected_list))
n_groups_216 = int(len(group_map))

summary_216 = pd.DataFrame([{
    "section":            "2.1.6",
    "section_name":       "Feature space scaffolding",
    "check":              "Feature space config (groups ‚Üí columns)",
    "level":              "info",
    "status":             "OK",
    "n_features":         int(n_features_216),
    "n_protected":        int(n_protected_216),
    "n_groups":           int(n_groups_216),
    "group_counts_json":  json.dumps(group_counts_216, sort_keys=True),
    "feature_space_json": getattr(fs_json_path, "name", str(fs_json_path)),
    "feature_space_yaml": getattr(fs_yaml_path, "name", str(fs_yaml_path))
                            if (yaml is not None and fs_yaml_path is not None)
                            else None,
    "source_catalog_csv": getattr(fg_csv_path, "name", str(fg_csv_path)),
    "timestamp":          pd.Timestamp.utcnow(),
    "detail":          (f"Feature space scaffolding built from "
                        f"{getattr(fg_csv_path, 'name', str(fg_csv_path))}; "
                        f"JSON: {getattr(fs_json_path, 'name', str(fs_json_path))}, "
                        f"YAML: {getattr(fs_yaml_path, 'name', str(fs_yaml_path)) if yaml is not None else 'skipped'}."),
}])

append_sec2(summary_216, SECTION2_REPORT_PATH)

display(summary_216)

# 2.1.5 üß¨ Feature Group Registration | Column-level Feature Catalog
print("\n2.1.5 üß¨ Feature Group Registration | Column-level Feature Catalog")

# REQUIREMENTS: special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
# TODO: fix numbering issues in 2.1.5 & PART B

# Guards
assert "df" in globals(), "‚ùå df not found. Run Section 2.0.0 first."
assert "CONFIG" in globals(), "‚ùå CONFIG not found. Run 2.0.0 first."
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing. Run 2.0.1 first."
assert "SEC2_REPORTS_DIR" in globals(), "‚ùå SEC2_REPORTS_DIR missing. Run 2.0.0/2.0.1 first."
assert "SEC2_ARTIFACTS_DIR" in globals(), "‚ùå SEC2_ARTIFACTS_DIR missing. Run 2.0.0 first."
assert "id_cols" in globals(), "‚ùå id_cols not found. Run 2.1.2 first."

SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# 1) Config + thresholds
fg_cfg = CONFIG.get("FEATURE_GROUPING", {}) or {}
low_card_threshold = int(fg_cfg.get("LOW_CARDINALITY_THRESHOLD", 20))
free_text_min_avg_len = int(fg_cfg.get("FREE_TEXT_MIN_AVG_LEN", 30))

ordinal_cfg = CONFIG.get("ORDINAL_COLUMNS", []) or []
if isinstance(ordinal_cfg, (str, bytes)):
    ordinal_cols = [ordinal_cfg]
else:
    ordinal_cols = list(ordinal_cfg)

protected_cfg = CONFIG.get("PROTECTED_COLUMNS", []) or []
if isinstance(protected_cfg, (str, bytes)):
    protected_from_config = {protected_cfg}
else:
    protected_from_config = set(protected_cfg)

# 2) Resolve target columns (prefer CONFIG, fallback to previously-resolved globals)
raw_target_col = None
encoded_target_col = None

_target_block = CONFIG.get("TARGET", {}) or {}
_raw_from_cfg = _target_block.get("RAW_COLUMN")
_enc_from_cfg = _target_block.get("COLUMN")

# Raw target (e.g., "Churn")
if _raw_from_cfg and _raw_from_cfg in df.columns:
    raw_target_col = _raw_from_cfg
else:
    _prev_raw = globals().get("raw_target_col")
    if isinstance(_prev_raw, str) and _prev_raw in df.columns:
        raw_target_col = _prev_raw

# Encoded target (e.g., "Churn_flag")
if _enc_from_cfg and _enc_from_cfg in df.columns:
    encoded_target_col = _enc_from_cfg
else:
    _prev_enc = globals().get("encoded_target_col")
    if isinstance(_prev_enc, str) and _prev_enc in df.columns:
        encoded_target_col = _prev_enc

print("üéØ raw_target_col:", raw_target_col, "| encoded_target_col:", encoded_target_col)

# 3) Load special numeric flags (from 2.1.3)
special_flags_path = SEC2_REPORTS_DIR / "special_numeric_flags.csv"
special_flag_cols = set()

if special_flags_path.exists() and special_flags_path.stat().st_size > 0:
    try:
        _special_flags_df = pd.read_csv(special_flags_path)
    except Exception:
        _special_flags_df = pd.DataFrame()
    if "column" in _special_flags_df.columns:
        special_flag_cols = set(
            _special_flags_df["column"].dropna().astype("string")
        )
else:
    _special_flags_df = pd.DataFrame(columns=["column", "role"])

# 4) Build protected-columns set
protected_cols = set()

# IDs
protected_cols.update(id_cols)

# Targets
if raw_target_col is not None:
    protected_cols.add(raw_target_col)
if encoded_target_col is not None:
    protected_cols.add(encoded_target_col)

# Special numeric flags
protected_cols.update(special_flag_cols)

# Config-driven protected list
protected_cols.update(protected_from_config)

# Ensure only columns that actually exist in df are considered protected
# Only keep those that actually exist in df
protected_cols = {c for c in protected_cols if c in df.columns}

# --- 2.1.5.5 Feature grouping logic (per-column catalog) --------------------
feature_group_rows = []

for col in df.columns:
    s = df[col]
    dtype_str = str(s.dtype)
    n_unique = int(s.nunique(dropna=True))
    is_protected = col in protected_cols

    # Base notes str we can enrich
    notes = []

    # Target / target_aux
    if encoded_target_col is not None and col == encoded_target_col:
        feature_group = "target"
        notes.append("binary target (encoded)")
    elif raw_target_col is not None and col == raw_target_col:
        feature_group = "target_aux"
        notes.append("raw target label")
    # ID / primary key
    elif col in id_cols:
        feature_group = "id"
        notes.append("ID / key candidate")
    # Explicit ordinal from config
    elif col in ordinal_cols:
        feature_group = "ordinal"
        notes.append("ordinal from CONFIG.ORDINAL_COLUMNS")
    # Special-case numeric flags from 2.1.3
    elif col in special_flag_cols:
        feature_group = "numeric_flag"
        notes.append("special numeric flag (2.1.3)")
    else:
        # Type-based rules
        if pd.api.types.is_datetime64_any_dtype(s):
            feature_group = "datetime"
            notes.append("datetime-like dtype")
        elif pd.api.types.is_bool_dtype(s):
            feature_group = "numeric_flag"
            notes.append("bool ‚Üí treated as flag")
        elif pd.api.types.is_numeric_dtype(s):
            # numeric: detect potential flag-like or discrete small-card
            if n_unique <= low_card_threshold and n_unique <= 10:
                feature_group = "numeric_flag"
                notes.append(
                    f"numeric small-card (n_unique={n_unique} ‚â§ {low_card_threshold})"
                )
            else:
                feature_group = "numeric_continuous"
                notes.append("numeric continuous / high-card")
        else:
            # object / string-like / category
            # quick heuristic for free text: high card + long strings
            try:
                avg_len = float(
                    s.dropna()
                    .astype("string")
                    .str.len()
                    .mean()
                )
            except Exception:
                avg_len = 0.0

            if n_unique <= low_card_threshold:
                feature_group = "categorical_low_card"
                notes.append(
                    f"low-card categorical (n_unique={n_unique} ‚â§ {low_card_threshold})"
                )
            else:
                if avg_len >= free_text_min_avg_len:
                    feature_group = "free_text"
                    notes.append(
                        f"free text (avg_len‚âà{avg_len:.1f} ‚â• {free_text_min_avg_len})"
                    )
                else:
                    feature_group = "categorical_high_card"
                    notes.append(
                        f"high-card categorical (n_unique={n_unique} > {low_card_threshold})"
                    )

    feature_group_rows.append(
        {
            "column": col,
            "dtype": dtype_str,
            "feature_group": feature_group,
            "n_unique": n_unique,
            "protected": bool(is_protected),
            "notes": "; ".join(notes),
        }
    )

#
feature_groups_df = pd.DataFrame(feature_group_rows)

# Just in case, ensure feature_group is not missing
feature_groups_df["feature_group"] = feature_groups_df["feature_group"].fillna("other")

# metrics?
n_features = int(len(feature_groups_df))
n_protected = int(feature_groups_df["protected"].sum())
n_unassigned = int((feature_groups_df["feature_group"] == "other").sum())

# --- 2.1.5.6 Persist CSV artifact under SEC2_REPORTS_DIR --------------------
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups_2_1_5.csv"
fg_tmp_csv = fg_csv_path.with_suffix(".tmp.csv")

feature_groups_df.to_csv(fg_tmp_csv, index=False)
os.replace(fg_tmp_csv, fg_csv_path)

print(f"2.1.5 feature groups CSV written ‚Üí {fg_csv_path} ‚úÖ")
display(feature_groups_df.head(30))

# Evidence helpers for notes/debug
_detected_id_rows = feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].tolist()

# CREATE ISSUES LIST
issues_215 = []

_detected_id_cols = set(
    feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].astype("string")
)
_config_id_cols = set(id_cols)  # canonical IDs from 2.1.2

missing_from_protection = sorted(_detected_id_cols - _config_id_cols)
if missing_from_protection:
    issues_215.append(f"id-like cols not in id_cols: {missing_from_protection}")

# Unassigned
if n_unassigned > 0:
    issues_215.append(f"{n_unassigned} column(s) unassigned ‚Üí feature_group='other'")

# Targets
if raw_target_col is None:
    issues_215.append("raw target not resolved (CONFIG.TARGET.RAW_COLUMN missing/invalid and no global fallback)")
if encoded_target_col is None:
    issues_215.append("encoded target not resolved (CONFIG.TARGET.COLUMN missing/invalid and no global fallback)")

# IDs (strong signal if empty) with evidence
if not id_cols:
    issues_215.append(
        "id_cols is empty (2.1.2 may not have detected IDs; ID protection may be incomplete). "
        f"Columns currently grouped as id: {sorted(_detected_id_rows)}"
    )

# FIXME: integrate into 2.1.6: 2.1.5.6 Persist YAML / JSON artifact under SEC2_ARTIFACTS_DIR
# Build group ‚Üí columns mapping (+ protected info) for YAML/JSON
group_map = {}
for grp, sub_df in feature_groups_df.groupby("feature_group"):
    group_map[str(grp)] = sorted(sub_df["column"].astype("string").tolist())

protected_list = sorted(feature_groups_df.loc[feature_groups_df["protected"], "column"].astype("string").tolist())
meta = {
    "section": "2.1.5",
    "description": "Feature group catalog at end of Section 2.1",
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

fg_struct = {
    "groups": group_map,
    "meta": meta,
}

# Create JSON (latest + snapshot)
SEC2_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

fg_json_path = SEC2_ARTIFACTS_DIR / "feature_groups.json"

# timestamped snapshot (UTC)
ts_snap = pd.Timestamp.utcnow().strftime("%Y-%m-%d__%H%M%S") + "Z"
fg_json_snapshot = SEC2_ARTIFACTS_DIR / f"feature_groups__{ts_snap}.json"

# write latest atomically
fg_tmp_json = fg_json_path.with_suffix(".tmp.json")
with open(fg_tmp_json, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_json, fg_json_path)

# write snapshot (also atomically)
fg_tmp_snap = fg_json_snapshot.with_suffix(".tmp.json")
with open(fg_tmp_snap, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_snap, fg_json_snapshot)

print(f"üíæ feature groups JSON written ‚Üí {fg_json_path}")
print(f"üìå snapshot JSON written ‚Üí {fg_json_snapshot}")

# -- YAML (optional)
fg_yaml_path = SEC2_ARTIFACTS_DIR / "feature_groups.yaml"
if yaml is not None:
    fg_tmp_yaml = fg_yaml_path.with_suffix(".tmp.yaml")
    with open(fg_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(fg_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fg_tmp_yaml, fg_yaml_path)
    print(f"üíæ feature groups YAML written ‚Üí {fg_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature groups.")

# -- 2.1.5.7 Summary metrics + unified diagnostics row
group_counts = (
    feature_groups_df["feature_group"]
    .value_counts()
    .sort_index()
    .to_dict()
)

# Determine status (severity ladder)
status_215 = "OK"

# Hard failures (ERROR)
if not id_cols:
    status_215 = "ERROR"
# Medium failures (WARN) if not already ERROR
elif missing_from_protection:
    status_215 = "WARN"
elif raw_target_col is None or encoded_target_col is None:
    status_215 = "WARN"
elif n_unassigned > 0:
    status_215 = "WARN"


# Level mapping
level_215 = "info"
if status_215 == "WARN":
    level_215 = "warn"
elif status_215 == "ERROR":
    level_215 = "error"

notes_215 = "; ".join(issues_215) if issues_215 else None

# use scalar values ([{}])
summary_215 = pd.DataFrame([{
    "section":             "2.1.5",
    "section_name":        "Feature group registration",
    "check":               "Feature group catalog (column-level)",
    "level":               level_215,
    "status":              status_215,
    "n_features":          n_features,
    "n_protected":         n_protected,
    "n_unassigned":        n_unassigned,
    "group_counts_json":   json.dumps(group_counts, sort_keys=True),
    "feature_groups_csv":  fg_csv_path.name,
    "feature_groups_json": fg_json_path.name,
    "feature_groups_yaml": fg_yaml_path.name if yaml is not None else None,
    "timestamp": pd.Timestamp.utcnow(),
    "detail": (
        f"Feature groups registered for {n_features} column(s); "
        f"catalog written to {fg_csv_path.name}."),
    "notes":                notes_215,
}])

append_sec2(summary_215, SECTION2_REPORT_PATH)
display(summary_215)

# 2.1.6 üß± Feature Space Scaffolding | Groups ‚Üí Config for Downstream
print("\n2.1.6 üß± Feature Space Scaffolding | Groups ‚Üí Config for Downstream")

# Guards
assert "SECTION2_REPORT_PATH" in globals(), "‚ùå SECTION2_REPORT_PATH missing. Run 2.0.1 first."
assert "SEC2_REPORTS_DIR" in globals(), "‚ùå SEC2_REPORTS_DIR missing. Run 2.0.0/2.0.1 first."
assert "ARTIFACTS_DIR" in globals(), "‚ùå ARTIFACTS_DIR missing. Run 2.0.0 first."
assert "CONFIG" in globals(), "‚ùå CONFIG not found. Run 2.0.0 first."

import json
try:
    import yaml
except Exception:
    yaml = None

# --- 2.1.6.1 Reload thresholds so they are embedded in meta ------------------
fg_cfg = CONFIG.get("FEATURE_GROUPING", {}) or {}
low_card_threshold = int(fg_cfg.get("LOW_CARDINALITY_THRESHOLD", 20))
free_text_min_avg_len = int(fg_cfg.get("FREE_TEXT_MIN_AVG_LEN", 30))

# --- 2.1.6.2 Load the 2.1.5 catalog -----------------------------------------
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups.csv"
assert fg_csv_path.exists(), f"‚ùå {fg_csv_path} not found. Run 2.1.5 first."
feature_groups_df_216 = pd.read_csv(fg_csv_path)

if "feature_group" not in feature_groups_df_216.columns:
    raise KeyError("‚ùå feature_groups_2_1_5.csv missing 'feature_group' column.")

# --- 2.1.6.3 Build group ‚Üí columns mapping + protected list ------------------
group_map = {}
for grp, sub_df in feature_groups_df_216.groupby("feature_group"):
    group_map[str(grp)] = sorted(sub_df["column"].astype("string").tolist())

protected_mask = feature_groups_df_216.get("protected", pd.Series([False] * len(feature_groups_df_216)))
protected_list = sorted(feature_groups_df_216.loc[protected_mask.astype(bool), "column"].astype("string").tolist())

meta = {
    "section": "2.1.6",
    "description": "Feature space scaffolding (groups ‚Üí columns) derived from 2.1.5 catalog.",
    "source_catalog": fg_csv_path.name,
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

feature_space_struct = {
    "groups": group_map,
    "meta": meta,
}

protected_list = sorted(feature_groups_df.loc[feature_groups_df["protected"], "column"].astype("string").tolist())
meta = {
    "section": "2.1.5",
    "description": "Feature group catalog at end of Section 2.1",
    "low_card_threshold": low_card_threshold,
    "free_text_min_avg_len": free_text_min_avg_len,
    "protected_columns": protected_list,
}

fg_struct = {
    "groups": group_map,
    "meta": meta,
}

# JSON
fg_json_path = ARTIFACTS_DIR / "feature_groups.json"
fg_tmp_json = fg_json_path.with_suffix(".tmp.json")
with open(fg_tmp_json, "w", encoding="utf-8") as f:
    json.dump(fg_struct, f, indent=2, ensure_ascii=False)
os.replace(fg_tmp_json, fg_json_path)

print(f"üíæ feature groups JSON written ‚Üí {fg_json_path}")

# YAML (optional)
fg_yaml_path = ARTIFACTS_DIR / "feature_groups_2_1_5.yaml"
if yaml is not None:
    fg_tmp_yaml = fg_yaml_path.with_suffix(".tmp.yaml")
    with open(fg_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(fg_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fg_tmp_yaml, fg_yaml_path)
    print(f"üíæ feature groups YAML written ‚Üí {fg_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature groups.")


# --- 2.1.6.4 Persist JSON / YAML artifacts under ARTIFACTS_DIR --------------
fs_json_path = ARTIFACTS_DIR / "feature_space_2_1_6.json"
fs_tmp_json = fs_json_path.with_suffix(".tmp.json")
with open(fs_tmp_json, "w", encoding="utf-8") as f:
    json.dump(feature_space_struct, f, indent=2, ensure_ascii=False)
os.replace(fs_tmp_json, fs_json_path)

print(f"üíæ 2.1.6 feature space JSON written ‚Üí {fs_json_path}")

fs_yaml_path = ARTIFACTS_DIR / "feature_space_2_1_6.yaml"
if yaml is not None:
    fs_tmp_yaml = fs_yaml_path.with_suffix(".tmp.yaml")
    with open(fs_tmp_yaml, "w", encoding="utf-8") as f:
        yaml.safe_dump(feature_space_struct, f, sort_keys=False, allow_unicode=True)
    os.replace(fs_tmp_yaml, fs_yaml_path)
    print(f"üíæ 2.1.6 feature space YAML written ‚Üí {fs_yaml_path}")
else:
    print("‚ö†Ô∏è yaml not available; skipping YAML export for feature space scaffolding.")

# --- 2.1.6.5 Diagnostics summary + SECTION2_REPORT_PATH append ---------------
group_counts_216 = {
    grp: len(cols) for grp, cols in group_map.items()
}

n_features_216 = int(len(feature_groups_df_216))
n_protected_216 = int(len(protected_list))
n_groups_216 = int(len(group_map))

summary_216 = pd.DataFrame([{
    "section":            "2.1.6",
    "section_name":       "Feature space scaffolding",
    "check":              "Feature space config (groups ‚Üí columns)",
    "level":              "info",
    "status":             "OK",
    "n_features":         int(n_features_216),
    "n_protected":        int(n_protected_216),
    "n_groups":           int(n_groups_216),
    "group_counts_json":  json.dumps(group_counts_216, sort_keys=True),
    "feature_space_json": getattr(fs_json_path, "name", str(fs_json_path)),
    "feature_space_yaml": getattr(fs_yaml_path, "name", str(fs_yaml_path))
                            if (yaml is not None and fs_yaml_path is not None)
                            else None,
    "source_catalog_csv": getattr(fg_csv_path, "name", str(fg_csv_path)),
    "timestamp":          pd.Timestamp.utcnow(),
    "detail":          (f"Feature space scaffolding built from "
                        f"{getattr(fg_csv_path, 'name', str(fg_csv_path))}; "
                        f"JSON: {getattr(fs_json_path, 'name', str(fs_json_path))}, "
                        f"YAML: {getattr(fs_yaml_path, 'name', str(fs_yaml_path)) if yaml is not None else 'skipped'}."),
}])

append_sec2(summary_216, SECTION2_REPORT_PATH)

display(summary_216)
```

># SOLUTION:
Yeah ‚Äî **your 2.1.6 is redundant (and a bit inconsistent)** *as written*, because it re-creates and re-writes artifacts you already produced in **2.1.5**, and it mixes paths + metadata in a way that will confuse future-you.

Here‚Äôs what‚Äôs redundant / risky, and the clean fix.

## What‚Äôs redundant in your 2.1.6 block

### 1) You already wrote `feature_groups.json/yaml` in 2.1.5

In 2.1.5 you write:

* `SEC2_ARTIFACTS_DIR / feature_groups.json`
* `SEC2_ARTIFACTS_DIR / feature_groups.yaml`
* plus a timestamped snapshot JSON

Then in 2.1.6 you write again:

* `ARTIFACTS_DIR / feature_groups.json`
* `ARTIFACTS_DIR / feature_groups_2_1_5.yaml`

That‚Äôs **duplicate outputs in a different folder** with slightly different naming. That‚Äôs a recipe for ‚Äúwhich one is the truth?‚Äù later.

‚úÖ Recommendation: **2.1.6 should NOT rewrite feature_groups.* at all.** It should only create the *next-level* scaffold artifact(s).

---

### 2) You rebuild `group_map`, `protected_list`, `meta` twice

You build:

* `feature_space_struct` (good)
  ‚Ä¶and then you immediately rebuild:
* `protected_list`, `meta`, and `fg_struct` again (not needed)

That whole mid-section is basically copy/pasted from 2.1.5 and should be removed from 2.1.6.

---

### 3) You load the wrong CSV filename

In 2.1.5 you saved:

```python
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups_2_1_5.csv"
```

But 2.1.6 tries to read:

```python
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups.csv"
```

That will break unless you coincidentally also have `feature_groups.csv`.

‚úÖ Fix: read `"feature_groups_2_1_5.csv"` (or better: store the name in a known place).

---

### 4) Your 2.1.6 meta section names are wrong

You set:

```python
meta = {"section": "2.1.6", ...}
feature_space_struct = {"groups": ..., "meta": meta}

protected_list = ...
meta = {"section": "2.1.5", ...}
fg_struct = ...
```

So 2.1.6 is mixing section metadata and creating 2.1.5 artifacts again. That‚Äôs conceptually messy.

---

## Cleanest structure

### What 2.1.5 should own

* The **catalog**: `feature_groups_2_1_5.csv`
* The **grouping config**: `SEC2_ARTIFACTS_DIR/feature_groups.json` (+ snapshot + yaml)

### What 2.1.6 should own (only)

* A **feature space scaffold** derived from 2.1.5, e.g.:

  * `feature_space_2_1_6.json`
  * optional yaml
* A summary row appended to `SECTION2_REPORT_PATH`

That‚Äôs it.

---

## Minimal ‚Äúnon-redundant‚Äù 2.1.6 patch (keep your style, no functions)

**Delete** everything in 2.1.6 from:

```python
protected_list = sorted(feature_groups_df.loc[feature_groups_df["protected"], ...
...
print(f"üíæ feature groups JSON written ‚Üí {fg_json_path}")
...
fg_yaml_path = ...
...
```

All of that is re-exporting `feature_groups.*` and should not exist in 2.1.6.

Then ensure 2.1.6 uses the correct input CSV:

```python
fg_csv_path = SEC2_REPORTS_DIR / "feature_groups_2_1_5.csv"
```

And only writes:

```python
fs_json_path = ARTIFACTS_DIR / "feature_space_2_1_6.json"
fs_yaml_path = ARTIFACTS_DIR / "feature_space_2_1_6.yaml"
```

(Or if you want it in section2 artifacts consistently: use `SEC2_ARTIFACTS_DIR` instead of `ARTIFACTS_DIR`.)

---

## One strong suggestion (consistency)

Pick **one** artifacts root for Section 2:

* either `SEC2_ARTIFACTS_DIR` for *all* Section 2 artifacts
  **or**
* `ARTIFACTS_DIR / section2/` (but then don‚Äôt also write to `SEC2_ARTIFACTS_DIR`)

Right now you‚Äôre doing both.

üí°üí° My vote: **keep everything Section 2 in `SEC2_ARTIFACTS_DIR`**. It makes ‚Äúchapter-local truth‚Äù super clear.

---

If you want, I can paste a cleaned 2.1.6 cell that only does: **load 2.1.5 CSV ‚Üí build scaffold ‚Üí write feature_space_2_1_6.* ‚Üí append summary_216**, using your exact conventions (atomic writes, `append_sec2`, no functions).
