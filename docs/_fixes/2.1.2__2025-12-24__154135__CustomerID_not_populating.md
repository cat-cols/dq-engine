
># candidate IDs not populating


```python
# 2.1.2 ğŸªª ID & Key Field Verification | def(reporting) no C()
print("\n2.1.2 ğŸªª ID & Key Field Verification")

# Guards
assert "df" in globals(), "âŒ df not found. Run Section 2.0.0 first."
assert "CONFIG" in globals(), "âŒ CONFIG not found. Run 2.0.0 first."
assert "SECTION2_REPORT_PATH" in globals(), "âŒ SECTION2_REPORT_PATH missing. Run 2.0.1 first."
assert "SEC2_REPORTS_DIR" in globals(), "âŒ SEC2_REPORTS_DIR missing. Run 2.0.0/2.0.1 first."

# Resolve ID columns from CONFIG with sensible fallback
id_cols_cfg = CONFIG.get("ID_COLUMNS", []) or ["customerID"]
if isinstance(id_cols_cfg, (str, bytes)):
    id_cols = [id_cols_cfg]
else:
    id_cols = list(id_cols_cfg)

# Build ID integrity table
id_rows = []
for col in id_cols:
    exists = col in df.columns
    if exists:
        s = df[col]
        non_null = int(s.notna().sum())
        n_nulls = int(s.isna().sum())
        n_dupes = int(df.duplicated(subset=[col]).sum())
        n_unique = int(s.nunique(dropna=True))
        unique_ok = bool(n_unique == non_null)
    else:
        non_null = 0
        n_nulls = np.nan if "np" in globals() else None
        n_dupes = np.nan if "np" in globals() else None
        unique_ok = False

    id_rows.append(
        {
            "id_column":   col,
            "exists":      bool(exists),
            "non_null":    non_null,
            "nulls":       n_nulls,
            "duplicates":  n_dupes,
            "unique_ok":   bool(unique_ok),
        }
    )

id_integrity_df = pd.DataFrame(id_rows)

# Write id_integrity_report.csv atomically
id_integrity_path = SEC2_REPORTS_DIR / "id_integrity_report.csv"
tmp_id_path = id_integrity_path.with_suffix(".tmp.csv")

id_integrity_df.to_csv(tmp_id_path, index=False)
os.replace(tmp_id_path, id_integrity_path)

print(f"âœ… 2.1.2 ID integrity report written â†’ {id_integrity_path}")
display(id_integrity_df)

# Build unified diagnostics chunk for 2.1.2
n_ids = len(id_cols)
n_non_unique = int((~id_integrity_df["unique_ok"]).sum()) if not id_integrity_df.empty else 0
status = "OK" if n_non_unique == 0 else "WARN"

sec2_chunk_212 = pd.DataFrame([{
        "section":        "2.1.2",
        "section_name":   "ID & key field verification",
        "check":          "ID & key field verification",
        "level":          "info",
        "status":         status,
        "n_ids":          n_ids,
        "n_non_unique":   n_non_unique,
        "timestamp":      pd.Timestamp.now(),
        "detail":         f"ID integrity report written to {id_integrity_path.name}; "
            f"{n_non_unique} non-unique or invalid ID columns.",
}])

display(sec2_chunk_212)
append_sec2(sec2_chunk_212, SECTION2_REPORT_PATH)
```

># SOLUTION:
Youâ€™re *very* close â€” 2.1.2 is computing the integrity report correctly, but it has **one key misbehavior**:

### The bug: you keep â€œcandidateâ€ IDs even when they donâ€™t exist

You set:

```python
id_cols_cfg = CONFIG.get("ID_COLUMNS", []) or ["customerID"]
...
id_cols = list(id_cols_cfg)
```

So `id_cols` becomes **whatever is in config** (or `["customerID"]`). But you **never filter** that list down to *existing* columns (and ideally *valid* IDs).

That means downstream steps can end up with:

* `id_cols` containing columns that **donâ€™t exist**
* or `id_cols` containing columns that **exist but are not unique**
* or (depending on earlier cells) `id_cols` can get overwritten/reset elsewhere and become `set()` again

Your 2.1.5 run showed `id_cols` was empty at runtime, which tells me something else is reassigning it later â€” but even in *this* cell, you should make `id_cols` end up as a **clean, canonical set** so later steps can trust it.

---

## âœ… Fix: produce a canonical `id_cols` (existing + unique_ok), and persist it

Drop this in **after** you build `id_integrity_df` (right after `display(id_integrity_df)` is fine):

```python
# Canonicalize id_cols for downstream: keep only existing + unique_ok IDs
existing_ids = set(id_integrity_df.loc[id_integrity_df["exists"], "id_column"].astype("string"))
unique_ids = set(id_integrity_df.loc[id_integrity_df["unique_ok"], "id_column"].astype("string"))

# Prefer unique IDs; if none, fall back to existing (still better than phantom cols)
id_cols = unique_ids if len(unique_ids) > 0 else existing_ids

print("ğŸªª Canonical id_cols â†’", sorted(id_cols))
```

Now `id_cols` will be a **set**, and it will never include phantom columns.

---

## ğŸ’¡ğŸ’¡ Make status + level reflect reality

Right now you do:

```python
n_non_unique = int((~id_integrity_df["unique_ok"]).sum())
status = "OK" if n_non_unique == 0 else "WARN"
```

But if the ID column doesnâ€™t exist, `unique_ok=False`, so it becomes WARN (good) â€” however you still report `level="info"` always.

Replace that part with:

```python
n_missing = int((~id_integrity_df["exists"]).sum())
n_non_unique = int((id_integrity_df["exists"] & (~id_integrity_df["unique_ok"])).sum())

status = "OK"
if n_missing > 0:
    status = "ERROR"   # missing ID columns is severe
elif n_non_unique > 0:
    status = "WARN"

level = "info"
if status == "WARN":
    level = "warn"
elif status == "ERROR":
    level = "error"
```

Then use `level` in your chunk.

---

## Why your 2.1.5 run showed `id_cols` empty

With the 2.1.2 cell as written, `id_cols` should at least be `["customerID"]` (unless your config sets `ID_COLUMNS` to empty string / None weirdly, or another cell later does `id_cols=set()`).

Two common ways `id_cols` becomes empty later:

1. another step redefines `id_cols = set()` (maybe a â€œprotect columnsâ€ builder)
2. youâ€™re running cells out of order and something resets state

Once you add the â€œcanonicalize id_colsâ€ print, youâ€™ll know immediately if 2.1.2 is producing the right value.

---

## Tiny extra: write the canonical list to an artifact (so you can always reload)

Add:

```python
id_cols_path = SEC2_REPORTS_DIR / "id_cols.txt"
with open(id_cols_path, "w", encoding="utf-8") as f:
    for c in sorted(id_cols):
        f.write(f"{c}\n")
print(f"ğŸ’¾ id_cols written â†’ {id_cols_path}")
```

Now even if a notebook state gets weird, you can recover.

---

If you run 2.1.2 with the canonicalization print and paste the output (just the table + the `ğŸªª Canonical id_cols` line), I can tell you immediately whether the issue is â€œ2.1.2 logicâ€ or â€œlater cell overwriting `id_cols`.â€



---
---

># SOLUTION:

Yes â€” and your new output proves *why* it matters: `id_cols` is empty, and **customerID got misclassified** as `categorical_high_card` + `protected=False`. Thatâ€™s exactly the kind of thing this robustness is meant to catch.

### What should change: â€œWARN vs ERRORâ€ + better notes

Right now you mark **WARN** when `id_cols` is empty. In practice, for your pipeline, thatâ€™s often closer to an **ERROR** because:

* IDs not protected â†’ downstream models can leak / memorize
* many checks (uniqueness, joins, duplicates) depend on IDs
* â€œfeature groupingâ€ is used to drive later DQ logic; misclassifying IDs breaks routing

So: treat missing IDs as **ERROR** (or at least â€œWARN-highâ€).

---

## Make status/level more robust (drop-in)

Replace your â€œDetermine statusâ€ block with this logic (inline, no functions):

```python
# Determine status (severity ladder)
status_215 = "OK"

# Hard failures (ERROR)
if not id_cols:
    status_215 = "ERROR"

# Medium failures (WARN) if not already ERROR
if status_215 != "ERROR":
    if raw_target_col is None or encoded_target_col is None:
        status_215 = "WARN"
    elif n_unassigned > 0:
        status_215 = "WARN"

# Level mapping
level_215 = "info"
if status_215 == "WARN":
    level_215 = "warn"
elif status_215 == "ERROR":
    level_215 = "error"
```

And keep `notes_215 = "; ".join(issues_215) if issues_215 else None`.

### Why this is better

* `id_cols` empty becomes **ERROR** (correctly reflects risk)
* target missing becomes **WARN** (important but not always fatal)
* â€œother/unassignedâ€ becomes **WARN** (cleanup signal)

---

## Make notes more actionable (add immediate evidence)

Right now notes say â€œid_cols is emptyâ€¦â€. Add â€œwhat did we *think* the ID was?â€ so you can debug fast.

Add this right after you build `feature_groups_df`:

```python
# Evidence helpers for notes/debug
_detected_id_rows = feature_groups_df.loc[feature_groups_df["feature_group"] == "id", "column"].tolist()
```

Then, when you append the id issue:

```python
if not id_cols:
    issues_215.append(
        "id_cols is empty (2.1.2 may not have detected IDs). "
        f"Columns currently grouped as id: {_detected_id_rows}"
    )
```

In your current run, `_detected_id_rows` will be `[]`, which is itself useful.

---

## The real fix you need next (why id_cols is empty)

Your warning is correct: **2.1.2 isnâ€™t populating `id_cols`** (or you didnâ€™t run it, or it got reset).

In this dataset, `customerID` is a textbook ID. Two options:

1. Fix 2.1.2 so it always detects `customerID` (best)
2. Add a fallback here (only if you want 2.1.5 to be resilient)

ğŸ’¡ğŸ’¡ If you want fallback inside 2.1.5 (still inline), add right after the `assert "id_cols" in globals()`:

```python
# Fallback: if id_cols is empty, attempt a minimal ID guess
if isinstance(id_cols, (set, list, tuple)) and len(id_cols) == 0:
    if "customerID" in df.columns:
        id_cols = {"customerID"}
        print("âš ï¸ id_cols was empty; fallback set id_cols={'customerID'}")
```

That keeps your pipeline moving, but it will also hide the fact 2.1.2 is brokenâ€”so only do this if you *also* keep the ERROR/WARN signaling.

---

### What Iâ€™d do given your goals

* Make `id_cols` empty = **ERROR**
* Do **not** silently fallback unless you really want to keep running
* Then go fix 2.1.2 so `customerID` becomes protected and grouped as `id`

If you paste your 2.1.2 cell, Iâ€™ll tell you exactly why itâ€™s producing an empty set and give you an inline fix.

This is a **big improvement** â€” but youâ€™ve got one subtle ordering issue and one â€œmissing signalâ€ that will bite you later.

## 1) Youâ€™re writing `id_cols.txt` too early

Right now you write:

```python
# write id_cols.txt
for c in sorted(id_cols): ...
```

**before** you canonicalize `id_cols`. So `id_cols.txt` records the *config candidates*, not the *validated* IDs.

âœ… Fix: move the `id_cols.txt` write block **after** canonicalization, and ideally write **both** â€œcandidatesâ€ and â€œcanonicalâ€ so debugging is easy.

Drop-in change:

```python
# --- after canonicalization ---
id_cols_path = SEC2_REPORTS_DIR / "id_cols.txt"
with open(id_cols_path, "w", encoding="utf-8") as f:
    for c in sorted(id_cols):
        f.write(f"{c}\n")
print(f"ğŸ’¾ canonical id_cols written â†’ {id_cols_path}")
```

If you want both:

```python
id_cols_candidates_path = SEC2_REPORTS_DIR / "id_cols__candidates.txt"
with open(id_cols_candidates_path, "w", encoding="utf-8") as f:
    for c in sorted(id_cols_cfg if isinstance(id_cols_cfg, (list, tuple, set)) else [id_cols_cfg]):
        f.write(f"{c}\n")
print(f"ğŸ’¾ id_cols candidates written â†’ {id_cols_candidates_path}")
```

(That second one is optional.)

---

## 2) Your diagnostics counts are now slightly inconsistent

You compute:

```python
n_ids = len(id_cols)
n_non_unique = int((exists & ~unique_ok).sum())
```

But `n_ids` is based on **canonical** IDs, while `n_non_unique` is based on **all candidate rows** in `id_integrity_df`.

Thatâ€™s okay, but your `detail` string says â€œnon-unique or invalid ID columnsâ€ while the metric is only â€œnon-unique among existingâ€. Also, if your candidate ID doesnâ€™t exist, it triggers `ERROR` (good), but you arenâ€™t reporting how many are missing in the row.

âœ… Fix: add `n_missing` into the output row, and make the detail string reflect both.

Change your `sec2_chunk_212` to:

```python
sec2_chunk_212 = pd.DataFrame([{
    "section":        "2.1.2",
    "section_name":   "ID & key field verification",
    "check":          "ID & key field verification",
    "level":          level_212,
    "status":         status_212,
    "n_id_candidates": int(len(id_integrity_df)),
    "n_ids_canonical": n_ids,
    "n_missing":       n_missing,
    "n_non_unique":    n_non_unique,
    "timestamp":       pd.Timestamp.utcnow(),
    "detail": (
        f"ID integrity report written to {id_integrity_path.name}; "
        f"{n_missing} missing; {n_non_unique} non-unique among existing."
    ),
}])
```

Also note I switched to `pd.Timestamp.utcnow()` so it matches your other UTC timestamps.

---

## 3) Why `customerID` still became non-ID in 2.1.5

If you run **this** 2.1.2 cell and it prints:

```
ğŸªª Canonical id_cols â†’ ['customerID']
```

â€¦but 2.1.5 still shows `id_cols is empty` and classifies `customerID` as high-card categorical, then **something later is overwriting `id_cols`** (or youâ€™re running 2.1.5 in a fresh kernel / different notebook).

### Fastest way to catch the overwrite

At the very top of 2.1.5 (right after guards), add:

```python
print("DEBUG id_cols type/value:", type(id_cols), id_cols)
```

If you ever see `set()` or `[]` there, you know 2.1.2 didnâ€™t â€œstickâ€ in that runtime.

ğŸ’¡ğŸ’¡ If you want 2.1.5 to be resilient even when state is weird, add this fallback right after that debug print:

```python
if not id_cols:
    _id_txt = SEC2_REPORTS_DIR / "id_cols.txt"
    if _id_txt.exists() and _id_txt.stat().st_size > 0:
        with open(_id_txt, "r", encoding="utf-8") as f:
            id_cols = {ln.strip() for ln in f if ln.strip()}
        print("âš ï¸ id_cols rehydrated from id_cols.txt â†’", sorted(id_cols))
```

That keeps 2.1.5 correct even if a notebook cell resets globals.

---

## Net: what you should do next

1. Move `id_cols.txt` write **after** canonicalization.
2. Add `n_missing` and split candidate vs canonical counts in the diagnostics row.
3. Add the `DEBUG id_cols` print in 2.1.5 to confirm whether overwrite/state is the real issue.

If you paste the output of `ğŸªª Canonical id_cols â†’ ...` from 2.1.2 plus the `DEBUG id_cols ...` line from 2.1.5, I can tell you exactly which scenario youâ€™re in (state reset vs overwrite vs wrong notebook order).
