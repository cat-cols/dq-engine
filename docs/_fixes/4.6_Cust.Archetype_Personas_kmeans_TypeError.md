># Q:
```py
print("\n6Ô∏è‚É£ Customer Archetype Personas ‚Äì k-Means clustering")

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Use few numeric features to define personas
feat_cols = ["tenure", "MonthlyCharges", "TotalCharges"]
feat_cols = [c for c in feat_cols if c in df_clean.columns]

persona_df = df_clean[feat_cols].dropna().copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(persona_df)

k = 5
kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto")
persona_labels = kmeans.fit_predict(X_scaled)

persona_df["cluster"] = persona_labels

# Attach back to original
df_personas = df_clean.copy()
df_personas = df_personas.loc[persona_df.index].copy()
df_personas["persona_cluster"] = persona_labels

# Quick cluster summary
summary = (
    df_personas
    .groupby("persona_cluster")[feat_cols + (["Churn"] if "Churn" in df_personas.columns else [])]
    .agg(["mean", "median"])
)

print("\nCluster summary:")
display(summary)

# 2D scatter: tenure vs MonthlyCharges
fig, ax = plt.subplots(figsize=(8, 6))
scatter = ax.scatter(
    df_personas["tenure"],
    df_personas["MonthlyCharges"],
    c=df_personas["persona_cluster"],
    cmap="tab10",
    alpha=0.6,
    s=15,
)

ax.set_title("Customer Archetype Personas\nClusters in Tenure vs MonthlyCharges space")
ax.set_xlabel("Tenure")
ax.set_ylabel("MonthlyCharges")

cbar = fig.colorbar(scatter, ax=ax)
cbar.set_label("Persona cluster")

plt.tight_layout()

path = FIG2 / "06_customer_persona_clusters.png"
fig.savefig(path, dpi=150)
plt.show()
plt.close(fig)

print(f"üíæ Saved ‚Üí {path}")

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1944, in GroupBy._agg_py_fallback(self, how, values, ndim, alt)
   1943 try:
-> 1944     res_values = self._grouper.agg_series(ser, alt, preserve_dtype=True)
   1945 except Exception as err:

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py:873, in BaseGrouper.agg_series(self, obj, func, preserve_dtype)
    871     preserve_dtype = True
--> 873 result = self._aggregate_series_pure_python(obj, func)
    875 npvalues = lib.maybe_convert_objects(result, try_float=False)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py:894, in BaseGrouper._aggregate_series_pure_python(self, obj, func)
    893 for i, group in enumerate(splitter):
--> 894     res = func(group)
    895     res = extract_result(res)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2461, in GroupBy.mean.<locals>.<lambda>(x)
   2458 else:
   2459     result = self._cython_agg_general(
   2460         "mean",
-> 2461         alt=lambda x: Series(x, copy=False).mean(numeric_only=numeric_only),
   2462         numeric_only=numeric_only,
   2463     )
   2464     return result.__finalize__(self.obj, method="groupby")

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/series.py:6570, in Series.mean(self, axis, skipna, numeric_only, **kwargs)
   6562 @doc(make_doc("mean", ndim=1))
   6563 def mean(
   6564     self,
   (...)   6568     **kwargs,
   6569 ):
-> 6570     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/generic.py:12485, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)
  12478 def mean(
  12479     self,
  12480     axis: Axis | None = 0,
   (...)  12483     **kwargs,
  12484 ) -> Series | float:
> 12485     return self._stat_function(
  12486         "mean", nanops.nanmean, axis, skipna, numeric_only, **kwargs
  12487     )

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/generic.py:12442, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)
  12440 validate_bool_kwarg(skipna, "skipna", none_allowed=False)
> 12442 return self._reduce(
  12443     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only
  12444 )

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/series.py:6478, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   6474     raise TypeError(
   6475         f"Series.{name} does not allow {kwd_name}={numeric_only} "
   6476         "with non-numeric dtypes."
   6477     )
-> 6478 return op(delegate, skipna=skipna, **kwds)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)
    146 else:
--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)
    149 return result

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)
    402     mask = isna(values)
--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)
    406 if datetimelike:

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:720, in nanmean(values, axis, skipna, mask)
    719 the_sum = values.sum(axis, dtype=dtype_sum)
--> 720 the_sum = _ensure_numeric(the_sum)
    722 if axis is not None and getattr(the_sum, "ndim", False):

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:1701, in _ensure_numeric(x)
   1699 if isinstance(x, str):
   1700     # GH#44008, GH#36703 avoid casting e.g. strings to numeric
-> 1701     raise TypeError(f"Could not convert string '{x}' to numeric")
   1702 try:

TypeError: Could not convert string 'NoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNo' to numeric

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Cell In[21], line 30
     24 df_personas["persona_cluster"] = persona_labels
     26 # Quick cluster summary
     27 summary = (
     28     df_personas
     29     .groupby("persona_cluster")[feat_cols + (["Churn"] if "Churn" in df_personas.columns else [])]
---> 30     .agg(["mean", "median"])
     31 )
     33 print("\nCluster summary:")
     34 display(summary)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1432, in DataFrameGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
   1429     kwargs["engine_kwargs"] = engine_kwargs
   1431 op = GroupByApply(self, func, args=args, kwargs=kwargs)
-> 1432 result = op.agg()
   1433 if not is_dict_like(func) and result is not None:
   1434     # GH #52849
   1435     if not self.as_index and is_list_like(func):

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/apply.py:193, in Apply.agg(self)
    190     return self.agg_dict_like()
    191 elif is_list_like(func):
    192     # we require a list, but not a 'str'
--> 193     return self.agg_list_like()
    195 if callable(func):
    196     f = com.get_cython_func(func)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/apply.py:326, in Apply.agg_list_like(self)
    318 def agg_list_like(self) -> DataFrame | Series:
    319     """
    320     Compute aggregation in the case of a list-like argument.
    321
   (...)    324     Result of aggregation.
    325     """
--> 326     return self.agg_or_apply_list_like(op_name="agg")

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1566, in GroupByApply.agg_or_apply_list_like(self, op_name)
   1561 # Only set as_index=True on groupby objects, not Window or Resample
   1562 # that inherit from this class.
   1563 with com.temp_setattr(
   1564     obj, "as_index", True, condition=hasattr(obj, "as_index")
   1565 ):
-> 1566     keys, results = self.compute_list_like(op_name, selected_obj, kwargs)
   1567 result = self.wrap_results_list_like(keys, results)
   1568 return result

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/apply.py:385, in Apply.compute_list_like(self, op_name, selected_obj, kwargs)
    379 colg = obj._gotitem(col, ndim=1, subset=selected_obj.iloc[:, index])
    380 args = (
    381     [self.axis, *self.args]
    382     if include_axis(op_name, colg)
    383     else self.args
    384 )
--> 385 new_res = getattr(colg, op_name)(func, *args, **kwargs)
    386 results.append(new_res)
    387 indices.append(index)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py:257, in SeriesGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    255 kwargs["engine"] = engine
    256 kwargs["engine_kwargs"] = engine_kwargs
--> 257 ret = self._aggregate_multiple_funcs(func, *args, **kwargs)
    258 if relabeling:
    259     # columns is not narrowed by mypy from relabeling flag
    260     assert columns is not None  # for mypy

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py:362, in SeriesGroupBy._aggregate_multiple_funcs(self, arg, *args, **kwargs)
    360     for idx, (name, func) in enumerate(arg):
    361         key = base.OutputKey(label=name, position=idx)
--> 362         results[key] = self.aggregate(func, *args, **kwargs)
    364 if any(isinstance(x, DataFrame) for x in results.values()):
    365     from pandas import concat

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py:249, in SeriesGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    247     if engine_kwargs is not None:
    248         kwargs["engine_kwargs"] = engine_kwargs
--> 249     return getattr(self, func)(*args, **kwargs)
    251 elif isinstance(func, abc.Iterable):
    252     # Catch instances of lists / tuples
    253     # but not the class list / tuple itself.
    254     func = maybe_mangle_lambdas(func)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2459, in GroupBy.mean(self, numeric_only, engine, engine_kwargs)
   2452     return self._numba_agg_general(
   2453         grouped_mean,
   2454         executor.float_dtype_mapping,
   2455         engine_kwargs,
   2456         min_periods=0,
   2457     )
   2458 else:
-> 2459     result = self._cython_agg_general(
   2460         "mean",
   2461         alt=lambda x: Series(x, copy=False).mean(numeric_only=numeric_only),
   2462         numeric_only=numeric_only,
   2463     )
   2464     return result.__finalize__(self.obj, method="groupby")

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2005, in GroupBy._cython_agg_general(self, how, alt, numeric_only, min_count, **kwargs)
   2002     result = self._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)
   2003     return result
-> 2005 new_mgr = data.grouped_reduce(array_func)
   2006 res = self._wrap_agged_manager(new_mgr)
   2007 if how in ["idxmin", "idxmax"]:

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/internals/base.py:367, in SingleDataManager.grouped_reduce(self, func)
    365 def grouped_reduce(self, func):
    366     arr = self.array
--> 367     res = func(arr)
    368     index = default_index(len(res))
    370     mgr = type(self).from_array(res, index)

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2002, in GroupBy._cython_agg_general.<locals>.array_func(values)
   1999     return result
   2001 assert alt is not None
-> 2002 result = self._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)
   2003 return result

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1948, in GroupBy._agg_py_fallback(self, how, values, ndim, alt)
   1946     msg = f"agg function failed [how->{how},dtype->{ser.dtype}]"
   1947     # preserve the kind of exception that raised
-> 1948     raise type(err)(msg) from err
   1950 dtype = ser.dtype
   1951 if dtype == object:

TypeError: agg function failed [how->mean,dtype->object]
```

---

># SOLUTION

> :)


Nice, this is exactly the kind of ‚Äúwire everything up cleanly‚Äù moment.

That assertion is just telling you:

> ‚ÄúAt least one of: `price_anomaly_bucket`, `rel_residual_pct`, `Churn_flag` does **not** exist on `df_clean` in this kernel.‚Äù

Let‚Äôs fix **all three prerequisites in one go** and then rerun your churn-by-bucket view.

---

## 1Ô∏è‚É£ One cell to fix / create all needed columns

Drop this **above** your `üìä Churn Rate by Pricing-Fairness Bucket` cell and run it:

```python
print("\nüõ† Preparing pricing-fairness features on df_clean")

import numpy as np
import pandas as pd

assert "df_clean" in globals(), "‚ùå df_clean not found."

# -------------------------------------------------------------------
# 0. Quick diagnostics
# -------------------------------------------------------------------
needed_core = ["MonthlyCharges", "tenure"]
missing_core = [c for c in needed_core if c not in df_clean.columns]
assert not missing_core, f"‚ùå df_clean is missing core columns: {missing_core}"

print("‚úÖ df_clean columns available for pricing:",
      [c for c in ["MonthlyCharges", "tenure", "Contract", "Churn", "Churn_flag",
                   "expected_monthly_charge", "price_residual", "rel_residual_pct",
                   "price_anomaly_bucket"] if c in df_clean.columns])

df_clean = df_clean.copy()

# -------------------------------------------------------------------
# 1. Ensure Churn_flag exists (0/1)
# -------------------------------------------------------------------
if "Churn_flag" not in df_clean.columns:
    if "Churn" in df_clean.columns:
        print("‚û°Ô∏è Building Churn_flag from Churn (Yes/No ‚Üí 1/0)")
        df_clean["Churn_flag"] = (
            df_clean["Churn"].astype(str).str.strip().str.lower().eq("yes")
        ).astype("Int8")
    else:
        raise AssertionError("‚ùå Neither Churn_flag nor Churn present; can‚Äôt compute churn metrics.")

# -------------------------------------------------------------------
# 2. Expected price + residuals (if missing)
# -------------------------------------------------------------------
need_expected_cols = (
    "expected_monthly_charge" not in df_clean.columns
    or "price_residual" not in df_clean.columns
    or "rel_residual_pct" not in df_clean.columns
)

if need_expected_cols:
    print("‚û°Ô∏è Computing expected_monthly_charge, price_residual, rel_residual_pct")

    use_contract = "Contract" in df_clean.columns
    work_cols = ["tenure", "MonthlyCharges"] + (["Contract"] if use_contract else [])
    work = df_clean[work_cols].dropna().copy()

    if use_contract:
        group_keys = ["Contract", "tenure"]
        print("   ‚Ä¢ Using (Contract, tenure) to define fair price.")
    else:
        group_keys = ["tenure"]
        print("   ‚Ä¢ Using tenure only to define fair price.")

    fair_table = (
        work
        .groupby(group_keys)["MonthlyCharges"]
        .median()
        .rename("expected_monthly_charge")
        .reset_index()
    )

    df_clean = df_clean.merge(
        fair_table,
        on=group_keys,
        how="left",
    )

    df_clean["price_residual"] = (
        df_clean["MonthlyCharges"] - df_clean["expected_monthly_charge"]
    )

    df_clean["rel_residual_pct"] = np.where(
        df_clean["expected_monthly_charge"] > 0,
        df_clean["price_residual"] / df_clean["expected_monthly_charge"],
        np.nan,
    )

else:
    print("‚úÖ Re-using existing expected_monthly_charge / residual columns.")

# -------------------------------------------------------------------
# 3. price_anomaly_bucket from rel_residual_pct (if missing)
# -------------------------------------------------------------------
if "price_anomaly_bucket" not in df_clean.columns:
    assert "rel_residual_pct" in df_clean.columns, "‚ùå rel_residual_pct still missing."

    print("‚û°Ô∏è Deriving price_anomaly_bucket from rel_residual_pct")

    df_clean["price_anomaly_bucket"] = pd.cut(
        df_clean["rel_residual_pct"],
        bins=[-1.0, 0.05, 0.15, 0.30, np.inf],
        labels=["Fair/Under", "Slightly High", "High", "Extreme"],
    )
else:
    print("‚úÖ Re-using existing price_anomaly_bucket")

print("\n‚úÖ Pricing-fairness features ready:")
display(
    df_clean[
        ["customerID", "tenure", "Contract", "MonthlyCharges",
         "expected_monthly_charge", "price_residual", "rel_residual_pct",
         "price_anomaly_bucket", "Churn_flag"]
    ].head(10)
)
```

That cell guarantees:

* `Churn_flag` exists (built from `Churn` if needed)
* `expected_monthly_charge`, `price_residual`, `rel_residual_pct` are created if missing
* `price_anomaly_bucket` is created as:

  * `Fair/Under`
  * `Slightly High`
  * `High`
  * `Extreme`

All wired off your actual `df_clean`.

---

## 2Ô∏è‚É£ Now re-run your bucket insight cell

Your original cell can stay exactly as-is:

```python
print("\nüìä Churn Rate by Pricing-Fairness Bucket")

assert all(col in df_clean.columns for col in [
    "price_anomaly_bucket", "rel_residual_pct", "Churn_flag"
]), "‚ùå Missing required columns."

bucket_summary = (
    df_clean
    .groupby("price_anomaly_bucket")
    .agg(
        n_customers=("customerID", "count"),
        churn_rate=("Churn_flag", "mean"),
        avg_rel_overcharge=("rel_residual_pct", "mean")
    )
    .reset_index()
    .sort_values("churn_rate", ascending=False)
)

bucket_summary["churn_rate_pct"] = (bucket_summary["churn_rate"] * 100).round(2)

display(bucket_summary)

print("\nüì£ Pricing ‚Üí Churn Insight")
for _, row in bucket_summary.iterrows():
    print(
        f"‚Ä¢ {row['price_anomaly_bucket']}: "
        f"Churn={row['churn_rate_pct']}% "
        f"with avg relative overcharge={row['avg_rel_overcharge']:.2f}"
    )
```

Now the assertion should pass, and you‚Äôll get exactly the storytelling you wanted:

> ‚ÄúCustomers in **Extreme** pricing anomalies churn at X%, vs Y% in **Fair/Under**.‚Äù

If you want to turn this into a **Section 2-style diagnostic row** (with `append_sec2` and a CSV saved under `SEC2_REPORTS_DIR`), I can wire that up in the same style as your other 2.x blocks.


