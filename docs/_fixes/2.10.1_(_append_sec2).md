```py
# Telco/Level_3/src/telco_churn/utils/reporting.py

from __future__ import annotations

from pathlib import Path
import os
from typing import Union
from datetime import datetime, timezone
import pandas as pd
from collections.abc import Mapping
from typing import Any
import json

# Columns we often want numerically coerced & rounded in the unified Section 2 report
_NUMERIC_NORMALIZE_COLS = (
    "percent",
    "imbalance_ratio",
    "pct_inconsistent",
    "top_freq",
    "pct_not_allowed",
)

# TODO: add inline version first?
def log_section_completion(section: str, status: str, **metrics) -> None:
    """
    Lightweight console logger for Section 2 cells.
    Example:
      log_section_completion("2.x.x", status_2xx, checked=n_cols, mismatched=n_unassigned)
    """
    parts = [f"‚úÖ [{section}]", f"status={status}"]
    for k, v in metrics.items():
        parts.append(f"{k}={v}")
    print(" | ".join(parts))


# 1
def append_sec2(
    chunk: pd.DataFrame,
    report_path: str | Path,
) -> Path:
    """
    Append a diagnostics chunk into the unified Section 2 report CSV.

    Parameters
    ----------
    chunk:
        1+ row DataFrame with Section 2 diagnostics.
    report_path:
        Path to the unified Section 2 CSV
        (usually SECTION2_REPORT_PATH from the notebook).

    Behaviour
    ---------
    * Ensures parent directory exists
    * If the report exists, merges columns and appends rows
    * Writes through a temporary file and uses os.replace for atomicity
    * Normalizes a few numeric columns if present
    """

    path = Path(report_path)
    tmp_path = path.with_suffix(path.suffix + ".tmp")

    try:
        path.parent.mkdir(parents=True, exist_ok=True)

        if path.exists():
            existing = pd.read_csv(path)
            all_cols = pd.Index(existing.columns).union(chunk.columns)
            out = pd.concat(
                [
                    existing.reindex(columns=all_cols),
                    chunk.reindex(columns=all_cols),
                ],
                ignore_index=True,
            )
        else:
            out = chunk.copy()

        # Optional: normalize some known numeric columns if present
        for col in _NUMERIC_NORMALIZE_COLS:
            if col in out.columns:
                out[col] = pd.to_numeric(out[col], errors="coerce").round(4)

        out.to_csv(tmp_path, index=False)
        os.replace(tmp_path, path)
        print(f"üßæ Appended diagnostics ‚Üí {path}")
    except Exception as e:
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        print(f"‚ö†Ô∏è Could not append diagnostics: {e}")

    return path


# 1
def log_section_completion(
    section: str,
    status: str,
    *,
    run_id: str | None = None,
    level: str = "info",
    log_dir: str | Path | None = None,
    log_name: str = "section2_runlog.jsonl",
    extra: Mapping[str, Any] | None = None,
    **metrics: Any,
) -> None:
    """
    Lightweight Section 2 logger.

    - Always prints a concise console line.
    - Optionally appends a JSONL record to a run log.

    Parameters
    ----------
    section:
        Section identifier, e.g. "2.1.5".
    status:
        Status string, e.g. "OK", "WARN", "FAIL", "SKIP".
    run_id:
        Optional run identifier (e.g. SECTION2_RUN_ID) to tie sections to a single run.
    level:
        Log level string. Mostly for future use ("info", "warn", "error").
    log_dir:
        If provided, JSONL will be appended to `log_dir / log_name`.
        If None, only console output is produced.
    log_name:
        File name for the JSONL run log (default: "section2_runlog.jsonl").
    extra:
        Optional mapping of additional metadata that should be included in the record.
    **metrics:
        Arbitrary key/value metrics, e.g. checked=..., mismatched=...
    """
    ts_utc = (
        datetime.now(timezone.utc)
        .isoformat(timespec="seconds")
        .replace("+00:00", "Z")
    )

    # Build record
    record: dict[str, Any] = {
        "timestamp_utc": ts_utc,
        "section": section,
        "status": status,
        "level": level,
    }
    if run_id is not None:
        record["run_id"] = run_id

    if extra:
        record.update(dict(extra))

    if metrics:
        record["metrics"] = dict(metrics)

    # ---- Console output (human friendly) -----------------------------------
    # Example: ‚úÖ [2.1.5] status=OK | checked=21 | mismatched=0
    parts = [f"‚úÖ [{section}]", f"status={status}"]
    for k, v in (metrics or {}).items():
        parts.append(f"{k}={v}")
    print(" | ".join(parts))

    # ---- Optional JSONL logging --------------------------------------------
    if log_dir is None:
        return

    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    log_path = log_dir / log_name

    try:
        with log_path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    except Exception as e:
        # We never want logging failures to break the notebook
        print(f"‚ö†Ô∏è log_section_completion: failed to write to {log_path}: {e}")

# 2.10.1 | Numeric Univariate Summary
print("2.10.1 ‚ñ∂ Numeric Univariate Summary")

default_univariate_numeric_cfg = {
    "ENABLED": True,
    "SKEW_THRESH_HIGH": 1.0,
    "KURTOSIS_THRESH_HIGH": 3.0,
    "ZERO_INFLATION_THRESH": 0.5,
    "OUTPUT_FILE": "univariate_numeric_summary.csv",
}
univariate_numeric_cfg = _get_cfg_210("UNIVARIATE_NUMERIC", default_univariate_numeric_cfg)

univ_num_enabled_2101 = bool(univariate_numeric_cfg.get("ENABLED", True))
univ_num_output_2101 = str(univariate_numeric_cfg.get("OUTPUT_FILE", "univariate_numeric_summary.csv"))
skew_thresh_2101 = float(univariate_numeric_cfg.get("SKEW_THRESH_HIGH", 1.0))
kurt_thresh_2101 = float(univariate_numeric_cfg.get("KURTOSIS_THRESH_HIGH", 3.0))
zero_thresh_2101 = float(univariate_numeric_cfg.get("ZERO_INFLATION_THRESH", 0.5))

numeric_summary_path_2101 = section2_reports_dir_210 / univ_num_output_2101

num_summary_df_2101 = pd.DataFrame()

if univ_num_enabled_2101:
    # Identify numeric columns (exclude IDs + booleans if schema hints exist)
    from pandas.api.types import is_numeric_dtype, is_bool_dtype

    numeric_cols_2101 = []
    for col in df_clean.columns:
        # err fix: added: and not is_bool_dtype(df_clean[col])
        if is_numeric_dtype(df_clean[col]) and not is_bool_dtype(df_clean[col]):
            numeric_cols_2101.append(col)

    # Optionally exclude ID-like columns if SCHEMA / CONFIG says so
    id_like_cols_2101 = set()
    if "SCHEMA" in globals():
        try:
            id_like_cols_2101.update(SCHEMA.get("ID_COLUMNS", []))
        except Exception:
            pass

    numeric_cols_2101 = [c for c in numeric_cols_2101 if c not in id_like_cols_2101]

    rows_2101 = []
    for col in numeric_cols_2101:
        s = df_clean[col].dropna()
        if s.empty:
            mean = median = std = min_val = max_val = iqr = np.nan
            skew = kurt = zero_frac = np.nan
        else:
            desc = s.describe(percentiles=[0.25, 0.75])
            mean = float(desc.get("mean", np.nan))
            median = float(desc.get("50%", np.nan))
            std = float(desc.get("std", np.nan))
            min_val = float(desc.get("min", np.nan))
            max_val = float(desc.get("max", np.nan))
            q25 = float(desc.get("25%", np.nan))
            q75 = float(desc.get("75%", np.nan))
            iqr = q75 - q25 if (not np.isnan(q75) and not np.isnan(q25)) else np.nan
            skew = float(s.skew()) if s.size > 1 else np.nan
            kurt = float(s.kurtosis()) if s.size > 1 else np.nan
            zero_frac = float((s == 0).mean()) if s.size > 0 else np.nan

        # Labels
        if np.isnan(skew):
            skew_label = "Unknown"
        elif skew >= skew_thresh_2101:
            skew_label = "High positive skew"
        elif skew <= -skew_thresh_2101:
            skew_label = "High negative skew"
        else:
            skew_label = "Approximately symmetric"

        if np.isnan(kurt):
            kurt_label = "Unknown"
        elif kurt >= kurt_thresh_2101:
            kurt_label = "Heavy-tailed"
        elif kurt <= 0:
            kurt_label = "Light-tailed"
        else:
            kurt_label = "Near-normal / moderate tail"

        zero_inflated_flag = (
            False if np.isnan(zero_frac) else (zero_frac >= zero_thresh_2101)
        )

        rows_2101.append(
            {
                "feature": col,
                "mean": mean,
                "median": median,
                "std": std,
                "min": min_val,
                "max": max_val,
                "iqr": iqr,
                "skewness": skew,
                "kurtosis": kurt,
                "zero_fraction": zero_frac,
                "skew_label": skew_label,
                "kurtosis_label": kurt_label,
                "zero_inflated_flag": bool(zero_inflated_flag),
            }
        )

    num_summary_df_2101 = pd.DataFrame(rows_2101)

    # Atomic write
    tmp_2101 = numeric_summary_path_2101.with_suffix(".tmp.csv")
    num_summary_df_2101.to_csv(tmp_2101, index=False)
    os.replace(tmp_2101, numeric_summary_path_2101)

# Diagnostics row for 2.10.1
n_numeric_2101 = int(num_summary_df_2101.shape[0]) if not num_summary_df_2101.empty else 0
n_high_skew_2101 = 0
n_heavy_tail_2101 = 0
if n_numeric_2101 > 0:
    n_high_skew_2101 = int(
        num_summary_df_2101["skew_label"].isin(["High positive skew", "High negative skew"]).sum()
    )
    n_heavy_tail_2101 = int(
        num_summary_df_2101["kurtosis_label"].isin(["Heavy-tailed"]).sum()
    )

if n_numeric_2101 == 0:
    status_2101 = "WARN"
else:
    frac_skew = n_high_skew_2101 / max(1, n_numeric_2101)
    frac_heavy = n_heavy_tail_2101 / max(1, n_numeric_2101)
    frac_problem = max(frac_skew, frac_heavy)
    if frac_problem <= 0.3:
        status_2101 = "OK"
    elif frac_problem <= 0.7:
        status_2101 = "WARN"
    else:
        status_2101 = "FAIL"

sec2_chunk_2101 = pd.DataFrame(
    {
        "section": ["2.10.1"],
        "section_name": ["Numeric univariate summary"],
        "check": ["Compute descriptive statistics and shape diagnostics for numeric features"],
        "level": ["info"],
        "n_numeric_features": [n_numeric_2101],
        "n_high_skew": [n_high_skew_2101],
        "n_heavy_tail": [n_heavy_tail_2101],
        "status": [status_2101],
        "detail": [str(numeric_summary_path_2101)],
    }
)

if "_append_sec2" in globals() and callable(_append_sec2):
    _append_sec2(sec2_chunk_2101)
else:
    print("‚ÑπÔ∏è _append_sec2 not available; 2.10.1 diagnostics not appended to Section 2 report.")

output:

2.10.1 ‚ñ∂ Numeric Univariate Summary
‚ÑπÔ∏è _append_sec2 not available; 2.10.1 diagnostics not appended to Section 2 report.
```

># &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

># FIX:
># Should I create a _append_sec2 function or change it to my reporting.py append_sec2?

Short answer: you don‚Äôt need to rename everything in your cells ‚Äî just **wire up `_append_sec2` as a thin wrapper around your `append_sec2`** once, and all those cells will start writing into the unified report.

---

### Why you‚Äôre seeing this

Your 2.10.1 cell ends with:

```python
if "_append_sec2" in globals() and callable(_append_sec2):
    _append_sec2(sec2_chunk_2101)
else:
    print("‚ÑπÔ∏è _append_sec2 not available; 2.10.1 diagnostics not appended to Section 2 report.")
```

The message:

```text
‚ÑπÔ∏è _append_sec2 not available; 2.10.1 diagnostics not appended to Section 2 report.
```

means:

* In the notebook‚Äôs global namespace there is **no function named `_append_sec2`**, so the `else` branch fires.
* Your `append_sec2` lives in `telco_churn/utils/reporting.py`, but you haven‚Äôt imported it (or wrapped it) as `_append_sec2` in this notebook.

So the cells are written assuming a helper called `_append_sec2(chunk_df)` already exists.

---

### Recommended pattern üí°üí°

Keep your `append_sec2` in `reporting.py` exactly as-is, and in your Section 2 notebook define `_append_sec2` once as a small wrapper that knows the **unified report path**.

Add this **once near the top of your Section 2 notebook** (or in a shared ‚Äú2.0 preamble‚Äù cell):

```python
from pathlib import Path
from telco_churn.utils.reporting import append_sec2 as _append_sec2_impl

# Decide where the unified Section 2 diagnostics CSV should live
if "section2_reports_dir_210" in globals():
    _sec2_report_dir = section2_reports_dir_210
elif "sec2_reports_dir" in globals():
    _sec2_report_dir = sec2_reports_dir
elif "REPORTS_DIR" in globals():
    _sec2_report_dir = (REPORTS_DIR / "section2").resolve()
else:
    _sec2_report_dir = Path("resources/reports/section2").resolve()

_sec2_report_dir.mkdir(parents=True, exist_ok=True)
SECTION2_UNIFIED_REPORT_PATH = _sec2_report_dir / "section2_unified_report.csv"

def _append_sec2(chunk: pd.DataFrame) -> Path:
    """
    Thin wrapper so all 2.x cells can just call `_append_sec2(chunk)`.
    Uses the shared SECTION2_UNIFIED_REPORT_PATH under the hood.
    """
    return _append_sec2_impl(chunk, SECTION2_UNIFIED_REPORT_PATH)
```

Now this line in your 2.10.1 cell:

```python
_append_sec2(sec2_chunk_2101)
```

will:

1. Use your `append_sec2` implementation from `reporting.py`.
2. Append into `SECTION2_UNIFIED_REPORT_PATH` (ex: `resources/reports/section2/section2_unified_report.csv`).
3. Do it atomically and with the numeric normalization you already coded.

No need to edit all your existing `_append_sec2(...)` calls.

---

### Alternative (more intrusive) option

You *could* instead:

* Replace every `_append_sec2(sec2_chunk_XXXX)` in your notebooks with:

  ```python
  from telco_churn.utils.reporting import append_sec2
  append_sec2(sec2_chunk_XXXX, SECTION2_UNIFIED_REPORT_PATH)
  ```

‚Ä¶but that‚Äôs more typing and more places to maintain. The thin wrapper is cleaner.

---

### One small extra note üí°üí°

You currently have **two** functions named `log_section_completion` in `reporting.py`. The second one overwrites the first. That‚Äôs not *broken*, but for clarity you might want to:

* Rename the simple console-only one to something like `log_section_console`, or
* Delete it if you‚Äôve fully migrated to the richer JSONL version.

Doesn‚Äôt affect `_append_sec2`, just a cleanup suggestion.

---

If you‚Äôd like, I can also give you a quick ‚Äúsmoke test‚Äù snippet to verify that `_append_sec2` is actually appending rows where you expect.
