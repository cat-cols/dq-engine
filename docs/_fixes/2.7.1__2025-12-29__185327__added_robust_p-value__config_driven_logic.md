Below is a drop-in upgrade for **2.7.1** that implements:

1. **Practical significance (“effect size”) flag per feature** using **max |pct_delta|**
2. **Config-driven WARN/FAIL thresholds** for both **p-value** *and* **pct_delta**
3. **Full-length p-values** (best practice: keep float + add a string column for display)

---

## Add these keys to `project_config.yaml` (under `SAMPLING_REPRESENTATIVENESS`)

Keep your existing keys; add these:

```yaml
SAMPLING_REPRESENTATIVENESS:
  ENABLED: true
  TEST_METHOD: chi_square

  # Existing (kept for backward compat; treated as WARN threshold)
  P_VALUE_THRESHOLD: 0.05

  # New: p-value thresholds
  P_VALUE_WARN_THRESHOLD: 0.05
  P_VALUE_FAIL_THRESHOLD: 0.01

  # New: practical significance thresholds on max abs pct_delta
  MAX_ABS_PCT_DELTA_WARN: 0.02     # 2 percentage points
  MAX_ABS_PCT_DELTA_FAIL: 0.05     # 5 percentage points

  # New: p-value display control
  P_VALUE_DISPLAY_PRECISION: 18    # 18 is plenty
  ADD_P_VALUE_STRING_COL: true

  OUTPUT_FILE: sample_representativeness_report.csv

  POPULATION_BENCHMARKS:
    gender:
      Male: 0.49
      Female: 0.51
    Contract:
      Month-to-month: 0.55
      One year: 0.20
      Two year: 0.25
```

---

## Code changes inside your 2.7.1 cell

### A) Replace your threshold reads with this (near the top where you parse config)

This keeps your old `P_VALUE_THRESHOLD` behavior but upgrades it:

```python
# ---- thresholds (config-driven; backward compatible) ----
p_warn_271 = float(sampling_cfg.get("P_VALUE_WARN_THRESHOLD", sampling_cfg.get("P_VALUE_THRESHOLD", 0.05)))
p_fail_271 = float(sampling_cfg.get("P_VALUE_FAIL_THRESHOLD", 0.01))

delta_warn_271 = float(sampling_cfg.get("MAX_ABS_PCT_DELTA_WARN", 0.02))
delta_fail_271 = float(sampling_cfg.get("MAX_ABS_PCT_DELTA_FAIL", 0.05))

pval_precision_271 = int(sampling_cfg.get("P_VALUE_DISPLAY_PRECISION", 18))
add_pval_str_271 = bool(sampling_cfg.get("ADD_P_VALUE_STRING_COL", True))
```

*(You can keep `p_thresh_271` if you want, but this replaces it cleanly.)*

---

### B) Inside the `for feature, pop_dist in population_benchmarks_271.items():` loop, add max-delta tracking

Right after you compute `total_n` and before the per-category loop, initialize:

```python
max_abs_delta_feat = 0.0
```

Then inside your per-category loop (where you compute `pop_pct`, `sample_pct`, `pct_delta`), update it:

```python
pct_delta_val = sample_pct - pop_pct
abs_delta_val = abs(pct_delta_val)
if abs_delta_val > max_abs_delta_feat:
    max_abs_delta_feat = abs_delta_val
```

---

### C) Replace your status logic (currently p-value only) with combined p-value + practical thresholds

Right after you compute `chi_stat, p_val = stats.chisquare(...)`, replace your current status selection with:

```python
# --- status by p-value ---
if p_val < p_fail_271:
    status_p = "FAIL"
elif p_val < p_warn_271:
    status_p = "WARN"
else:
    status_p = "OK"

# --- status by practical delta (max abs pct_delta across categories) ---
if max_abs_delta_feat >= delta_fail_271:
    status_d = "FAIL"
elif max_abs_delta_feat >= delta_warn_271:
    status_d = "WARN"
else:
    status_d = "OK"

# --- final feature status = worst of the two ---
if ("FAIL" in (status_p, status_d)):
    status_feat = "FAIL"
    n_fail_271 += 1
elif ("WARN" in (status_p, status_d)):
    status_feat = "WARN"
    n_warn_271 += 1
else:
    status_feat = "OK"

n_features_tested_271 += 1

# helpful note (same for all categories of this feature)
notes_feat = ""
if status_p != "OK":
    notes_feat += f"p={p_val:.6g} (< warn={p_warn_271} fail={p_fail_271}); "
if status_d != "OK":
    notes_feat += f"max_abs_pct_delta={max_abs_delta_feat:.6g} (>= warn={delta_warn_271} fail={delta_fail_271}); "
notes_feat = notes_feat.strip()
```

**Important:** because `max_abs_delta_feat` is computed in the per-category loop, you have two clean options:

* **Option 1 (recommended):** compute `pct_delta`/`max_abs_delta_feat` *first*, then do chi-square + status, then append rows.
* **Option 2 (minimal edits):** do a first pass to compute `max_abs_delta_feat`, then do chi-square + status, then do a second pass to append rows.

To avoid re-looping, do **Option 1** by moving the “append row” part to after you’ve decided `status_feat`. Here’s the simplest pattern:

1. build a temporary list of per-category values
2. compute max_abs_delta_feat
3. run chi-square
4. assign status_feat + notes_feat
5. append rows using the stored per-category values

Drop-in version of that inner section:

```python
# compute per-category pct deltas first (store rows), track max abs delta
tmp_rows = []
max_abs_delta_feat = 0.0

for cat in all_categories:
    pop_pct = float(pop_probs_aligned.loc[cat])
    sample_pct = float(sample_counts_aligned.loc[cat] / total_n)
    pct_delta_val = sample_pct - pop_pct
    abs_delta_val = abs(pct_delta_val)
    if abs_delta_val > max_abs_delta_feat:
        max_abs_delta_feat = abs_delta_val
    tmp_rows.append((cat, pop_pct, sample_pct, pct_delta_val, abs_delta_val))

# chi-square
chi_stat, p_val = stats.chisquare(
    f_obs=sample_counts_aligned.values,
    f_exp=expected_counts_safe.values
)
test_name = "chi_square"

# status by p-value
if p_val < p_fail_271:
    status_p = "FAIL"
elif p_val < p_warn_271:
    status_p = "WARN"
else:
    status_p = "OK"

# status by delta
if max_abs_delta_feat >= delta_fail_271:
    status_d = "FAIL"
elif max_abs_delta_feat >= delta_warn_271:
    status_d = "WARN"
else:
    status_d = "OK"

# final
if ("FAIL" in (status_p, status_d)):
    status_feat = "FAIL"
    n_fail_271 += 1
elif ("WARN" in (status_p, status_d)):
    status_feat = "WARN"
    n_warn_271 += 1
else:
    status_feat = "OK"

n_features_tested_271 += 1

notes_feat = ""
if status_p != "OK":
    notes_feat += f"p={p_val:.6g} (< warn={p_warn_271} fail={p_fail_271}); "
if status_d != "OK":
    notes_feat += f"max_abs_pct_delta={max_abs_delta_feat:.6g} (>= warn={delta_warn_271} fail={delta_fail_271}); "
notes_feat = notes_feat.strip()

# full-length p-value string (optional)
p_val_str = None
if add_pval_str_271:
    p_val_str = np.format_float_positional(float(p_val), precision=pval_precision_271, unique=False, trim='k')

# append final rows
for (cat, pop_pct, sample_pct, pct_delta_val, abs_delta_val) in tmp_rows:
    row = {
        "feature": feature,
        "category": cat,
        "population_pct": pop_pct,
        "sample_pct": sample_pct,
        "pct_delta": pct_delta_val,
        "abs_pct_delta": abs_delta_val,
        "feature_max_abs_pct_delta": float(max_abs_delta_feat),

        "test_method": test_name,
        "test_statistic": float(chi_stat),
        "p_value": float(p_val),

        # thresholds used
        "p_warn_threshold": float(p_warn_271),
        "p_fail_threshold": float(p_fail_271),
        "delta_warn_threshold": float(delta_warn_271),
        "delta_fail_threshold": float(delta_fail_271),

        # combined status
        "status": status_feat,
        "notes": notes_feat,
    }
    if add_pval_str_271:
        row["p_value_str"] = p_val_str

    sample_representativeness_rows.append(row)
```

That implements #1 and #2 cleanly, and #3 via `p_value_str`.

---

## About “full-length p-values” (is it a good move?)

Yes, **as a string column**.

* Keep `p_value` as a float for computation/sorting.
* Add `p_value_str` for human display, audits, and diffing across runs.
* Don’t obsess over 30+ digits of p-value: beyond ~1e-16 you’re mostly seeing floating-point limits, not “truth.”

Your report now supports both worlds: machines + eyeballs.

---

## Where to handle WARN?

Don’t “fix” WARN here. This cell should **measure + label**. Enforcement belongs downstream (contracts/run health). With the delta thresholds added, WARN now means “either statistically detectable or practically meaningful,” which is finally a sane signal.

If you want, the next logical add is a downstream contract like:

* FAIL if any representativeness FAIL
* WARN if any representativeness WARN
* FAIL if `n_warn > K` (config-driven)

But you don’t need that inside 2.7.1.

# 2.7.1 | Sampling Representativeness Audit
print("2.7.1 | Sampling Representativeness Audit")

# ---- config ----
sampling_cfg = (CONFIG.get("SAMPLING_REPRESENTATIVENESS", {}) or {})

sampling_enabled_271       = bool(sampling_cfg.get("ENABLED", True))
population_benchmarks_271  = sampling_cfg.get("POPULATION_BENCHMARKS", {}) or {}
sampling_test_method_271   = str(sampling_cfg.get("TEST_METHOD", "chi_square"))
# p_thresh_271               = float(sampling_cfg.get("P_VALUE_THRESHOLD", 0.05))
output_file_271            = sampling_cfg.get("OUTPUT_FILE", "sample_representativeness_report.csv")

# ---- thresholds (config-driven; backward compatible) ----
p_warn_271 = float(sampling_cfg.get("P_VALUE_WARN_THRESHOLD", sampling_cfg.get("P_VALUE_THRESHOLD", 0.05)))
p_fail_271 = float(sampling_cfg.get("P_VALUE_FAIL_THRESHOLD", 0.01))

delta_warn_271 = float(sampling_cfg.get("MAX_ABS_PCT_DELTA_WARN", 0.02))
delta_fail_271 = float(sampling_cfg.get("MAX_ABS_PCT_DELTA_FAIL", 0.05))

pval_precision_271 = int(sampling_cfg.get("P_VALUE_DISPLAY_PRECISION", 18))
add_pval_str_271 = bool(sampling_cfg.get("ADD_P_VALUE_STRING_COL", True))


# ---- paths (make sure sec2_27_dir exists) ----
# Prefer your shared directory map if present
if "SEC2_REPORT_DIRS" in globals() and isinstance(SEC2_REPORT_DIRS, dict) and "2.7" in SEC2_REPORT_DIRS:
    sec2_27_dir = SEC2_REPORT_DIRS["2.7"]
else:
    # fallback to Section 2 reports root
    sec2_27_dir = (SEC2_REPORTS_DIR / "2_7").resolve() if "SEC2_REPORTS_DIR" in globals() else Path("sec2_2_7_reports").resolve()
sec2_27_dir.mkdir(parents=True, exist_ok=True)

# ---- choose df ----
if "df_27" in globals():
    df_for_271 = df_27
elif "df_clean" in globals():
    df_for_271 = df_clean
elif "df" in globals():
    df_for_271 = df
else:
    raise RuntimeError("❌ No dataframe found for 2.7.1 (expected df_27, df_clean, or df).")

# ---- early exits (log exactly once) ----
if not sampling_enabled_271:
    notes_271 = "Disabled via config"
    status_271 = "SKIPPED"
    detail_271 = None
    n_features_tested_271 = n_fail_271 = n_warn_271 = 0

elif not population_benchmarks_271:
    notes_271 = "No population benchmarks configured (add CONFIG.SAMPLING_REPRESENTATIVENESS.POPULATION_BENCHMARKS)"
    status_271 = "WARN"
    detail_271 = None
    n_features_tested_271 = n_fail_271 = n_warn_271 = 0

elif (sampling_test_method_271.lower() == "chi_square") and (not HAS_SCIPY or stats is None):
    notes_271 = "SciPy not available (required for chi-square)."
    status_271 = "SKIPPED"
    detail_271 = None
    n_features_tested_271 = n_fail_271 = n_warn_271 = 0

else:
    # ---- run tests ----
    sample_representativeness_rows = []
    n_features_tested_271 = 0
    n_fail_271 = 0
    n_warn_271 = 0

    for feature, pop_dist in population_benchmarks_271.items():
        if feature not in df_for_271.columns:
            print(f"   ⚠️ 2.7.1: feature '{feature}' not found; skipping.")
            continue

        pop_series = pd.Series(pop_dist, dtype=float)
        if pop_series.sum() <= 0:
            print(f"   ⚠️ 2.7.1: feature '{feature}' benchmark sums to 0; skipping.")
            continue
        pop_series = pop_series / pop_series.sum()

        sample_counts = df_for_271[feature].value_counts(dropna=False)
        sample_counts = sample_counts.rename(index=lambda x: "NaN" if pd.isna(x) else x)

        all_categories = sorted(set(pop_series.index).union(sample_counts.index))
        pop_probs_aligned = pop_series.reindex(all_categories).fillna(0.0)
        sample_counts_aligned = sample_counts.reindex(all_categories).fillna(0.0)

        total_n = float(sample_counts_aligned.sum())
        if total_n <= 0:
            print(f"   ⚠️ 2.7.1: feature '{feature}' has zero total count; skipping.")
            continue

        expected_counts = pop_probs_aligned * total_n

        #
        max_abs_delta_feat = 0.0

        # avoid zero expected counts
        expected_counts_safe = expected_counts.copy()
        zero_mask = expected_counts_safe <= 0
        if zero_mask.any():
            tiny = 1e-8
            expected_counts_safe[zero_mask] = tiny
            expected_counts_safe = expected_counts_safe * (total_n / expected_counts_safe.sum())

        # chi-square
        chi_stat, p_val = stats.chisquare(
            f_obs=sample_counts_aligned.values,
            f_exp=expected_counts_safe.values
        )
        test_name = "chi_square"

        # --- status by p-value ---
        if p_val < p_fail_271:
            status_p = "FAIL"
        elif p_val < p_warn_271:
            status_p = "WARN"
        else:
            status_p = "OK"

        # --- status by practical delta (max abs pct_delta across categories) ---
        if max_abs_delta_feat >= delta_fail_271:
            status_d = "FAIL"
        elif max_abs_delta_feat >= delta_warn_271:
            status_d = "WARN"
        else:
            status_d = "OK"

        # --- final feature status = worst of the two ---
        if ("FAIL" in (status_p, status_d)):
            status_feat = "FAIL"
            n_fail_271 += 1
        elif ("WARN" in (status_p, status_d)):
            status_feat = "WARN"
            n_warn_271 += 1
        else:
            status_feat = "OK"

        n_features_tested_271 += 1

        # helpful note (same for all categories of this feature)
        notes_feat = ""
        if status_p != "OK":
            notes_feat += f"p={p_val:.6g} (< warn={p_warn_271} fail={p_fail_271}); "
        if status_d != "OK":
            notes_feat += f"max_abs_pct_delta={max_abs_delta_feat:.6g} (>= warn={delta_warn_271} fail={delta_fail_271}); "
        notes_feat = notes_feat.strip()

        # compute per-category pct deltas first (store rows), track max abs delta
        tmp_rows = []
        max_abs_delta_feat = 0.0

        for cat in all_categories:
            pop_pct = float(pop_probs_aligned.loc[cat])
            sample_pct = float(sample_counts_aligned.loc[cat] / total_n)
            pct_delta_val = sample_pct - pop_pct
            abs_delta_val = abs(pct_delta_val)
            if abs_delta_val > max_abs_delta_feat:
                max_abs_delta_feat = abs_delta_val
            tmp_rows.append((cat, pop_pct, sample_pct, pct_delta_val, abs_delta_val))

        # chi-square
        chi_stat, p_val = stats.chisquare(
            f_obs=sample_counts_aligned.values,
            f_exp=expected_counts_safe.values
        )
        test_name = "chi_square"

        # status by p-value
        if p_val < p_fail_271:
            status_p = "FAIL"
        elif p_val < p_warn_271:
            status_p = "WARN"
        else:
            status_p = "OK"

        # status by delta
        if max_abs_delta_feat >= delta_fail_271:
            status_d = "FAIL"
        elif max_abs_delta_feat >= delta_warn_271:
            status_d = "WARN"
        else:
            status_d = "OK"

        # final
        if ("FAIL" in (status_p, status_d)):
            status_feat = "FAIL"
            n_fail_271 += 1
        elif ("WARN" in (status_p, status_d)):
            status_feat = "WARN"
            n_warn_271 += 1
        else:
            status_feat = "OK"

        n_features_tested_271 += 1

        notes_feat = ""
        if status_p != "OK":
            notes_feat += f"p={p_val:.6g} (< warn={p_warn_271} fail={p_fail_271}); "
        if status_d != "OK":
            notes_feat += f"max_abs_pct_delta={max_abs_delta_feat:.6g} (>= warn={delta_warn_271} fail={delta_fail_271}); "
        notes_feat = notes_feat.strip()

        # full-length p-value string (optional)
        p_val_str = None
        if add_pval_str_271:
            p_val_str = np.format_float_positional(float(p_val), precision=pval_precision_271, unique=False, trim='k')

        # append final rows
        for (cat, pop_pct, sample_pct, pct_delta_val, abs_delta_val) in tmp_rows:
            row = {
                "feature": feature,
                "category": cat,
                "population_pct": pop_pct,
                "sample_pct": sample_pct,
                "pct_delta": pct_delta_val,
                "abs_pct_delta": abs_delta_val,
                "feature_max_abs_pct_delta": float(max_abs_delta_feat),

                "test_method": test_name,
                "test_statistic": float(chi_stat),
                "p_value": float(p_val),

                # thresholds used
                "p_warn_threshold": float(p_warn_271),
                "p_fail_threshold": float(p_fail_271),
                "delta_warn_threshold": float(delta_warn_271),
                "delta_fail_threshold": float(delta_fail_271),

                # combined status
                "status": status_feat,
                "notes": notes_feat,
            }
            if add_pval_str_271:
                row["p_value_str"] = p_val_str

            sample_representativeness_rows.append(row)

        # for cat in all_categories:
        #     pop_pct = float(pop_probs_aligned.loc[cat])
        #     pct_delta_val = sample_pct - pop_pct
        #     abs_delta_val = abs(pct_delta_val)
        #     if abs_delta_val > max_abs_delta_feat:
        #         max_abs_delta_feat = abs_delta_val

        #     sample_pct = float(sample_counts_aligned.loc[cat] / total_n)
        #     sample_representativeness_rows.append({
        #         "feature": feature,
        #         "category": cat,
        #         "population_pct": pop_pct,
        #         "sample_pct": sample_pct,
        #         "pct_delta": sample_pct - pop_pct,
        #         "test_method": test_name,
        #         "test_statistic": float(chi_stat),
        #         "p_value": float(p_val),
        #         "status": status_feat,
        #         "notes": ""
        #     })

    if sample_representativeness_rows:
        df_sample_rep_271 = pd.DataFrame(sample_representativeness_rows)
        path_271 = (sec2_27_dir / output_file_271).resolve()
        df_sample_rep_271.to_csv(path_271, index=False)
        print(f"   ✅ 2.7.1 report written to: {path_271}")
        detail_271 = str(path_271.name)
        notes_271 = ""
    else:
        detail_271 = None
        notes_271 = "No valid benchmarks/features produced rows."

    if n_fail_271 > 0:
        status_271 = "FAIL"
    elif n_warn_271 > 0:
        status_271 = "WARN"
    elif n_features_tested_271 == 0:
        status_271 = "SKIPPED"
    else:
        status_271 = "OK"

# ---- single summary row (always exactly one) ----
summary_271 = pd.DataFrame([{
    "section": "2.7.1",
    "section_name": "Sampling representativeness audit",
    "check": "Compare sample distributions to population benchmark via statistical tests",
    "level": "info",
    "status": status_271,
    "n_features_tested": int(n_features_tested_271),
    "n_fail": int(n_fail_271),
    "n_warn": int(n_warn_271),
    "detail": detail_271,
    "notes": notes_271,
    "timestamp": pd.Timestamp.utcnow(),
}])

# master sink
append_sec2(summary_271, SECTION2_REPORT_PATH)
display(summary_271)