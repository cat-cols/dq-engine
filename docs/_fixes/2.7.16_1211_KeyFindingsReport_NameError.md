```python
# 2.7.16 | Key Findings Report (Markdown)
print("2.7.16 | Key Findings Report (Markdown)")

rep_cfg = CONFIG.get("INFERENTIAL_SUMMARY_REPORT", {})

rep_enabled_2716 = bool(rep_cfg.get("ENABLED", True))
rep_format_2716 = rep_cfg.get("FORMAT", "markdown")
rep_output_file_2716 = rep_cfg.get("OUTPUT_FILE", "inferential_summary_report.md")

include_sections_2716 = rep_cfg.get("INCLUDE_SECTIONS", {
    "REPRESENTATIVENESS": True,
    "NORMALITY": True,
    "VARIANCE": True,
    "GROUP_TESTS": True,
    "EFFECT_SIZES": True,
    "MULTICOLLINEARITY": True,
    "INTERACTIONS": True
})

rep_detail_2716 = None
rep_status_2716 = "SKIPPED"
n_sections_included_2716 = 0

if not rep_enabled_2716:
    print("   ⚠️ 2.7.16 disabled via CONFIG.INFERENTIAL_SUMMARY_REPORT.ENABLED = False")
else:
    # Reuse dfs & paths from dashboard section
    df_rep = dfs["representativeness"]
    df_norm = dfs["normality"]
    df_var = dfs["variance"]
    df_anova = dfs["anova_kruskal"]
    df_chi = dfs["chi_square"]
    df_t = dfs["t_tests"]
    df_nonparam = dfs["nonparametric"]
    df_prop = dfs["proportion"]
    df_eff = dfs["effect_sizes"]
    df_vif = dfs["vif"]
    df_int = dfs["interactions"]

    lines = []

    # Header
    lines.append("# Section 2.7 – Inferential Statistics Summary Report\n")
    lines.append(f"_Generated: {now_str}_\n")
    lines.append(
        textwrap.dedent(
            """
            This report summarizes key inferential diagnostics from Section 2.7,
            including representativeness, distribution shape, group differences,
            effect sizes, multicollinearity, and interaction effects.
            """
        ).strip()
    )
    lines.append("")

    # ---------- Representativeness ----------
    if include_sections_2716.get("REPRESENTATIVENESS", False):
        lines.append("## 1. Representativeness & Sample Bias\n")
        if df_rep.empty:
            lines.append("- No representativeness benchmark file (`sample_representativeness_report.csv`) was found.\n")
        else:
            n_features = df_rep["feature"].nunique() if "feature" in df_rep.columns else df_rep.shape[0]
            if "status" in df_rep.columns:
                n_warn = int(df_rep["status"].eq("WARN").sum())
                n_fail = int(df_rep["status"].eq("FAIL").sum())
            else:
                n_warn = n_fail = 0

            lines.append(
                f"- The sampling representativeness audit covered **{n_features}** benchmarked features.\n"
            )
            if n_fail > 0 or n_warn > 0:
                lines.append(
                    f"- Some population benchmarks deviated from the sample: "
                    f"**{n_warn} WARN** and **{n_fail} FAIL** tests were detected.\n"
                )
            else:
                lines.append("- No serious sampling bias was detected for the configured benchmarks.\n")

            # Highlight largest absolute deviations if available
            if {"feature", "category", "pct_delta"}.issubset(df_rep.columns):
                df_rep_abs = df_rep.copy()
                df_rep_abs["abs_delta"] = df_rep_abs["pct_delta"].abs()
                top_rep = df_rep_abs.sort_values("abs_delta", ascending=False).head(5)
                lines.append("**Largest absolute sample vs population deviations:**\n")
                for _, r in top_rep.iterrows():
                    lines.append(
                        f"- `{r['feature']}` – category `{r['category']}`: "
                        f"sample is {r['pct_delta']:.2f} percentage points away from population."
                    )
            lines.append("")
        n_sections_included_2716 += 1

    # ---------- Normality & distribution ----------
    if include_sections_2716.get("NORMALITY", False):
        lines.append("## 2. Normality & Distribution Shape\n")
        if df_norm.empty:
            lines.append("- No normality test artifact (`normality_tests.csv`) was found.\n")
        else:
            n_features = df_norm["feature"].nunique() if "feature" in df_norm.columns else df_norm.shape[0]
            if "normality_label" in df_norm.columns:
                n_non_normal = int(df_norm["normality_label"].isin(["Non-normal", "Heavy-tailed"]).sum())
            else:
                n_non_normal = np.nan
            lines.append(
                f"- Normality tests were run on **{n_features}** numeric features.\n"
            )
            if not np.isnan(n_non_normal):
                lines.append(
                    f"- **{n_non_normal}** features were flagged as clearly non-normal or heavy-tailed.\n"
                )
            lines.append(
                "- Non-normal variables may require transformation or nonparametric modeling downstream.\n"
            )
        lines.append("")
        n_sections_included_2716 += 1

    # ---------- Variance ----------
    if include_sections_2716.get("VARIANCE", False):
        lines.append("## 3. Variance Homogeneity\n")
        if df_var.empty:
            lines.append("- No variance homogeneity artifact (`variance_homogeneity_report.csv`) was found.\n")
        else:
            n_tests = df_var.shape[0]
            if "variance_label" in df_var.columns:
                n_hetero = int(df_var["variance_label"].isin(["Strongly Heterogeneous"]).sum())
            else:
                n_hetero = np.nan
            lines.append(
                f"- Variance homogeneity tests were run across **{n_tests}** (numeric, group) combinations.\n"
            )
            if not np.isnan(n_hetero) and n_hetero > 0:
                lines.append(
                    f"- **{n_hetero}** tests indicated strong heteroskedasticity, which may affect linear model assumptions.\n"
                )
            else:
                lines.append(
                    "- No major heteroskedasticity issues were detected among the configured tests.\n"
                )
        lines.append("")
        n_sections_included_2716 += 1

    # ---------- Group tests (ANOVA, chi-square, t-tests, proportions) ----------
    if include_sections_2716.get("GROUP_TESTS", False):
        lines.append("## 4. Group Differences & Comparative Tests\n")

        # ANOVA / Kruskal
        if df_anova.empty:
            lines.append("- ANOVA/Kruskal results (`anova_kruskal_results.csv`) not found.\n")
        else:
            n_tests = df_anova.shape[0]
            if {"p_value", "method"}.issubset(df_anova.columns):
                n_sig = int((df_anova["p_value"] <= 0.05).sum())
            else:
                n_sig = np.nan
            lines.append(
                f"- ANOVA/Kruskal tests were run for **{n_tests}** (group, numeric) combinations.\n"
            )
            if not np.isnan(n_sig):
                lines.append(
                    f"- **{n_sig}** of these tests showed statistically significant group differences (p ≤ 0.05).\n"
                )

        # Chi-square
        if df_chi.empty:
            lines.append("- Chi-square relationship results (`chi_square_results.csv`) not found.\n")
        else:
            n_tests = df_chi.shape[0]
            if "p_value" in df_chi.columns:
                n_sig = int((df_chi["p_value"] <= 0.05).sum())
            else:
                n_sig = np.nan
            lines.append(
                f"- Chi-square tests were run for **{n_tests}** categorical pairs to assess association.\n"
            )
            if not np.isnan(n_sig):
                lines.append(
                    f"- **{n_sig}** categorical pairs showed significant dependence (p ≤ 0.05).\n"
                )

        # Parametric t-tests
        if df_t.empty:
            lines.append("- Parametric t-test results (`t_test_results.csv`) not found.\n")
        else:
            n_tests = df_t.shape[0]
            if "p_value" in df_t.columns:
                n_sig = int((df_t["p_value"] <= 0.05).sum())
            else:
                n_sig = np.nan
            lines.append(
                f"- Parametric t-tests were configured for **{n_tests}** group comparisons.\n"
            )
            if not np.isnan(n_sig):
                lines.append(
                    f"- **{n_sig}** comparisons showed statistically significant mean differences.\n"
                )

        # Nonparametric
        if df_nonparam.empty:
            lines.append("- Nonparametric test results (`nonparametric_results.csv`) not found.\n")
        else:
            n_tests = df_nonparam.shape[0]
            if "p_value" in df_nonparam.columns:
                n_sig = int((df_nonparam["p_value"] <= 0.05).sum())
            else:
                n_sig = np.nan
            lines.append(
                f"- Nonparametric tests (Mann–Whitney/Wilcoxon) were run for **{n_tests}** comparisons.\n"
            )
            if not np.isnan(n_sig):
                lines.append(
                    f"- **{n_sig}** nonparametric tests indicated significant group differences.\n"
                )

        # Proportion tests
        if df_prop.empty:
            lines.append("- Proportion / rate comparison results (`proportion_tests.csv`) not found.\n")
        else:
            n_tests = df_prop.shape[0]
            if "p_value" in df_prop.columns:
                n_sig = int((df_prop["p_value"] <= 0.05).sum())
            else:
                n_sig = np.nan
            lines.append(
                f"- Two-proportion tests were run for **{n_tests}** scenarios (e.g., churn or adoption rates).\n"
            )
            if not np.isnan(n_sig):
                lines.append(
                    f"- **{n_sig}** scenarios showed statistically significant rate differences.\n"
                )

        lines.append("")
        n_sections_included_2716 += 1

    # ---------- Effect sizes ----------
    if include_sections_2716.get("EFFECT_SIZES", False):
        lines.append("## 5. Effect Sizes & Practical Significance\n")
        if df_eff.empty:
            lines.append("- No effect size artifact (`effect_size_report.csv`) was found.\n")
        else:
            n_tests = df_eff["test_name"].nunique() if "test_name" in df_eff.columns else df_eff.shape[0]
            if "magnitude_label" in df_eff.columns:
                n_large = int(df_eff["magnitude_label"].isin(["large", "very large"]).sum())
            else:
                n_large = np.nan
            lines.append(
                f"- Standardized effect sizes were computed for **{n_tests}** unique tests.\n"
            )
            if not np.isnan(n_large):
                lines.append(
                    f"- **{n_large}** effects were classified as large or very large (substantial practical impact).\n"
                )

            # Highlight a few largest
            if {"test_name", "effect_type", "effect_value"}.issubset(df_eff.columns):
                df_num = df_eff.copy()
                df_num["abs_val"] = pd.to_numeric(df_num["effect_value"], errors="coerce").abs()
                df_num = df_num.dropna(subset=["abs_val"])
                top_eff = df_num.sort_values("abs_val", ascending=False).head(5)
                lines.append("**Top effect magnitude examples:**")
                for _, r in top_eff.iterrows():
                    lines.append(
                        f"- `{r['test_name']}` – {r['effect_type']}: "
                        f"effect ≈ {r['effect_value']:.3f} "
                        f"(magnitude: {r.get('magnitude_label', 'unknown')})."
                    )
        lines.append("")
        n_sections_included_2716 += 1

    # ---------- Multicollinearity ----------
    if include_sections_2716.get("MULTICOLLINEARITY", False):
        lines.append("## 6. Multicollinearity (VIF)\n")
        if df_vif.empty:
            lines.append("- No VIF artifact (`vif_report.csv`) was found.\n")
        else:
            n_cols = df_vif.shape[0]
            if "vif_value" in df_vif.columns:
                n_high = int((df_vif["vif_value"] >= 10.0).sum())
            else:
                n_high = np.nan
            lines.append(
                f"- Variance Inflation Factors were computed for **{n_cols}** candidate predictors.\n"
            )
            if not np.isnan(n_high) and n_high > 0:
                lines.append(
                    f"- **{n_high}** predictors exceeded the high-VIF threshold (≥ 10), "
                    "suggesting redundancy or instability.\n"
                )
            else:
                lines.append(
                    "- No predictors exhibited problematic VIF values at the configured threshold.\n"
                )
            # Show the worst offenders if available
            if {"column", "vif_value"}.issubset(df_vif.columns):
                df_vif_sorted = df_vif.sort_values("vif_value", ascending=False).head(5)
                lines.append("**Highest VIF predictors:**")
                for _, r in df_vif_sorted.iterrows():
                    lines.append(
                        f"- `{r['column']}` – VIF ≈ {r['vif_value']:.2f} "
                        f"({r.get('vif_category', 'unknown')}), {r.get('notes', '').strip()}"
                    )
        lines.append("")
        n_sections_included_2716 += 1

    # ---------- Interactions ----------
    if include_sections_2716.get("INTERACTIONS", False):
        lines.append("## 7. Interaction Effects\n")
        if df_int.empty:
            lines.append("- No interaction artifact (`interaction_effects.csv`) was found.\n")
        else:
            n_int = df_int.shape[0]
            if "significant_interaction" in df_int.columns:
                n_sig = int(df_int["significant_interaction"].fillna(False).astype(bool).sum())
            else:
                n_sig = np.nan
            lines.append(
                f"- Two-way interaction models were evaluated for **{n_int}** (outcome, factor A, factor B) scenarios.\n"
            )
            if not np.isnan(n_sig):
                lines.append(
                    f"- **{n_sig}** scenarios showed statistically significant interaction terms (p < 0.05).\n"
                )

            # Describe a few strongest
            if {"outcome", "factor_a", "factor_b", "interaction_p"}.issubset(df_int.columns):
                df_int_sorted = df_int.sort_values("interaction_p", ascending=True).head(5)
                lines.append("**Strongest interaction candidates:**")
                for _, r in df_int_sorted.iterrows():
                    pval = r.get("interaction_p", np.nan)
                    lines.append(
                        f"- Outcome `{r['outcome']}` with factors `{r['factor_a']}` × `{r['factor_b']}` "
                        f"(p ≈ {pval:.3g})."
                    )
        lines.append("")
        n_sections_included_2716 += 1

    # ---------- Modeling recommendations & caveats ----------
    lines.append("## 8. Modeling Recommendations & Caveats\n")
    lines.append(
        "- Use non-normal or heavy-tailed variables with caution; consider transformations or nonparametric models.\n"
    )
    lines.append(
        "- Address high-VIF predictors via feature selection, regularization, or dimensionality reduction to avoid unstable coefficients.\n"
    )
    lines.append(
        "- Prioritize predictors and group splits that show both statistical significance **and** meaningful effect sizes.\n"
    )
    lines.append(
        "- Incorporate significant interaction terms into modeling where they have clear business interpretation "
        "and adequate sample support.\n"
    )
    lines.append(
        "- Interpret non-significant results carefully in scenarios flagged as potentially underpowered in the power analysis.\n"
    )
    lines.append("")
    n_sections_included_2716 += 1

    # ---------- Write markdown (and optionally PDF placeholder) ----------
    if rep_format_2716.lower() == "markdown":
        rep_path = sec2_27_dir / rep_output_file_2716
        with rep_path.open("w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        print(f"   ✅ 2.7.16 markdown summary written to: {rep_path}")
        rep_detail_2716 = str(rep_path)
        rep_status_2716 = "OK" if n_sections_included_2716 > 0 else "FAIL"
    elif rep_format_2716.lower() == "pdf":
        # We don't implement PDF rendering here; write markdown instead and note this
        rep_path = sec2_27_dir / rep_output_file_2716.replace(".pdf", ".md")
        with rep_path.open("w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        print(f"   ✅ 2.7.16 markdown summary written (PDF generation not implemented) to: {rep_path}")
        rep_detail_2716 = str(rep_path)
        rep_status_2716 = "WARN"
    else:
        print(f"   ⚠️ 2.7.16: unknown FORMAT='{rep_format_2716}', wrote markdown fallback.")
        rep_path = sec2_27_dir / "inferential_summary_report.md"
        with rep_path.open("w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        rep_detail_2716 = str(rep_path)
        rep_status_2716 = "WARN"

sec2_diagnostics_rows.append({
    "section": "2.7.16",
    "section_name": "Key findings report",
    "check": "Generate narrative summary of inferential diagnostics (markdown/pdf)",
    "level": "info",
    "n_sections_included": n_sections_included_2716,
    "status": rep_status_2716,
    "detail": rep_detail_2716,
    "notes": None
})

display()

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[194], line 46
     43 lines.append("# Section 2.7 – Inferential Statistics Summary Report\n")
     44 lines.append(f"_Generated: {now_str}_\n")
     45 lines.append(
---> 46     textwrap.dedent(
     47         """
     48         This report summarizes key inferential diagnostics from Section 2.7,
     49         including representativeness, distribution shape, group differences,
     50         effect sizes, multicollinearity, and interaction effects.
     51         """
     52     ).strip()
     53 )
     54 lines.append("")
     56 # ---------- Representativeness ----------

NameError: name 'textwrap' is not defined
```

># SOLUTION:

