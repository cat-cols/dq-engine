{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420a56a2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "# 1.2.1-1.2.3 | PART B Path Setup & Directory Management FIXME: 1113_924\n",
    "</summary>\n",
    "\n",
    "This looks really solid already ‚Äî this is exactly the kind of ‚ÄúPart B: environment bootstrap‚Äù that makes a project feel real. There are just a few small things I‚Äôd tweak for clarity/maintainability and ‚Äúfuture-you won‚Äôt hate this‚Äù vibes.\n",
    "\n",
    "I‚Äôll point them out and then show a tightened version.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. PROJECT_ROOT resolution\n",
    "\n",
    "This part is good:\n",
    "\n",
    "```python\n",
    "CURRENT_PATH = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "\n",
    "for parent in [CURRENT_PATH] + list(CURRENT_PATH.parents):\n",
    "    # Heuristic 1: repo folder name is \"Telco\"\n",
    "    if parent.name == \"Telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "```\n",
    "\n",
    "Two small ideas:\n",
    "\n",
    "1. **Be case-insensitive**, just in case the folder gets renamed `telco` at some point:\n",
    "\n",
    "   ```python\n",
    "   if parent.name.lower() == \"telco\":\n",
    "   ```\n",
    "\n",
    "2. **Consider an override** (env var) later, but that can wait until you have the package; your comment about `config/project_config.yaml` is already a nice ‚Äúfuture heuristic.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Magic `\"Level_3\"` string everywhere\n",
    "\n",
    "Right now:\n",
    "\n",
    "```python\n",
    "CONFIG_DIR = PROJECT_ROOT / \"Level_3\" / \"config\"\n",
    "\n",
    "CONVENTION_DIRS = [\n",
    "    CONFIG_DIR,\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"reports\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"figures\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"models\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"outputs\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"artifacts\",\n",
    "]\n",
    "```\n",
    "\n",
    "You‚Äôre going to copy this pattern for Level_4, Level_5, etc, so I‚Äôd pull `\"Level_3\"` into a tiny constant at the top of the notebook:\n",
    "\n",
    "```python\n",
    "LEVEL_NAME = \"Level_3\"\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "\n",
    "CONFIG_DIR = LEVEL_ROOT / \"config\"\n",
    "\n",
    "CONVENTION_DIRS = [\n",
    "    CONFIG_DIR,\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    LEVEL_ROOT / \"resources\" / \"reports\",\n",
    "    LEVEL_ROOT / \"resources\" / \"figures\",\n",
    "    LEVEL_ROOT / \"resources\" / \"models\",\n",
    "    LEVEL_ROOT / \"resources\" / \"outputs\",\n",
    "    LEVEL_ROOT / \"resources\" / \"artifacts\",\n",
    "]\n",
    "```\n",
    "\n",
    "Same behavior, much easier to generalize later.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Directory creation + writable check\n",
    "\n",
    "This bit is great:\n",
    "\n",
    "```python\n",
    "for d in CONVENTION_DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "```\n",
    "\n",
    "and:\n",
    "\n",
    "```python\n",
    "writable_checks = []\n",
    "for d in CONVENTION_DIRS:\n",
    "    try:\n",
    "        test_file = d / \".write_test.tmp\"\n",
    "        test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "        test_file.unlink(missing_ok=True)\n",
    "        writable_checks.append((str(d), True))\n",
    "    except Exception:\n",
    "        writable_checks.append((str(d), False))\n",
    "```\n",
    "\n",
    "Two micro-tweaks:\n",
    "\n",
    "1. **Narrow the exception** if you want (e.g. `OSError`), but `Exception` is fine for a ‚Äúhealth check‚Äù if you just log it.\n",
    "2. Add a tiny comment that explains *why* you‚Äôre doing the test write (you know now ‚Äî future-you might not):\n",
    "\n",
    "```python\n",
    "# 1.2.3 Verify Writable Paths (simple can-I-write-here check)\n",
    "```\n",
    "\n",
    "You already have the printout at the end, which is üëå.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. A slightly polished version\n",
    "\n",
    "Here‚Äôs your same logic with those tiny tweaks:\n",
    "\n",
    "```python\n",
    "# 1.2.1‚Äì1.2.3 | PART B Path Setup & Directory Management\n",
    "\n",
    "# 1.2.1 Resolve Project Root\n",
    "CURRENT_PATH = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "LEVEL_NAME = \"Level_3\"  # current working level\n",
    "LEVEL_ROOT = None\n",
    "\n",
    "for parent in [CURRENT_PATH] + list(CURRENT_PATH.parents):\n",
    "    # Heuristic 1: repo folder name is \"Telco\"\n",
    "    if parent.name.lower() == \"telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "        break\n",
    "    # Heuristic 2 (future): parent contains config/project_config.yaml\n",
    "    # if (parent / \"config\" / \"project_config.yaml\").exists():\n",
    "    #     PROJECT_ROOT = parent\n",
    "    #     LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "    #     break\n",
    "\n",
    "if PROJECT_ROOT is None or LEVEL_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Could not locate Telco project root from {CURRENT_PATH}. \"\n",
    "        \"Expected a 'Telco' folder or 'config/project_config.yaml' above.\"\n",
    "    )\n",
    "\n",
    "print(f\"üìÅ 1.2.1 Project root resolved   ‚Üí {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ 1.2.1 Level root resolved     ‚Üí {LEVEL_ROOT}\")\n",
    "\n",
    "# 1.2.2 Define Core Subdirectories (convention-level, before config)\n",
    "CONFIG_DIR = LEVEL_ROOT / \"config\"\n",
    "\n",
    "CONVENTION_DIRS = [\n",
    "    CONFIG_DIR,\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    LEVEL_ROOT / \"resources\" / \"reports\",\n",
    "    LEVEL_ROOT / \"resources\" / \"figures\",\n",
    "    LEVEL_ROOT / \"resources\" / \"models\",\n",
    "    LEVEL_ROOT / \"resources\" / \"outputs\",\n",
    "    LEVEL_ROOT / \"resources\" / \"artifacts\",\n",
    "]\n",
    "\n",
    "for d in CONVENTION_DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1.2.3 Verify Writable Paths\n",
    "writable_checks = []\n",
    "for d in CONVENTION_DIRS:\n",
    "    try:\n",
    "        test_file = d / \".write_test.tmp\"\n",
    "        test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "        test_file.unlink(missing_ok=True)\n",
    "        writable_checks.append((str(d), True))\n",
    "    except Exception:\n",
    "        writable_checks.append((str(d), False))\n",
    "\n",
    "print(\"\\nüìÇ 1.2.2‚Äì1.2.3 Directory structure & write access:\")\n",
    "for path_str, ok in writable_checks:\n",
    "    status = \"‚úÖ writable\" if ok else \"‚ö†Ô∏è not writable\"\n",
    "    print(f\"   {status}: {path_str}\")\n",
    "```\n",
    "\n",
    "Functionally identical to what you have, but:\n",
    "\n",
    "* `\"Level_3\"` is centralized\n",
    "* Case-insensitive root detection\n",
    "* Slightly more explicit prints\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next step I can help you turn this 1.2 block into a tiny `telco_churn.config.paths` module (e.g. `get_project_root()`, `get_level_root()`) so your notebooks can just call a function instead of carrying the for-loop logic inline.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c254cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "`# 1.5.2  Hash / Snapshot Validation TODO: proper desc? Load registry`\n",
    "</summary>\n",
    "\n",
    "Nice, this is looking really clean. That `# TODO: proper desc? Load registry` is basically asking to describe this whole ‚Äúload-or-init version registry‚Äù behavior.\n",
    "\n",
    "Here‚Äôs a good, professional wording + a slightly tidied block you can drop in.\n",
    "\n",
    "### Suggested comment for that TODO\n",
    "\n",
    "Replace:\n",
    "\n",
    "```python\n",
    "# TODO: proper desc? Load registry\n",
    "if registry_path.exists():\n",
    "```\n",
    "\n",
    "with something like:\n",
    "\n",
    "```python\n",
    "# Load existing dataset version registry if present; otherwise initialize an empty one\n",
    "if registry_path.exists():\n",
    "```\n",
    "\n",
    "If you want to be even more explicit about semantics:\n",
    "\n",
    "```python\n",
    "# Load dataset version registry (file_hash ‚Üí version_id); initialize empty registry if not present\n",
    "if registry_path.exists():\n",
    "```\n",
    "\n",
    "### Slightly polished version of your whole block\n",
    "\n",
    "Here‚Äôs your code with that comment dropped in and minor tighten-ups:\n",
    "\n",
    "```python\n",
    "# 1.5.2 Hash / Snapshot Validation\n",
    "print(\"\\n1.5.2) üîê Hash / snapshot validation\")\n",
    "\n",
    "def compute_hash(path, algo: str = \"sha256\", chunk_size: int = 1 << 20) -> str:\n",
    "    \"\"\"Streaming file hash to avoid loading whole file into memory.\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "file_hash = compute_hash(RAW_DATA, \"sha256\")\n",
    "now_utc = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "# Ensure artifacts directory exists so atomic writes (tmp + replace) don't fail\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGISTRY_DIR = ARTIFACTS_DIR / \"registry\"\n",
    "REGISTRY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "registry_path = REGISTRY_DIR / \"dataset_version_registry.csv\"\n",
    "\n",
    "# Load dataset version registry (file_hash ‚Üí version_id); initialize empty registry if not present\n",
    "if registry_path.exists():\n",
    "    registry = pd.read_csv(registry_path)\n",
    "else:\n",
    "    registry = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"version_id\",\n",
    "            \"dataset_path\",\n",
    "            \"file_hash\",\n",
    "            \"first_seen_utc\",\n",
    "            \"last_seen_utc\",\n",
    "            \"n_rows\",\n",
    "            \"n_cols\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "existing = registry.loc[registry[\"file_hash\"] == file_hash]\n",
    "\n",
    "if not existing.empty:\n",
    "    version_id = str(existing.iloc[0][\"version_id\"])\n",
    "    registry.loc[registry[\"file_hash\"] == file_hash, \"last_seen_utc\"] = now_utc\n",
    "    print(f\"‚ÑπÔ∏è Existing dataset version found ‚Üí version_id={version_id}\")\n",
    "else:\n",
    "    if registry.empty:\n",
    "        next_id = 1\n",
    "    else:\n",
    "        max_existing = pd.to_numeric(registry[\"version_id\"], errors=\"coerce\").fillna(0).max()\n",
    "        next_id = int(max_existing) + 1\n",
    "\n",
    "    version_id = str(next_id)\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"version_id\": [version_id],\n",
    "            \"dataset_path\": [str(RAW_DATA)],\n",
    "            \"file_hash\": [file_hash],\n",
    "            \"first_seen_utc\": [now_utc],\n",
    "            \"last_seen_utc\": [now_utc],\n",
    "            \"n_rows\": [n_rows],\n",
    "            \"n_cols\": [n_cols],\n",
    "        }\n",
    "    )\n",
    "    registry = pd.concat([registry, new_row], ignore_index=True)\n",
    "    print(f\"üÜï New dataset version registered ‚Üí version_id={version_id}\")\n",
    "\n",
    "tmp_reg = registry_path.with_suffix(\".tmp\")\n",
    "registry.to_csv(tmp_reg, index=False)\n",
    "os.replace(tmp_reg, registry_path)\n",
    "print(f\"‚úÖ Version registry updated ‚Üí {registry_path}\")\n",
    "```\n",
    "\n",
    "One tiny note: `load_log_path` is now unused in this snippet; you can either:\n",
    "\n",
    "* Remove it from this section, or\n",
    "* Move its path + mkdir into the 1.5.3 ‚Äúload log‚Äù block where you actually use it.\n",
    "\n",
    "But for the TODO itself, ‚ÄúLoad dataset version registry (file_hash ‚Üí version_id); initialize empty registry if not present‚Äù is a very recruiter-friendly description of what‚Äôs going on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7750f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "# 1.4.1 TODO: FIX_ID: 11132019\n",
    "Artifacts: Where to put\n",
    "</summary>\n",
    "\n",
    "Love this section‚Äîthis is very ‚Äúgrown-up pipeline‚Äù code already. üß™üì¶\n",
    "\n",
    "You‚Äôre basically producing three *kinds* of artifacts here:\n",
    "\n",
    "1. **Dataset version registry** ‚Üí long-lived registry\n",
    "2. **Dataset load log** ‚Üí append-only event log\n",
    "3. **`df.info` snapshots** ‚Üí diagnostic snapshots\n",
    "\n",
    "Right now all three live flat in `ARTIFACTS_DIR`. I‚Äôd split them a bit so they *explain themselves* just by the path.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Where to put `dataset_version_registry.csv`\n",
    "\n",
    "Conceptually this is a **registry**, not a one-off log or report.\n",
    "\n",
    "I‚Äôd give it its own subfolder:\n",
    "\n",
    "```text\n",
    "resources/artifacts/\n",
    "  registry/\n",
    "    dataset_version_registry.csv\n",
    "```\n",
    "\n",
    "Code tweak:\n",
    "\n",
    "```python\n",
    "REGISTRY_DIR = ARTIFACTS_DIR / \"registry\"\n",
    "REGISTRY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "registry_path = REGISTRY_DIR / \"dataset_version_registry.csv\"\n",
    "```\n",
    "\n",
    "Everything else about how you write it (temp file + `os.replace`) is already üíØ atomic and professional.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Where to put `dataset_load_log.csv`\n",
    "\n",
    "This one is an **event log**: every run appends one row.\n",
    "\n",
    "I‚Äôd put it under a **logs** or **diagnostics** subtree:\n",
    "\n",
    "```text\n",
    "resources/artifacts/\n",
    "  logs/\n",
    "    dataset_load_log.csv\n",
    "```\n",
    "\n",
    "or, if you‚Äôre following the diagnostics pattern we talked about earlier:\n",
    "\n",
    "```text\n",
    "resources/artifacts/\n",
    "  diagnostics/\n",
    "    loads/\n",
    "      dataset_load_log.csv\n",
    "```\n",
    "\n",
    "Code tweak:\n",
    "\n",
    "```python\n",
    "LOAD_LOG_DIR = ARTIFACTS_DIR / \"diagnostics\" / \"loads\"\n",
    "LOAD_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load_log_path = LOAD_LOG_DIR / \"dataset_load_log.csv\"\n",
    "```\n",
    "\n",
    "Again, your atomic write pattern using `.tmp` + `os.replace` is exactly what I‚Äôd want here.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Where to put `df_raw_info.txt` / `df_working_info.txt`\n",
    "\n",
    "These are **diagnostic snapshots** about the dataset, and they‚Äôre tied to a specific version + run.\n",
    "\n",
    "Two improvements:\n",
    "\n",
    "1. **Folder:** tuck them under `diagnostics/dataset_info/`\n",
    "2. **Filename:** include `version_id` and maybe timestamp so you don‚Äôt just overwrite every run.\n",
    "\n",
    "Suggested layout:\n",
    "\n",
    "```text\n",
    "resources/artifacts/\n",
    "  diagnostics/\n",
    "    dataset_info/\n",
    "      df_raw_info_v001_2025-11-13T01-23-45Z.txt\n",
    "      df_working_info_v001_2025-11-13T01-23-45Z.txt\n",
    "```\n",
    "\n",
    "Code tweak (and fix the open-handle leak):\n",
    "\n",
    "```python\n",
    "INFO_DIR = ARTIFACTS_DIR / \"diagnostics\" / \"dataset_info\"\n",
    "INFO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = now_utc.replace(\":\", \"-\")  # safer for filenames\n",
    "\n",
    "raw_info_path     = INFO_DIR / f\"df_raw_info_v{version_id}_{stamp}.txt\"\n",
    "working_info_path = INFO_DIR / f\"df_working_info_v{version_id}_{stamp}.txt\"\n",
    "\n",
    "print(f\"\\n1.5.5) üß¨ Writing df.info() snapshots ‚Üí {INFO_DIR}\")\n",
    "\n",
    "with raw_info_path.open(\"w\") as f:\n",
    "    df_raw.info(buf=f)\n",
    "\n",
    "with working_info_path.open(\"w\") as f:\n",
    "    df.info(buf=f)\n",
    "\n",
    "print(f\"   ‚úî df_raw.info()    ‚Üí {raw_info_path}\")\n",
    "print(f\"   ‚úî df.info()        ‚Üí {working_info_path}\")\n",
    "```\n",
    "\n",
    "This fixes two subtle things:\n",
    "\n",
    "* No more `buf=open(...)` without closing the file\n",
    "* Paths and names tell a future you:\n",
    "\n",
    "  * which dataset version\n",
    "  * which run\n",
    "  * which frame (raw vs working)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Recap of ‚Äúmore professional‚Äù destinations\n",
    "\n",
    "Inside `ARTIFACTS_DIR` something like:\n",
    "\n",
    "```text\n",
    "artifacts/\n",
    "  registry/\n",
    "    dataset_version_registry.csv\n",
    "  diagnostics/\n",
    "    loads/\n",
    "      dataset_load_log.csv\n",
    "    dataset_info/\n",
    "      df_raw_info_v001_2025-11-13T01-23-45Z.txt\n",
    "      df_working_info_v001_2025-11-13T01-23-45Z.txt\n",
    "```\n",
    "\n",
    "Your 1.5 logic is already doing the right *behaviors* (hashing, atomic writes, defensive copy). These path tweaks just make the filesystem tell the same story your code does: registry vs logs vs diagnostics.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d0aac",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "# 1.5.1‚Äì1.5.3 | PART E üìä Dataset Load & Version Verification FIXME: 1113_1013\n",
    "</summary>\n",
    "This looks really good already ‚Äî you‚Äôve got the right structure and nice logging. You‚Äôre basically 90% of the way there; I‚Äôd just tighten a couple of things and make the CSV vs Parquet behavior really explicit.\n",
    "\n",
    "Here‚Äôs what I‚Äôd tweak:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Make the CSV vs Parquet comment actually say what you do\n",
    "\n",
    "Right now:\n",
    "\n",
    "```python\n",
    "# 1.5.1 Load Raw Dataset (CSV: Config-Driven, with READ_OPTS) / (PARQUET: )\n",
    "```\n",
    "\n",
    "I‚Äôd make that a mini-contract:\n",
    "\n",
    "```python\n",
    "# 1.5.1 Load Raw Dataset\n",
    "# - CSV:    config-driven via READ_OPTS (e.g. delimiter, encoding, dtypes)\n",
    "# - Parquet: loaded via default pd.read_parquet() options\n",
    "```\n",
    "\n",
    "So anyone reading the code/glossary knows ‚Äúonly CSV uses READ_OPTS right now.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Don‚Äôt reuse `READ_OPTS` for Parquet (yet)\n",
    "\n",
    "What you‚Äôve got is good:\n",
    "\n",
    "```python\n",
    "READ_OPTS = C(\"READ_OPTS\", {}) or {}\n",
    "\n",
    "suffix = RAW_DATA.suffix.lower()\n",
    "if suffix == \".csv\":\n",
    "    print(f\"   Using READ_OPTS from config: {READ_OPTS}\")\n",
    "    df_raw = pd.read_csv(RAW_DATA, **READ_OPTS)\n",
    "elif suffix in {\".parquet\", \".pq\"}:\n",
    "    df_raw = pd.read_parquet(RAW_DATA)\n",
    "```\n",
    "\n",
    "I‚Äôd keep it that way (i.e., don‚Äôt try to feed `READ_OPTS` into `read_parquet`) unless you explicitly add a `PARQUET_READ_OPTS` config. If you want to future-proof:\n",
    "\n",
    "```python\n",
    "CSV_READ_OPTS = C(\"READ_OPTS.CSV\", {}) or {}\n",
    "PARQUET_READ_OPTS = C(\"READ_OPTS.PARQUET\", {}) or {}\n",
    "```\n",
    "\n",
    "‚Ä¶but that‚Äôs optional for now.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Slightly polished version you can drop in\n",
    "\n",
    "Here‚Äôs your block with just those small improvements and a tiny comment on what READ_OPTS is for:\n",
    "\n",
    "```python\n",
    "# 1.5.1‚Äì1.5.3 | PART E üìä Dataset Load & Version Verification\n",
    "print(\"1.5 üß© DATASET LOAD & VERSION VERIFICATION\")\n",
    "\n",
    "# 1.5.1 Load Raw Dataset\n",
    "# - CSV:     config-driven via READ_OPTS (e.g. delimiter, encoding, dtypes)\n",
    "# - Parquet: loaded via default pd.read_parquet() options\n",
    "\n",
    "if not RAW_DATA.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå RAW_DATA file not found at {RAW_DATA}\")\n",
    "\n",
    "print(f\"1.5.1) üì• Loading raw dataset from: {RAW_DATA}\")\n",
    "\n",
    "# Pull CSV read options from config (READ_OPTS is optional)\n",
    "READ_OPTS = C(\"READ_OPTS\", {}) or {}\n",
    "\n",
    "suffix = RAW_DATA.suffix.lower()\n",
    "if suffix == \".csv\":\n",
    "    print(f\"   Using READ_OPTS from config: {READ_OPTS}\")\n",
    "    df_raw = pd.read_csv(RAW_DATA, **READ_OPTS)\n",
    "elif suffix in {\".parquet\", \".pq\"}:\n",
    "    df_raw = pd.read_parquet(RAW_DATA)\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå Unsupported file format for RAW_DATA: {suffix}\")\n",
    "\n",
    "n_rows, n_cols = df_raw.shape\n",
    "print(f\"‚úÖ Loaded dataset: {n_rows:,} rows √ó {n_cols:,} columns\")\n",
    "```\n",
    "\n",
    "If you want to go one step further later, we can add:\n",
    "\n",
    "* `load_format = \"csv\"`/`\"parquet\"`\n",
    "* And log that into your `dataset_load_log.csv` alongside `READ_OPTS`, so the registry knows how each version was ingested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3712a87",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.5.3‚Äì1.5.5 | PART E üìä Dataset Load & Version Verification FIXME: 1113_2324\n",
    "</summary>\n",
    "\n",
    "Nice, this block is doing all the right things conceptually ‚Äî append-only log, atomic write, defensive copy, info snapshots. There are just a few things to polish:\n",
    "\n",
    "* Where `dataset_load_log.csv` lives\n",
    "* Making sure its directory exists\n",
    "* Not leaking file handles with `buf=open(...)`\n",
    "* Not overwriting the same `df_raw_info.txt` on every run\n",
    "\n",
    "I‚Äôll show you an improved version of **1.5.3‚Äì1.5.5** and then point out what changed.\n",
    "\n",
    "---\n",
    "\n",
    "## Polished version (drop-in replacement)\n",
    "\n",
    "```python\n",
    "# 1.5.3 Schema / Load Metadata Logging\n",
    "\n",
    "print(\"\\n1.5.3) üßæ Appending load metadata to dataset_load_log.csv\")\n",
    "\n",
    "# Put load logs under artifacts/diagnostics/loads\n",
    "LOADS_DIR = ARTIFACTS_DIR / \"diagnostics\" / \"loads\"\n",
    "LOADS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load_log_path = LOADS_DIR / \"dataset_load_log.csv\"\n",
    "\n",
    "log_row = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp_utc\": [now_utc],\n",
    "        \"dataset_path\": [str(RAW_DATA)],\n",
    "        \"version_id\": [version_id],\n",
    "        \"file_hash\": [file_hash],\n",
    "        \"n_rows\": [n_rows],\n",
    "        \"n_cols\": [n_cols],\n",
    "        \"read_opts\": [str(READ_OPTS)],\n",
    "    }\n",
    ")\n",
    "\n",
    "if load_log_path.exists():\n",
    "    existing_log = pd.read_csv(load_log_path)\n",
    "    # Union of columns so the log is forward-compatible with new fields\n",
    "    all_cols = pd.Index(existing_log.columns).union(log_row.columns)\n",
    "    out_log = pd.concat(\n",
    "        [existing_log.reindex(columns=all_cols), log_row.reindex(columns=all_cols)],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "else:\n",
    "    out_log = log_row\n",
    "\n",
    "tmp_log = load_log_path.with_suffix(\".tmp\")\n",
    "out_log.to_csv(tmp_log, index=False)\n",
    "os.replace(tmp_log, load_log_path)\n",
    "\n",
    "print(f\"‚úÖ Load log updated ‚Üí {load_log_path}\")\n",
    "print(f\"üìä Dataset hash: {file_hash[:12]}... | version_id: {version_id}\")\n",
    "\n",
    "# df_raw is your canonical raw DataFrame for 1.6+\n",
    "\n",
    "# 1.5.4 Defensive Copy for Processing\n",
    "df = df_raw.copy(deep=True)\n",
    "print(\"\\n‚úÖ Created deep working copy 'df' from 'df_raw'\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]:,} columns\")\n",
    "\n",
    "# 1.5.5 Metadata Snapshots (df.info for raw + working)\n",
    "INFO_DIR = ARTIFACTS_DIR / \"diagnostics\" / \"dataset_info\"\n",
    "INFO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = now_utc.replace(\":\", \"-\")  # safer for filenames on most OSes\n",
    "\n",
    "raw_info_path     = INFO_DIR / f\"df_raw_info_v{version_id}_{stamp}.txt\"\n",
    "working_info_path = INFO_DIR / f\"df_working_info_v{version_id}_{stamp}.txt\"\n",
    "\n",
    "print(\"\\n1.5.5) üß¨ Writing df.info() snapshots:\")\n",
    "print(f\"   ‚Ä¢ df_raw.info() ‚Üí {raw_info_path}\")\n",
    "print(f\"   ‚Ä¢ df.info()     ‚Üí {working_info_path}\")\n",
    "\n",
    "with raw_info_path.open(\"w\") as f:\n",
    "    df_raw.info(buf=f)\n",
    "\n",
    "with working_info_path.open(\"w\") as f:\n",
    "    df.info(buf=f)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What changed and why\n",
    "\n",
    "### 1. Log file location + mkdir\n",
    "\n",
    "**Before:**\n",
    "\n",
    "* `load_log_path = ARTIFACTS_DIR / \"dataset_load_log.csv\"`\n",
    "* Implicitly assumed `ARTIFACTS_DIR` exists (you now guarantee that in 1.5.2, so this is okay but a bit flat).\n",
    "\n",
    "**After:**\n",
    "\n",
    "```python\n",
    "LOADS_DIR = ARTIFACTS_DIR / \"diagnostics\" / \"loads\"\n",
    "LOADS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "load_log_path = LOADS_DIR / \"dataset_load_log.csv\"\n",
    "```\n",
    "\n",
    "* Gives the log a **clear home**: `artifacts/diagnostics/loads/dataset_load_log.csv`\n",
    "* Guarantees the directory exists right where you write (self-contained section)\n",
    "* Matches the mental model: *this is a diagnostic load log, not a human report*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Atomic write is preserved\n",
    "\n",
    "You already had a nice atomic pattern:\n",
    "\n",
    "```python\n",
    "tmp_log = load_log_path.with_suffix(\".tmp\")\n",
    "out_log.to_csv(tmp_log, index=False)\n",
    "os.replace(tmp_log, load_log_path)\n",
    "```\n",
    "\n",
    "That stays exactly the same ‚Äî just now `load_log_path` lives under `diagnostics/loads`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `df.info` snapshots: no more open-handle weirdness, no silent overwrites\n",
    "\n",
    "**Before:**\n",
    "\n",
    "```python\n",
    "df_raw.info(buf=open(ARTIFACTS_DIR / \"df_raw_info.txt\", \"w\"))\n",
    "df.info(buf=open(ARTIFACTS_DIR / \"df_working_info.txt\", \"w\"))\n",
    "```\n",
    "\n",
    "Issues:\n",
    "\n",
    "* Opens files without closing them explicitly (works, but messy)\n",
    "* Overwrites the same filenames every run (no link to version/timestamp)\n",
    "* Lives flat under `ARTIFACTS_DIR`, mixed with other stuff\n",
    "\n",
    "**After:**\n",
    "\n",
    "```python\n",
    "INFO_DIR = ARTIFACTS_DIR / \"diagnostics\" / \"dataset_info\"\n",
    "INFO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = now_utc.replace(\":\", \"-\")\n",
    "\n",
    "raw_info_path     = INFO_DIR / f\"df_raw_info_v{version_id}_{stamp}.txt\"\n",
    "working_info_path = INFO_DIR / f\"df_working_info_v{version_id}_{stamp}.txt\"\n",
    "\n",
    "with raw_info_path.open(\"w\") as f:\n",
    "    df_raw.info(buf=f)\n",
    "\n",
    "with working_info_path.open(\"w\") as f:\n",
    "    df.info(buf=f)\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Files go under a **logical diagnostics directory**.\n",
    "* Filenames encode **version + timestamp**:\n",
    "\n",
    "  * `df_raw_info_v3_2025-11-13T01-34-12Z.txt`\n",
    "* Uses `with` context managers ‚Üí files are properly closed.\n",
    "* You can browse history by file name and instantly see what‚Äôs from which version.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next step we can add a tiny description to your glossary:\n",
    "\n",
    "> **`dataset_load_log.csv`** ‚Äì append-only log of dataset loads (timestamp, path, version_id, hash, row/col counts, READ_OPTS). Lives under `artifacts/diagnostics/loads`.\n",
    "\n",
    "> **`df_raw_info` / `df_working_info` snapshots** ‚Äì text dumps of `df.info()` for raw and working DataFrames, versioned by `version_id` + timestamp, under `artifacts/diagnostics/dataset_info`.\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fb7c1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    ".gitignore | How do I edit .gitignore and remove the Level_3 ignore rule? FIXME: 11/14/2025_12:00am\n",
    "</summary>\n",
    "\n",
    "I have 2 .gitignore's\n",
    "\n",
    "```ignore\n",
    "# -` ./Users/b/DATA/PROJECTS/Telco/.gitignore`\n",
    "cat > .gitignore << 'EOF'\n",
    "# ALLOW LIST\n",
    "# --- Default: ignore everything ---\n",
    "*\n",
    "\n",
    "# --- Allow these at repo root ---\n",
    "!.gitignore\n",
    "!README.md\n",
    "!setup_env.sh\n",
    "!requirements.txt\n",
    "\n",
    "# --- Allow Level_0 (recursively) ---\n",
    "!Level_0/\n",
    "!Level_0/**\n",
    "\n",
    "!Level_0/README.md\n",
    "!Level_0/01_EDA/\n",
    "!Level_0/01_EDA/**\n",
    "\n",
    "# --- Allow Level_1 (recursively) ---\n",
    "# !Level_1/\n",
    "# !Level_1/**\n",
    "\n",
    "# --- Allow Level_2 (recursively) ---\n",
    "# !Level_2/\n",
    "# !Level_2/**\n",
    "\n",
    "# # --- Allow ALL Level_3 (recursively) ---\n",
    "# !Level_3/\n",
    "# !Level_3/**\n",
    "\n",
    "# --- Ignore everything in Level_3 ---\n",
    "# But Allow README.md and 01_EDA directory ---\n",
    "Level_3/*\n",
    "\n",
    "!Level_3/README.md\n",
    "!Level_3/01_EDA/\n",
    "!Level_3/01_EDA/**\n",
    "!Level_3/resources/_assets/churn.png\n",
    "\n",
    "##############################################\n",
    "# --- Safety: ignore heavy/local anywhere ---#\n",
    "##############################################\n",
    "**/.venv/\n",
    "**/.env\n",
    "**/__pycache__/\n",
    "**/.ipynb_checkpoints/\n",
    "**/data/\n",
    "**/outputs/\n",
    "**/logs/\n",
    "**/.cache/\n",
    "**/*.csv\n",
    "**/*.tsv\n",
    "**/*.zip\n",
    "**/*.parquet\n",
    "**/*.pkl\n",
    "**/*.h5\n",
    ".vscode/\n",
    ".DS_Store\n",
    "\n",
    "# --- Never commit tokens ---\n",
    "github_pat_*\n",
    "*.token\n",
    "*.key\n",
    "\n",
    "# Data (too large for Git)\n",
    "*.csv\n",
    "*.xlsx\n",
    "*.xls\n",
    "data/raw/\n",
    "data/processed/\n",
    "\n",
    "# Data & outputs (keep folders, ignore large files)\n",
    "data/raw/*\n",
    "data/processed/*\n",
    "!data/.gitkeep\n",
    "outputs/*\n",
    "!outputs/figures/.gitkeep\n",
    "!outputs/reports/.gitkeep\n",
    "\n",
    "# Models (too large)\n",
    "*.pkl\n",
    "*.h5\n",
    "*.joblib\n",
    "\n",
    "# Byte-compiled / cache\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*.ipynb_checkpoints/\n",
    "\n",
    "# Environments\n",
    ".venv/\n",
    ".env\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "\n",
    "# Secrets\n",
    "*.env\n",
    "config/*.secrets.*\n",
    "\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "venv/\n",
    "env/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "*.ipynb_checkpoints\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    " \n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "EOF\n",
    "```\n",
    "\n",
    "\n",
    "> Q: How do I edit the .gitignore and remove the Level_3 ignore rule?\n",
    "\n",
    "I have 2 .gitignore's\n",
    "\n",
    "```yaml\n",
    "# -` ./Users/b/DATA/PROJECTS/Telco/.gitignore`\n",
    "cat > .gitignore << 'EOF'\n",
    "# ALLOW LIST\n",
    "# --- Default: ignore everything ---\n",
    "*\n",
    "\n",
    "# --- Allow these at repo root ---\n",
    "!.gitignore\n",
    "!README.md\n",
    "!setup_env.sh\n",
    "!requirements.txt\n",
    "\n",
    "# --- Allow Level_0 (recursively) ---\n",
    "!Level_0/\n",
    "!Level_0/**\n",
    "\n",
    "!Level_0/README.md\n",
    "!Level_0/01_EDA/\n",
    "!Level_0/01_EDA/**\n",
    "\n",
    "# --- Allow Level_1 (recursively) ---\n",
    "# !Level_1/\n",
    "# !Level_1/**\n",
    "\n",
    "# --- Allow Level_2 (recursively) ---\n",
    "# !Level_2/\n",
    "# !Level_2/**\n",
    "\n",
    "# # --- Allow ALL Level_3 (recursively) ---\n",
    "# !Level_3/\n",
    "# !Level_3/**\n",
    "\n",
    "# --- Ignore everything in Level_3 ---\n",
    "# But Allow README.md and 01_EDA directory ---\n",
    "Level_3/*\n",
    "\n",
    "!Level_3/README.md\n",
    "!Level_3/01_EDA/\n",
    "!Level_3/01_EDA/**\n",
    "!Level_3/resources/_assets/churn.png\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# --- Safety: ignore heavy/local anywhere ---#\n",
    "##############################################\n",
    "**/.venv/\n",
    "**/.env\n",
    "**/__pycache__/\n",
    "**/.ipynb_checkpoints/\n",
    "**/data/\n",
    "**/outputs/\n",
    "**/logs/\n",
    "**/.cache/\n",
    "**/*.csv\n",
    "**/*.tsv\n",
    "**/*.zip\n",
    "**/*.parquet\n",
    "**/*.pkl\n",
    "**/*.h5\n",
    ".vscode/\n",
    ".DS_Store\n",
    "\n",
    "# --- Never commit tokens ---\n",
    "github_pat_*\n",
    "*.token\n",
    "*.key\n",
    "\n",
    "# Data (too large for Git)\n",
    "*.csv\n",
    "*.xlsx\n",
    "*.xls\n",
    "data/raw/\n",
    "data/processed/\n",
    "\n",
    "# Data & outputs (keep folders, ignore large files)\n",
    "data/raw/*\n",
    "data/processed/*\n",
    "!data/.gitkeep\n",
    "outputs/*\n",
    "!outputs/figures/.gitkeep\n",
    "!outputs/reports/.gitkeep\n",
    "\n",
    "# Models (too large)\n",
    "*.pkl\n",
    "*.h5\n",
    "*.joblib\n",
    "\n",
    "# Byte-compiled / cache\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*.ipynb_checkpoints/\n",
    "\n",
    "# Environments\n",
    ".venv/\n",
    ".env\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "\n",
    "# Secrets\n",
    "*.env\n",
    "config/*.secrets.*\n",
    "\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "venv/\n",
    "env/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "*.ipynb_checkpoints\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "EOF\n",
    "\n",
    "#2) `.gitignore` \n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    "\n",
    "# Virtual env\n",
    ".venv/\n",
    "\n",
    "# Logs and outputs\n",
    "logs/\n",
    "figures/\n",
    "*.log\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "\n",
    "# data/*.csv   # optional if large\n",
    "\n",
    "# Logs\n",
    "logs/\n",
    "*.log\n",
    "\n",
    "# Data\n",
    "data/\n",
    "*.csv\n",
    "\n",
    "# Jupyter\n",
    "*.ipynbvenv/\n",
    "venv/\n",
    "data/\n",
    "logs/\n",
    "```\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Good catch past-you with that allow-list pattern‚Ä¶ and also: rude of past-you to block Level_3 üòÑ\n",
    "\n",
    "You‚Äôre very close. The TL;DR is:\n",
    "\n",
    "> **Root `.gitignore` is ignoring `Level_3/*`**.\n",
    "> You need to either **allow all of Level_3** or at least **unignore `Level_3/notebooks/**`**.\n",
    "\n",
    "Let‚Äôs fix it cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Don‚Äôt re-run the `cat > .gitignore << 'EOF'` thing\n",
    "\n",
    "That snippet you pasted:\n",
    "\n",
    "```bash\n",
    "cat > .gitignore << 'EOF'\n",
    "...\n",
    "EOF\n",
    "```\n",
    "\n",
    "was just how you CREATED the file earlier.\n",
    "Don‚Äôt re-run that now ‚Äî it would overwrite any edits.\n",
    "\n",
    "We‚Äôre going to **edit the existing `.gitignore` in-place** instead.\n",
    "\n",
    "From repo root:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco\n",
    "nano .gitignore\n",
    "```\n",
    "\n",
    "(or use VS Code / Windsurf / whatever editor you like)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The important part of your root `.gitignore`\n",
    "\n",
    "You currently have this block (simplified):\n",
    "\n",
    "```gitignore\n",
    "# --- Default: ignore everything ---\n",
    "*\n",
    "\n",
    "...\n",
    "\n",
    "# # --- Allow ALL Level_3 (recursively) ---\n",
    "# !Level_3/\n",
    "# !Level_3/**\n",
    "\n",
    "# --- Ignore everything in Level_3 ---\n",
    "# But Allow README.md and 01_EDA directory ---\n",
    "Level_3/*\n",
    "\n",
    "!Level_3/README.md\n",
    "!Level_3/01_EDA/\n",
    "!Level_3/01_EDA/**\n",
    "!Level_3/resources/_assets/churn.png\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "* `*` at the top = **ignore everything by default**\n",
    "* You then **explicitly unignore** certain paths with `!something/`\n",
    "* You have `Level_3/*` ‚Üí ‚Äúignore *everything* in Level_3‚Äù\n",
    "* You only unignore:\n",
    "\n",
    "  * `Level_3/README.md`\n",
    "  * `Level_3/01_EDA/` and its contents\n",
    "  * `Level_3/resources/_assets/churn.png`\n",
    "\n",
    "But your notebook is at:\n",
    "\n",
    "```text\n",
    "Level_3/notebooks/01_EDA.ipynb\n",
    "```\n",
    "\n",
    "That is **not** under `Level_3/01_EDA/`, so it‚Äôs still ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Option A (recommended): allow the whole `Level_3` tree\n",
    "\n",
    "Since Level_3 is a real, active project, the cleanest move is to **track all of it** and let your generic ignore rules handle big files.\n",
    "\n",
    "In `.gitignore`, change this part:\n",
    "\n",
    "```gitignore\n",
    "# # --- Allow ALL Level_3 (recursively) ---\n",
    "# !Level_3/\n",
    "# !Level_3/**\n",
    "\n",
    "# --- Ignore everything in Level_3 ---\n",
    "# But Allow README.md and 01_EDA directory ---\n",
    "Level_3/*\n",
    "\n",
    "!Level_3/README.md\n",
    "!Level_3/01_EDA/\n",
    "!Level_3/01_EDA/**\n",
    "!Level_3/resources/_assets/churn.png\n",
    "```\n",
    "\n",
    "to this:\n",
    "\n",
    "```gitignore\n",
    "# --- Allow ALL Level_3 (recursively) ---\n",
    "!Level_3/\n",
    "!Level_3/**\n",
    "\n",
    "# (Old per-file Level_3 rules no longer needed)\n",
    "# Level_3/*\n",
    "# !Level_3/README.md\n",
    "# !Level_3/01_EDA/\n",
    "# !Level_3/01_EDA/**\n",
    "# !Level_3/resources/_assets/churn.png\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "1. **Uncomment** the `!Level_3/` and `!Level_3/**` lines\n",
    "2. **Comment out or delete** the `Level_3/*` and specific `!Level_3/...` lines\n",
    "\n",
    "This means:\n",
    "\n",
    "* `*` says: ignore everything\n",
    "* `!Level_3/` and `!Level_3/**` say: **except everything under Level_3** ‚Äì track it all\n",
    "\n",
    "Your ‚Äúsafety‚Äù patterns at the bottom (`**/*.csv`, `**/.ipynb_checkpoints/`, etc.) will still prevent big/annoying stuff from being committed, including inside Level_3.\n",
    "\n",
    "Save and close `.gitignore`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Stage and commit your notebook (and `.gitignore` change)\n",
    "\n",
    "Back in the repo root:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco\n",
    "\n",
    "# See what changed\n",
    "git status\n",
    "\n",
    "# Add the updated .gitignore\n",
    "git add .gitignore\n",
    "\n",
    "# Add your notebook now that it's no longer ignored\n",
    "git add Level_3/notebooks/01_EDA.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"Allow Level_3 in git and restore 01_EDA notebook\"\n",
    "\n",
    "# Push to GitHub\n",
    "git push\n",
    "```\n",
    "\n",
    "Now `Telco/Level_3/notebooks/01_EDA.ipynb` should show up on GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. What about that second `.gitignore`?\n",
    "\n",
    "The ‚Äú#2) .gitignore‚Äù you pasted (with things like `__pycache__/`, `.venv/`, etc.) is probably:\n",
    "\n",
    "* Either in a subdirectory (e.g. `Level_3/.gitignore`), or\n",
    "* What you *intended* as a generic `.gitignore` originally\n",
    "\n",
    "It‚Äôs fine to have a nested `.gitignore`; Git applies the root one, then refines rules with deeper ones. The main culprit was the **root** allow-list `.gitignore` ignoring Level_3.\n",
    "\n",
    "Once you fix the root one as above, the second .gitignore‚Äôs patterns (caches, env, logs, etc.) will just do their job.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, you can paste the *path* to that second `.gitignore` (e.g. `Level_3/.gitignore` or `Level_3/notebooks/.gitignore`), and I can suggest a super-clean layout where:\n",
    "\n",
    "* Root `.gitignore` handles global patterns & ‚Äúallow list by level‚Äù\n",
    "* Nested `.gitignore` handles level-specific data/artifacts ignores\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "ME:\n",
    "/Users/b/DATA/PROJECTS/Telco/Level_3/.gitignore\n",
    "\n",
    "Nice, thanks for confirming that path. So:\n",
    "\n",
    "* **Root `.gitignore`**: `/Users/b/DATA/PROJECTS/Telco/.gitignore`\n",
    "* **Level-specific `.gitignore`**: `/Users/b/DATA/PROJECTS/Telco/Level_3/.gitignore`\n",
    "\n",
    "Only the **root** one is blocking `Level_3` right now.\n",
    "The `Level_3/.gitignore` doesn‚Äôt ignore `*.ipynb`, so your notebook is fine there.\n",
    "\n",
    "Let‚Äôs fix this cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Edit the **root** `.gitignore` to allow `Level_3`\n",
    "\n",
    "From your Telco repo root:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco\n",
    "```\n",
    "\n",
    "Open `.gitignore` in an editor (Windsurf/VS Code/nano, whatever you like). In terminal with nano:\n",
    "\n",
    "```bash\n",
    "nano .gitignore\n",
    "```\n",
    "\n",
    "Find this block (you pasted it earlier):\n",
    "\n",
    "```gitignore\n",
    "# # --- Allow ALL Level_3 (recursively) ---\n",
    "# !Level_3/\n",
    "# !Level_3/**\n",
    "\n",
    "# --- Ignore everything in Level_3 ---\n",
    "# But Allow README.md and 01_EDA directory ---\n",
    "Level_3/*\n",
    "\n",
    "!Level_3/README.md\n",
    "!Level_3/01_EDA/\n",
    "!Level_3/01_EDA/**\n",
    "!Level_3/resources/_assets/churn.png\n",
    "```\n",
    "\n",
    "### Change it to this:\n",
    "\n",
    "```gitignore\n",
    "# --- Allow ALL Level_3 (recursively) ---\n",
    "!Level_3/\n",
    "!Level_3/**\n",
    "\n",
    "# (Old per-file Level_3 rules no longer needed)\n",
    "# Level_3/*\n",
    "# !Level_3/README.md\n",
    "# !Level_3/01_EDA/\n",
    "# !Level_3/01_EDA/**\n",
    "# !Level_3/resources/_assets/churn.png\n",
    "```\n",
    "\n",
    "So you:\n",
    "\n",
    "* **Uncomment** the `!Level_3/` and `!Level_3/**` lines\n",
    "* **Comment out or delete** the `Level_3/*` line and the specific exceptions\n",
    "\n",
    "This makes Git track **everything under `Level_3/`**, except stuff ignored by your generic patterns (e.g. `**/*.csv`, `.venv`, etc.).\n",
    "\n",
    "Save and exit (`Ctrl+O`, `Enter`, `Ctrl+X` in nano).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Confirm `Level_3/.gitignore` is not blocking notebooks\n",
    "\n",
    "Your `Level_3/.gitignore` contains things like:\n",
    "\n",
    "```gitignore\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".venv/\n",
    "logs/\n",
    "figures/\n",
    "*.log\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "data/\n",
    "*.csv\n",
    "venv/\n",
    "```\n",
    "\n",
    "There is **no `*.ipynb` rule**, so `Level_3/notebooks/01_EDA.ipynb` is **not** being ignored here. Good.\n",
    "\n",
    "(There‚Äôs a weird `*.ipynbvenv/` at the end, but that doesn‚Äôt match `*.ipynb`, so it doesn‚Äôt affect your notebook.)\n",
    "\n",
    "You don‚Äôt *have* to change Level_3/.gitignore right now for this to work.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Add the notebook + updated `.gitignore`, commit, push\n",
    "\n",
    "Back in the repo root:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco\n",
    "\n",
    "# See status\n",
    "git status\n",
    "```\n",
    "\n",
    "Now add:\n",
    "\n",
    "```bash\n",
    "# Add the updated root .gitignore\n",
    "git add .gitignore\n",
    "\n",
    "# Add your notebook now that Level_3 is no longer ignored\n",
    "git add Level_3/notebooks/01_EDA.ipynb\n",
    "\n",
    "# Commit\n",
    "git commit -m \"Allow Level_3 in git and restore 01_EDA notebook\"\n",
    "\n",
    "# Push to GitHub\n",
    "git push\n",
    "```\n",
    "\n",
    "After that, you should see:\n",
    "\n",
    "```text\n",
    "Telco/Level_3/notebooks/01_EDA.ipynb\n",
    "```\n",
    "\n",
    "on GitHub again.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "> If you want, next step I can help you tidy the `Level_3/.gitignore` into a very clean ‚Äúignore only junk + data, keep code and notebooks‚Äù version, but the critical blocker for getting 01_EDA back is just that `Level_3/*` line in the **root** `.gitignore`.\n",
    "\n",
    "yes please\n",
    "\n",
    "Nice, let‚Äôs clean this up. Right now your `Level_3/.gitignore` is a bit duplicated and slightly messy, but the good news is: nothing in it is blocking notebooks.\n",
    "\n",
    "You‚Äôve got two good options:\n",
    "\n",
    "---\n",
    "\n",
    "## Option 1 (simplest): delete `Level_3/.gitignore`\n",
    "\n",
    "Because your **root** `.gitignore` is already aggressive (allow-list + global ignores like `**/.venv/`, `**/.ipynb_checkpoints/`, `**/*.csv`, etc.), you don‚Äôt *need* a second one here at all.\n",
    "\n",
    "If you want max simplicity:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "rm .gitignore\n",
    "```\n",
    "\n",
    "Git will then only use the root rules, and Level_3 will behave like the rest of the repo.\n",
    "\n",
    "If you like that idea, you can stop here.\n",
    "If you *do* want a Level_3-specific `.gitignore` (e.g., to keep local logs/data out even if root changes), use Option 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Option 2: keep `Level_3/.gitignore`, but make it clean\n",
    "\n",
    "Let‚Äôs rewrite `/Users/b/DATA/PROJECTS/Telco/Level_3/.gitignore` to:\n",
    "\n",
    "* Ignore only junk / env / logs / data\n",
    "* **Not** ignore `.ipynb`\n",
    "* Avoid duplicates and typos\n",
    "\n",
    "Suggested content:\n",
    "\n",
    "```gitignore\n",
    "# Level_3/.gitignore\n",
    "# Ignore local junk, envs, data, and logs ‚Äî keep code and notebooks.\n",
    "\n",
    "###########################\n",
    "# Python bytecode / cache #\n",
    "###########################\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "\n",
    "###################\n",
    "# Virtual envs    #\n",
    "###################\n",
    ".venv/\n",
    "venv/\n",
    "\n",
    "###################\n",
    "# Logs & figures  #\n",
    "###################\n",
    "logs/\n",
    "*.log\n",
    "figures/\n",
    "\n",
    "###################\n",
    "# Data            #\n",
    "###################\n",
    "data/\n",
    "# (Raw / processed data for this level should not be committed)\n",
    "\n",
    "###################\n",
    "# Jupyter         #\n",
    "###################\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "###################\n",
    "# OS / editor     #\n",
    "###################\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "```\n",
    "\n",
    "How to apply it:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco/Level_3\n",
    "nano .gitignore\n",
    "```\n",
    "\n",
    "Paste the cleaned version above, save, exit.\n",
    "\n",
    "Then from repo root:\n",
    "\n",
    "```bash\n",
    "cd /Users/b/DATA/PROJECTS/Telco\n",
    "git status\n",
    "git add Level_3/.gitignore\n",
    "git commit -m \"Tidy Level_3 .gitignore to ignore junk but keep notebooks\"\n",
    "git push\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Which should you choose?\n",
    "\n",
    "* If you want **one source of truth** for ignores ‚Üí **Option 1** (delete Level_3/.gitignore).\n",
    "* If you like the idea that each level can have its own ‚Äúsafety net‚Äù for data/logs ‚Üí **Option 2**.\n",
    "\n",
    "Either way, your `Level_3/notebooks/01_EDA.ipynb` will be safe and trackable now that the root `.gitignore` no longer ignores `Level_3/*`.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aec320",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "FIX_ID: 120125_1111 | 2.3.8 | DQ Rule Catalog\n",
    "</summary>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "ERROR\n",
    "</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "# üìö DQ rule catalog (joined with numeric profile) | TODO: PLACE in 2.3.8?\n",
    "print(\"\\n üìö DQ rule catalog\")\n",
    "\n",
    "# Load rule confidence artifact\n",
    "rule_conf_path = NUMERIC_DIR / \"rule_confidence_scores.csv\"\n",
    "if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\n",
    "    rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "else:\n",
    "    rule_conf_df = pd.DataFrame()\n",
    "\n",
    "# Try to load numeric_profile_df (fallback if needed)\n",
    "numeric_profile_path = NUMERIC_DIR / \"numeric_profile_df.csv\"\n",
    "if numeric_profile_path.exists() and numeric_profile_path.stat().st_size > 0:\n",
    "    numeric_profile_df_238 = pd.read_csv(numeric_profile_path)\n",
    "else:\n",
    "    numeric_profile_df_238 = pd.DataFrame()\n",
    "\n",
    "if not rule_conf_df.empty and not numeric_profile_df_238.empty:\n",
    "    dq_rule_catalog_df = (\n",
    "        numeric_profile_df_238\n",
    "        .rename(columns={\"column\": \"feature\"})\n",
    "        .merge(rule_conf_df, on=\"feature\", how=\"left\")\n",
    "        .sort_values([\"feature\", \"rule_type\", \"rule_id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    dq_rule_catalog_df = rule_conf_df.copy()\n",
    "\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "tmp_238 = dq_rule_catalog_path.with_suffix(\".tmp.csv\")\n",
    "dq_rule_catalog_df.to_csv(tmp_238, index=False)\n",
    "os.replace(tmp_238, dq_rule_catalog_path)\n",
    "\n",
    "print(f\"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}\")\n",
    "if not dq_rule_catalog_df.empty:\n",
    "    print(\"\\nüìä 2.3.8 DQ rule catalog (head):\")\n",
    "    display(\n",
    "        dq_rule_catalog_df[\n",
    "            [\n",
    "                \"feature\",\n",
    "                \"role\" if \"role\" in dq_rule_catalog_df.columns else \"feature\",\n",
    "                \"rule_type\",\n",
    "                \"rule_id\",\n",
    "                \"confidence_score\",\n",
    "                \"hard_vs_soft\",\n",
    "            ]\n",
    "        ].head(30)\n",
    "    )\n",
    "else:\n",
    "    print(\"   (no rules to catalog)\")\n",
    "\n",
    "print(\"‚úÖ 2.3.8 complete.\")\n",
    "\n",
    "###\n",
    "# ‚ÄúDQ rules‚Äù tab in your report\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "dq_rules_df = pd.read_csv(dq_rule_catalog_path)\n",
    "\n",
    "# Example: one-row-per-feature view with max hard/soft confidence\n",
    "agg_rules_df = (\n",
    "    dq_rules_df\n",
    "    .groupby(\"feature\", as_index=False)\n",
    "    .agg(\n",
    "        n_rules=(\"rule_id\", \"nunique\"),\n",
    "        max_hard_conf=(\"confidence_score\", lambda s: s[dq_rules_df.loc[s.index, \"hard_vs_soft\"] == \"hard\"].max()),\n",
    "        max_soft_conf=(\"confidence_score\", lambda s: s[dq_rules_df.loc[s.index, \"hard_vs_soft\"] == \"soft\"].max()),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(agg_rules_df.head(20))\n",
    "```\n",
    "\n",
    "> # OUTPUT:\n",
    "\n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "EmptyDataError                            Traceback (most recent call last)\n",
    "Cell In[53], line 7\n",
    "      5 rule_conf_path = NUMERIC_DIR / \"rule_confidence_scores.csv\"\n",
    "      6 if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\n",
    "----> 7     rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "      8 else:\n",
    "      9     rule_conf_df = pd.DataFrame()\n",
    "\n",
    "File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n",
    "   1013 kwds_defaults = _refine_defaults_read(\n",
    "   1014     dialect,\n",
    "   1015     delimiter,\n",
    "   (...)   1022     dtype_backend=dtype_backend,\n",
    "   1023 )\n",
    "   1024 kwds.update(kwds_defaults)\n",
    "-> 1026 return _read(filepath_or_buffer, kwds)\n",
    "\n",
    "File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n",
    "    617 _validate_names(kwds.get(\"names\", None))\n",
    "    619 # Create the parser.\n",
    "--> 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n",
    "    622 if chunksize or iterator:\n",
    "    623     return parser\n",
    "\n",
    "File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n",
    "   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n",
    "   1619 self.handles: IOHandles | None = None\n",
    "-> 1620 self._engine = self._make_engine(f, self.engine)\n",
    "\n",
    "File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898, in TextFileReader._make_engine(self, f, engine)\n",
    "   1895     raise ValueError(msg)\n",
    "   1897 try:\n",
    "-> 1898     return mapping[engine](f, **self.options)\n",
    "   1899 except Exception:\n",
    "   1900     if self.handles is not None:\n",
    "\n",
    "File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93, in CParserWrapper.__init__(self, src, **kwds)\n",
    "     90 if kwds[\"dtype_backend\"] == \"pyarrow\":\n",
    "     91     # Fail here loudly instead of in cython after reading\n",
    "     92     import_optional_dependency(\"pyarrow\")\n",
    "---> 93 self._reader = parsers.TextReader(src, **kwds)\n",
    "     95 self.unnamed_cols = self._reader.unnamed_cols\n",
    "     97 # error: Cannot determine type of 'names'\n",
    "\n",
    "File pandas/_libs/parsers.pyx:581, in pandas._libs.parsers.TextReader.__cinit__()\n",
    "\n",
    "EmptyDataError: No columns to parse from file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "SOLUTION 1\n",
    "</summary>\n",
    "\n",
    "Right, the stack trace is actually super helpful here.\n",
    "\n",
    "The problem isn‚Äôt your *logic* ‚Äî it‚Äôs that `pd.read_csv()` is being called on a file that **technically exists, has size > 0, but has no parsable columns** (empty or just whitespace), which triggers:\n",
    "\n",
    "```text\n",
    "pandas.errors.EmptyDataError: No columns to parse from file\n",
    "```\n",
    "\n",
    "Your current guard:\n",
    "\n",
    "```python\n",
    "if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\n",
    "    rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "else:\n",
    "    rule_conf_df = pd.DataFrame()\n",
    "```\n",
    "\n",
    "only protects against:\n",
    "\n",
    "* file missing\n",
    "* file truly 0 bytes\n",
    "\n",
    "It does **not** protect against: ‚Äúfile has 1 newline or garbage but no columns.‚Äù\n",
    "\n",
    "Same story later here:\n",
    "\n",
    "```python\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "dq_rules_df = pd.read_csv(dq_rule_catalog_path)\n",
    "```\n",
    "\n",
    "If that file is empty / malformed ‚Üí same error.\n",
    "\n",
    "---\n",
    "\n",
    "## Fix: wrap the reads in try/except for `EmptyDataError`\n",
    "\n",
    "Here‚Äôs a hardened version of your whole block that:\n",
    "\n",
    "* gracefully handles empty / malformed CSVs\n",
    "* only runs the aggregation when the required columns exist\n",
    "* keeps your style (atomic writes, previews, etc.)\n",
    "\n",
    "Add this once near the top of the notebook (with other imports):\n",
    "\n",
    "```python\n",
    "from pandas.errors import EmptyDataError\n",
    "```\n",
    "\n",
    "Then replace your DQ rule catalog section with:\n",
    "\n",
    "```python\n",
    "# 2.3.8 üìö DQ rule catalog (joined with numeric profile)\n",
    "print(\"\\n2.3.8 üìö DQ rule catalog\")\n",
    "\n",
    "# --- 1) Load rule confidence artifact (safe) -------------------------------\n",
    "rule_conf_path = NUMERIC_DIR / \"rule_confidence_scores.csv\"\n",
    "\n",
    "try:\n",
    "    if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\n",
    "        rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "    else:\n",
    "        rule_conf_df = pd.DataFrame()\n",
    "except EmptyDataError:\n",
    "    print(f\"‚ö†Ô∏è {rule_conf_path} is empty or has no columns. Treating as no rules.\")\n",
    "    rule_conf_df = pd.DataFrame()\n",
    "\n",
    "# --- 2) Load numeric profile (safe) ---------------------------------------\n",
    "numeric_profile_path = NUMERIC_DIR / \"numeric_profile_df.csv\"\n",
    "\n",
    "try:\n",
    "    if numeric_profile_path.exists() and numeric_profile_path.stat().st_size > 0:\n",
    "        numeric_profile_df_238 = pd.read_csv(numeric_profile_path)\n",
    "    else:\n",
    "        numeric_profile_df_238 = pd.DataFrame()\n",
    "except EmptyDataError:\n",
    "    print(f\"‚ö†Ô∏è {numeric_profile_path} is empty or has no columns. Skipping join.\")\n",
    "    numeric_profile_df_238 = pd.DataFrame()\n",
    "\n",
    "# --- 3) Build DQ rule catalog ---------------------------------------------\n",
    "if not rule_conf_df.empty and not numeric_profile_df_238.empty:\n",
    "    if \"column\" in numeric_profile_df_238.columns:\n",
    "        dq_rule_catalog_df = (\n",
    "            numeric_profile_df_238\n",
    "            .rename(columns={\"column\": \"feature\"})\n",
    "            .merge(rule_conf_df, on=\"feature\", how=\"left\")\n",
    "            .sort_values([\"feature\", \"rule_type\", \"rule_id\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è numeric_profile_df_238 missing 'column' col; using rule_conf_df only.\")\n",
    "        dq_rule_catalog_df = rule_conf_df.copy()\n",
    "else:\n",
    "    dq_rule_catalog_df = rule_conf_df.copy()\n",
    "\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "tmp_238 = dq_rule_catalog_path.with_suffix(\".tmp.csv\")\n",
    "dq_rule_catalog_df.to_csv(tmp_238, index=False)\n",
    "os.replace(tmp_238, dq_rule_catalog_path)\n",
    "\n",
    "print(f\"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}\")\n",
    "\n",
    "if not dq_rule_catalog_df.empty:\n",
    "    print(\"\\nüìä 2.3.8 DQ rule catalog (head):\")\n",
    "    cols_preview = [\n",
    "        \"feature\",\n",
    "        \"role\" if \"role\" in dq_rule_catalog_df.columns else \"feature\",\n",
    "        \"rule_type\",\n",
    "        \"rule_id\",\n",
    "        \"confidence_score\",\n",
    "        \"hard_vs_soft\",\n",
    "    ]\n",
    "    cols_preview = [c for c in cols_preview if c in dq_rule_catalog_df.columns]\n",
    "    display(dq_rule_catalog_df[cols_preview].head(30))\n",
    "else:\n",
    "    print(\"   (no rules to catalog)\")\n",
    "\n",
    "print(\"‚úÖ 2.3.8 complete.\")\n",
    "\n",
    "# --- 4) ‚ÄúDQ rules‚Äù tab in your report (aggregated view) -------------------\n",
    "dq_rules_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "\n",
    "try:\n",
    "    if dq_rules_path.exists() and dq_rules_path.stat().st_size > 0:\n",
    "        dq_rules_df = pd.read_csv(dq_rules_path)\n",
    "    else:\n",
    "        dq_rules_df = pd.DataFrame()\n",
    "except EmptyDataError:\n",
    "    print(f\"‚ö†Ô∏è {dq_rules_path} is empty or has no columns. Skipping aggregation.\")\n",
    "    dq_rules_df = pd.DataFrame()\n",
    "\n",
    "if not dq_rules_df.empty and {\"feature\", \"rule_id\", \"confidence_score\", \"hard_vs_soft\"}.issubset(dq_rules_df.columns):\n",
    "    agg_rules_df = (\n",
    "        dq_rules_df\n",
    "        .groupby(\"feature\", as_index=False)\n",
    "        .agg(\n",
    "            n_rules=(\"rule_id\", \"nunique\"),\n",
    "            max_hard_conf=(\"confidence_score\", lambda s: s[dq_rules_df.loc[s.index, \"hard_vs_soft\"] == \"hard\"].max()),\n",
    "            max_soft_conf=(\"confidence_score\", lambda s: s[dq_rules_df.loc[s.index, \"hard_vs_soft\"] == \"soft\"].max()),\n",
    "        )\n",
    "    )\n",
    "    print(\"\\nüìä Aggregated DQ rules per feature (head):\")\n",
    "    display(agg_rules_df.head(20))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough columns / data to build aggregated DQ rules view.\")\n",
    "```\n",
    "\n",
    "### What this gives you\n",
    "\n",
    "* No more `EmptyDataError` when a CSV is present but empty or junky.\n",
    "* You still get your **catalog CSV** + **preview** when there *is* data.\n",
    "* You get a nice ‚Äúaggregated DQ rules‚Äù view **only when** the necessary columns exist.\n",
    "\n",
    "If you want, we can also wrap this into a reusable helper like:\n",
    "\n",
    "```python\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame: ...\n",
    "```\n",
    "\n",
    "and use that everywhere in Section 2 so any half-baked artifact can‚Äôt blow up the run.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Did you remove anything?\n",
    "</summary>\n",
    "\n",
    "Short answer: no, I didn‚Äôt remove any of your *behavior*, I only wrapped it in padding so it can‚Äôt blow up on empty/bad CSVs and added a couple of defensive branches.\n",
    "\n",
    "Here‚Äôs what stayed the same vs what changed:\n",
    "\n",
    "### What stayed the same ‚úÖ\n",
    "\n",
    "All of this is still there:\n",
    "\n",
    "* You still:\n",
    "\n",
    "  * Load `rule_confidence_scores.csv` ‚Üí `rule_conf_df`\n",
    "  * Load `numeric_profile_df.csv` ‚Üí `numeric_profile_df_238`\n",
    "  * Build `dq_rule_catalog_df` by:\n",
    "\n",
    "    * renaming `column ‚Üí feature` on the numeric profile\n",
    "    * left-merging on `\"feature\"`\n",
    "    * sorting by `[\"feature\", \"rule_type\", \"rule_id\"]`\n",
    "  * Write `dq_rule_catalog.csv` atomically via temp file + `os.replace`\n",
    "  * Print:\n",
    "\n",
    "    * path it wrote to\n",
    "    * a `.head()` preview when there‚Äôs data\n",
    "  * Build an aggregated ‚ÄúDQ rules‚Äù view with:\n",
    "\n",
    "    * `n_rules` per feature\n",
    "    * `max_hard_conf`, `max_soft_conf`\n",
    "    * and display `agg_rules_df.head(20)`\n",
    "\n",
    "So your core idea ‚Äî ‚Äújoin profile + rule confidence, persist catalog, then create an aggregated summary view‚Äù ‚Äî is intact.\n",
    "\n",
    "### What I *added* üß©\n",
    "\n",
    "I added *defensive wrapping* and minor guards:\n",
    "\n",
    "1. **Safe CSV reading**\n",
    "   Instead of:\n",
    "\n",
    "   ```python\n",
    "   rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "   ```\n",
    "\n",
    "   I wrapped it:\n",
    "\n",
    "   ```python\n",
    "   try:\n",
    "       if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\n",
    "           rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "       else:\n",
    "           rule_conf_df = pd.DataFrame()\n",
    "   except EmptyDataError:\n",
    "       print(\"‚ö†Ô∏è ... is empty or has no columns. Treating as no rules.\")\n",
    "       rule_conf_df = pd.DataFrame()\n",
    "   ```\n",
    "\n",
    "   Same for `numeric_profile_df_238` and later for `dq_rules_df`.\n",
    "\n",
    "2. **Column-existence checks for the join / aggregation**\n",
    "\n",
    "   * Before, you assumed `numeric_profile_df_238` has a `\"column\"` column.\n",
    "     Now:\n",
    "\n",
    "     ```python\n",
    "     if \"column\" in numeric_profile_df_238.columns:\n",
    "         # do the rename + merge\n",
    "     else:\n",
    "         print(\"‚ö†Ô∏è numeric_profile_df_238 missing 'column' col; using rule_conf_df only.\")\n",
    "         dq_rule_catalog_df = rule_conf_df.copy()\n",
    "     ```\n",
    "\n",
    "   * For the aggregated view I added:\n",
    "\n",
    "     ```python\n",
    "     if not dq_rules_df.empty and {\"feature\", \"rule_id\", \"confidence_score\", \"hard_vs_soft\"}.issubset(dq_rules_df.columns):\n",
    "         # do the groupby/agg\n",
    "     else:\n",
    "         print(\"‚ö†Ô∏è Not enough columns / data to build aggregated DQ rules view.\")\n",
    "     ```\n",
    "\n",
    "   That way you don‚Äôt get mysterious KeyErrors when a file exists but isn‚Äôt in the expected shape.\n",
    "\n",
    "3. **Preview column selection is now safe**\n",
    "\n",
    "   Instead of blindly indexing:\n",
    "\n",
    "   ```python\n",
    "   dq_rule_catalog_df[\n",
    "       [\"feature\", \"role\", \"rule_type\", \"rule_id\", \"confidence_score\", \"hard_vs_soft\"]\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "   I compute `cols_preview` and filter it to columns that actually exist:\n",
    "\n",
    "   ```python\n",
    "   cols_preview = [\n",
    "       \"feature\",\n",
    "       \"role\" if \"role\" in dq_rule_catalog_df.columns else \"feature\",\n",
    "       \"rule_type\",\n",
    "       \"rule_id\",\n",
    "       \"confidence_score\",\n",
    "       \"hard_vs_soft\",\n",
    "   ]\n",
    "   cols_preview = [c for c in cols_preview if c in dq_rule_catalog_df.columns]\n",
    "   ```\n",
    "\n",
    "   So the preview never dies if one column is missing.\n",
    "\n",
    "### What I **didn‚Äôt** include (because it wasn‚Äôt in your snippet)\n",
    "\n",
    "* I didn‚Äôt add a `sec2_chunk_238` or `append_sec2` logging block ‚Äî your original snippet didn‚Äôt have a Section 2 diagnostics row for this cell, so I left that alone.\n",
    "* I didn‚Äôt touch any of your `NUMERIC_DIR` setup, `section2_nodes`, etc.\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Add a matching `sec2_chunk_238` + `log_section_completion(...)` + `append_sec2(...)` block so `2.3.8` shows up cleanly in your unified `section2_data_quality_*.csv`, **and**\n",
    "* Turn `safe_read_csv(path)` into a reusable helper in `telco_churn.utils.io` (or similar) so you can call it everywhere instead of repeating the try/except.\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "SOLUTION 2\n",
    "</summary>\n",
    "\n",
    "Short version: `EmptyDataError` is happening because the CSV file exists but has no parseable columns (empty file, or just newlines). Your size check (`stat().st_size > 0`) isn‚Äôt enough to guarantee that `read_csv` will work, so you need a **safe loader** that catches `EmptyDataError` and returns an empty DataFrame (with the right columns if you want), and you should also protect the later `groupby` from running on an empty DF.\n",
    "\n",
    "Below is a clean, ‚ÄúSection-2-grade‚Äù way to do it.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Add a safe CSV loader\n",
    "\n",
    "You already import `EmptyDataError` ‚Äì let‚Äôs actually use it and centralize the logic:\n",
    "\n",
    "```python\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "def safe_read_csv(path, *, expected_columns=None):\n",
    "    \"\"\"\n",
    "    Robust CSV loader:\n",
    "    - Returns empty DataFrame if file missing, empty, or unparsable.\n",
    "    - Optionally enforces a known column schema.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"‚ö†Ô∏è {path} does not exist; returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=expected_columns or None)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except EmptyDataError:\n",
    "        print(f\"‚ö†Ô∏è {path} exists but is empty / has no columns; returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=expected_columns or None)\n",
    "\n",
    "    # Optionally align columns to expected schema\n",
    "    if expected_columns is not None:\n",
    "        # Ensure all expected columns are present (others are kept)\n",
    "        for col in expected_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "        # You can also reorder if you want:\n",
    "        df = df[[c for c in expected_columns if c in df.columns] +\n",
    "                [c for c in df.columns if c not in expected_columns]]\n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Use the safe loader for your artifacts\n",
    "\n",
    "Replace your current `read_csv` calls with `safe_read_csv`.\n",
    "\n",
    "```python\n",
    "print(\"\\n üìö DQ rule catalog\")\n",
    "\n",
    "# --- 2.3.8.1 Load artifacts safely -----------------------------------------\n",
    "rule_conf_path = NUMERIC_DIR / \"rule_confidence_scores.csv\"\n",
    "numeric_profile_path = NUMERIC_DIR / \"numeric_profile_df.csv\"\n",
    "\n",
    "rule_conf_df = safe_read_csv(\n",
    "    rule_conf_path,\n",
    "    expected_columns=[\n",
    "        \"feature\",      # or \"column\" upstream, see rename below\n",
    "        \"rule_type\",\n",
    "        \"rule_id\",\n",
    "        \"confidence_score\",\n",
    "        \"hard_vs_soft\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "numeric_profile_df_238 = safe_read_csv(\n",
    "    numeric_profile_path,\n",
    "    expected_columns=[\n",
    "        \"column\",       # we rename ‚Üí \"feature\" in the merge\n",
    "        # any other profile columns you care about\n",
    "    ],\n",
    ")\n",
    "\n",
    "# --- 2.3.8.2 Build catalog if we have both pieces --------------------------\n",
    "if not rule_conf_df.empty and not numeric_profile_df_238.empty:\n",
    "    dq_rule_catalog_df = (\n",
    "        numeric_profile_df_238\n",
    "        .rename(columns={\"column\": \"feature\"})\n",
    "        .merge(rule_conf_df, on=\"feature\", how=\"left\")\n",
    "        .sort_values([\"feature\", \"rule_type\", \"rule_id\"], na_position=\"last\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    # Fall back: just whatever rule_conf_df has (maybe even still empty)\n",
    "    dq_rule_catalog_df = rule_conf_df.copy()\n",
    "\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "tmp_238 = dq_rule_catalog_path.with_suffix(\".tmp.csv\")\n",
    "dq_rule_catalog_df.to_csv(tmp_238, index=False)\n",
    "os.replace(tmp_238, dq_rule_catalog_path)\n",
    "\n",
    "print(f\"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}\")\n",
    "\n",
    "if not dq_rule_catalog_df.empty:\n",
    "    cols_to_show = [\n",
    "        \"feature\",\n",
    "        \"role\" if \"role\" in dq_rule_catalog_df.columns else \"feature\",\n",
    "        \"rule_type\",\n",
    "        \"rule_id\",\n",
    "        \"confidence_score\",\n",
    "        \"hard_vs_soft\",\n",
    "    ]\n",
    "    print(\"\\nüìä 2.3.8 DQ rule catalog (head):\")\n",
    "    display(dq_rule_catalog_df[cols_to_show].head(30))\n",
    "else:\n",
    "    print(\"   (no rules to catalog)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Safely build the aggregated ‚Äúone-row-per-feature‚Äù view\n",
    "\n",
    "Then, when you read the catalog back, also use the safe loader and guard the `groupby`:\n",
    "\n",
    "```python\n",
    "# ‚ÄúDQ rules‚Äù tab in your report\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "dq_rules_df = safe_read_csv(\n",
    "    dq_rule_catalog_path,\n",
    "    expected_columns=[\n",
    "        \"feature\",\n",
    "        \"rule_type\",\n",
    "        \"rule_id\",\n",
    "        \"confidence_score\",\n",
    "        \"hard_vs_soft\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "if dq_rules_df.empty:\n",
    "    print(\"   (no rules to aggregate for feature-level view)\")\n",
    "    agg_rules_df = pd.DataFrame()\n",
    "else:\n",
    "    # Clean groupby (no outer-scope df gymnastics)\n",
    "    def _agg_group(g):\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"n_rules\": g[\"rule_id\"].nunique(),\n",
    "                \"max_hard_conf\": g.loc[g[\"hard_vs_soft\"] == \"hard\", \"confidence_score\"].max(),\n",
    "                \"max_soft_conf\": g.loc[g[\"hard_vs_soft\"] == \"soft\", \"confidence_score\"].max(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    agg_rules_df = (\n",
    "        dq_rules_df\n",
    "        .groupby(\"feature\", as_index=False)\n",
    "        .apply(_agg_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    display(agg_rules_df.head(20))\n",
    "\n",
    "print(\"‚úÖ 2.3.8 complete.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Upstream ‚Äúbest practice‚Äù (optional but üíØ data-engineer-y)\n",
    "\n",
    "Two extra robustness moves you *can* add upstream so this class of problem basically disappears:\n",
    "\n",
    "1. **Don‚Äôt write truly empty artifacts**\n",
    "   In the step that creates `rule_confidence_scores.csv`, do something like:\n",
    "\n",
    "   ```python\n",
    "   if rule_conf_df.empty:\n",
    "       # Either delete old file or write a header-only schema\n",
    "       if rule_conf_path.exists():\n",
    "           rule_conf_path.unlink()\n",
    "   else:\n",
    "       rule_conf_df.to_csv(rule_conf_path.with_suffix(\".tmp.csv\"), index=False)\n",
    "       os.replace(rule_conf_path.with_suffix(\".tmp.csv\"), rule_conf_path)\n",
    "   ```\n",
    "\n",
    "2. **Or: always write header-only schema**\n",
    "   If you *want* the file to exist even when empty, write an empty DF with the right columns instead of a completely blank file. `read_csv` will then succeed and return an empty DF with those columns.\n",
    "\n",
    "   ```python\n",
    "   if rule_conf_df.empty:\n",
    "       schema_df = pd.DataFrame(\n",
    "           columns=[\"feature\", \"rule_type\", \"rule_id\", \"confidence_score\", \"hard_vs_soft\"]\n",
    "       )\n",
    "       schema_df.to_csv(rule_conf_path, index=False)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "* Your `stat().st_size > 0` guard isn‚Äôt enough; you need to **catch `EmptyDataError`**.\n",
    "* Implement a `safe_read_csv` helper and use it consistently for all Section-2 artifacts.\n",
    "* Guard `groupby`/aggregations with `if df.empty` so they don‚Äôt blow up when there‚Äôs nothing to aggregate.\n",
    "* (Nice-to-have) Standardize upstream artifact writing so empty files are either removed or written with headers only.\n",
    "\n",
    "If you paste your current ‚Äúrule confidence writer‚Äù snippet, I can wire the upstream part to match your exact artifact contracts.\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Can this be fixed without using a function?\n",
    "</summary>\n",
    "\n",
    "Yep, totally fixable without a helper function üëç.\n",
    "You just need:\n",
    "\n",
    "1. `try/except EmptyDataError` around `read_csv`\n",
    "2. When you *do* create an empty DataFrame, give it the expected columns so later `to_csv` writes a header (so it‚Äôs still parseable)\n",
    "3. Guard the `groupby` with an `if not df.empty`\n",
    "\n",
    "Here‚Äôs a drop-in rewrite of your cell, no functions, just inline logic.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Build + write the DQ rule catalog safely\n",
    "\n",
    "```python\n",
    "print(\"\\n üìö DQ rule catalog\")\n",
    "\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "# Define the canonical rule columns so that empty DFs still have a schema\n",
    "RULE_COLS = [\"feature\", \"rule_type\", \"rule_id\", \"confidence_score\", \"hard_vs_soft\"]\n",
    "\n",
    "# --- Load rule_confidence_scores.csv safely -------------------------------\n",
    "rule_conf_path = NUMERIC_DIR / \"rule_confidence_scores.csv\"\n",
    "\n",
    "if rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\n",
    "    try:\n",
    "        rule_conf_df = pd.read_csv(rule_conf_path)\n",
    "    except EmptyDataError:\n",
    "        print(f\"‚ö†Ô∏è {rule_conf_path} exists but is empty / unparsable. Treating as no rules.\")\n",
    "        rule_conf_df = pd.DataFrame(columns=RULE_COLS)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {rule_conf_path} missing or zero bytes. Treating as no rules.\")\n",
    "    rule_conf_df = pd.DataFrame(columns=RULE_COLS)\n",
    "\n",
    "# Ensure rule_conf_df at least has the expected columns\n",
    "for col in RULE_COLS:\n",
    "    if col not in rule_conf_df.columns:\n",
    "        rule_conf_df[col] = pd.NA\n",
    "\n",
    "# --- Load numeric_profile_df safely ---------------------------------------\n",
    "numeric_profile_path = NUMERIC_DIR / \"numeric_profile_df.csv\"\n",
    "\n",
    "if numeric_profile_path.exists() and numeric_profile_path.stat().st_size > 0:\n",
    "    try:\n",
    "        numeric_profile_df_238 = pd.read_csv(numeric_profile_path)\n",
    "    except EmptyDataError:\n",
    "        print(f\"‚ö†Ô∏è {numeric_profile_path} exists but is empty / unparsable. No profile info.\")\n",
    "        numeric_profile_df_238 = pd.DataFrame(columns=[\"column\"])\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {numeric_profile_path} missing or zero bytes. No profile info.\")\n",
    "    numeric_profile_df_238 = pd.DataFrame(columns=[\"column\"])\n",
    "\n",
    "# --- Build the catalog ----------------------------------------------------\n",
    "if not rule_conf_df.empty and not numeric_profile_df_238.empty:\n",
    "    dq_rule_catalog_df = (\n",
    "        numeric_profile_df_238\n",
    "        .rename(columns={\"column\": \"feature\"})\n",
    "        .merge(rule_conf_df, on=\"feature\", how=\"left\")\n",
    "        .sort_values([\"feature\", \"rule_type\", \"rule_id\"], na_position=\"last\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    # Fall back to rules only (may be empty but has schema)\n",
    "    dq_rule_catalog_df = rule_conf_df.copy()\n",
    "\n",
    "# If we somehow ended up entirely column-less, enforce schema before writing\n",
    "if dq_rule_catalog_df.columns.size == 0:\n",
    "    dq_rule_catalog_df = pd.DataFrame(columns=RULE_COLS)\n",
    "\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "tmp_238 = dq_rule_catalog_path.with_suffix(\".tmp.csv\")\n",
    "dq_rule_catalog_df.to_csv(tmp_238, index=False)\n",
    "os.replace(tmp_238, dq_rule_catalog_path)\n",
    "\n",
    "print(f\"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}\")\n",
    "\n",
    "if not dq_rule_catalog_df.empty:\n",
    "    cols_to_show = [\n",
    "        \"feature\",\n",
    "        \"role\" if \"role\" in dq_rule_catalog_df.columns else \"feature\",\n",
    "        \"rule_type\",\n",
    "        \"rule_id\",\n",
    "        \"confidence_score\",\n",
    "        \"hard_vs_soft\",\n",
    "    ]\n",
    "    print(\"\\nüìä 2.3.8 DQ rule catalog (head):\")\n",
    "    display(dq_rule_catalog_df[cols_to_show].head(30))\n",
    "else:\n",
    "    print(\"   (no rules to catalog)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Safe read + aggregate view for the ‚ÄúDQ rules‚Äù tab\n",
    "\n",
    "```python\n",
    "# ‚ÄúDQ rules‚Äù tab in your report\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "\n",
    "if dq_rule_catalog_path.exists() and dq_rule_catalog_path.stat().st_size > 0:\n",
    "    try:\n",
    "        dq_rules_df = pd.read_csv(dq_rule_catalog_path)\n",
    "    except EmptyDataError:\n",
    "        print(f\"‚ö†Ô∏è {dq_rule_catalog_path} exists but is empty / unparsable. No rules to aggregate.\")\n",
    "        dq_rules_df = pd.DataFrame(columns=RULE_COLS)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {dq_rule_catalog_path} missing or zero bytes. No rules to aggregate.\")\n",
    "    dq_rules_df = pd.DataFrame(columns=RULE_COLS)\n",
    "\n",
    "# Ensure schema\n",
    "for col in RULE_COLS:\n",
    "    if col not in dq_rules_df.columns:\n",
    "        dq_rules_df[col] = pd.NA\n",
    "\n",
    "if dq_rules_df.empty:\n",
    "    print(\"   (no rules to aggregate for feature-level view)\")\n",
    "    agg_rules_df = pd.DataFrame()\n",
    "else:\n",
    "    # One row per feature, clean groupby\n",
    "    agg_rules_df = (\n",
    "        dq_rules_df\n",
    "        .groupby(\"feature\", as_index=False)\n",
    "        .agg(\n",
    "            n_rules=(\"rule_id\", \"nunique\"),\n",
    "            max_hard_conf=(\n",
    "                \"confidence_score\",\n",
    "                lambda s: s[dq_rules_df.loc[s.index, \"hard_vs_soft\"] == \"hard\"].max()\n",
    "            ),\n",
    "            max_soft_conf=(\n",
    "                \"confidence_score\",\n",
    "                lambda s: s[dq_rules_df.loc[s.index, \"hard_vs_soft\"] == \"soft\"].max()\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    display(agg_rules_df.head(20))\n",
    "\n",
    "print(\"‚úÖ 2.3.8 complete.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üí°üí° **Key idea going forward (no functions needed):**\n",
    "\n",
    "* Every time you `read_csv` on a pipeline artifact, wrap it with:\n",
    "\n",
    "  * `if path.exists() and path.stat().st_size > 0:`\n",
    "  * `try: df = pd.read_csv(path) except EmptyDataError: df = pd.DataFrame(columns=[...])`\n",
    "* Any time you *create* an empty artifact, make sure it has a column schema so the written CSV has a header line, not a truly blank file.\n",
    "\n",
    "If you want, I can do the same pattern for your other Section 2 artifacts (profiles, outliers, drift, etc.) so they all behave consistently.\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Should I place the DQ rule catalog in 2.3.8 'Model Readiness Impact Summary'\n",
    "</summary>\n",
    "\n",
    "Short answer: I‚Äôd *keep the DQ rule catalog as its own mini-step* (2.3.7.x) and let 2.3.8 *consume* it, rather than burying it inside ‚ÄúModel readiness impact summary‚Äù.\n",
    "\n",
    "Here‚Äôs why and how.\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptually: what each piece ‚Äúis‚Äù\n",
    "\n",
    "* **DQ rule catalog (your first block):**\n",
    "\n",
    "  * Artifact: `dq_rule_catalog.csv`\n",
    "  * Grain: *one row per (feature, rule)*\n",
    "  * Purpose: human and ops visibility on *what rules exist per feature*, with scores, types, hard vs soft, etc.\n",
    "  * This is a **dictionary / catalog** output.\n",
    "\n",
    "* **2.3.8 Model readiness impact summary (second big block):**\n",
    "\n",
    "  * Artifact: `model_readiness_report.csv`\n",
    "  * Grain: *one row per feature*\n",
    "  * Purpose: aggregate *all numeric quality signals* (nulls, range, outliers, rule confidence) into a **readiness_score** and ‚Äúpct_rows_impacted‚Äù.\n",
    "\n",
    "Those are different levels:\n",
    "\n",
    "* catalog = detailed rules\n",
    "* readiness = per-feature score\n",
    "\n",
    "So architecturally:\n",
    "\n",
    "* The catalog belongs with the **rule-scoring & integrity diagnostics** (2.3.7.x / early 2.3.8).\n",
    "* The readiness summary is a **downstream consumer** of those diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "## What I‚Äôd do concretely\n",
    "\n",
    "### 1. Treat the catalog as ‚Äú2.3.8.A DQ rule catalog‚Äù\n",
    "\n",
    "At the very top of this cell, before the `print(\"\\n2.3.8 üßÆ Model readiness impact summary\")`, I‚Äôd:\n",
    "\n",
    "* Keep this block, but label it clearly as a sub-step:\n",
    "\n",
    "```python\n",
    "# 2.3.8.A üìö DQ rule catalog (joined with numeric profile)\n",
    "print(\"\\n2.3.8.A üìö DQ rule catalog\")\n",
    "\n",
    "...\n",
    "dq_rule_catalog_path = NUMERIC_DIR / \"dq_rule_catalog.csv\"\n",
    "...\n",
    "print(f\"üíæ Wrote DQ rule catalog ‚Üí {dq_rule_catalog_path}\")\n",
    "...\n",
    "```\n",
    "\n",
    "So in your execution map you‚Äôll list:\n",
    "\n",
    "* **2.3.8** ‚Äì *Model readiness impact summary*\n",
    "* With an internal sub-step 2.3.8.A for the catalog.\n",
    "\n",
    "Or, if you want finer granularity:\n",
    "\n",
    "* Rename it to **2.3.7.4 DQ rule catalog**, and have 2.3.8 depend on 2.3.7.4. That‚Äôs even cleaner from a DAG perspective.\n",
    "\n",
    "### 2. Let 2.3.8 *consume* either `rule_conf_df` or `dq_rule_catalog_df`\n",
    "\n",
    "Right now, your readiness block recomputes everything from:\n",
    "\n",
    "```python\n",
    "rule_conf_path_238      = NUMERIC_DIR / \"rule_confidence_scores.csv\"\n",
    "rule_conf_df_238 = pd.read_csv(rule_conf_path_238)\n",
    "...\n",
    "# groupby feature, compute n_hard_rules, avg_confidence, etc.\n",
    "```\n",
    "\n",
    "You‚Äôve got two options:\n",
    "\n",
    "* **Option A (simple):**\n",
    "  Keep readiness built directly from `rule_conf_df_238` (what you‚Äôre doing now). The catalog is ‚Äúextra‚Äù for humans & BI tools. This is totally fine.\n",
    "\n",
    "* **Option B (tighter coupling):**\n",
    "  Build `agg_rule_conf_238` from `dq_rule_catalog_df` instead of directly from `rule_conf_df_238`. This shows a clear ‚Äúsingle source of truth‚Äù for rule data.\n",
    "\n",
    "Either way, the catalog still *belongs* to the ‚Äúrule diagnostics‚Äù layer, not the ‚Äúfinal readiness score‚Äù layer.\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR recommendation\n",
    "\n",
    "* **Don‚Äôt** make ‚ÄúDQ rule catalog‚Äù *the same* thing as 2.3.8 ‚ÄúModel readiness impact summary‚Äù.\n",
    "* **Do**:\n",
    "\n",
    "  * Keep the catalog as a sub-step right before 2.3.8 (2.3.7.x or 2.3.8.A).\n",
    "  * Have 2.3.8 *read* `rule_confidence_scores.csv` (or `dq_rule_catalog.csv`) and turn it into readiness scores.\n",
    "  * Mention in your 2.3.8 summary `detail` field something like:\n",
    "    `\"detail\": \"model_readiness_report.csv; dq_rule_catalog.csv\"`\n",
    "\n",
    "If you want, I can rewrite the top of that cell to:\n",
    "\n",
    "* add a clean `2.3.8.A` catalog summary row for `append_sec2`\n",
    "* and adjust your execution map comments so it shows up nicely as ‚Äú2.3.8.A ‚Üí 2.3.8 ‚Üí 2.3.9 ‚Üí 2.3.10‚Äù.\n",
    "\n",
    "</details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c47835",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "FIXID: 120225_1131\n",
    "</summary>\n",
    "\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00cfaa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "2025-11-28_dq-rule-catalog-edge-case.md\n",
    "</summary>\n",
    "\n",
    "Should I document the fix that we just did for the data quality summary/ edge case issues? how would you recommend documenting fixes like this and other errors/fixes going forward?\n",
    "\n",
    "\n",
    "**Yes ‚Äî you absolutely should document the fix.**\n",
    "Not because you‚Äôll forget *how* you fixed it (you might), but because this is already functioning like a **real production-quality DQ pipeline**, and the fastest-rising engineers are the ones who leave a breadcrumb trail of:\n",
    "\n",
    "> **Problem ‚Üí Why it happened ‚Üí How we fixed it ‚Üí How to prevent it again**\n",
    "\n",
    "Documenting fixes builds maintainability, future-you context, and is *excellent* portfolio evidence of engineering maturity.\n",
    "\n",
    "---\n",
    "\n",
    "# üî• Recommended structure for documenting this fix\n",
    "\n",
    "You don‚Äôt need essays. You need **repeatable structure.**\n",
    "Store each fix as an entry like this:\n",
    "\n",
    "### üìÑ `docs/fixes/2025-11-28_dq-rule-catalog-edge-case.md`\n",
    "\n",
    "| Field                    | Content                                                                                                                                                                 |\n",
    "| ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Issue**                | DQ summary section failed when rule/feature files existed but were empty or missing.                                                                                    |\n",
    "| **Root Cause**           | `pd.read_csv()` raised `EmptyDataError` when CSV had size >0 but no valid header/rows.                                                                                  |\n",
    "| **Fix**                  | Added existence + non-empty guard pattern for rule catalog + numeric_profile before merge & summary.                                                                    |\n",
    "| **Code Snippet (final)** | `python\\nif rule_conf_path.exists() and rule_conf_path.stat().st_size > 0:\\n    rule_conf_df = pd.read_csv(rule_conf_path)\\nelse:\\n    rule_conf_df = pd.DataFrame()\\n` |\n",
    "| **Edge Cases Covered**   | Empty CSV, missing files, one-sided merge, no overlapping features.                                                                                                     |\n",
    "| **Prevents Recurrence**  | Safe fallback to empty DF avoids pipeline crash + produces SKIP status instead of failure.                                                                              |\n",
    "| **Notes for Future**     | Consider extracting into `utils.safe_load_csv()` or YAML-driven edge-case thresholds.                                                                                   |\n",
    "\n",
    "Now you have a **repeatable log format.**\n",
    "\n",
    "---\n",
    "\n",
    "## Where should fixes live?\n",
    "\n",
    "| Type                                   | Best place                |\n",
    "| -------------------------------------- | ------------------------- |\n",
    "| Single fix like the one we just solved | `docs/fixes/**.md`        |\n",
    "| Multiple fixes per section             | `docs/section2_notes.md`  |\n",
    "| Long-term improvements / enhancements  | `ROADMAP.md`              |\n",
    "| Quick wins or one-liners               | Inline `# TODO:` comments |\n",
    "\n",
    "---\n",
    "\n",
    "## Why this matters long-term\n",
    "\n",
    "### Without documentation:\n",
    "\n",
    "You (future-you) re-debate fixes.\n",
    "Recruiters skim code and see brilliance, but no iteration.\n",
    "\n",
    "### With documentation:\n",
    "\n",
    "You look like someone who builds pipelines as **products**, not notebooks.\n",
    "Documentation becomes an asset you can show off.\n",
    "\n",
    "This is senior thinking.\n",
    "\n",
    "---\n",
    "\n",
    "## üí°üí° Suggested automation (worth adding soon)\n",
    "\n",
    "Create a reusable function for future fixes:\n",
    "\n",
    "```python\n",
    "def safe_read(path, empty_ok=True, **kwargs):\n",
    "    \"\"\"Read CSV safely ‚Äî avoids EmptyDataError, returns empty DF if needed.\"\"\"\n",
    "    from pandas.errors import EmptyDataError\n",
    "    try:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    except (FileNotFoundError, EmptyDataError, ValueError):\n",
    "        return pd.DataFrame() if empty_ok else None\n",
    "```\n",
    "\n",
    "Replace **every** risky `pd.read_csv()` with:\n",
    "\n",
    "```python\n",
    "df = safe_read(NUMERIC_DIR / \"rule_confidence_scores.csv\")\n",
    "```\n",
    "\n",
    "Then you‚Äôll **never** fight this class of issue again.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can generate:\n",
    "\n",
    "### üî• A full repo-ready fix log document for this and future incidents\n",
    "\n",
    "including structure, templates, file layout, and how to tag them in Git commits.\n",
    "\n",
    "Just say **\"generate fix-log layout\"**.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac26ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "</summary>\n",
    "\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
