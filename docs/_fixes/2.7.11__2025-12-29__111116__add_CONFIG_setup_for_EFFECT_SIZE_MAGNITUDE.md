># Decision to Handle Nans "Cleanly":

Because `NaN` magnitude labels break counting and messaging in subtle ways.

What â€œcleanlyâ€ means here:

* **Counts stay correct.** If some rows have `magnitude_label = NaN`, a straight comparison `(series == mag)` treats them as `False`. That is fine, but you lose visibility into how many rows are unlabeled. Filling lets you optionally count and report `"unknown"` explicitly.
* **Your output stays deterministic.** Upstream parsing can yield missing labels if:

  * an effect value is `NaN` or non-numeric
  * a row was partially computed (missing SD, missing N, division by zero)
  * a source file schema changed and a column read fails
* **You can add an â€œunknownâ€ bucket.** That is useful for QA: it tells you â€œX effect sizes were produced but not interpretable.â€ That is a signal to fix upstream inputs, not to trust the report.

If you do not care about surfacing unlabeled rows, you can skip the fill and just do:

```python
count = int((df_effect_2711["magnitude_label"] == mag).sum())
```

If you want the QA benefit, add `"unknown"` to your action_guide:

```python
action_guide["unknown"] = "âš ï¸ CHECK: Missing magnitude label"
```
The error is on this line:

```python
count = int(df_effect_2711["magnitude_label"] == mag).sum()
```

`df_effect_2711["magnitude_label"] == mag` is a boolean Series. You cannot cast a Series to `int`.

Replace it with either of these:

### Fix 1 (simplest)

```python
count = int((df_effect_2711["magnitude_label"] == mag).sum())
```

### Fix 2 (explicit)

```python
count = int((df_effect_2711["magnitude_label"].fillna("unknown").eq(mag)).sum())
```

Use Fix 2 if you want NaNs handled cleanly.

So your loop becomes:

```python
for mag, action in action_guide.items():
    count = int((df_effect_2711["magnitude_label"].fillna("unknown").eq(mag)).sum())
    if count > 0:
        print(f"  {action} ({count}): {mag.upper()}")
```

```python
# 2.7.11 | Effect Size Computations (Cohenâ€™s d, Î·Â², rÂ², Î¦/V, risk measures)
print("2.7.11 | Effect Size Computations (Cohenâ€™s d, Î·Â², rÂ², Î¦/V, risk measures)")

# IPython-safe display
try:
    from IPython.display import display
except Exception:
    display = print

# ðŸ”’ ROBUST GUARDS
if "sec2_27_dir" not in globals() or sec2_27_dir is None:
    raise NameError("âŒ sec2_27_dir missing. Run the 2.7 directory bootstrap first.")
if ("df_27" not in globals()) or (df_27 is None) or (getattr(df_27, "empty", True)):
    raise NameError("âŒ df_27 missing/None/empty. Build df_27 (or df_base) first.")

# Load CONFIG (your YAML loads here)
effect_cfg = CONFIG.get("EFFECT_SIZE", {}) if isinstance(CONFIG, dict) else {}
effect_enabled_2711 = bool(effect_cfg.get("ENABLED", True))

effect_sources_2711 = effect_cfg.get("SOURCES", [
    "t_test_results.csv",
    "anova_kruskal_results.csv",
    "chi_square_results.csv",
    "point_biserial_results.csv",
    "proportion_tests.csv",
])

# Add input validation summary
missing_sources = [s for s in effect_sources_2711 if not (sec2_27_dir/s).exists()]
if missing_sources:
    print(f"   âš ï¸ Missing {len(missing_sources)}/{len(effect_sources_2711)} sources: {missing_sources}")

effect_metrics_2711 = effect_cfg.get("METRICS", {
    "COHENS_D": True,
    "ETA_SQUARED": True,
    "PARTIAL_ETA_SQUARED": False,
    "R_SQUARED": True,
    "PHI_CRAMER_V": True
})
effect_output_file_2711 = effect_cfg.get("OUTPUT_FILE", "effect_size_report.csv")

effect_rows_2711 = []
n_tests_covered_2711 = 0
n_large_effects_2711 = 0
effect_detail_2711 = None
effect_status_2711 = "SKIPPED"

# Magnitude helpers
def _label_magnitude_d(d_abs: float) -> str:
    if np.isnan(d_abs):
        return "unknown"
    if d_abs < 0.1:
        return "negligible"
    if d_abs < 0.3:
        return "small"
    if d_abs < 0.5:
        return "small/medium"
    if d_abs < 0.8:
        return "medium"
    if d_abs < 1.2:
        return "large"
    return "very large"

def _label_magnitude_r(r_abs: float) -> str:
    if np.isnan(r_abs):
        return "unknown"
    if r_abs < 0.1:
        return "negligible"
    if r_abs < 0.3:
        return "small"
    if r_abs < 0.5:
        return "medium"
    if r_abs < 0.7:
        return "large"
    return "very large"

def _label_magnitude_r2(r2: float) -> str:
    if np.isnan(r2):
        return "unknown"
    # Interpret via sqrt(r2)
    return _label_magnitude_r(math.sqrt(r2))

def _label_magnitude_risk_diff(diff_abs: float) -> str:
    if np.isnan(diff_abs):
        return "unknown"
    if diff_abs < 0.02:
        return "negligible"
    if diff_abs < 0.05:
        return "small"
    if diff_abs < 0.15:
        return "medium"
    if diff_abs < 0.30:
        return "large"
    return "very large"

def _label_magnitude_ratio(rr: float) -> str:
    if np.isnan(rr) or rr <= 0:
        return "unknown"
    dist = abs(math.log(rr))
    if dist < 0.1:
        return "negligible"
    if dist < 0.25:
        return "small"
    if dist < 0.5:
        return "medium"
    if dist < 0.9:
        return "large"
    return "very large"

# Main effect size computation ----------------------------------------
if not effect_enabled_2711:
    print("   âš ï¸ 2.7.11 disabled via CONFIG.EFFECT_SIZE.ENABLED = False")
else:
    # Try to read source files if they exist
    source_dfs_2711 = {}

    for src_name in effect_sources_2711:
        src_path = sec2_27_dir / src_name
        if src_path.exists():
            try:
                source_dfs_2711[src_name] = pd.read_csv(src_path)
            except Exception as e:
                print(f"   âš ï¸ 2.7.11: failed to read {src_path}: {e}")
        else:
            print(f"   â„¹ï¸ 2.7.11: source file not found (skip): {src_path}")

    # ---------- 1) t-test effects: Cohen's d, d_z ----------
    if effect_metrics_2711.get("COHENS_D", True) and "t_test_results.csv" in source_dfs_2711:
        df_t = source_dfs_2711["t_test_results.csv"]
        for _, row in df_t.iterrows():
            test_name = row.get("test_name", None)
            test_type = row.get("test_type", None)
            p_val = row.get("p_value", np.nan)
            t_stat = row.get("t_statistic", np.nan)

            if test_type == "independent":
                nA = row.get("n_group_A", np.nan)
                nB = row.get("n_group_B", np.nan)
                mA = row.get("mean_group_A", np.nan)
                mB = row.get("mean_group_B", np.nan)
                sA = row.get("std_group_A", np.nan)
                sB = row.get("std_group_B", np.nan)

                if any(pd.isna([nA, nB, mA, mB, sA, sB])) or (nA <= 1) or (nB <= 1):
                    continue

                try:
                    nA = float(nA)
                    nB = float(nB)
                    sA2 = float(sA) ** 2
                    sB2 = float(sB) ** 2
                    sp2 = ((nA - 1) * sA2 + (nB - 1) * sB2) / (nA + nB - 2)
                    if sp2 <= 0:
                        d_val = np.nan
                    else:
                        sp = math.sqrt(sp2)
                        d_val = (float(mA) - float(mB)) / sp
                except Exception:
                    d_val = np.nan

                d_abs = abs(d_val) if not pd.isna(d_val) else np.nan
                mag = _label_magnitude_d(d_abs)

                effect_rows_2711.append({
                    "source_file": "t_test_results.csv",
                    "test_name": test_name,
                    "test_type": test_type,
                    "outcome_col": row.get("numeric_col", None),
                    "group_col": row.get("group_col", None),
                    "effect_type": "cohens_d",
                    "effect_value": d_val,
                    "magnitude_label": mag,
                    "p_value": p_val,
                    "statistic": t_stat,
                    "notes": "Cohen's d from pooled SD (independent t-test)"
                })

            elif test_type == "paired":
                n_pairs = row.get("n_pairs", np.nan)
                if pd.isna(t_stat) or pd.isna(n_pairs) or n_pairs <= 0:
                    continue
                try:
                    n_pairs = float(n_pairs)
                    d_val = float(t_stat) / math.sqrt(n_pairs)
                except Exception:
                    d_val = np.nan

                d_abs = abs(d_val) if not pd.isna(d_val) else np.nan
                mag = _label_magnitude_d(d_abs)

                effect_rows_2711.append({
                    "source_file": "t_test_results.csv",
                    "test_name": test_name,
                    "test_type": test_type,
                    "outcome_col": None,
                    "group_col": None,
                    "effect_type": "cohens_d_z",
                    "effect_value": d_val,
                    "magnitude_label": mag,
                    "p_value": p_val,
                    "statistic": t_stat,
                    "notes": "Approximate d_z = t / sqrt(n_pairs) (paired design)"
                })

    # ---------- 2) Chi-square effects: Phi / CramÃ©râ€™s V ----------
    if effect_metrics_2711.get("PHI_CRAMER_V", True) and "chi_square_results.csv" in source_dfs_2711:
        df_chi = source_dfs_2711["chi_square_results.csv"]
        for _, row in df_chi.iterrows():
            test_name = f"{row.get('feature_1')}__{row.get('feature_2')}"
            f1 = row.get("feature_1", None)
            f2 = row.get("feature_2", None)
            chi2 = row.get("statistic", np.nan)
            p_val = row.get("p_value", np.nan)

            if f1 not in df_27.columns or f2 not in df_27.columns or pd.isna(chi2):
                continue

            # Reconstruct contingency to get N and table shape
            sub = df_27[[f1, f2]].dropna()
            if sub.empty:
                continue
            contingency = pd.crosstab(sub[f1], sub[f2])
            r, c = contingency.shape
            N = contingency.to_numpy().sum()

            if N <= 0:
                continue

            chi2_val = float(chi2)
            phi = math.sqrt(chi2_val / N)

            if r == 2 and c == 2:
                effect_rows_2711.append({
                    "source_file": "chi_square_results.csv",
                    "test_name": test_name,
                    "test_type": "chi_square_2x2",
                    "outcome_col": f2,
                    "group_col": f1,
                    "effect_type": "phi",
                    "effect_value": phi,
                    "magnitude_label": _label_magnitude_r(abs(phi)),
                    "p_value": p_val,
                    "statistic": chi2_val,
                    "notes": "Phi coefficient for 2x2 table"
                })
            else:
                k = min(r - 1, c - 1)
                if k <= 0:
                    continue
                V = math.sqrt(chi2_val / (N * k))
                effect_rows_2711.append({
                    "source_file": "chi_square_results.csv",
                    "test_name": test_name,
                    "test_type": "chi_square",
                    "outcome_col": f2,
                    "group_col": f1,
                    "effect_type": "cramers_v",
                    "effect_value": V,
                    "magnitude_label": _label_magnitude_r(abs(V)),
                    "p_value": p_val,
                    "statistic": chi2_val,
                    "notes": f"CramÃ©r's V (r={r}, c={c})"
                })

    # ---------- 3) Point-biserial: r and rÂ² ----------
    if effect_metrics_2711.get("R_SQUARED", True) and "point_biserial_results.csv" in source_dfs_2711:
        df_pb = source_dfs_2711["point_biserial_results.csv"]
        for _, row in df_pb.iterrows():
            test_name = row.get("numeric_feature", None)
            r_val = row.get("correlation", np.nan)
            p_val = row.get("p_value", np.nan)

            if pd.isna(r_val):
                continue

            r_abs = abs(r_val)
            r2 = r_val ** 2

            effect_rows_2711.append({
                "source_file": "point_biserial_results.csv",
                "test_name": test_name,
                "test_type": "point_biserial",
                "outcome_col": None,
                "group_col": None,
                "effect_type": "r",
                "effect_value": r_val,
                "magnitude_label": _label_magnitude_r(r_abs),
                "p_value": p_val,
                "statistic": None,
                "notes": "Point-biserial correlation"
            })

            effect_rows_2711.append({
                "source_file": "point_biserial_results.csv",
                "test_name": test_name,
                "test_type": "point_biserial",
                "outcome_col": None,
                "group_col": None,
                "effect_type": "r_squared",
                "effect_value": r2,
                "magnitude_label": _label_magnitude_r2(r2),
                "p_value": p_val,
                "statistic": None,
                "notes": "Variance explained (r^2)"
            })

    # ---------- 4) Proportion tests: risk diff, RR, OR ----------
    if "proportion_tests.csv" in source_dfs_2711:
        df_prop = source_dfs_2711["proportion_tests.csv"]
        for _, row in df_prop.iterrows():
            test_name = row.get("test_name", None)
            outcome_col = row.get("outcome_col", None)
            group_col = row.get("group_col", None)
            p_val = row.get("p_value", np.nan)

            rate_A = row.get("rate_A", np.nan)
            rate_B = row.get("rate_B", np.nan)
            abs_diff = row.get("absolute_diff", np.nan)
            rr = row.get("relative_risk", np.nan)
            nA = row.get("n_A", np.nan)
            nB = row.get("n_B", np.nan)
            sA = row.get("success_A", np.nan)
            sB = row.get("success_B", np.nan)

            # Risk difference
            if not pd.isna(abs_diff):
                mag = _label_magnitude_risk_diff(abs(abs_diff))
                effect_rows_2711.append({
                    "source_file": "proportion_tests.csv",
                    "test_name": test_name,
                    "test_type": "two_proportion_z",
                    "outcome_col": outcome_col,
                    "group_col": group_col,
                    "effect_type": "risk_difference",
                    "effect_value": abs_diff,
                    "magnitude_label": mag,
                    "p_value": p_val,
                    "statistic": row.get("z_statistic", np.nan),
                    "notes": "Absolute difference in proportions (rate_A - rate_B)"
                })

            # Relative risk
            if not pd.isna(rr):
                mag_rr = _label_magnitude_ratio(rr)
                effect_rows_2711.append({
                    "source_file": "proportion_tests.csv",
                    "test_name": test_name,
                    "test_type": "two_proportion_z",
                    "outcome_col": outcome_col,
                    "group_col": group_col,
                    "effect_type": "relative_risk",
                    "effect_value": rr,
                    "magnitude_label": mag_rr,
                    "p_value": p_val,
                    "statistic": row.get("z_statistic", np.nan),
                    "notes": "Relative risk (rate_A / rate_B)"
                })

            # Odds ratio (if computable)
            try:
                if not any(pd.isna([nA, nB, sA, sB])):
                    nA = float(nA); nB = float(nB)
                    sA = float(sA); sB = float(sB)
                    fA = nA - sA
                    fB = nB - sB
                    if sA > 0 and sB > 0 and fA > 0 and fB > 0:
                        or_val = (sA / fA) / (sB / fB)
                        mag_or = _label_magnitude_ratio(or_val)
                        effect_rows_2711.append({
                            "source_file": "proportion_tests.csv",
                            "test_name": test_name,
                            "test_type": "two_proportion_z",
                            "outcome_col": outcome_col,
                            "group_col": group_col,
                            "effect_type": "odds_ratio",
                            "effect_value": or_val,
                            "magnitude_label": mag_or,
                            "p_value": p_val,
                            "statistic": row.get("z_statistic", np.nan),
                            "notes": "Odds ratio from 2x2 table"
                        })
            except Exception:
                # If OR fails, just skip
                pass

    # NOTE: ANOVA / Kruskal and nonparametric U/W tests could have
    # more nuanced effect sizes (eta^2, rank-biserial, etc.), but we
    # donâ€™t have sums-of-squares or z-statistics logged, so we skip them
    # for now with a conservative design.

    if effect_rows_2711:
        df_effect_2711 = pd.DataFrame(effect_rows_2711)

        # derive n_tests_covered and n_large_effects
        n_tests_covered_2711 = df_effect_2711["test_name"].nunique()

        # count "large" / "very large" magnitude labels
        n_large_effects_2711 = int(
            df_effect_2711["magnitude_label"]
            .fillna("unknown")
            .isin(["large", "very large"])
            .sum()
        )

        # effect_path_2711 = sec2_27_dir / effect_output_file_2711
        # df_effect_2711.to_csv(effect_path_2711, index=False)

        # Add BEFORE your existing if effect_rows_2711 block:
        # print("\nðŸ“Š EFFECT SIZE SUMMARY:")
        # if effect_rows_2711:
        #     print(f"âœ… {len(effect_rows_2711)} effect sizes computed across {n_tests_covered_2711} tests")
        #     large_pct = n_large_effects_2711 / n_tests_covered_2711 * 100
        #     print(f"ðŸ”¥ {n_large_effects_2711}/{n_tests_covered_2711} ({large_pct:.1f}%) LARGE effects")

        #     # Show top 5
        #     top5 = df_effect_2711.nlargest(5, 'effect_value', keep='all')
        #     print("\nTOP 5 EFFECTS:")
        #     for _, row in top5.iterrows():
        #         print(f"  â€¢ {row.test_name}: {row.effect_type}={row.effect_value:.3f} ({row.magnitude_label})")
        # else:
        #     print("âš ï¸ No effects (run upstream tests first)")

        # effect_status_2711 = "OK" if effect_rows_2711 else "WARN"

        # TOP 10 Effects Table (NEW)
    #     if effect_rows_2711:
    #         print("\nðŸ“Š TOP EFFECT SIZES:")
    #         top_effects = (df_effect_2711
    #             .assign(magnitude_score=lambda df: df['effect_value'].abs())
    #             .nlargest(10, 'magnitude_score')
    #             .loc[:, ['test_name', 'effect_type', 'effect_value', 'magnitude_label', 'p_value']]
    #             .round(3)
    #         )
    #         display(top_effects)

    #         # Executive Summary
    #         print(f"\nðŸŽ¯ SUMMARY: {n_tests_covered_2711} tests â†’ {n_large_effects_2711} large/very large effects")
    #         print(f"   ðŸ“ˆ Largest: {df_effect_2711['effect_value'].abs().max():.3f}")

    #     print(f"   âœ… 2.7.11 effect size report written to: {effect_path_2711}")
    #     effect_detail_2711 = str(effect_path_2711)
    #     effect_status_2711 = "OK"
    # else:
    #     print("   âš ï¸ 2.7.11: no effect sizes computed (no usable test inputs).")
    #     effect_status_2711 = "FAIL"

        # ðŸŽ¯ NEW: ENHANCED OUTPUT WITH ACTIONABLE INSIGHTS
        if effect_rows_2711:
            df_effect_2711 = pd.DataFrame(effect_rows_2711)
            n_tests_covered_2711 = df_effect_2711["test_name"].nunique()
            n_large_effects_2711 = int(df_effect_2711["magnitude_label"].fillna("unknown").isin(["large", "very large"]).sum())

            effect_path_2711 = sec2_27_dir / effect_output_file_2711
            df_effect_2711.to_csv(effect_path_2711, index=False)
            print(f"\nâœ… {len(effect_rows_2711)} effects | {n_tests_covered_2711} tests | ðŸ”¥ {n_large_effects_2711} LARGE")

            # ðŸ“Š TOP 10 TABLE
            top_effects = (df_effect_2711
                .assign(magnitude_score=df_effect_2711['effect_value'].abs())
                .nlargest(10, 'magnitude_score')
                [['test_name', 'effect_type', 'effect_value', 'magnitude_label', 'p_value']]
                .round(3))
            display(top_effects)

            # ðŸš€ ACTIONABLE RECOMMENDATIONS
            print("\nðŸŽ¯ PRIORITY ACTIONS BY EFFECT SIZE:")

            action_guide = {
                "very large": "ðŸš¨ CRITICAL: Immediate feature engineering / business intervention",
                "large": "ðŸ”¥ HIGH: Top priority for modeling / A/B testing", 
                "medium": "âš¡ MEDIUM: Strong signal, include in modeling",
                "small": "ðŸ“ˆ LOW: Monitor, may compound with interactions",
                "negligible": "âž¡ï¸ IGNORE: Not practically meaningful"
            }

            for mag, action in action_guide.items():
                count = int(df_effect_2711["magnitude_label"] == mag).sum()
                if count > 0:
                    print(f"  {action} ({count}): {mag.upper()}")

            # ðŸ’¼ BUSINESS TRANSLATION TABLE
            print("\nðŸ’¼ BUSINESS IMPACT REFERENCE:")
            impact_table = pd.DataFrame({
                "Effect Type": ["Cohen's d > 0.8", "Relative Risk > 3.0", "CramÃ©r's V > 0.3", "rÂ² > 0.15"],
                "Meaning": ["Large group difference", "3x risk multiplier", "Strong categorical association", "15% variance explained"],
                "Action": ["Segment + target", "Risk mitigation", "Feature engineering", "Primary predictor"]
            })
            display(impact_table)

            effect_status_2711 = "OK"
        else:
            print("âš ï¸ No upstream test results. Run 2.7.8-2.7.10 first.")
            effect_status_2711 = "WARN"
            effect_detail_2711 = "No upstream test results"

summary_2711 = pd.DataFrame([{
    "section": "2.7.11",
    "section_name": "Effect size computations",
    "check": "Compute standardized effect sizes (d, Î·Â², rÂ², Î¦/V) for key tests",
    "level": "info",
    "n_tests_covered": n_tests_covered_2711,
    "n_large_effects": n_large_effects_2711,
    "status": effect_status_2711,
    "detail": effect_detail_2711,
    "notes": None,
    "notes2": "Effect sizes computed for t-tests, chi-square, point-biserial, and proportion tests",
    "notes3": "Cohen's d, Phi/V, r, rÂ², risk difference, relative risk, odds ratio"
}])
append_sec2(summary_2711, SECTION2_REPORT_PATH)

display(summary_2711)

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[200], line 477
    468 action_guide = {
    469     "very large": "ðŸš¨ CRITICAL: Immediate feature engineering / business intervention",
    470     "large": "ðŸ”¥ HIGH: Top priority for modeling / A/B testing", 
   (...)    473     "negligible": "âž¡ï¸ IGNORE: Not practically meaningful"
    474 }
    476 for mag, action in action_guide.items():
--> 477     count = int(df_effect_2711["magnitude_label"] == mag).sum()
    478     if count > 0:
    479         print(f"  {action} ({count}): {mag.upper()}")

File ~/DATA/PROJECTS/Telco/.venv/lib/python3.12/site-packages/pandas/core/series.py:251, in _coerce_method.<locals>.wrapper(self)
    243     warnings.warn(
    244         f"Calling {converter.__name__} on a single element Series is "
    245         "deprecated and will raise a TypeError in the future. "
   (...)    248         stacklevel=find_stack_level(),
    249     )
    250     return converter(self.iloc[0])
--> 251 raise TypeError(f"cannot convert the series to {converter}")

TypeError: cannot convert the series to <class 'int'>
```