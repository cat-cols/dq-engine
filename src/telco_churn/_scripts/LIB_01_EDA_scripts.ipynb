{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_like_columns(df, protect=(\"customerID\",), thresh=0.95):\n",
    "    cand = [c for c in df.select_dtypes(include=\"object\").columns if c not in protect]\n",
    "    out = []\n",
    "    for c in cand:\n",
    "        s = df[c].astype(\"string\").str.strip()\n",
    "        s = s.str.replace(r\"[,$%]\", \"\", regex=True).str.replace(r\"\\(([^)]+)\\)\", r\"-\\1\", regex=True)\n",
    "        ratio = pd.to_numeric(s, errors=\"coerce\").notna().mean()\n",
    "        if ratio >= thresh:\n",
    "            out.append(c)\n",
    "    return sorted(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb08ddb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> 2.0.2 schema consistency check </summary>\n",
    "\n",
    "\n",
    "You don‚Äôt *have* to change it‚Äîthe current check is correct.\n",
    "If what you want is **richer diagnostics** (which values caused >2 uniques, sample counts, and optional normalization without mutating `df`), swap that block for this drop-in:\n",
    "\n",
    "```python\n",
    "# --------- Optional integrity checks (binary columns should be 2-unique) ----------\n",
    "# richer diagnostics + safe normalization (temp only)\n",
    "def _normalize_binary_series(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype == \"O\" or pd.api.types.is_string_dtype(s):\n",
    "        m = {\"yes\":\"yes\",\"y\":\"yes\",\"1\":\"yes\",\"true\":\"yes\",\n",
    "             \"no\":\"no\",\"n\":\"no\",\"0\":\"no\",\"false\":\"no\"}\n",
    "        return (s.astype(\"string\").str.strip().str.lower().map(m).fillna(s))\n",
    "    return s  # leave numeric/boolean as-is\n",
    "\n",
    "binary_not_two = []\n",
    "binary_details = []  # for reporting\n",
    "\n",
    "for c in schema.get(\"binary\", {}).get(\"columns\", []):\n",
    "    if c not in df.columns:\n",
    "        continue\n",
    "    s = _normalize_binary_series(df[c])\n",
    "    vals = s.dropna().value_counts()\n",
    "    nunq = int(vals.size)\n",
    "\n",
    "    if nunq != 2:\n",
    "        binary_not_two.append((c, nunq))\n",
    "        # capture a compact snapshot for the report (top few values)\n",
    "        binary_details.append({\n",
    "            \"column\": c,\n",
    "            \"nunique\": nunq,\n",
    "            \"top_values\": vals.head(5).to_dict()\n",
    "        })\n",
    "```\n",
    "\n",
    "Then, in your print/report section, you can show more helpful info:\n",
    "\n",
    "```python\n",
    "if binary_not_two:\n",
    "    print(f\"\\n‚ùå Binary columns not 2-unique ({len(binary_not_two)}): {binary_not_two}\")\n",
    "    for d in binary_details:\n",
    "        print(f\"   ‚Ä¢ {d['column']}: top values ‚Üí {d['top_values']}\")\n",
    "else:\n",
    "    print(\"‚úÖ All schema binary columns are 2-unique\")\n",
    "```\n",
    "\n",
    "### What this improves\n",
    "\n",
    "* **Explains why** a column isn‚Äôt binary (you see the offending values).\n",
    "* **Handles common yes/no variants** temporarily (no mutation of `df`).\n",
    "* Keeps your existing `binary_not_two` list so the rest of your pipeline doesn‚Äôt change.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e29cac",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> 2.0.2 schema consistency check function </summary>\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "def run_dataset_guard(\n",
    "    df: pd.DataFrame | None = None,\n",
    "    *,\n",
    "    schema_path: Path = Path(\"config/feature_schema.yaml\"),\n",
    "    inline_schema: dict | None = None,\n",
    "    auto_load_patterns: list[str] = (\"**/telco_*.parquet\", \"**/telco_*.csv\"),\n",
    "    raise_on_critical: bool = True,\n",
    "    section2_report_path: str | Path | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Notebook- & script-friendly dataset guard:\n",
    "      - Auto-loads a dataset if df is None\n",
    "      - Loads schema (YAML with robust fallback)\n",
    "      - Derives/validates target\n",
    "      - Checks schema consistency\n",
    "      - Optionally appends to a unified CSV report\n",
    "      - Returns a structured summary dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          status, target, schema_version, rows, cols,\n",
    "          missing_cols, unexpected_cols, binary_not_two,\n",
    "          bad_numeric (if any), near_miss (if any)\n",
    "        }\n",
    "    \"\"\"\n",
    "    # -------------------- Settings & defaults --------------------\n",
    "    if inline_schema is None:\n",
    "        inline_schema = {\n",
    "            \"target\": \"Churn_flag\",\n",
    "            \"binary\": {\n",
    "                \"columns\": [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]\n",
    "            },\n",
    "            \"continuous\": {\n",
    "                \"columns\": [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "            },\n",
    "            \"categorical\": {\n",
    "                \"columns\": [\n",
    "                    \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "                    \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\",\n",
    "                    \"Contract\", \"PaymentMethod\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # -------------------- Load schema with fallback --------------------\n",
    "    if schema_path.exists():\n",
    "        try:\n",
    "            schema = yaml.safe_load(schema_path.read_text(encoding=\"utf-8\"))\n",
    "            if verbose: print(f\"üìò Loaded schema from {schema_path}\")\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"‚ö†Ô∏è Failed to parse schema ({e}); using inline fallback.\")\n",
    "            schema = inline_schema\n",
    "    else:\n",
    "        if verbose: print(\"üìò Using inline fallback schema (YAML not found)\")\n",
    "        schema = inline_schema\n",
    "\n",
    "    schema_version = schema.get(\"version\", \"n/a\")\n",
    "    if verbose: print(f\"Schema version: {schema_version}\")\n",
    "\n",
    "    target_name = schema.get(\"target\", \"Churn_flag\")\n",
    "\n",
    "    # -------------------- Load dataframe if needed --------------------\n",
    "    files = []\n",
    "    if df is None:\n",
    "        root = Path.cwd()\n",
    "        files = [p for pat in auto_load_patterns for p in root.glob(pat)]\n",
    "        files = sorted(files, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if verbose: print(f\"üîé Candidate data files found: {len(files)}\")\n",
    "\n",
    "        if not files:\n",
    "            raise FileNotFoundError(\"‚ùå No dataset found (looked for telco_*.parquet/csv).\")\n",
    "\n",
    "        latest = files[0]\n",
    "        if verbose: print(f\"üì¶ Auto-loading dataset: {latest}\")\n",
    "\n",
    "        ext = latest.suffix.lower()\n",
    "        if ext == \".parquet\":\n",
    "            df = pd.read_parquet(latest)\n",
    "        elif ext == \".csv\":\n",
    "            df = pd.read_csv(latest)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {ext}\")\n",
    "    else:\n",
    "        if verbose: print(\"‚úÖ Using dataset already in memory (df)\")\n",
    "\n",
    "    # Safe working copy & normalized columns\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Duplicate column names (warn + dedupe)\n",
    "    dupes = df.columns[df.columns.duplicated()].tolist()\n",
    "    if dupes:\n",
    "        if verbose: print(f\"‚ö†Ô∏è Duplicate column names detected: {dupes}\")\n",
    "        df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "        if verbose: print(f\"‚ÑπÔ∏è Dropped {len(dupes)} duplicate column(s)\")\n",
    "\n",
    "    # -------------------- Target derive/validate --------------------\n",
    "    if target_name not in df.columns:\n",
    "        if \"Churn\" in df.columns:\n",
    "            if verbose: print(f\"‚öôÔ∏è Creating {target_name} from 'Churn' ‚Ä¶\")\n",
    "            churn_norm = (\n",
    "                df[\"Churn\"]\n",
    "                .map({True: \"yes\", False: \"no\"})       # handle booleans first\n",
    "                .astype(\"string\").str.strip().str.lower()\n",
    "                .map({\n",
    "                    \"yes\": \"yes\", \"y\": \"yes\", \"1\": \"yes\", \"true\": \"yes\",\n",
    "                    \"no\": \"no\", \"n\": \"no\", \"0\": \"no\", \"false\": \"no\"\n",
    "                })\n",
    "            )\n",
    "            # Map to 0/1 (nullable for diagnostics)\n",
    "            df[target_name] = churn_norm.map({\"no\": 0, \"yes\": 1}).astype(\"Int8\")\n",
    "\n",
    "            bad_mask = df[target_name].isna()\n",
    "            if bad_mask.any():\n",
    "                unmapped = (\n",
    "                    df.loc[bad_mask, \"Churn\"]\n",
    "                      .astype(\"string\").str.strip().str.lower()\n",
    "                      .value_counts().to_dict()\n",
    "                )\n",
    "                msg = f\"‚ùå Could not map some 'Churn' values to 0/1. Unmapped: {unmapped}\"\n",
    "                if raise_on_critical:\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    print(msg)\n",
    "\n",
    "            # If fully mapped, enforce compact dtype\n",
    "            if df[target_name].notna().all():\n",
    "                df[target_name] = df[target_name].astype(\"int8\")\n",
    "                if verbose: print(f\"‚úÖ Created {target_name} and verified dtype int8\")\n",
    "        else:\n",
    "            msg = f\"‚ùå Target '{target_name}' missing and no 'Churn' to derive from.\"\n",
    "            if raise_on_critical:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "    # Binary integrity\n",
    "    if target_name in df.columns:\n",
    "        u = sorted(df[target_name].dropna().unique().tolist())\n",
    "        if u != [0, 1]:\n",
    "            msg = f\"‚ùå Target '{target_name}' must be binary 0/1. Found unique={u}\"\n",
    "            if raise_on_critical: \n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "        else:\n",
    "            if verbose: print(f\"‚úÖ Target '{target_name}' verified binary (0/1)\")\n",
    "\n",
    "        # Fail fast if entirely NA\n",
    "        if df[target_name].isna().all():\n",
    "            msg = f\"‚ùå Target '{target_name}' is entirely NA after mapping.\"\n",
    "            if raise_on_critical: \n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "    # -------------------- Schema consistency --------------------\n",
    "    groups = [g for g in (\"binary\", \"continuous\", \"categorical\") if g in schema]\n",
    "    expected = pd.Index([target_name])\n",
    "    for g in groups:\n",
    "        expected = expected.union(pd.Index(schema[g].get(\"columns\", [])))\n",
    "\n",
    "    cols = pd.Index(df.columns)\n",
    "    missing_cols = expected.difference(cols).tolist()\n",
    "    unexpected_cols = cols.difference(expected).tolist()\n",
    "\n",
    "    # Name-similar warning for unexpected cols\n",
    "    expected_lower = set(map(str.lower, expected.tolist()))\n",
    "    near_miss = [c for c in unexpected_cols if c.lower().strip() in expected_lower]\n",
    "    if near_miss and verbose:\n",
    "        print(f\"‚ÑπÔ∏è Unexpected but name-similar columns (check casing/whitespace): {near_miss}\")\n",
    "\n",
    "    # Optional integrity: declared binary columns should be 2-unique\n",
    "    binary_not_two = []\n",
    "    for c in schema.get(\"binary\", {}).get(\"columns\", []):\n",
    "        if c in df.columns:\n",
    "            nunq = int(df[c].dropna().nunique())\n",
    "            if nunq != 2:\n",
    "                binary_not_two.append((c, nunq))\n",
    "\n",
    "    # Optional: coerce continuous to numeric and report new NaNs\n",
    "    coerced = []\n",
    "    bad_numeric = {}\n",
    "    for c in schema.get(\"continuous\", {}).get(\"columns\", []):\n",
    "        if c in df.columns:\n",
    "            before_nulls = df[c].isna().sum()\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            after_nulls = df[c].isna().sum()\n",
    "            if after_nulls > before_nulls:\n",
    "                bad_numeric[c] = int(after_nulls - before_nulls)\n",
    "            coerced.append(c)\n",
    "    if bad_numeric and verbose:\n",
    "        print(f\"‚ö†Ô∏è Continuous columns coerced to numeric with new NaNs: {bad_numeric}\")\n",
    "\n",
    "    # -------------------- Print summary --------------------\n",
    "    if verbose:\n",
    "        print(\"\\nüìã Schema Consistency Check\")\n",
    "        print(f\"Expected columns: {len(expected)} | Found: {len(cols)}\")\n",
    "        print(f\"‚ùå Missing columns: {missing_cols}\" if missing_cols else \"‚úÖ No missing columns\")\n",
    "        if unexpected_cols:\n",
    "            N = 20\n",
    "            head = unexpected_cols[:N]\n",
    "            tail = unexpected_cols[-N:] if len(unexpected_cols) > N else []\n",
    "            print(f\"\\n‚ö†Ô∏è Unexpected columns ({len(unexpected_cols)}):\")\n",
    "            for c in head: print(f\"  - {c}\")\n",
    "            if tail and tail != head:\n",
    "                print(\"  ...\")\n",
    "                for c in tail: print(f\"  - {c}\")\n",
    "        else:\n",
    "            print(\"‚úÖ No unexpected columns\")\n",
    "        print(f\"‚ùå Binary columns not 2-unique: {binary_not_two}\" if binary_not_two else \"‚úÖ All binary columns are 2-unique\")\n",
    "\n",
    "    # Status & raising\n",
    "    critical = bool(missing_cols or binary_not_two)\n",
    "    status = \"FAIL\" if critical else (\"WARN\" if unexpected_cols else \"OK\")\n",
    "    if verbose: print(f\"\\nStatus: {status}\")\n",
    "\n",
    "    if critical and raise_on_critical:\n",
    "        raise ValueError(\"Schema validation failed (critical issues above).\")\n",
    "\n",
    "    # -------------------- Atomic append to report (optional) --------------------\n",
    "    if section2_report_path is not None:\n",
    "        report_path = Path(section2_report_path)\n",
    "        report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if target_name in df.columns:\n",
    "            counts = df[target_name].value_counts(dropna=False)\n",
    "            ratio = float(counts.get(1, 0)) / float(counts.sum()) if counts.sum() else 0.0\n",
    "            balance_str = f\"{counts.to_dict()} | churn_rate={ratio:.4f}\"\n",
    "        else:\n",
    "            balance_str = \"target_missing\"\n",
    "\n",
    "        schema_chunk = pd.DataFrame([\n",
    "            {\"section\":\"0.1_schema_check\",\"rule\":\"schema_version\",\"value\": schema_version},\n",
    "            {\"section\":\"0.1_schema_check\",\"rule\":\"missing_cols\",\"value\":\";\".join(missing_cols) or \"none\"},\n",
    "            {\"section\":\"0.1_schema_check\",\"rule\":\"unexpected_cols\",\"value\":\";\".join(unexpected_cols) or \"none\"},\n",
    "            {\"section\":\"0.1_schema_check\",\"rule\":\"binary_not_two\",\"value\":\";\".join(f\"{c}:{n}\" for c,n in binary_not_two) or \"none\"},\n",
    "            {\"section\":\"0.1_schema_check\",\"rule\":\"class_balance\",\"value\": balance_str},\n",
    "            {\"section\":\"0.1_schema_check\",\"rule\":\"status\",\"value\": status}\n",
    "        ])\n",
    "\n",
    "        write_header = not report_path.exists()\n",
    "        schema_chunk.to_csv(report_path, mode=\"a\", header=write_header, index=False, encoding=\"utf-8\")\n",
    "        if verbose: print(f\"üìù Appended schema check to: {report_path}\")\n",
    "\n",
    "    # -------------------- Summary dict (return) --------------------\n",
    "    n_rows, n_cols = int(df.shape[0]), int(df.shape[1])\n",
    "    summary = {\n",
    "        \"status\": status,\n",
    "        \"target\": target_name,\n",
    "        \"schema_version\": schema_version,\n",
    "        \"rows\": n_rows,\n",
    "        \"cols\": n_cols,\n",
    "        \"missing_cols\": missing_cols,\n",
    "        \"unexpected_cols\": unexpected_cols,\n",
    "        \"binary_not_two\": binary_not_two,\n",
    "    }\n",
    "    if bad_numeric:\n",
    "        summary[\"bad_numeric\"] = bad_numeric\n",
    "    if near_miss:\n",
    "        summary[\"near_miss\"] = near_miss\n",
    "    if files:\n",
    "        summary[\"auto_loaded_from\"] = str(files[0])\n",
    "\n",
    "    return summary\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eed4ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> tidy, minimal refactor of your Section-2 work into **3 small, reusable functions** </summary>\n",
    "\n",
    "<!-- awesome ‚Äî here‚Äôs a tidy, minimal refactor of your Section-2 work into **3 small, reusable functions** (+ one tiny helper). they‚Äôre pure (no globals), notebook-friendly, and write nothing unless you call the append helper. -->\n",
    "\n",
    "---\n",
    "\n",
    "# `src/quality/section2.py`\n",
    "\n",
    "# src/quality/section2.py\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Sequence\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Small helper: atomic append\n",
    "# ----------------------------\n",
    "def atomic_append_csv(path: Path, chunk: pd.DataFrame) -> Path:\n",
    "    \"\"\"\n",
    "    Append `chunk` to CSV at `path` atomically, aligning columns.\n",
    "    Creates file if missing. Returns the final path.\n",
    "    \"\"\"\n",
    "\n",
    "    path = Path(path)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if path.exists():\n",
    "        existing = pd.read_csv(path)\n",
    "        all_cols = pd.Index(existing.columns).union(chunk.columns)\n",
    "        out = pd.concat(\n",
    "            [existing.reindex(columns=all_cols), chunk.reindex(columns=all_cols)],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    else:\n",
    "        out = chunk\n",
    "\n",
    "    out.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "    return path\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1) Numeric-like object detector (no I/O)\n",
    "# ------------------------------------------\n",
    "def numeric_like_columns(\n",
    "    df: pd.DataFrame,\n",
    "    protect: Sequence[str] = (\"customerID\",),\n",
    "    thresh: float = 0.95,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return object columns whose trimmed values are ‚â• `thresh` parsable as numbers.\n",
    "    Used to (a) include in blank/whitespace scans, and (b) decide later coercion.\n",
    "    \"\"\"\n",
    "    obj_cols = [c for c in df.select_dtypes(include=\"object\").columns if c not in protect]\n",
    "    out: List[str] = []\n",
    "\n",
    "    for c in obj_cols:\n",
    "        s = df[c].astype(\"string\")\n",
    "        cleaned = (\n",
    "            s.str.strip()\n",
    "             .str.replace(r\"[,$%]\", \"\", regex=True)\n",
    "             .str.replace(r\"\\(([^)]+)\\)\", r\"-\\1\", regex=True)  # (123) -> -123\n",
    "        )\n",
    "        nn = cleaned.dropna()\n",
    "        if nn.empty:\n",
    "            continue\n",
    "        ratio = pd.to_numeric(nn, errors=\"coerce\").notna().mean()\n",
    "        if ratio >= thresh:\n",
    "            out.append(c)\n",
    "\n",
    "    return sorted(set(out))\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Missing / Null / Blank scan (report, no I/O)\n",
    "# -------------------------------------------------\n",
    "def missing_blank_report(\n",
    "    df: pd.DataFrame,\n",
    "    id_cols: Iterable[str] = (\"customerID\",),\n",
    "    extra_blank_scan_cols: Optional[Iterable[str]] = None,\n",
    "    section_tag: str = \"2.1_missing_null_blank\",\n",
    "    run_ts: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a per-column report with:\n",
    "      - nulls, empty_strings (\"\"), whitespace_only (\"  \")\n",
    "      - total_issues, pct_missing, pct_blank\n",
    "    Returns a tidy DataFrame ready to append to your unified CSV.\n",
    "    \"\"\"\n",
    "    N = len(df)\n",
    "    run_ts = run_ts or pd.Timestamp.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "    nulls = df.isna().sum()\n",
    "\n",
    "    empty = pd.Series(0, index=df.columns, dtype=\"int64\")\n",
    "    spaces = pd.Series(0, index=df.columns, dtype=\"int64\")\n",
    "\n",
    "    # string-like columns to scan for blanks/whitespace\n",
    "    base_scan = set(df.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "    if extra_blank_scan_cols:\n",
    "        base_scan |= set(extra_blank_scan_cols)\n",
    "    scan_cols = sorted(base_scan.difference(set(id_cols)))\n",
    "\n",
    "    for c in scan_cols:\n",
    "        s = df[c].astype(\"string\", copy=False)\n",
    "        empty[c]  = s.eq(\"\").sum()\n",
    "        spaces[c] = s.str.strip().eq(\"\").sum()\n",
    "\n",
    "    rep = (\n",
    "        pd.DataFrame({\"nulls\": nulls, \"empty_strings\": empty, \"whitespace_only\": spaces})\n",
    "          .assign(\n",
    "              total_issues=lambda x: x[[\"nulls\",\"empty_strings\",\"whitespace_only\"]].sum(axis=1),\n",
    "              pct_missing=lambda x: (x[\"nulls\"] / max(1, N) * 100).round(2),\n",
    "              pct_blank=lambda x: ((x[\"empty_strings\"] + x[\"whitespace_only\"]) / max(1, N) * 100).round(2),\n",
    "              section=section_tag,\n",
    "              rule=\"missing_null_blank\",\n",
    "              run_ts=run_ts,\n",
    "          )\n",
    "          .reset_index(names=\"column\")\n",
    "          .sort_values(\"total_issues\", ascending=False)\n",
    "    )\n",
    "    return rep\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Constant / Nearly-Constant scan (report, no I/O)\n",
    "# -------------------------------------------------------\n",
    "def low_variance_report(\n",
    "    df: pd.DataFrame,\n",
    "    id_cols: Iterable[str] = (\"customerID\",),\n",
    "    nearly_const_thresh: float = 0.98,\n",
    "    section_tag: str = \"2.2_constant_lowvariance\",\n",
    "    run_ts: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify columns as all_null / constant / nearly_constant (‚â• threshold of one level).\n",
    "    Returns a tidy report aligned to your unified schema.\n",
    "    \"\"\"\n",
    "    run_ts = run_ts or pd.Timestamp.now().isoformat(timespec=\"seconds\")\n",
    "    n_rows = len(df)\n",
    "    rows = []\n",
    "\n",
    "    scan_cols = [c for c in df.columns if c not in set(id_cols)]\n",
    "    for c in scan_cols:\n",
    "        vc = df[c].value_counts(dropna=False)\n",
    "        if vc.empty:\n",
    "            continue\n",
    "        n_unique = int(vc.size)\n",
    "        top_val = vc.index[0]\n",
    "        top_count = int(vc.iloc[0])\n",
    "        top_freq = top_count / n_rows if n_rows else np.nan\n",
    "\n",
    "        if df[c].isna().all():\n",
    "            rule = \"all_null\"\n",
    "        elif n_unique == 1:\n",
    "            rule = \"constant\"\n",
    "        elif top_freq >= nearly_const_thresh:\n",
    "            rule = \"nearly_constant\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"column\": c,\n",
    "            \"dtype\": str(df[c].dtype),\n",
    "            \"rule\": rule,\n",
    "            \"unique_count\": n_unique,\n",
    "            \"top_value\": top_val,\n",
    "            \"top_count\": top_count,\n",
    "            \"top_freq\": top_freq,\n",
    "        })\n",
    "\n",
    "    base = pd.DataFrame(rows)\n",
    "    if base.empty:\n",
    "        # return an aligned-but-empty frame (helps with appends)\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"column\",\"dtype\",\"rule\",\"section\",\"run_ts\",\"n_rows\",\"threshold\",\n",
    "            \"unique_count\",\"top_value\",\"top_count\",\"top_freq\",\n",
    "            \"nulls\",\"empty_strings\",\"whitespace_only\",\"total_issues\",\"pct_missing\",\"pct_blank\",\n",
    "        ])\n",
    "\n",
    "    out = (\n",
    "        base.assign(\n",
    "            section=section_tag,\n",
    "            run_ts=run_ts,\n",
    "            n_rows=n_rows,\n",
    "            threshold=nearly_const_thresh,\n",
    "            # placeholders to align with missing/blank schema\n",
    "            nulls=pd.NA, empty_strings=pd.NA, whitespace_only=pd.NA,\n",
    "            total_issues=pd.NA, pct_missing=pd.NA, pct_blank=pd.NA,\n",
    "        )[[\n",
    "            \"column\",\"dtype\",\"rule\",\"section\",\"run_ts\",\"n_rows\",\"threshold\",\n",
    "            \"unique_count\",\"top_value\",\"top_count\",\"top_freq\",\n",
    "            \"nulls\",\"empty_strings\",\"whitespace_only\",\"total_issues\",\"pct_missing\",\"pct_blank\",\n",
    "        ]]\n",
    "    )\n",
    "    # numeric tidy\n",
    "    if \"top_freq\" in out.columns:\n",
    "        out[\"top_freq\"] = pd.to_numeric(out[\"top_freq\"], errors=\"coerce\").round(4)\n",
    "    return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# how to use in your notebook\n",
    "\n",
    "```python\n",
    "# In a cell near the top (after Section 1 paths):\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from quality.section2 import (\n",
    "    numeric_like_columns, missing_blank_report, low_variance_report, atomic_append_csv\n",
    ")\n",
    "\n",
    "# unified Section-2 CSV once per run\n",
    "SECTION2_REPORT_PATH = REPORTS / f\"section2_data_quality_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "\n",
    "# --- 2.0.2 Numeric-like probe (for blanks scan & later coercion)\n",
    "possible_numeric = numeric_like_columns(df, protect=(\"customerID\",), thresh=0.95)\n",
    "print(\"üîé Numeric-like object columns:\", possible_numeric)\n",
    "\n",
    "# --- 2.1 Missing / Null / Blank\n",
    "rep_21 = missing_blank_report(\n",
    "    df,\n",
    "    id_cols=(\"customerID\",),\n",
    "    extra_blank_scan_cols=possible_numeric,   # ensure e.g. TotalCharges gets scanned\n",
    "    section_tag=\"2.1_missing_null_blank\",\n",
    ")\n",
    "atomic_append_csv(SECTION2_REPORT_PATH, rep_21)\n",
    "display(rep_21.query(\"total_issues > 0\").head(20))\n",
    "\n",
    "# --- 2.2 Constant / Nearly-Constant\n",
    "rep_22 = low_variance_report(\n",
    "    df,\n",
    "    id_cols=(\"customerID\",),\n",
    "    nearly_const_thresh=0.98,\n",
    "    section_tag=\"2.2_constant_lowvariance\",\n",
    ")\n",
    "if not rep_22.empty:\n",
    "    atomic_append_csv(SECTION2_REPORT_PATH, rep_22)\n",
    "    display(rep_22.sort_values([\"rule\",\"top_freq\"], ascending=[True, False]).head(20))\n",
    "else:\n",
    "    print(\"‚úÖ No constant / nearly-constant columns\")\n",
    "\n",
    "print(f\"üßæ Unified report: {SECTION2_REPORT_PATH}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## why this helps (and keeps hiring managers happy)\n",
    "\n",
    "* **tiny, purpose-built functions** w/ docstrings ‚úÖ\n",
    "* **pure + testable** (no hidden globals, return DataFrames) ‚úÖ\n",
    "* **consistent unified schema** across steps ‚úÖ\n",
    "* **atomic append helper** you can reuse everywhere ‚úÖ\n",
    "\n",
    "if you want, i can add a fourth optional function later for **schema guard** (taking a Python `schema` dict or YAML) that returns a compact ‚Äúguard report‚Äù chunk you can also append with the same helper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027f969",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> 2.0.1A üß©üîí Dataset Guard drop-in upgrade / that adds a `problems = []` collector and captures **row samples** for each failing rule.  </summary>\n",
    "\n",
    "# 2.0.1A üß©üîí Dataset Guard (Auto-Load + Target + Schema Consistency Check /Guard/ report)\n",
    "\n",
    "Awesome idea. Here‚Äôs a **drop-in upgrade** that adds a `problems = []` collector and captures **row samples** for each failing rule. I kept it minimal and notebook-friendly.\n",
    "\n",
    "Paste the block below into your validator (it‚Äôs self-contained). It assumes you‚Äôre using the latest version we just cleaned; you can replace the corresponding sections, or paste and tweak where noted.\n",
    "\n",
    "```python\n",
    "# =========================\n",
    "# NEW: problems[] collector\n",
    "# =========================\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "problems: List[Dict[str, Any]] = []\n",
    "\n",
    "def _sample_rows(df, mask=None, cols=None, n=5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return up to n row samples as list-of-dicts. If mask is None, returns empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if mask is None:\n",
    "            return []\n",
    "        sub = df.loc[mask, cols] if cols is not None else df.loc[mask]\n",
    "        if sub.empty:\n",
    "            return []\n",
    "        return sub.head(n).to_dict(orient=\"records\")\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _add_problem(rule: str,\n",
    "                 severity: str,\n",
    "                 message: str,\n",
    "                 affected_cols: List[str] = None,\n",
    "                 sample_rows: List[Dict[str, Any]] = None,\n",
    "                 extras: Dict[str, Any] = None):\n",
    "    problems.append({\n",
    "        \"rule\": rule,\n",
    "        \"severity\": severity,                  # \"CRITICAL\" | \"WARN\" | \"INFO\"\n",
    "        \"message\": message,\n",
    "        \"affected_cols\": affected_cols or [],\n",
    "        \"sample_rows\": sample_rows or [],\n",
    "        \"extras\": extras or {},\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# Hook 1: Duplicate columns\n",
    "# =========================\n",
    "dupes = df.columns[df.columns.duplicated()].tolist()\n",
    "if dupes:\n",
    "    # Keep an audit before dropping\n",
    "    _add_problem(\n",
    "        rule=\"duplicate_columns\",\n",
    "        severity=\"WARN\",\n",
    "        message=f\"Duplicate column names detected: {dupes}\",\n",
    "        affected_cols=dupes\n",
    "    )\n",
    "    df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "    print(f\"‚ÑπÔ∏è Dropped {len(dupes)} duplicate column(s)\")\n",
    "\n",
    "# ==================================================\n",
    "# Hook 2: Churn -> TARGET mapping (unmapped samples)\n",
    "# ==================================================\n",
    "unmapped = None           # we'll set if we discover any unmapped\n",
    "unmapped_mask = None      # boolean mask to sample rows\n",
    "\n",
    "if TARGET_NAME not in df.columns and \"Churn\" in df.columns:\n",
    "    # after your churn mapping logic...\n",
    "    # ... you already set df[TARGET_NAME] = Int8 with possible NAs\n",
    "    unmapped_mask = df[TARGET_NAME].isna()\n",
    "    if unmapped_mask.any():\n",
    "        # Collect distribution of the *original* Churn values that failed\n",
    "        unmapped = (\n",
    "            df.loc[unmapped_mask, \"Churn\"]\n",
    "              .astype(\"string\").str.strip().str.lower()\n",
    "              .value_counts().to_dict()\n",
    "        )\n",
    "        # Prefer sampling key ID columns if present\n",
    "        sample_cols = [\"customerID\", \"Churn\"] if \"customerID\" in df.columns else [\"Churn\"]\n",
    "        samples = _sample_rows(df, unmapped_mask, cols=sample_cols, n=5)\n",
    "        _add_problem(\n",
    "            rule=\"target_mapping_unmapped\",\n",
    "            severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "            message=f\"Could not map some 'Churn' values to 0/1.\",\n",
    "            affected_cols=[TARGET_NAME],\n",
    "            sample_rows=samples,\n",
    "            extras={\"unmapped_value_counts\": unmapped}\n",
    "        )\n",
    "\n",
    "# =======================================================\n",
    "# Hook 3: Target not strictly binary (0/1) ‚Äî show sample\n",
    "# =======================================================\n",
    "if TARGET_NAME in df.columns:\n",
    "    u = sorted(df[TARGET_NAME].dropna().unique().tolist())\n",
    "    if u != [0, 1]:\n",
    "        # sample rows where values are not in {0,1}\n",
    "        bad_mask = df[TARGET_NAME].notna() & ~df[TARGET_NAME].isin([0, 1])\n",
    "        samples = _sample_rows(\n",
    "            df, bad_mask,\n",
    "            cols=[\"customerID\", TARGET_NAME] if \"customerID\" in df.columns else [TARGET_NAME],\n",
    "            n=5\n",
    "        )\n",
    "        _add_problem(\n",
    "            rule=\"target_not_binary\",\n",
    "            severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "            message=f\"Target '{TARGET_NAME}' must be binary 0/1. Found unique={u}\",\n",
    "            affected_cols=[TARGET_NAME],\n",
    "            sample_rows=samples,\n",
    "            extras={\"unique_values\": u}\n",
    "        )\n",
    "\n",
    "# ==============================================\n",
    "# Hook 4: Target is entirely NA ‚Äî no row samples\n",
    "# ==============================================\n",
    "if TARGET_NAME in df.columns and df[TARGET_NAME].isna().all():\n",
    "    _add_problem(\n",
    "        rule=\"target_all_na\",\n",
    "        severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "        message=f\"Target '{TARGET_NAME}' is entirely NA after mapping.\",\n",
    "        affected_cols=[TARGET_NAME],\n",
    "    )\n",
    "\n",
    "# ===========================================\n",
    "# Hook 5: Missing columns ‚Äî suggest near-miss\n",
    "# ===========================================\n",
    "if missing_cols:\n",
    "    # Try to find near-miss names (case/trim variants) among existing columns\n",
    "    lower_cols = {c.lower().strip(): c for c in df.columns}\n",
    "    suggestions = {}\n",
    "    for m in missing_cols:\n",
    "        key = m.lower().strip()\n",
    "        if key in lower_cols:\n",
    "            suggestions[m] = lower_cols[key]\n",
    "    _add_problem(\n",
    "        rule=\"missing_columns\",\n",
    "        severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "        message=f\"Missing expected columns: {missing_cols}\",\n",
    "        affected_cols=missing_cols,\n",
    "        extras={\"near_miss_suggestions\": suggestions}\n",
    "    )\n",
    "\n",
    "# ===========================================\n",
    "# Hook 6: Unexpected columns ‚Äî sample a few\n",
    "# ===========================================\n",
    "if unexpected_cols:\n",
    "    # show a few rows of just the unexpected columns\n",
    "    keep = unexpected_cols[: min(6, len(unexpected_cols))]\n",
    "    samples = _sample_rows(df, mask=df.index == df.index, cols=keep, n=3)  # first 3 rows\n",
    "    _add_problem(\n",
    "        rule=\"unexpected_columns\",\n",
    "        severity=\"WARN\",\n",
    "        message=f\"Found {len(unexpected_cols)} unexpected columns.\",\n",
    "        affected_cols=unexpected_cols,\n",
    "        sample_rows=samples\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# Hook 7: Binary columns not exactly 2-unique ‚Äî show examples\n",
    "# ============================================================\n",
    "for col, nunq in (binary_not_two or []):\n",
    "    counts = (\n",
    "        df[col].astype(\"string\").str.strip().str.lower()\n",
    "          .value_counts(dropna=False).to_dict()\n",
    "    ) if col in df.columns else {}\n",
    "    # sample 5 raw values for inspection\n",
    "    samples = _sample_rows(\n",
    "        df, mask=df[col].notna() if col in df.columns else None,\n",
    "        cols=[\"customerID\", col] if \"customerID\" in df.columns and col in df.columns else ([col] if col in df.columns else None),\n",
    "        n=5\n",
    "    )\n",
    "    _add_problem(\n",
    "        rule=\"binary_not_two_unique\",\n",
    "        severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "        message=f\"Binary column '{col}' has {nunq} unique values (expected 2).\",\n",
    "        affected_cols=[col],\n",
    "        sample_rows=samples,\n",
    "        extras={\"value_counts\": counts}\n",
    "    )\n",
    "\n",
    "# =========================================================================\n",
    "# Hook 8: Continuous coercion created NaNs ‚Äî capture which rows became NaN\n",
    "# =========================================================================\n",
    "# To support row sampling here, keep a copy *before* coercion when you coerce.\n",
    "# Replace your coercion loop with this version:\n",
    "bad_numeric = {}\n",
    "coercion_samples = {}   # col -> sample rows that became NaN after coercion\n",
    "\n",
    "for c in schema.get(\"continuous\", {}).get(\"columns\", []):\n",
    "    if c in df.columns:\n",
    "        s_before = df[c].copy()\n",
    "        before_nulls = s_before.isna().sum()\n",
    "\n",
    "        # attempt coercion\n",
    "        s_after = pd.to_numeric(s_before, errors=\"coerce\")\n",
    "        df[c] = s_after\n",
    "\n",
    "        after_nulls = s_after.isna().sum()\n",
    "        if after_nulls > before_nulls:\n",
    "            # Rows that were non-null before but became NaN after coercion\n",
    "            new_nan_mask = s_before.notna() & s_after.isna()\n",
    "            bad_numeric[c] = int(after_nulls - before_nulls)\n",
    "            # sample a few original \"bad\" values (with ID if present)\n",
    "            sample_cols = [\"customerID\", c] if \"customerID\" in df.columns else [c]\n",
    "            coercion_samples[c] = _sample_rows(df.assign(_before=s_before), new_nan_mask, cols=[\"customerID\", \"_before\"] if \"customerID\" in df.columns else [\"_before\"], n=5)\n",
    "\n",
    "if bad_numeric:\n",
    "    print(f\"‚ö†Ô∏è Continuous columns coerced to numeric with new NaNs: {bad_numeric}\")\n",
    "    # Log one consolidated problem with per-column samples\n",
    "    _add_problem(\n",
    "        rule=\"continuous_coercion_new_nans\",\n",
    "        severity=\"WARN\",\n",
    "        message=\"Numeric coercion introduced NaNs in continuous columns.\",\n",
    "        affected_cols=list(bad_numeric.keys()),\n",
    "        sample_rows=[],  # keep consolidated samples in extras\n",
    "        extras={\"new_nan_counts\": bad_numeric, \"samples_per_column\": coercion_samples}\n",
    "    )\n",
    "\n",
    "# ===========================\n",
    "# (Optional) Near-miss helper\n",
    "# ===========================\n",
    "if 'near_miss' in locals() and near_miss:\n",
    "    _add_problem(\n",
    "        rule=\"name_similarity_warnings\",\n",
    "        severity=\"INFO\",\n",
    "        message=\"Columns that are unexpected but look like case/whitespace variants.\",\n",
    "        affected_cols=near_miss\n",
    "    )\n",
    "\n",
    "# ================================================\n",
    "# Final: pretty-print problems & (optionally) save\n",
    "# ================================================\n",
    "print(f\"\\nüßæ Problems collected: {len(problems)}\")\n",
    "for i, p in enumerate(problems, 1):\n",
    "    print(f\"\\n[{i}] {p['severity']} ‚Äî {p['rule']}\")\n",
    "    print(f\"    {p['message']}\")\n",
    "    if p.get(\"affected_cols\"):\n",
    "        print(f\"    Affected cols: {p['affected_cols'][:8]}{' ‚Ä¶' if len(p['affected_cols'])>8 else ''}\")\n",
    "    # print a tiny sample if present\n",
    "    if p.get(\"sample_rows\"):\n",
    "        print(f\"    Samples (up to 5 rows):\")\n",
    "        for r in p[\"sample_rows\"][:5]:\n",
    "            print(f\"      - {r}\")\n",
    "    if p.get(\"extras\"):\n",
    "        # avoid dumping huge dicts ‚Äî show keys only\n",
    "        keys = list(p[\"extras\"].keys())\n",
    "        print(f\"    Extras keys: {keys}\")\n",
    "\n",
    "# If you'd like to persist the full problems payload alongside your CSV:\n",
    "if \"SECTION2_REPORT_PATH\" in globals():\n",
    "    try:\n",
    "        import json\n",
    "        report_path = Path(SECTION2_REPORT_PATH)\n",
    "        issues_json = report_path.with_name(report_path.stem.replace(\".csv\",\"\") + \"_issues.json\")\n",
    "        issues_json.write_text(json.dumps(problems, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"üíæ Wrote detailed issues JSON to: {issues_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to write issues JSON: {e}\")\n",
    "```\n",
    "\n",
    "### What this captures (with samples)\n",
    "\n",
    "* `duplicate_columns` ‚Äî list of dupes before dropping.\n",
    "* `target_mapping_unmapped` ‚Äî original `Churn` values that failed, with sample rows.\n",
    "* `target_not_binary` ‚Äî rows where target isn‚Äôt 0/1.\n",
    "* `target_all_na` ‚Äî whole target NA (no samples).\n",
    "* `missing_columns` ‚Äî includes near-miss suggestions (case/space).\n",
    "* `unexpected_columns` ‚Äî quick 3-row sample of unexpected columns.\n",
    "* `binary_not_two_unique` ‚Äî per-column value counts + row samples.\n",
    "* `continuous_coercion_new_nans` ‚Äî counts and **row samples per column** showing original values that became NaN after coercion.\n",
    "\n",
    "If you want me to **merge this into your full 2.0.1A cell** and return a single consolidated block, I can do that too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2d1ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "------- 2.0.1: schema guard: Full V5 + 'problems=[]' collector-----------\n",
    "</summary>\n",
    "\n",
    "Got it‚Äîhere‚Äôs your **single, consolidated 2.0.1A** validator cell with a `problems = []` collector and **row samples** for failing rules. It logs to your CSV (if `SECTION2_REPORT_PATH` is set) **before** raising, and also writes a sidecar JSON (`*_issues.json`) with full details.\n",
    "\n",
    "```python\n",
    "# 2.0.1A üß©üîí Dataset Guard (Auto-Load + Target + Schema Consistency Check /Guard/ report)\n",
    "# - Validates df against config/feature_schema.yaml\n",
    "# - problems[] collector with row samples for failing rules\n",
    "# - Appends concise CSV row to SECTION2_REPORT_PATH and writes *_issues.json (if path provided)\n",
    "# - Notebook-friendly: fails after logging when RAISE_ON_CRITICAL=True\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# --------- Settings ----------\n",
    "SCHEMA_PATH = Path(\"config/feature_schema.yaml\")\n",
    "AUTO_LOAD_PATTERN = [\"**/telco_*.parquet\", \"**/telco_*.csv\"]\n",
    "RAISE_ON_CRITICAL = True   # flip to False if you want to continue on FAIL\n",
    "\n",
    "# ------- Inline Schema Fallback -----------\n",
    "INLINE_SCHEMA = {\n",
    "    \"target\": \"Churn_flag\",\n",
    "    \"binary\": {\n",
    "        \"columns\": [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]\n",
    "    },\n",
    "    \"continuous\": {\n",
    "        \"columns\": [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "    },\n",
    "    \"categorical\": {\n",
    "        \"columns\": [\n",
    "            \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "            \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\",\n",
    "            \"Contract\", \"PaymentMethod\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# problems[] collector\n",
    "# =========================\n",
    "problems: List[Dict[str, Any]] = []\n",
    "\n",
    "def _sample_rows(df, mask=None, cols=None, n=5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return up to n row samples as list-of-dicts. If mask is None/empty, returns [].\"\"\"\n",
    "    try:\n",
    "        if mask is None:\n",
    "            return []\n",
    "        sub = df.loc[mask, cols] if cols is not None else df.loc[mask]\n",
    "        if sub.empty:\n",
    "            return []\n",
    "        return sub.head(n).to_dict(orient=\"records\")\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _add_problem(rule: str,\n",
    "                 severity: str,\n",
    "                 message: str,\n",
    "                 affected_cols: List[str] = None,\n",
    "                 sample_rows: List[Dict[str, Any]] = None,\n",
    "                 extras: Dict[str, Any] = None):\n",
    "    problems.append({\n",
    "        \"rule\": rule,\n",
    "        \"severity\": severity,                  # \"CRITICAL\" | \"WARN\" | \"INFO\"\n",
    "        \"message\": message,\n",
    "        \"affected_cols\": affected_cols or [],\n",
    "        \"sample_rows\": sample_rows or [],\n",
    "        \"extras\": extras or {},\n",
    "    })\n",
    "\n",
    "# --------- Load schema with fallback ----------\n",
    "if SCHEMA_PATH.exists():\n",
    "    try:\n",
    "        schema = yaml.safe_load(SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "        print(f\"üìò Loaded schema from {SCHEMA_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to parse schema ({e}); using inline fallback.\")\n",
    "        schema = INLINE_SCHEMA\n",
    "else:\n",
    "    print(\"üìò Using inline fallback schema (YAML not found)\")\n",
    "    schema = INLINE_SCHEMA\n",
    "\n",
    "schema_version = schema.get(\"version\", \"n/a\")\n",
    "print(f\"Schema version: {schema_version}\")\n",
    "TARGET_NAME = schema.get(\"target\", \"Churn_flag\")  # set early\n",
    "\n",
    "# --------- Locate / load dataframe if df not present ----------\n",
    "files = []  # safe to print even if df existed\n",
    "if \"df\" not in locals():\n",
    "    root = Path.cwd()\n",
    "    files = [p for pat in AUTO_LOAD_PATTERN for p in root.glob(pat)]\n",
    "    files = sorted(files, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"‚ùå No dataset found (looked for telco_*.parquet/csv).\")\n",
    "    latest = files[0]\n",
    "    print(f\"üì¶ Auto-loading dataset: {latest}\")\n",
    "    ext = latest.suffix.lower()\n",
    "    if ext == \".parquet\":\n",
    "        df = pd.read_parquet(latest)\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(latest)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {ext}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using dataset already in memory (df)\")\n",
    "\n",
    "print(f\"üîé Candidate data files found: {len(files)}\")\n",
    "\n",
    "# Make a safe working copy\n",
    "df = df.copy()\n",
    "\n",
    "# Normalize columns (trim only; you can also lower/underscores if desired)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# =========================\n",
    "# Hook 1: Duplicate columns\n",
    "# =========================\n",
    "dupes = df.columns[df.columns.duplicated()].tolist()\n",
    "if dupes:\n",
    "    _add_problem(\n",
    "        rule=\"duplicate_columns\",\n",
    "        severity=\"WARN\",\n",
    "        message=f\"Duplicate column names detected: {dupes}\",\n",
    "        affected_cols=dupes\n",
    "    )\n",
    "    df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "    print(f\"‚ÑπÔ∏è Dropped {len(dupes)} duplicate column(s)\")\n",
    "\n",
    "# ==================================================\n",
    "# Target creation: Churn -> TARGET_NAME (robust map)\n",
    "# ==================================================\n",
    "# Create target if missing, from 'Churn' if available\n",
    "unmapped_mask = None  # to sample failures later\n",
    "if TARGET_NAME not in df.columns:\n",
    "    if \"Churn\" in df.columns:\n",
    "        print(f\"‚öôÔ∏è Creating {TARGET_NAME} from 'Churn' ‚Ä¶\")\n",
    "        s = df[\"Churn\"]\n",
    "\n",
    "        # Build a textual yes/no series using both bools and strings numerics safely\n",
    "        if pd.api.types.is_bool_dtype(s):\n",
    "            churn_text = s.map({True: \"yes\", False: \"no\"}).astype(\"string\")\n",
    "        else:\n",
    "            # First map actual Python bools present in object dtype, else NA\n",
    "            churn_text = s.map({True: \"yes\", False: \"no\"})\n",
    "            # Where unmapped, use normalized string\n",
    "            needs_text = churn_text.isna()\n",
    "            churn_text = churn_text.astype(\"string\")\n",
    "            churn_text.loc[needs_text] = (\n",
    "                s.astype(\"string\").str.strip().str.lower()\n",
    "            )\n",
    "\n",
    "        # Normalize typical variants ‚Üí canonical yes/no\n",
    "        churn_text = churn_text.map({\n",
    "            \"yes\":\"yes\",\"y\":\"yes\",\"1\":\"yes\",\"true\":\"yes\",\n",
    "            \"no\":\"no\",\"n\":\"no\",\"0\":\"no\",\"false\":\"no\"\n",
    "        })\n",
    "\n",
    "        # Map to 0/1 nullable\n",
    "        df[TARGET_NAME] = churn_text.map({\"no\": 0, \"yes\": 1}).astype(\"Int8\")\n",
    "\n",
    "        # Collect unmapped sample rows (NA after mapping)\n",
    "        unmapped_mask = df[TARGET_NAME].isna()\n",
    "        if unmapped_mask.any():\n",
    "            unmapped_counts = (\n",
    "                df.loc[unmapped_mask, \"Churn\"]\n",
    "                  .astype(\"string\").str.strip().str.lower()\n",
    "                  .value_counts().to_dict()\n",
    "            )\n",
    "            sample_cols = [\"customerID\", \"Churn\"] if \"customerID\" in df.columns else [\"Churn\"]\n",
    "            samples = _sample_rows(df, unmapped_mask, cols=sample_cols, n=5)\n",
    "            _add_problem(\n",
    "                rule=\"target_mapping_unmapped\",\n",
    "                severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "                message=\"Could not map some 'Churn' values to 0/1.\",\n",
    "                affected_cols=[TARGET_NAME],\n",
    "                sample_rows=samples,\n",
    "                extras={\"unmapped_value_counts\": unmapped_counts}\n",
    "            )\n",
    "\n",
    "        # If everything mapped, compact the dtype\n",
    "        if df[TARGET_NAME].notna().all():\n",
    "            df[TARGET_NAME] = df[TARGET_NAME].astype(\"int8\")\n",
    "            print(f\"‚úÖ Created {TARGET_NAME} and verified dtype int8\")\n",
    "    else:\n",
    "        _add_problem(\n",
    "            rule=\"target_missing\",\n",
    "            severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "            message=f\"Target '{TARGET_NAME}' missing and no 'Churn' to derive from.\",\n",
    "            affected_cols=[TARGET_NAME]\n",
    "        )\n",
    "        print(f\"‚ùå Target '{TARGET_NAME}' missing and no 'Churn' to derive from.\")\n",
    "\n",
    "# -------- Binary integrity of target ---------\n",
    "target_binary_problem = None\n",
    "if TARGET_NAME in df.columns:\n",
    "    u = sorted(df[TARGET_NAME].dropna().unique().tolist())\n",
    "    if u != [0, 1]:\n",
    "        target_binary_problem = f\"Target '{TARGET_NAME}' must be binary 0/1. Found unique={u}\"\n",
    "        # sample rows where target is not 0/1\n",
    "        bad_mask = df[TARGET_NAME].notna() & ~df[TARGET_NAME].isin([0, 1])\n",
    "        samples = _sample_rows(\n",
    "            df, bad_mask,\n",
    "            cols=[\"customerID\", TARGET_NAME] if \"customerID\" in df.columns else [TARGET_NAME],\n",
    "            n=5\n",
    "        )\n",
    "        _add_problem(\n",
    "            rule=\"target_not_binary\",\n",
    "            severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "            message=target_binary_problem,\n",
    "            affected_cols=[TARGET_NAME],\n",
    "            sample_rows=samples,\n",
    "            extras={\"unique_values\": u}\n",
    "        )\n",
    "        print(f\"‚ùå {target_binary_problem}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Target '{TARGET_NAME}' verified binary (0/1)\")\n",
    "\n",
    "# Fully NA target after mapping\n",
    "if TARGET_NAME in df.columns and df[TARGET_NAME].isna().all():\n",
    "    _add_problem(\n",
    "        rule=\"target_all_na\",\n",
    "        severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "        message=f\"Target '{TARGET_NAME}' is entirely NA after mapping.\",\n",
    "        affected_cols=[TARGET_NAME],\n",
    "    )\n",
    "    print(f\"‚ùå Target '{TARGET_NAME}' is entirely NA after mapping.\")\n",
    "\n",
    "# --------- Expected columns from schema ----------\n",
    "groups = [g for g in (\"binary\",\"continuous\",\"categorical\") if g in schema]\n",
    "expected = pd.Index([TARGET_NAME])\n",
    "for g in groups:\n",
    "    expected = expected.union(pd.Index(schema[g].get(\"columns\", [])))\n",
    "\n",
    "cols = pd.Index(df.columns)\n",
    "missing_cols = expected.difference(cols).tolist()\n",
    "unexpected_cols = cols.difference(expected).tolist()\n",
    "\n",
    "# Near-miss suggestions (case/trim variants)\n",
    "expected_lower = set(map(str.lower, expected.tolist()))\n",
    "near_miss = [c for c in unexpected_cols if c.lower().strip() in expected_lower]\n",
    "if near_miss:\n",
    "    print(f\"‚ÑπÔ∏è Unexpected but name-similar columns (check casing/whitespace): {near_miss}\")\n",
    "\n",
    "# Log missing columns w/ near-miss suggestions\n",
    "if missing_cols:\n",
    "    lower_cols = {c.lower().strip(): c for c in df.columns}\n",
    "    suggestions = {m: lower_cols[m.lower().strip()] for m in missing_cols if m.lower().strip() in lower_cols}\n",
    "    _add_problem(\n",
    "        rule=\"missing_columns\",\n",
    "        severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "        message=f\"Missing expected columns: {missing_cols}\",\n",
    "        affected_cols=missing_cols,\n",
    "        extras={\"near_miss_suggestions\": suggestions}\n",
    "    )\n",
    "\n",
    "# Log unexpected columns (show quick first-rows sample)\n",
    "if unexpected_cols:\n",
    "    keep = unexpected_cols[: min(6, len(unexpected_cols))]\n",
    "    samples = _sample_rows(df, mask=df.index == df.index, cols=keep, n=3)  # first 3 rows\n",
    "    _add_problem(\n",
    "        rule=\"unexpected_columns\",\n",
    "        severity=\"WARN\",\n",
    "        message=f\"Found {len(unexpected_cols)} unexpected columns.\",\n",
    "        affected_cols=unexpected_cols,\n",
    "        sample_rows=samples\n",
    "    )\n",
    "\n",
    "# --------- Binary columns (should be 2-unique) ---------\n",
    "binary_not_two = []\n",
    "for c in schema.get(\"binary\", {}).get(\"columns\", []):\n",
    "    if c in df.columns:\n",
    "        nunq = int(df[c].dropna().nunique())\n",
    "        if nunq != 2:\n",
    "            binary_not_two.append((c, nunq))\n",
    "\n",
    "for col, nunq in (binary_not_two or []):\n",
    "    counts = (\n",
    "        df[col].astype(\"string\").str.strip().str.lower()\n",
    "          .value_counts(dropna=False).to_dict()\n",
    "    ) if col in df.columns else {}\n",
    "    samples = _sample_rows(\n",
    "        df,\n",
    "        mask=df[col].notna() if col in df.columns else None,\n",
    "        cols=[\"customerID\", col] if \"customerID\" in df.columns and col in df.columns else ([col] if col in df.columns else None),\n",
    "        n=5\n",
    "    )\n",
    "    _add_problem(\n",
    "        rule=\"binary_not_two_unique\",\n",
    "        severity=\"CRITICAL\" if RAISE_ON_CRITICAL else \"WARN\",\n",
    "        message=f\"Binary column '{col}' has {nunq} unique values (expected 2).\",\n",
    "        affected_cols=[col],\n",
    "        sample_rows=samples,\n",
    "        extras={\"value_counts\": counts}\n",
    "    )\n",
    "\n",
    "# --------- Enforce numeric dtype for continuous columns (with samples) ---------\n",
    "bad_numeric = {}\n",
    "coercion_samples = {}   # col -> list-of-row dicts (original values that became NaN)\n",
    "\n",
    "for c in schema.get(\"continuous\", {}).get(\"columns\", []):\n",
    "    if c in df.columns:\n",
    "        s_before = df[c].copy()\n",
    "        before_nulls = s_before.isna().sum()\n",
    "\n",
    "        s_after = pd.to_numeric(s_before, errors=\"coerce\")\n",
    "        df[c] = s_after\n",
    "\n",
    "        after_nulls = s_after.isna().sum()\n",
    "        if after_nulls > before_nulls:\n",
    "            new_nan_mask = s_before.notna() & s_after.isna()\n",
    "            bad_numeric[c] = int(after_nulls - before_nulls)\n",
    "            # capture original values that failed coercion\n",
    "            sample_cols = [\"customerID\", c] if \"customerID\" in df.columns else [c]\n",
    "            # show original (pre-coercion) values via an aux column\n",
    "            coercion_samples[c] = _sample_rows(\n",
    "                df.assign(_before=s_before),\n",
    "                new_nan_mask,\n",
    "                cols=([\"customerID\", \"_before\"] if \"customerID\" in df.columns else [\"_before\"]),\n",
    "                n=5\n",
    "            )\n",
    "\n",
    "if bad_numeric:\n",
    "    print(f\"‚ö†Ô∏è Continuous columns coerced to numeric with new NaNs: {bad_numeric}\")\n",
    "    _add_problem(\n",
    "        rule=\"continuous_coercion_new_nans\",\n",
    "        severity=\"WARN\",\n",
    "        message=\"Numeric coercion introduced NaNs in continuous columns.\",\n",
    "        affected_cols=list(bad_numeric.keys()),\n",
    "        sample_rows=[],  # samples are large; keep in extras\n",
    "        extras={\"new_nan_counts\": bad_numeric, \"samples_per_column\": coercion_samples}\n",
    "    )\n",
    "\n",
    "# --------- Name-similarity info ---------\n",
    "if near_miss:\n",
    "    _add_problem(\n",
    "        rule=\"name_similarity_warnings\",\n",
    "        severity=\"INFO\",\n",
    "        message=\"Columns that are unexpected but look like case/whitespace variants.\",\n",
    "        affected_cols=near_miss\n",
    "    )\n",
    "\n",
    "# --------- Print concise summary (pre-raise) ----------\n",
    "print(\"\\nüìã Schema Consistency Check\")\n",
    "print(f\"Expected columns: {len(expected)} | Found: {len(cols)}\")\n",
    "print(f\"‚ùå Missing columns: {missing_cols}\" if missing_cols else \"‚úÖ No missing columns\")\n",
    "if unexpected_cols:\n",
    "    N = 20\n",
    "    head = unexpected_cols[:N]\n",
    "    tail = unexpected_cols[-N:] if len(unexpected_cols) > N else []\n",
    "    print(f\"\\n‚ö†Ô∏è Unexpected columns ({len(unexpected_cols)}):\")\n",
    "    for c in head: print(f\"  - {c}\")\n",
    "    if tail and tail != head:\n",
    "        print(\"  ...\")\n",
    "        for c in tail: print(f\"  - {c}\")\n",
    "else:\n",
    "    print(\"‚úÖ No unexpected columns\")\n",
    "print(f\"‚ùå Binary columns not 2-unique: {binary_not_two}\" if binary_not_two else \"‚úÖ All binary columns are 2-unique\")\n",
    "\n",
    "# --------- Compute status ----------\n",
    "critical_reasons = []\n",
    "if missing_cols: critical_reasons.append(\"missing_cols\")\n",
    "if binary_not_two: critical_reasons.append(\"binary_not_two\")\n",
    "if target_binary_problem: critical_reasons.append(\"target_not_binary\")\n",
    "if TARGET_NAME in df.columns and df[TARGET_NAME].isna().all():\n",
    "    critical_reasons.append(\"target_all_na\")\n",
    "\n",
    "critical = bool(critical_reasons)\n",
    "status = \"FAIL\" if critical else (\"WARN\" if unexpected_cols or problems else \"OK\")\n",
    "print(f\"\\nStatus: {status}\" + (f\" | reasons: {', '.join(critical_reasons)}\" if critical else \"\"))\n",
    "\n",
    "# --------- Append to SECTION2_REPORT_PATH (BEFORE any raise) ----------\n",
    "if \"SECTION2_REPORT_PATH\" in globals():\n",
    "    report_path = Path(SECTION2_REPORT_PATH)\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if TARGET_NAME in df.columns:\n",
    "        counts = df[TARGET_NAME].value_counts(dropna=False)\n",
    "        ratio = float(counts.get(1, 0)) / float(counts.sum()) if counts.sum() else 0.0\n",
    "        balance_str = f\"{counts.to_dict()} | churn_rate={ratio:.4f}\"\n",
    "    else:\n",
    "        balance_str = \"target_missing\"\n",
    "\n",
    "    schema_chunk = pd.DataFrame([\n",
    "        {\"section\":\"0.1_schema_check\",\"rule\":\"schema_version\",\"value\": schema_version},\n",
    "        {\"section\":\"0.1_schema_check\",\"rule\":\"missing_cols\",\"value\":\";\".join(missing_cols) or \"none\"},\n",
    "        {\"section\":\"0.1_schema_check\",\"rule\":\"unexpected_cols\",\"value\":\";\".join(unexpected_cols) or \"none\"},\n",
    "        {\"section\":\"0.1_schema_check\",\"rule\":\"binary_not_two\",\"value\":\";\".join(f\"{c}:{n}\" for c,n in binary_not_two) or \"none\"},\n",
    "        {\"section\":\"0.1_schema_check\",\"rule\":\"class_balance\",\"value\": balance_str},\n",
    "        {\"section\":\"0.1_schema_check\",\"rule\":\"status\",\"value\": status}\n",
    "    ])\n",
    "    write_header = not report_path.exists()\n",
    "    schema_chunk.to_csv(report_path, mode=\"a\", header=write_header, index=False, encoding=\"utf-8\")\n",
    "    print(f\"üìù Appended schema check to: {report_path}\")\n",
    "\n",
    "    # Write issues JSON alongside the CSV\n",
    "    try:\n",
    "        import json\n",
    "        issues_json = report_path.with_name(report_path.stem.replace(\".csv\",\"\") + \"_issues.json\")\n",
    "        issues_json.write_text(json.dumps(problems, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"üíæ Wrote detailed issues JSON to: {issues_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to write issues JSON: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è SECTION2_REPORT_PATH not set ‚Äî skipping report & issues JSON append.\")\n",
    "\n",
    "# --------- Pretty-print problems now ----------\n",
    "print(f\"\\nüßæ Problems collected: {len(problems)}\")\n",
    "for i, p in enumerate(problems, 1):\n",
    "    print(f\"\\n[{i}] {p['severity']} ‚Äî {p['rule']}\")\n",
    "    print(f\"    {p['message']}\")\n",
    "    if p.get(\"affected_cols\"):\n",
    "        print(f\"    Affected cols: {p['affected_cols'][:8]}{' ‚Ä¶' if len(p['affected_cols'])>8 else ''}\")\n",
    "    if p.get(\"sample_rows\"):\n",
    "        print(f\"    Samples (up to 5 rows):\")\n",
    "        for r in p[\"sample_rows\"][:5]:\n",
    "            print(f\"      - {r}\")\n",
    "    if p.get(\"extras\"):\n",
    "        keys = list(p[\"extras\"].keys())\n",
    "        print(f\"    Extras keys: {keys}\")\n",
    "\n",
    "# --------- Raise if critical AFTER logging ---------\n",
    "if critical and RAISE_ON_CRITICAL:\n",
    "    raise ValueError(\"Schema validation failed (critical issues above).\")\n",
    "\n",
    "# -------------------- Summary dict (print) --------------------\n",
    "schema_summary = {\n",
    "    \"status\": status,\n",
    "    \"target\": TARGET_NAME,\n",
    "    \"schema_version\": schema_version,\n",
    "    \"missing_cols\": missing_cols,\n",
    "    \"unexpected_cols\": unexpected_cols,\n",
    "    \"binary_not_two\": binary_not_two,\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "}\n",
    "print(\"\\nüì¶ Schema summary:\")\n",
    "for k, v in schema_summary.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "```\n",
    "\n",
    "Tip: if you want lowercased, underscore column names across the board, change the normalize line to:\n",
    "\n",
    "```python\n",
    "df.columns = (df.columns\n",
    "              .str.strip()\n",
    "              .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "              .str.lower())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3bb02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIB_01_EDA_scripts\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Elegant data quality checking for TotalCharges.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.issues = []\n",
    "        \n",
    "    def check_total_charges(self):\n",
    "        \"\"\"Comprehensive TotalCharges validation.\"\"\"\n",
    "        col = 'TotalCharges'\n",
    "        \n",
    "        # Type check\n",
    "        if self.df[col].dtype == 'object':\n",
    "            # Find non-numeric\n",
    "            numeric_mask = pd.to_numeric(self.df[col], errors='coerce').notna()\n",
    "            invalid_count = (~numeric_mask).sum()\n",
    "            \n",
    "            if invalid_count > 0:\n",
    "                # Analyze pattern\n",
    "                invalid_df = self.df[~numeric_mask]\n",
    "                \n",
    "                self.issues.append({\n",
    "                    'column': col,\n",
    "                    'issue_type': 'non_numeric_values',\n",
    "                    'count': invalid_count,\n",
    "                    'percentage': invalid_count / len(self.df) * 100,\n",
    "                    'unique_invalid_values': invalid_df[col].unique().tolist(),\n",
    "                    'pattern': {\n",
    "                        'all_tenure_zero': invalid_df['tenure'].eq(0).all(),\n",
    "                        'avg_monthly_charges': invalid_df['MonthlyCharges'].mean()\n",
    "                    },\n",
    "                    'recommendation': 'Replace with MonthlyCharges for tenure=0 customers'\n",
    "                })\n",
    "\n",
    "        # Logic check\n",
    "        numeric_charges = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        logic_issues = (numeric_charges < self.df['MonthlyCharges']) & (self.df['tenure'] > 1)\n",
    "        \n",
    "        if logic_issues.any():\n",
    "            self.issues.append({\n",
    "                'column': col,\n",
    "                'issue_type': 'logical_inconsistency',\n",
    "                'count': logic_issues.sum(),\n",
    "                'description': 'TotalCharges less than MonthlyCharges for tenure > 1'\n",
    "            })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Generate elegant report.\"\"\"\n",
    "        if not self.issues:\n",
    "            return \"‚úÖ No issues found in TotalCharges\"\n",
    "        \n",
    "        report = \"üîç TotalCharges Quality Report\\n\" + \"=\"*40 + \"\\n\"\n",
    "        for issue in self.issues:\n",
    "            report += f\"\\n‚ö†Ô∏è Issue: {issue['issue_type']}\\n\"\n",
    "            report += f\"   Count: {issue['count']} ({issue['percentage']:.1f}%)\\n\"\n",
    "            if 'unique_invalid_values' in issue:\n",
    "                report += f\"   Values found: {issue['unique_invalid_values']}\\n\"\n",
    "            if 'pattern' in issue:\n",
    "                report += f\"   Pattern: All tenure=0: {issue['pattern']['all_tenure_zero']}\\n\"\n",
    "            if 'recommendation' in issue:\n",
    "                report += f\"   ‚ú® Fix: {issue['recommendation']}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Usage\n",
    "checker = DataQualityChecker(df)\n",
    "print(checker.check_total_charges().get_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1367961",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><h2> \n",
    "\n",
    "### üîß `Operators in Python` </h2></summary>\n",
    "\n",
    "<h4>\n",
    "\n",
    "### üîß Bitwise Assignment Operators in Python`\n",
    "</h4>\n",
    "<ul>\n",
    "  <li>Create virtual environment, install dependencies</li>\n",
    "  <li>Load data from BigQuery / local CSV</li>\n",
    "</ul>\n",
    "\n",
    "| Operator | Name                       | Example     | Equivalent To    |\n",
    "|----------|----------------------------|-------------|------------------|\n",
    "| `&=`     | Bitwise AND assignment     | `a &= b`    | `a = a & b`      |\n",
    "| `|=`     | Bitwise OR assignment      | `a |= b`    | `a = a | b`      |\n",
    "| `^=`     | Bitwise XOR assignment     | `a ^= b`    | `a = a ^ b`      |\n",
    "| `<<=`    | Left shift assignment      | `a <<= b`   | `a = a << b`     |\n",
    "| `>>=`    | Right shift assignment     | `a >>= b`   | `a = a >> b`     |\n",
    "\n",
    "># üîç Focus on ^=\n",
    "**^=** toggles bits where the other operand is 1.\n",
    "- Common use: flipping a single bit (like toggling between 0 and 1 for parity checks).\n",
    "\n",
    "|Example:|\n",
    "\n",
    "```python\n",
    "x = 10      # 0b1010\n",
    "x ^= 3      # 0b0011\n",
    "print(x)    # 9 (0b1001)\n",
    "```\n",
    "\n",
    ">### ‚úÖ Tip: When to Use ^=\n",
    "\n",
    "Toggling flags or parity (like even/odd checks)\n",
    "\n",
    "Swapping values without a temporary variable (though not recommended for readability)\n",
    "- Bitmask operations\n",
    "- Competitive programming or algorithm optimizations\n",
    "\n",
    "Let me know if you‚Äôd like a deep dive into how these work with actual bit patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77a00c",
   "metadata": {},
   "source": [
    "## VERSION 2\n",
    "def perform_complete_eda(df, save_figures=False):\n",
    "    \"\"\"\n",
    "    Complete EDA workflow using our extracted functions.\n",
    "    This is the Level 2 culmination:\n",
    "    - Use functions instead of repetitive code\n",
    "    - Systematic approach to analysis\n",
    "    - Clear documentation of findings\n",
    "    \"\"\"\n",
    "    # 1. Data Validation\n",
    "    validate_dataset(df)\n",
    "    \n",
    "    # 2. Separate variable types\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # 3. Analyze each type systematically\n",
    "    print(\"\\n=== Categorical Variables Analysis ===\")\n",
    "    for col in categorical_cols:\n",
    "        if col not in ['customerID']:  # Skip ID\n",
    "            fig = analyze_categorical(df, col)\n",
    "            if save_figures:\n",
    "                fig.savefig(f'figures/categorical_{col}.png')\n",
    "    \n",
    "    print(\"\\n=== Numerical Variables Analysis ===\")\n",
    "    for col in numerical_cols:\n",
    "        fig = analyze_numerical(df, col)\n",
    "        if save_figures:\n",
    "            fig.savefig(f'figures/numerical_{col}.png')\n",
    "    \n",
    "    # 4. Feature Engineering\n",
    "    df_enhanced = create_customer_segments(df)\n",
    "    \n",
    "    # 5. Final validation\n",
    "    print(\"\\n=== Enhanced Dataset ===\")\n",
    "    validate_dataset(df_enhanced)\n",
    "    \n",
    "    return df_enhanced\n",
    "# utils.py (moved under /Users/b/DATA/PROJECTS/Telco/L2/src/)\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Project paths\n",
    "# ---------------------------\n",
    "# Always resolve project root relative to this utils.py file\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]   # /Users/b/DATA/PROJECTS/Telco/L2\n",
    "FIGURES_DIR = PROJECT_ROOT / \"figures\"\n",
    "\n",
    "def ensure_dir(path: str | Path) -> None:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def memory_report(df: pd.DataFrame) -> str:\n",
    "    mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    return f\"{mb:.2f} MB\"\n",
    "\n",
    "# ---------------------------\n",
    "# (your existing data prep + validation code unchanged)\n",
    "# ---------------------------\n",
    "# ... load_telco_data, validate_dataset, etc. ...\n",
    "\n",
    "# ---------------------------\n",
    "# Complete EDA workflow\n",
    "# ---------------------------\n",
    "def perform_complete_eda(\n",
    "    df: pd.DataFrame,\n",
    "    figures_dir: Path = FIGURES_DIR,  # default fixed to project root /figures\n",
    "    save_figures: bool = False,\n",
    "    skip_cols: Optional[Iterable[str]] = (\"customerID\",),\n",
    "    target: str = \"Churn\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run the Level-2 EDA end-to-end:\n",
    "      1) Validate\n",
    "      2) Analyze categoricals & numerics\n",
    "      3) Apply business logic\n",
    "      4) Feature engineering\n",
    "      5) Final validation\n",
    "    \"\"\"\n",
    "    ensure_dir(figures_dir)\n",
    "\n",
    "    print(\"1) Validation (raw)\")\n",
    "    validate_dataset(df)\n",
    "\n",
    "    # Separate types\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if skip_cols:\n",
    "        categorical_cols = [c for c in categorical_cols if c not in skip_cols]\n",
    "        numerical_cols = [c for c in numerical_cols if c not in skip_cols]\n",
    "\n",
    "    print(\"\\n2) Categorical analysis\")\n",
    "    for c in categorical_cols:\n",
    "        fig = analyze_categorical(df, c, target=target)\n",
    "        if save_figures:\n",
    "            fig.savefig(figures_dir / f\"categorical_{c}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"\\n3) Numerical analysis\")\n",
    "    for c in numerical_cols:\n",
    "        fig = analyze_numerical(df, c, target=target)\n",
    "        if save_figures:\n",
    "            fig.savefig(figures_dir / f\"numerical_{c}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"\\n4) Business logic corrections\")\n",
    "    df2 = apply_business_logic(df)\n",
    "\n",
    "    print(\"\\n5) Feature engineering\")\n",
    "    df3 = create_customer_segments(df2)\n",
    "\n",
    "    print(\"\\n6) Validation (enhanced)\")\n",
    "    validate_dataset(df3)\n",
    "\n",
    "    return df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "---\n",
    "\n",
    "Parameters\n",
    "Great question üëå ‚Äî let‚Äôs break down exactly what‚Äôs happening in that function definition:\n",
    "def load_telco_data(filepath, optimize_memory=True):\n",
    "\n",
    "1. Function definition\n",
    "def ‚Üí defines a new function.\n",
    "load_telco_data ‚Üí the name of the function (you‚Äôll call it later).\n",
    "\n",
    "2. Parameters\n",
    "Inside the parentheses are the parameters (inputs the function expects):\n",
    "filepath\n",
    "A positional parameter.\n",
    "When you call the function, you must supply this argument.\n",
    " load_telco_data(\"data/telco.csv\")\n",
    "In this case, it‚Äôs probably expected to be a string path to a CSV file.\n",
    "optimize_memory=True\n",
    "This is a keyword parameter with a default value.\n",
    "=True means that if you don‚Äôt pass anything for it, Python automatically sets it to True.\n",
    "You can override it when calling the function:\n",
    "\n",
    "load_telco_data(\"data/telco.csv\")               # optimize_memory defaults to True\n",
    "load_telco_data(\"data/telco.csv\", False)        # pass by position\n",
    "load_telco_data(\"data/telco.csv\", optimize_memory=False)  # pass by keyword\n",
    "So:\n",
    "filepath is required.\n",
    "optimize_memory is optional, defaulting to True.\n",
    "\n",
    "3. The trailing colon :\n",
    "The colon ends the function header and tells Python that what follows (indented) will be the function body.\n",
    "Example body (just illustrative):\n",
    "def load_telco_data(filepath, optimize_memory=True):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if optimize_memory:\n",
    "        # do some memory optimization steps\n",
    "        df = df.astype({\"customerID\": \"category\"})\n",
    "    return df\n",
    "\n",
    "4. Why use a default argument?\n",
    "Convenience: Most of the time, you may want memory optimization ‚Üí so True is the default.\n",
    "Flexibility: If you don‚Äôt want optimization (e.g., for debugging), you can set it to False.\n",
    "\n",
    "‚úÖ Summary:\n",
    "filepath ‚Üí required argument.\n",
    "optimize_memory=True ‚Üí optional argument, defaults to True unless you override it.\n",
    "The colon : starts the function‚Äôs body.\n",
    "\n",
    "---\n",
    "\n",
    "Absolutely! Moving from a monolithic to a modular project structure is a smart goal. There are several excellent resources to guide you through planning, organizing, and executing that transformation‚Äîespecially in Python. Here‚Äôs a curated guide to get you started:\n",
    "\n",
    "1. Python Structure & Modular Refactoring\n",
    "Python Guide: Structuring Your Project\n",
    "A foundational primer on organizing Python projects with clean modules, packages, clear layouts, and import structure.\n",
    " Provides solid guidance on how modules and filesystem layout affect maintainability and clarity.\n",
    " (docs.python-guide.org)\n",
    "Dagster Blog: Best Practices for Structuring Python Projects\n",
    "Covers nine practical best practices‚Äîfrom organizing folders, naming conventions, to modularization. Great if you're aiming to apply modular design with maintainable patterns.\n",
    " (Dagster)\n",
    "\n",
    "2. Modular Monolith Architecture\n",
    "Breadcrumbs Collector: Modular Monolith in Python\n",
    "Explores how to divide a monolith into thoughtfully separated, encapsulated components‚Äîeach with its own API and internal structure.\n",
    " Discusses leveraging Clean Architecture selectively per component for pragmatic modularization.\n",
    " (breadcrumbs collector.tech)\n",
    "Dev.to: Structuring Modular Monoliths\n",
    "Emphasizes domain-based modules (e.g., Product, Order) with clear boundaries, own persistence, and controlled inter-module communication‚Äîeven within a single codebase.\n",
    " (DEV Community)\n",
    "Medium: Modular Folder Organization\n",
    "Proposes a modular folder layout aligned with Clean or Hexagonal Architecture‚Äîeach module containing UI, business logic, and data access enabling easier testing and future extraction.\n",
    " (Medium)\n",
    "\n",
    "3. Transition Path & Reasoning\n",
    "Medium ‚Äì Modular Monolithic Architecture\n",
    " A practical architecture pattern that divides the monolith into independent modules with explicit boundaries. Useful as an intermediate step before microservices.\n",
    " (Medium)\n",
    "\n",
    "\n",
    "Miljan Jovanovic ‚Äì What Is a Modular Monolith?\n",
    " A recent overview of modular monolith benefits and why it's often better to start here rather than microservices. Includes caution using Martin Fowler's notes and scalability points.\n",
    " (milanjovanovic.tech)\n",
    "\n",
    "\n",
    "\n",
    "4. Community Wisdom\n",
    "From a popular Reddit discussion on structuring monoliths before splitting into services:\n",
    "‚ÄúGet the monolith right first‚Ä¶ if you can‚Äôt get one repo in the right state, ‚Ä¶ you‚Äôll have lots of things in a bad state instead of just one‚Ä¶‚Äù\n",
    " ‚ÄúPut your focus first on finding bounded contexts‚Äîwhat is service A, service B‚Ä¶‚Äù\n",
    " Very practical advice: refactor internally, identify domain boundaries, then modularize.\n",
    " (Reddit)\n",
    "\n",
    "5. Academic / Tool-supported Methods\n",
    "For long-term or large-scale projects, automated tools may help:\n",
    "Hierarchical-DBSCAN for Extracting Microservices (research paper)\n",
    " Uses clustering to suggest how classes could form microservices. Useful in mapping dependencies before refactor.\n",
    " (arXiv)\n",
    "\n",
    "\n",
    "Feature Table Approach for Decomposition\n",
    " Semi-automated methodology to help breakdown monolith into service candidates based on feature correlations.\n",
    " (arXiv)\n",
    "\n",
    "\n",
    "CARGO: AI-Guided Dependency Analysis\n",
    " Advanced ML tool that analyzes code dependencies to guide microservice extraction with minimal cross-service coupling.\n",
    " (arXiv)\n",
    "\n",
    "\n",
    "\n",
    "Summary Table: Resources at a Glance\n",
    "Stage\n",
    "Resource & Focus\n",
    "Basic Structuring\n",
    "Python Guide, Dagster's Best Practices\n",
    "Python Modular Monolith\n",
    "Breadcrumbs Collector, Dev.to, Medium tutorial\n",
    "Why Modular First?\n",
    "Medium architecture overview, modular monolith benefits\n",
    "Domain Modeling/Planning\n",
    "Reddit discussion on bounded contexts and modular breakdown\n",
    "Tool-assisted Refactoring\n",
    "Academic research (DBSCAN, Feature Table, CARGO)\n",
    "\n",
    "\n",
    "Next Steps for You\n",
    "Start Small\n",
    " Begin by reorganizing your monolith into modules by feature or domain. Use the Breadcrumbs Collector or Dev.to guides as templates.\n",
    "\n",
    "\n",
    "Enforce Boundaries\n",
    " Ensure each module only exposes public APIs. Avoid breaking encapsulation‚Äîsee the Clean Architecture distinction in some parts of your system.\n",
    "\n",
    "\n",
    "Test Rigorously\n",
    " Move functions into modules gradually and validate via tests (unit/integration).\n",
    "\n",
    "\n",
    "Iterate Toward Modularity\n",
    " Don‚Äôt aim for perfection upfront. Refactor iteratively, using domain clarity and testing as your guardrails.\n",
    "\n",
    "\n",
    "Prepare for Potential Extraction\n",
    " When modules are clean, they‚Äôre easier to ‚Äúextract‚Äù into services later‚Äîthanks to good boundaries and folder layout.\n",
    "\n",
    "# Quick Reference Commands\n",
    ">### Once everything is set up, you can download any Kaggle dataset with:\n",
    "```py\n",
    "## Command line\n",
    "kaggle datasets download -d [dataset-slug] -p [destination-path]\n",
    "\n",
    "# Python\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files(dataset='[dataset-slug]', path='[destination]', unzip=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee25bb",
   "metadata": {},
   "source": [
    "**Best practices for setting up a project in Windsurf (your IDE on Mac)** so it‚Äôs clean, reproducible, and scalable. Here‚Äôs a checklist that blends **data science / software engineering conventions** with the Windsurf environment:\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 1. Create a Clean Project Structure\n",
    "\n",
    "Inside Windsurf, set up a folder like:\n",
    "\n",
    "```\n",
    "my_project/\n",
    "‚îú‚îÄ‚îÄ src/             # source code (modules, utils, pipelines)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ my_project/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ data_utils.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ model_utils.py\n",
    "‚îú‚îÄ‚îÄ notebooks/       # Jupyter notebooks (exploration, EDA)\n",
    "‚îú‚îÄ‚îÄ data/            # (gitignored) raw & processed datasets\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ processed/\n",
    "‚îú‚îÄ‚îÄ logs/            # logging outputs\n",
    "‚îú‚îÄ‚îÄ tests/           # pytest unit tests\n",
    "‚îú‚îÄ‚îÄ .gitignore\n",
    "‚îú‚îÄ‚îÄ requirements.txt # or pyproject.toml\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îî‚îÄ‚îÄ venv/ or .venv/  # virtual environment (gitignored)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 2. Set Up Git for Version Control\n",
    "\n",
    "In Windsurf terminal:\n",
    "\n",
    "```bash\n",
    "git init\n",
    "echo \"venv/\" >> .gitignore\n",
    "echo \"data/\" >> .gitignore\n",
    "echo \"logs/\" >> .gitignore\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "```\n",
    "\n",
    "Then link to GitHub if needed:\n",
    "\n",
    "```bash\n",
    "git remote add origin https://github.com/username/repo.git\n",
    "git branch -M main\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 3. Create & Activate a Virtual Environment\n",
    "\n",
    "From Windsurf terminal:\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate   # Mac/Linux\n",
    "```\n",
    "\n",
    "Then install basics:\n",
    "\n",
    "```bash\n",
    "pip install -U pip wheel setuptools\n",
    "pip install jupyter pandas numpy matplotlib scikit-learn\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 4. Configure Windsurf to Use the Venv\n",
    "\n",
    "* In **Command Palette** (`Cmd+Shift+P`), search for:\n",
    "  **Python: Select Interpreter** ‚Üí choose your `.venv`.\n",
    "* For notebooks, install kernel:\n",
    "\n",
    "  ```bash\n",
    "  python -m ipykernel install --user --name=my_project\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 5. Add Essential Config Files\n",
    "\n",
    "* **`.gitignore`** ‚Üí ignore `venv/`, `data/`, `logs/`, `__pycache__/`.\n",
    "* **`README.md`** ‚Üí quick overview (purpose, setup, usage).\n",
    "* **`requirements.txt`** or **`pyproject.toml`** ‚Üí environment reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 6. Logging & Output Management\n",
    "\n",
    "* Direct logs to `/logs/` (not `src/`).\n",
    "* Save notebooks outputs to `/notebooks/`.\n",
    "* Save raw vs processed data to `/data/` (with subfolders).\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 7. Testing Setup (Optional but Powerful)\n",
    "\n",
    "* Use `pytest`:\n",
    "\n",
    "  ```bash\n",
    "  pip install pytest\n",
    "  ```\n",
    "* Add tests under `/tests/`:\n",
    "\n",
    "  ```\n",
    "  tests/\n",
    "    test_data_utils.py\n",
    "  ```\n",
    "\n",
    "Run with:\n",
    "\n",
    "```bash\n",
    "pytest\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 8. Keep Notebooks Monolithic ‚Üí then Modularize\n",
    "\n",
    "* Do quick experiments in `/notebooks/`.\n",
    "* Once stable, move reusable code into `/src/my_project/`.\n",
    "* Import it back into notebooks with:\n",
    "\n",
    "  ```python\n",
    "  from my_project import data_utils\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 9. Optional (but useful)\n",
    "\n",
    "* **Pre-commit hooks** for formatting/linting (`black`, `flake8`).\n",
    "* **.env file** for secrets/keys (don‚Äôt commit it).\n",
    "* **Makefile** or **task runner** for common commands (`make clean`, `make run`).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **TL;DR**:\n",
    "When setting up a project in Windsurf, focus on:\n",
    "\n",
    "1. Clean folder layout.\n",
    "2. Virtual environment tied to the IDE.\n",
    "3. Git + `.gitignore` from day one.\n",
    "4. Reproducibility (`requirements.txt`).\n",
    "5. Separation of code (`src/`), data (`data/`), logs (`logs/`), and notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to make you a **ready-to-run Windsurf project template** (folders, `.gitignore`, `requirements.txt`, and a sample `src/utils.py`) that you can drop into your next project?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f889b2c",
   "metadata": {},
   "source": [
    "# How to Organize Your Code Snippets Library\n",
    "\n",
    "## Option 1: Markdown Files by Category (Recommended for Beginners)\n",
    "\n",
    "Create separate `.md` files for each category:\n",
    "\n",
    "### File Structure:\n",
    "```\n",
    "code_library/\n",
    "‚îú‚îÄ‚îÄ missing_values.md\n",
    "‚îú‚îÄ‚îÄ data_types.md\n",
    "‚îú‚îÄ‚îÄ anomaly_detection.md\n",
    "‚îú‚îÄ‚îÄ data_cleaning.md\n",
    "‚îú‚îÄ‚îÄ basic_exploration.md\n",
    "‚îî‚îÄ‚îÄ visualization.md\n",
    "```\n",
    "\n",
    "### Example: `missing_values.md`\n",
    "```markdown\n",
    "# Missing Values Code Snippets\n",
    "\n",
    "## Quick Count\n",
    "```python\n",
    "# Most concise\n",
    "df.isna().sum().sum()\n",
    "```\n",
    "\n",
    "## Detailed Analysis\n",
    "```python\n",
    "# Show which columns have missing values\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n",
    "```\n",
    "\n",
    "## Business Context Check\n",
    "```python\n",
    "# Check if missing values make business sense\n",
    "missing_mask = df['TotalCharges'].isnull()\n",
    "print(df[missing_mask][['customerID', 'tenure', 'MonthlyCharges']])\n",
    "```\n",
    "\n",
    "## When to Use Each:\n",
    "- **Quick count**: Initial dataset assessment\n",
    "- **Detailed analysis**: When you find missing values\n",
    "- **Business context**: Understanding WHY values are missing\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Option 2: Jupyter Notebook Library\n",
    "\n",
    "Create `code_snippets.ipynb` with sections:\n",
    "\n",
    "```python\n",
    "# =============================================================================\n",
    "# MISSING VALUES TOOLKIT\n",
    "# =============================================================================\n",
    "\n",
    "# Quick check\n",
    "def quick_missing_check(df):\n",
    "    \"\"\"Return total missing values count\"\"\"\n",
    "    return df.isna().sum().sum()\n",
    "\n",
    "# Detailed check  \n",
    "def detailed_missing_check(df):\n",
    "    \"\"\"Show missing values by column\"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    return missing[missing > 0]\n",
    "\n",
    "# Usage examples and when to use each method\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Option 3: Python Module (Advanced)\n",
    "\n",
    "Create `data_toolkit.py`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "class DataExplorer:\n",
    "    \"\"\"Collection of data exploration utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def missing_values_quick(df):\n",
    "        \"\"\"Quick missing values count\"\"\"\n",
    "        return df.isna().sum().sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def missing_values_detailed(df):\n",
    "        \"\"\"Detailed missing values analysis\"\"\"\n",
    "        missing = df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            return missing[missing > 0]\n",
    "        else:\n",
    "            print(\"No missing values found\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_anomalies_in_column(df, column):\n",
    "        \"\"\"Find non-numeric values in supposedly numeric columns\"\"\"\n",
    "        anomalies = []\n",
    "        for val in df[column].unique():\n",
    "            if pd.isna(pd.to_numeric(val, errors='coerce')):\n",
    "                count = (df[column] == val).sum()\n",
    "                anomalies.append((val, count))\n",
    "        return anomalies\n",
    "\n",
    "# Usage: from data_toolkit import DataExplorer\n",
    "# DataExplorer.missing_values_quick(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Option 4: GitHub Repository Structure\n",
    "\n",
    "```\n",
    "data_analysis_toolkit/\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ missing_values/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ README.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ quick_checks.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ detailed_analysis.py\n",
    "‚îú‚îÄ‚îÄ data_cleaning/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ README.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ text_to_numeric.py\n",
    "‚îú‚îÄ‚îÄ anomaly_detection/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ README.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ find_weird_values.py\n",
    "‚îî‚îÄ‚îÄ examples/\n",
    "    ‚îú‚îÄ‚îÄ telco_dataset_examples.ipynb\n",
    "    ‚îî‚îÄ‚îÄ general_examples.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## My Recommendation for You: Start with Option 1\n",
    "\n",
    "### Why Markdown Files Work Best Initially:\n",
    "1. **Easy to search** - Ctrl+F across files\n",
    "2. **Copy-paste friendly** - Code blocks ready to use\n",
    "3. **Documentation built-in** - Context and when-to-use notes\n",
    "4. **Version control friendly** - Works great with Git\n",
    "5. **Portable** - Works on any platform\n",
    "\n",
    "### Sample Categories for Your Library:\n",
    "\n",
    "#### `missing_values.md`\n",
    "- Quick count methods\n",
    "- Detailed analysis\n",
    "- Business logic checks\n",
    "\n",
    "#### `data_types.md`  \n",
    "- Check data types\n",
    "- Convert text to numeric\n",
    "- Handle conversion errors\n",
    "\n",
    "#### `anomaly_detection.md`\n",
    "- Find weird values\n",
    "- Spot empty spaces\n",
    "- Identify non-convertible entries\n",
    "\n",
    "#### `data_cleaning.md`\n",
    "- Replace missing values\n",
    "- Handle empty spaces\n",
    "- Fix data type issues\n",
    "\n",
    "#### `quick_exploration.md`\n",
    "- Dataset shape and info\n",
    "- Sample data viewing\n",
    "- Basic statistics\n",
    "\n",
    "#### `visualization.md`\n",
    "- Simple plots\n",
    "- Quick correlation checks\n",
    "- Distribution analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Template for Each Category File:\n",
    "\n",
    "```markdown\n",
    "# [Category Name] - Code Snippets\n",
    "\n",
    "## Quick Reference\n",
    "[One-liner solutions]\n",
    "\n",
    "## Detailed Methods  \n",
    "[More comprehensive approaches]\n",
    "\n",
    "## When to Use Each\n",
    "[Decision guide]\n",
    "\n",
    "## Real Examples\n",
    "[From actual datasets you've worked with]\n",
    "\n",
    "## Common Pitfalls\n",
    "[Mistakes to avoid]\n",
    "\n",
    "## Related Categories\n",
    "[Links to other relevant snippet categories]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Pro Tips:\n",
    "\n",
    "### 1. Include Context\n",
    "Don't just store code - store the **why** and **when**:\n",
    "```markdown\n",
    "## Quick Missing Values Check\n",
    "```python\n",
    "df.isna().sum().sum()\n",
    "```\n",
    "**When to use:** Initial dataset assessment, want just a number\n",
    "**Returns:** Total count of missing values across entire dataset\n",
    "**Example output:** `42` (means 42 missing values total)\n",
    "```\n",
    "\n",
    "### 2. Tag by Use Case\n",
    "```markdown\n",
    "**Tags:** #initial-exploration #data-quality #quick-check\n",
    "```\n",
    "\n",
    "### 3. Include Failed Attempts\n",
    "```markdown\n",
    "## What Doesn't Work\n",
    "```python\n",
    "# DON'T DO THIS - checks length, not missing count\n",
    "if len(missing_data) > 0:  # Always true!\n",
    "```\n",
    "**Why it fails:** Length is number of columns, not missing values\n",
    "```\n",
    "\n",
    "### 4. Version Your Learning\n",
    "```markdown\n",
    "## Evolution of My Approach\n",
    "- **Level 0:** `df.isnull().sum()`  \n",
    "- **Level 1:** `df.isnull().sum()[df.isnull().sum() > 0]`\n",
    "- **Level 2:** Business context analysis with missing patterns\n",
    "```\n",
    "\n",
    "This way, you're not just collecting code - you're building a **learning journey** that shows your progression and thinking process to hiring managers!\n",
    "\n",
    "Start with simple markdown files, and as your library grows, you can always reorganize into more sophisticated structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387a551",
   "metadata": {},
   "source": [
    "You're building a **personal code library** - this is exactly what professional data scientists do. Here are several effective approaches:## üéØ **My Recommendation: Start Simple with Markdown Files**\n",
    "\n",
    "Create a folder structure like this:\n",
    "```\n",
    "my_code_library/\n",
    "‚îú‚îÄ‚îÄ missing_values.md\n",
    "‚îú‚îÄ‚îÄ data_types.md\n",
    "‚îú‚îÄ‚îÄ anomaly_detection.md\n",
    "‚îú‚îÄ‚îÄ data_cleaning.md\n",
    "‚îî‚îÄ‚îÄ quick_exploration.md\n",
    "```\n",
    "\n",
    "## üìù **Why Markdown Files Work Best:**\n",
    "\n",
    "1. **Easy to search** - Ctrl+F to find what you need\n",
    "2. **Copy-paste ready** - Code blocks ready to use  \n",
    "3. **Context included** - Notes on when/why to use each method\n",
    "4. **GitHub friendly** - Looks professional in your portfolio\n",
    "\n",
    "## üí° **Example: `missing_values.md`**\n",
    "\n",
    "```markdown\n",
    "# Missing Values Toolkit\n",
    "\n",
    "## Quick Count (Most Used)\n",
    "```python\n",
    "df.isna().sum().sum()  # 18 characters, fastest\n",
    "```\n",
    "\n",
    "## Show Details When Needed  \n",
    "```python\n",
    "df.isnull().sum()[df.isnull().sum() > 0]  # Only non-zero columns\n",
    "```\n",
    "\n",
    "## When to Use Each:\n",
    "- **Quick count**: Initial assessment, just need total\n",
    "- **Detailed view**: When you find missing values and need to know where\n",
    "```\n",
    "\n",
    "## üöÄ **Start Your Library Today:**\n",
    "\n",
    "1. **Create the folder structure**\n",
    "2. **Start with one file** - `missing_values.md`\n",
    "3. **Add your variations** from today's session\n",
    "4. **Include context** - when to use each method\n",
    "5. **Build as you learn** - add new categories in future levels\n",
    "\n",
    "## üìà **Benefits for Your Goals:**\n",
    "\n",
    "- **Goal #2**: Documents your learning progression\n",
    "- **Goal #4**: Creates your personal reference library\n",
    "- **Portfolio value**: Shows systematic thinking to hiring managers\n",
    "\n",
    "This approach scales perfectly - start simple, add complexity as you grow!\n",
    "\n",
    "Here are several ways to check for empty values across the entire dataset:\n",
    "\n",
    "## **Quick One-Liners for Different Types of Empty:**\n",
    "\n",
    "### **Check NaN/Null values:**\n",
    "```python\n",
    "df.isnull().sum().sum()  # Total count\n",
    "df.isnull().sum()        # By column\n",
    "```\n",
    "\n",
    "### **Check empty strings (''):**\n",
    "```python\n",
    "(df == '').sum().sum()   # Total count\n",
    "(df == '').sum()         # By column\n",
    "```\n",
    "\n",
    "### **Check empty spaces (' '):**\n",
    "```python\n",
    "(df == ' ').sum().sum()  # Total count\n",
    "(df == ' ').sum()        # By column\n",
    "```\n",
    "\n",
    "### **Ultimate one-liner for all types:**\n",
    "```python\n",
    "print(f\"NaN: {df.isnull().sum().sum()}, Empty: {(df == '').sum().sum()}, Spaces: {(df == ' ').sum().sum()}\")\n",
    "```\n",
    "\n",
    "## **Most Comprehensive Check:**\n",
    "\n",
    "```python\n",
    "# Check every column for all types of empty values\n",
    "for col in df.columns:\n",
    "    nan_count = df[col].isnull().sum()\n",
    "    empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "    space_count = (df[col] == ' ').sum() if df[col].dtype == 'object' else 0\n",
    "    \n",
    "    total = nan_count + empty_count + space_count\n",
    "    if total > 0:\n",
    "        print(f\"{col}: {total} empty values\")\n",
    "```\n",
    "\n",
    "## **Show Actual Problem Rows:**\n",
    "\n",
    "```python\n",
    "# Find and display rows with any empty values\n",
    "problem_rows = df[df.isnull().any(axis=1) | (df == '').any(axis=1) | (df == ' ').any(axis=1)]\n",
    "print(f\"Rows with empty values: {len(problem_rows)}\")\n",
    "if len(problem_rows) > 0:\n",
    "    print(problem_rows[['customerID']].head())  # Show first few problem customer IDs\n",
    "```\n",
    "\n",
    "**After your TotalCharges fix, you should see:**\n",
    "- **NaN: 0**\n",
    "- **Empty strings: 0** \n",
    "- **Spaces: 0**\n",
    "\n",
    "If you see any non-zero counts, that tells you where you still have empty value issues!\n",
    "\n",
    "---\n",
    "\n",
    "># **#2: Keep a snippet library handy across projects/workspaces**\n",
    "\n",
    "    Here are three ‚Äúprofessional and portable‚Äù ways to keep a snippet library handy across projects/workspaces on your Mac‚Äîordered from most editor-native to most universal. Pick one or combine B+C for a killer setup.\n",
    "\n",
    "# A) VS Code user snippets + built-in Sync (simple, editor-native)\n",
    "\n",
    "**Best if you primarily use VS Code.**\n",
    "\n",
    "1. Open Command Palette ‚Üí ‚ÄúPreferences: Configure User Snippets‚Äù.\n",
    "2. Create language or global snippets. Example (`python.json`):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Py docstring\": {\n",
    "    \"prefix\": \"pydoc\",\n",
    "    \"body\": [\n",
    "      \"\\\"\\\"\\\"${1:Summary}\",\n",
    "      \"\",\n",
    "      \"Args:\",\n",
    "      \"    ${2:param}: ${3:desc}\",\n",
    "      \"\",\n",
    "      \"Returns:\",\n",
    "      \"    ${4:type}: ${5:desc}\",\n",
    "      \"\\\"\\\"\\\"\"\n",
    "    ],\n",
    "    \"description\": \"Docstring scaffold\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Turn on **Settings Sync** (Account icon ‚Üí Turn On Sync). Your snippets follow you on any machine you sign into VS Code with.\n",
    "\n",
    "**Pro tip (shared across repos):** keep your snippet JSON files in a Git repo and **symlink** them into VS Code‚Äôs snippets folder so you can version-control them:\n",
    "\n",
    "```bash\n",
    "# Paths (macOS)\n",
    "VS_SNIPS=\"$HOME/Library/Application Support/Code/User/snippets\"\n",
    "mkdir -p ~/snippets/vscode\n",
    "ln -s ~/snippets/vscode/python.json \"$VS_SNIPS/python.json\"\n",
    "```\n",
    "\n",
    "# B) Git + Markdown snippets + fzf CLI (portable, editor-agnostic)\n",
    "\n",
    "**Best if you hop between tools (VS Code, Windsurf, terminals, notebooks).**\n",
    "\n",
    "1. Make a repo:\n",
    "\n",
    "```\n",
    "~/snippets/\n",
    "  python/\n",
    "  sql/\n",
    "  dbt/\n",
    "  shell/\n",
    "  README.md\n",
    "```\n",
    "\n",
    "Each snippet is a small `.md` with a clear title and a fenced code block.\n",
    "\n",
    "2. Install tools (free):\n",
    "\n",
    "```bash\n",
    "brew install fzf ripgrep\n",
    "```\n",
    "\n",
    "3. Add a tiny **`snip`** helper to your `~/.zshrc`:\n",
    "\n",
    "````zsh\n",
    "snip() {\n",
    "  local file\n",
    "  file=$(rg -l --hidden --glob \"!*.git/*\" \"${*:-.}\" \"$HOME/snippets\" | fzf --prompt=\"Snip> \") || return\n",
    "  # copy the code block contents to clipboard\n",
    "  awk '/^```/{f=!f; next} f' \"$file\" | pbcopy\n",
    "  echo \"üìã Copied from: $file\"\n",
    "}\n",
    "````\n",
    "\n",
    "Usage:\n",
    "\n",
    "````bash\n",
    "snip pandas groupby\n",
    "# copies the code inside the first ``` block from the chosen file\n",
    "# ‚åò+V (paste) in any editor/terminal\n",
    "````\n",
    "\n",
    "**Why this rocks:** works everywhere (terminal, notebooks, different editors), versioned with Git, searchable via fuzzy find. You can keep private snippets in a private repo.\n",
    "\n",
    "# C) Text expander (Espanso) for instant boilerplate (cross-app)\n",
    "\n",
    "**Great for short, frequently typed patterns.** Free & open-source.\n",
    "\n",
    "```bash\n",
    "brew install espanso\n",
    "espanso start\n",
    "espanso edit\n",
    "```\n",
    "\n",
    "Add triggers in `default.yml`:\n",
    "\n",
    "```yaml\n",
    "matches:\n",
    "  - trigger: \":sqlsel\"\n",
    "    replace: |\n",
    "      SELECT ${1:*}\n",
    "      FROM ${2:table}\n",
    "      WHERE ${3:cond};\n",
    "    vars:\n",
    "      - name: cursor\n",
    "        type: cursor\n",
    "  - trigger: \":dbtmodel\"\n",
    "    replace: |\n",
    "      {{ config(materialized='table') }}\n",
    "      WITH src AS (\n",
    "          SELECT * FROM {{ ref('${1:stg_table}') }}\n",
    "      )\n",
    "      SELECT * FROM src;\n",
    "    propagate_case: true\n",
    "filters:\n",
    "  apps:\n",
    "    only: [\"com.microsoft.VSCode\"]  # limit expansions to VS Code\n",
    "```\n",
    "\n",
    "Now typing `:sqlsel` in VS Code expands into a SQL skeleton with tabstops‚Äîwithout touching your editor‚Äôs snippet system.\n",
    "\n",
    "---\n",
    "\n",
    "## What I recommend for you\n",
    "\n",
    "Given you‚Äôre on macOS + zsh and bounce across projects:\n",
    "\n",
    "* **Primary:** VS Code user snippets + Sync (A) for language-aware tab-trigger snippets.\n",
    "* **Universal:** Git + `snip` (B) so you can search/paste any snippet in *any* workspace, terminal, or notebook.\n",
    "* **Boilerplate:** Espanso (C) for lightning-fast expansions you want everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## Starter pack you can paste in today\n",
    "\n",
    "**Repo skeleton**\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/snippets/{python,sql,dbt,shell}\n",
    "git init ~/snippets\n",
    "```\n",
    "\n",
    "**Example files**\n",
    "`~/snippets/sql/select-where.md`\n",
    "\n",
    "````md\n",
    "# Select with WHERE & limit\n",
    "```sql\n",
    "SELECT ${1:cols}\n",
    "FROM ${2:schema.table}\n",
    "WHERE ${3:cond}\n",
    "LIMIT ${4:100};\n",
    "````\n",
    "\n",
    "````\n",
    "\n",
    "`~/snippets/python/pandas-null-check.md`\n",
    "```md\n",
    "# Null audit (top offenders)\n",
    "```python\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(na[na>0].head(10))\n",
    "````\n",
    "\n",
    "````\n",
    "\n",
    "**VS Code snippet (dbt model)**\n",
    "`~/snippets/vscode/sql.json`\n",
    "```json\n",
    "{\n",
    "  \"dbt model\": {\n",
    "    \"prefix\": \"dbtmodel\",\n",
    "    \"body\": [\n",
    "      \"{{ config(materialized='table') }}\",\n",
    "      \"WITH src AS (\",\n",
    "      \"  SELECT * FROM {{ ref('${1:stg_table}') }}\",\n",
    "      \")\",\n",
    "      \"SELECT * FROM src;\"\n",
    "    ],\n",
    "    \"description\": \"dbt model scaffold\"\n",
    "  }\n",
    "}\n",
    "````\n",
    "\n",
    "Then symlink it as shown above.\n",
    "\n",
    "---\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "* **Secrets:** never store tokens/keys in snippets; use placeholders like `${SECRET_NAME}`.\n",
    "* **Licensing:** if you paste from blogs/StackOverflow, keep a note/link at the top of the snippet file.\n",
    "* **Naming:** use a consistent prefix (e.g., `:sql*`, `:py*`) for Espanso triggers to avoid accidental expansion.\n",
    "\n",
    ">If you want, tell me your top 10 snippet needs (Python/pandas, SQL/dbt, shell, GCP/AWS), and I‚Äôll generate the files to drop into `~/snippets/` plus the matching VS Code/Espanso entries.\n",
    "\n",
    "---\n",
    "\n",
    "># **#3: Windsurf with VS Code‚Äìstyle snippets**\n",
    "\n",
    "Yep‚ÄîWindsurf plays nicely with **VS Code‚Äìstyle snippets**, plus you can layer on a git repo and a text expander for portability. Here‚Äôs a clean setup that works across projects/workspaces.\n",
    "\n",
    "# 1) Built-in (VS Code‚Äìstyle) user & workspace snippets\n",
    "\n",
    "* Open the **Command Palette ‚Üí ‚ÄúPreferences: Configure User Snippets.‚Äù** Create a language file (e.g., `python.json`) or a **global** snippets file. Snippet format is the same as VS Code‚Äôs JSON (prefix/body/description). ([Arsturn][1], [Visual Studio Code][2])\n",
    "* Bringing snippets from VS Code? On first run Windsurf lets you **import settings from VS Code/Cursor**, which includes snippets. ([DEV Community][3])\n",
    "* For repo-scoped snippets, add a workspace file (e.g., `.vscode/snippets.code-snippets`) so they travel with the project in Git. (Same mechanism as VS Code.) ([Stack Overflow][4])\n",
    "\n",
    "Example snippet body (works in Windsurf since it‚Äôs VS Code syntax):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Docstring scaffold\": {\n",
    "    \"prefix\": \"pydoc\",\n",
    "    \"body\": [\n",
    "      \"\\\"\\\"\\\"${1:Summary}\",\n",
    "      \"\",\n",
    "      \"Args:\",\n",
    "      \"    ${2:param}: ${3:desc}\",\n",
    "      \"\",\n",
    "      \"Returns:\",\n",
    "      \"    ${4:type}: ${5:desc}\",\n",
    "      \"\\\"\\\"\\\"\"\n",
    "    ],\n",
    "    \"description\": \"Python docstring\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "# 2) Manage snippets with extensions (Open VSX)\n",
    "\n",
    "Windsurf uses the **Open VSX** marketplace, so install snippet helpers from there in the Extensions view:\n",
    "\n",
    "* **Snippets Manager** ‚Äì simple UI to create/edit snippets. ([open-vsx.org][5])\n",
    "* **Snippets Viewer** ‚Äì browse built-in, extension, and user snippets in a tree view. ([open-vsx.org][6])\n",
    "* **Snippets Ranger** ‚Äì create/select snippet files quickly. ([open-vsx.org][7])\n",
    "  (If you‚Äôre used to VS Code‚Äôs marketplace, note the store difference‚ÄîWindsurf surfaces Open VSX by default.) ([Reddit][8])\n",
    "\n",
    "# 3) Keep them portable with Git (+ optional symlink)\n",
    "\n",
    "* Put all snippet JSON (and markdown examples) in `~/snippets/` under Git.\n",
    "* In Windsurf, open one of your snippet files and copy its on-disk path; then **symlink** your repo files to that location so edits are versioned automatically (works on Mac/Linux).\n",
    "  This avoids guessing paths and keeps snippets consistent across workspaces.\n",
    "\n",
    "# 4) Universal quick-paste option (works in any app)\n",
    "\n",
    "If you want snippets outside the editor too:\n",
    "\n",
    "* **Espanso** (free text expander): type `:sqlsel` ‚Üí expands to a SQL skeleton anywhere.\n",
    "* Or a tiny **fzf** picker in your terminal that copies code blocks from `~/snippets/*.md` to clipboard (great inside Windsurf‚Äôs integrated terminal).\n",
    "\n",
    "# 5) Bonus: make snippets ‚Äúsearchable‚Äù by the AI\n",
    "\n",
    "If you keep a **Google Doc** of your favorite patterns, Windsurf‚Äôs **Knowledge Base / docs context** can ingest docs for retrieval in chat. It‚Äôs not an inserter like snippets, but handy for ‚Äúremind me of the dbt model scaffold‚Äù prompts. ([Windsurf Docs][9])\n",
    "\n",
    "---\n",
    "\n",
    "## A setup I‚Äôd recommend for you\n",
    "\n",
    "1. **User & workspace snippets** for tab-trigger speed. ([Arsturn][1], [Visual Studio Code][2])\n",
    "2. **Git repo** at `~/snippets/` + **symlink** to Windsurf‚Äôs snippet files so changes sync across workspaces.\n",
    "3. Install **Snippets Manager** (UI), plus **Snippets Viewer** (browse). ([open-vsx.org][5])\n",
    "4. Add **Espanso** for cross-app boilerplate (optional).\n",
    "5. (Nice-to-have) A **Google Doc** of patterns connected to Windsurf‚Äôs knowledge base for quick AI recall. ([Windsurf Docs][9])\n",
    "\n",
    "If you want, tell me your top 10 snippets (Python/pandas, SQL/dbt, shell/GCP) and I‚Äôll generate the JSON files + a starter `~/snippets/` repo structure to drop in.\n",
    "\n",
    "[1]: https://www.arsturn.com/blog/creating-custom-code-snippets-in-windsurf-for-reusability \"Mastering Code Snippets in Windsurf: Reusability Techniques\"\n",
    "[2]: https://code.visualstudio.com/docs/editing/userdefinedsnippets?utm_source=chatgpt.com \"Snippets in Visual Studio Code\"\n",
    "[3]: https://dev.to/proflead/this-ai-ide-can-code-for-you-windsurf-ai-full-tutorial-4p94 \"This AI IDE Can Code For You ‚Äì Windsurf AI Full Tutorial - DEV Community\"\n",
    "[4]: https://stackoverflow.com/questions/29995863/how-to-add-custom-code-snippets-in-vscode?utm_source=chatgpt.com \"How to add custom code snippets in VSCode?\"\n",
    "[5]: https://open-vsx.org/extension/zjffun/snippetsmanager?utm_source=chatgpt.com \"Snippets Manager\"\n",
    "[6]: https://open-vsx.org/extension/RandomFractalsInc/snippets-viewer?utm_source=chatgpt.com \"Snippets Viewer\"\n",
    "[7]: https://open-vsx.org/extension/robole/snippets-ranger?utm_source=chatgpt.com \"Snippets Ranger\"\n",
    "[8]: https://www.reddit.com/r/RooCode/comments/1i6ns94/new_users_discovery_of_roocode_on_vscode/?utm_source=chatgpt.com \"New users' discovery of RooCode on vscode marketplace ...\"\n",
    "[9]: https://docs.windsurf.com/context-awareness/overview?utm_source=chatgpt.com \"Overview - Windsurf Docs\"\n",
    "\n",
    "---\n",
    "\n",
    "># **#4: create a custome library that gives me suggestions in a jupyter cell or python file when I type # to call the library then call a specific snippet by name**\n",
    "\n",
    "Yes‚Äîyou can make a tiny IPython/Jupyter extension that:\n",
    "\n",
    "* lets you write a **comment directive** like `# snip <name>` which gets **expanded into code** before execution, and\n",
    "* gives you **Tab-completion suggestions** for snippet names after you type `# snip ` in a Jupyter cell (works in JupyterLab/Notebook and VS Code‚Äôs Jupyter).\n",
    "\n",
    "Below is a minimal, drop-in ‚Äúsnippet library‚Äù you can keep in `~/.snippets/`. It uses a YAML file for your snippets, a transformer to expand `# snip ...` lines, and a custom completer to suggest names.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Snippet storage (YAML)\n",
    "\n",
    "`~/.snippets/snippets.yaml`\n",
    "\n",
    "```yaml\n",
    "pandas.null_audit: |\n",
    "  na = df.isna().sum().sort_values(ascending=False)\n",
    "  print(na[na>0].head(10))\n",
    "sql.select_where: |\n",
    "  SELECT ${1:cols}\n",
    "  FROM ${2:schema.table}\n",
    "  WHERE ${3:cond}\n",
    "  LIMIT ${4:100};\n",
    "dbt.model_scaffold: |\n",
    "  {{ config(materialized='table') }}\n",
    "  WITH src AS (\n",
    "    SELECT * FROM {{ ref('${1:stg_table}') }}\n",
    "  )\n",
    "  SELECT * FROM src;\n",
    "```\n",
    "\n",
    "> You can keep multiple YAML files under `~/.snippets/`‚Äîthe loader will merge them.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) The extension (`snipline.py`)\n",
    "\n",
    "Put this file anywhere on your PYTHONPATH (e.g., `~/snipline/snipline.py`) and `pip install pyyaml` if you don‚Äôt have it.\n",
    "\n",
    "```python\n",
    "# snipline.py\n",
    "from __future__ import annotations\n",
    "import os, re, glob, time, yaml, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from IPython.core.magic import Magics, magics_class, line_magic\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "SNIP_DIR = Path(os.environ.get(\"SNIP_DIR\", \"~/.snippets\")).expanduser()\n",
    "\n",
    "class SnipStore:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.dir = Path(directory)\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._mtimes: Dict[str, float] = {}\n",
    "        self.reload()\n",
    "\n",
    "    def reload(self):\n",
    "        self.cache.clear()\n",
    "        self._mtimes.clear()\n",
    "        for yml in glob.glob(str(self.dir / \"*.y*ml\")):\n",
    "            with open(yml, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f) or {}\n",
    "            for k, v in data.items():\n",
    "                self.cache[str(k)] = str(v)\n",
    "            self._mtimes[yml] = os.path.getmtime(yml)\n",
    "\n",
    "    def maybe_reload(self):\n",
    "        for yml, old in list(self._mtimes.items()):\n",
    "            if os.path.exists(yml) and os.path.getmtime(yml) != old:\n",
    "                self.reload()\n",
    "                break\n",
    "\n",
    "    def names(self, prefix: str = \"\"):\n",
    "        self.maybe_reload()\n",
    "        return sorted([k for k in self.cache if k.startswith(prefix)])\n",
    "\n",
    "    def get(self, name: str) -> Optional[str]:\n",
    "        self.maybe_reload()\n",
    "        return self.cache.get(name)\n",
    "\n",
    "STORE = SnipStore(SNIP_DIR)\n",
    "\n",
    "# --- Input transformer: replace lines like \"# snip <name>\" with the snippet text\n",
    "_SNIP_LINE = re.compile(r\"^[ \\t]*#\\s*snip\\s+([A-Za-z0-9_.\\-/]+)[ \\t]*$\", re.MULTILINE)\n",
    "\n",
    "def _expand_snips(cell: str) -> str:\n",
    "    def repl(m):\n",
    "        name = m.group(1)\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            # Leave a visible marker if missing\n",
    "            return f\"# [snip: '{name}' not found]\"\n",
    "        # Preserve indentation of the directive line\n",
    "        indent = len(m.string[:m.start()].splitlines()[-1]) - len(m.string[:m.start()].splitlines()[-1].lstrip())\n",
    "        ind = \" \" * max(indent, 0)\n",
    "        expanded = textwrap.indent(body.rstrip(\"\\n\"), ind)\n",
    "        return expanded\n",
    "    return _SNIP_LINE.sub(repl, cell)\n",
    "\n",
    "# --- Tab completion for \"# snip \" or \"%snip \" prefixes\n",
    "def _snip_completer(self, event):\n",
    "    # Works for both comments and magic usage\n",
    "    line = getattr(event, \"line\", \"\")\n",
    "    # capture partial name user is typing\n",
    "    m = re.search(r\"(#\\s*snip|%snip)\\s+([A-Za-z0-9_.\\-/]*)$\", line[:getattr(event, \"cursor_position\", len(line))])\n",
    "    if not m:\n",
    "        return []\n",
    "    prefix = m.group(2)\n",
    "    return STORE.names(prefix)\n",
    "\n",
    "@magics_class\n",
    "class SnipMagics(Magics):\n",
    "    @line_magic(\"snip\")\n",
    "    def snip(self, line):\n",
    "        \"\"\"Usage: %snip <name>  -> prints the snippet body\n",
    "        With 'insert' in classic Notebook, it replaces the current cell.\"\"\"\n",
    "        name = line.strip()\n",
    "        if not name:\n",
    "            print(\"\\n\".join(STORE.names()))\n",
    "            return\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            print(f\"[snip: '{name}' not found]\")\n",
    "            return\n",
    "        # Print for copy/paste\n",
    "        print(body)\n",
    "        # Optional: try to replace current cell in classic Notebook UI\n",
    "        try:\n",
    "            js = Javascript(\"\"\"\n",
    "                if (typeof Jupyter !== 'undefined') {\n",
    "                  var cell = Jupyter.notebook.get_selected_cell();\n",
    "                  cell.set_text(atob('%s'));\n",
    "                }\n",
    "            \"\"\" % body.encode(\"utf-8\").hex())\n",
    "            display(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    # Expand \"# snip ...\" lines before execution\n",
    "    ip.input_transformers_post.append(_expand_snips)\n",
    "    # Add completer (works in Jupyter and VS Code's Jupyter)\n",
    "    try:\n",
    "        ip.set_hook('complete_command', _snip_completer, re_key='.*')\n",
    "    except Exception:\n",
    "        # Fallback: older IPython versions\n",
    "        completer = getattr(ip, 'Completer', None)\n",
    "        if completer and hasattr(completer, 'matchers'):\n",
    "            completer.matchers.insert(0, lambda self, text: [])\n",
    "    ip.register_magics(SnipMagics)\n",
    "\n",
    "def unload_ipython_extension(ip):\n",
    "    # Best-effort cleanup; new sessions usually just restart\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Use it in Jupyter/VS Code‚Äôs Jupyter\n",
    "\n",
    "In a cell:\n",
    "\n",
    "```python\n",
    "%load_ext snipline   # run once per session (or auto-load, see below)\n",
    "```\n",
    "\n",
    "Now you can:\n",
    "\n",
    "* Type: `# snip pandas.null_audit` then press **Tab** after `# snip ` to see snippet **name suggestions**.\n",
    "* Run the cell: that line is **replaced** with the snippet‚Äôs code before execution.\n",
    "* Or: `%snip pandas.null_audit` to print/insert the snippet body.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# snip pandas.null_audit\n",
    "```\n",
    "\n",
    "‚Ä¶expands to:\n",
    "\n",
    "```python\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(na[na>0].head(10))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Auto-load every session (optional)\n",
    "\n",
    "Create (or edit) `~/.ipython/profile_default/ipython_config.py`:\n",
    "\n",
    "```python\n",
    "c = get_config()\n",
    "c.InteractiveShellApp.extensions = ['snipline']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes & limits\n",
    "\n",
    "* **Where suggestions work:** in **Jupyter contexts** (classic, Lab, or VS Code‚Äôs Jupyter) because completion comes from the kernel. In a plain `.py` file editor view, completion is handled by the editor‚Äôs language server, so you won‚Äôt get the `# snip` suggestions there. For `.py` editing, pair this with **editor snippets** (VS Code/Windsurf) or a text expander (Espanso).\n",
    "* **Variables/placeholders:** You can embed `${1:placeholder}` in snippets for editor tabstops (useful when you paste into VS Code). The transformer leaves those as-is; your editor‚Äôs snippet engine will handle them if present.\n",
    "* **Versioning:** Keep `~/.snippets/` under Git so you can sync across machines. The loader auto-reloads when YAML files change.\n",
    "\n",
    "If you want, tell me your top 10 snippet names and I‚Äôll generate the YAML ready to drop into `~/.snippets/snippets.yaml`.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    Heck yes‚Äîlet‚Äôs make **snipples** your cross-workspace snippet engine. Below are two files:\n",
    "\n",
    "1. **`snipples.py`** ‚Äì an IPython/Jupyter extension so you can type `# snipples <name>` in a cell and it expands before execution (with tab-complete for names).\n",
    "2. **`~/.snipples/snippets.yaml`** ‚Äì a chunky library covering the Telco Churn project end-to-end (ingest ‚Üí EDA ‚Üí cleaning ‚Üí features ‚Üí ML ‚Üí explainability ‚Üí dbt/SQL ‚Üí ops).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) `snipples.py` (drop anywhere on your PYTHONPATH)\n",
    "\n",
    "```python\n",
    "# snipples.py\n",
    "from __future__ import annotations\n",
    "import os, re, glob, yaml, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from IPython.core.magic import Magics, magics_class, line_magic\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "SNIPS_DIR = Path(os.environ.get(\"SNIPPLES_DIR\", \"~/.snipples\")).expanduser()\n",
    "\n",
    "class SnipStore:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.dir = Path(directory)\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._mtimes: Dict[str, float] = {}\n",
    "        self.reload()\n",
    "\n",
    "    def reload(self):\n",
    "        self.cache.clear()\n",
    "        self._mtimes.clear()\n",
    "        self.dir.mkdir(parents=True, exist_ok=True)\n",
    "        for yml in sorted(self.dir.glob(\"*.y*ml\")):\n",
    "            with open(yml, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f) or {}\n",
    "            for k, v in data.items():\n",
    "                self.cache[str(k)] = str(v)\n",
    "            self._mtimes[str(yml)] = yml.stat().st_mtime\n",
    "\n",
    "    def maybe_reload(self):\n",
    "        for yml, old in list(self._mtimes.items()):\n",
    "            p = Path(yml)\n",
    "            if p.exists() and p.stat().st_mtime != old:\n",
    "                self.reload()\n",
    "                break\n",
    "\n",
    "    def names(self, prefix: str = \"\"):\n",
    "        self.maybe_reload()\n",
    "        return sorted([k for k in self.cache if k.startswith(prefix)])\n",
    "\n",
    "    def get(self, name: str) -> Optional[str]:\n",
    "        self.maybe_reload()\n",
    "        return self.cache.get(name)\n",
    "\n",
    "STORE = SnipStore(SNIPS_DIR)\n",
    "\n",
    "_SNIPLINE = re.compile(r\"^[ \\t]*#\\s*snipples\\s+([A-Za-z0-9_.\\-/]+)[ \\t]*$\", re.MULTILINE)\n",
    "\n",
    "def _expand_snips(cell: str) -> str:\n",
    "    def repl(m):\n",
    "        name = m.group(1)\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            return f\"# [snipples: '{name}' not found]\"\n",
    "        # Preserve indentation level of the directive line\n",
    "        line = cell[:m.start()].splitlines()[-1] if cell[:m.start()].splitlines() else \"\"\n",
    "        indent = len(line) - len(line.lstrip(\" \"))\n",
    "        expanded = textwrap.indent(body.rstrip(\"\\n\"), \" \" * max(indent, 0))\n",
    "        return expanded\n",
    "    return _SNIPLINE.sub(repl, cell)\n",
    "\n",
    "def _snipples_completer(self, event):\n",
    "    line = getattr(event, \"line\", \"\")\n",
    "    cur = getattr(event, \"cursor_position\", len(line))\n",
    "    m = re.search(r\"(#\\s*snipples|%snipples)\\s+([A-Za-z0-9_.\\-/]*)$\", line[:cur])\n",
    "    if not m:\n",
    "        return []\n",
    "    return STORE.names(m.group(2))\n",
    "\n",
    "@magics_class\n",
    "class SnipplesMagics(Magics):\n",
    "    @line_magic(\"snipples\")\n",
    "    def snipples(self, line):\n",
    "        \"\"\"%snipples <name>  -> prints/optionally inserts the snippet body\"\"\"\n",
    "        name = line.strip()\n",
    "        if not name:\n",
    "            print(\"\\n\".join(STORE.names()))\n",
    "            return\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            print(f\"[snipples: '{name}' not found]\")\n",
    "            return\n",
    "        print(body)\n",
    "        try:\n",
    "            js = Javascript(\"\"\"\n",
    "                if (typeof Jupyter !== 'undefined') {\n",
    "                  var cell = Jupyter.notebook.get_selected_cell();\n",
    "                  cell.set_text(atob('%s'));\n",
    "                }\n",
    "            \"\"\" % body.encode(\"utf-8\").hex())\n",
    "            display(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    ip.input_transformers_post.append(_expand_snips)\n",
    "    try:\n",
    "        ip.set_hook('complete_command', _snipples_completer, re_key='.*')\n",
    "    except Exception:\n",
    "        pass\n",
    "    ip.register_magics(SnipplesMagics)\n",
    "\n",
    "def unload_ipython_extension(ip):\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Use:** in a Jupyter cell run once per session:\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "# then type:  # snipples telco.qa.null_audit   (Tab to see suggestions)\n",
    "```\n",
    "\n",
    "To autoload every time, add to `~/.ipython/profile_default/ipython_config.py`:\n",
    "\n",
    "```python\n",
    "c = get_config()\n",
    "c.InteractiveShellApp.extensions = ['snipples']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) `~/.snipples/snippets.yaml` (Telco Churn end-to-end)\n",
    "\n",
    "Create the folder and file:\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.snipples\n",
    "# paste the following into ~/.snipples/snippets.yaml\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# =========================\n",
    "# Project scaffolding / setup\n",
    "# =========================\n",
    "proj.readme.telco: |\n",
    "  # Telco Churn ‚Äî Analytics & ML\n",
    "  ## Stack\n",
    "  - Python 3.11, pandas, scikit-learn, xgboost, catboost, imbalanced-learn, shap\n",
    "  - BigQuery + dbt (analytics)\n",
    "  - Great Expectations or Pandera (ingest DQ)\n",
    "  - MLflow (experiment tracking)\n",
    "\n",
    "  ## Structure\n",
    "  .\n",
    "  ‚îú‚îÄ data/{raw,interim,processed}\n",
    "  ‚îú‚îÄ notebooks/\n",
    "  ‚îú‚îÄ src/telco/...\n",
    "  ‚îú‚îÄ models/ (dbt)\n",
    "  ‚îî‚îÄ reports/\n",
    "\n",
    "  ## Targets\n",
    "  - Clean EDA + feature marts\n",
    "  - Baseline + tree models\n",
    "  - Explainability (SHAP)\n",
    "  - Reproducible pipelines + tests\n",
    "\n",
    "env.venv.setup: |\n",
    "  python3 -m venv .venv\n",
    "  source .venv/bin/activate\n",
    "  python -m pip install --upgrade pip\n",
    "  pip install pandas numpy scikit-learn imbalanced-learn xgboost catboost shap mlflow matplotlib seaborn pandera great-expectations python-dotenv pandas-gbq google-cloud-bigquery pyarrow\n",
    "\n",
    "nb.header.imports: |\n",
    "  import os, sys, math, json, textwrap, warnings\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  from pathlib import Path\n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "  pd.set_option(\"display.max_columns\", 100)\n",
    "  plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "# =========================\n",
    "# Ingest / IO\n",
    "# =========================\n",
    "ingest.csv.read_telco: |\n",
    "  dtype_map = {\n",
    "      \"customerID\": \"string\",\n",
    "      \"gender\": \"string\",\n",
    "      \"SeniorCitizen\": \"Int64\",\n",
    "      \"Partner\": \"string\",\n",
    "      \"Dependents\": \"string\",\n",
    "      \"tenure\": \"Int64\",\n",
    "      \"PhoneService\": \"string\",\n",
    "      \"MultipleLines\": \"string\",\n",
    "      \"InternetService\": \"string\",\n",
    "      \"OnlineSecurity\": \"string\",\n",
    "      \"OnlineBackup\": \"string\",\n",
    "      \"DeviceProtection\": \"string\",\n",
    "      \"TechSupport\": \"string\",\n",
    "      \"StreamingTV\": \"string\",\n",
    "      \"StreamingMovies\": \"string\",\n",
    "      \"Contract\": \"string\",\n",
    "      \"PaperlessBilling\": \"string\",\n",
    "      \"PaymentMethod\": \"string\",\n",
    "      \"MonthlyCharges\": \"float64\",\n",
    "      \"TotalCharges\": \"string\",   # coercion later\n",
    "      \"Churn\": \"string\"\n",
    "  }\n",
    "  df = pd.read_csv(\"data/raw/Telco-Customer-Churn.csv\", dtype=dtype_map)\n",
    "  print(df.shape)\n",
    "\n",
    "ingest.bigquery.read_table: |\n",
    "  # pip install pandas-gbq google-cloud-bigquery\n",
    "  from pandas_gbq import read_gbq\n",
    "  df = read_gbq(\"\"\"\n",
    "      SELECT * FROM `PROJECT.DATASET.telco_customers`\n",
    "  \"\"\", project_id=os.environ.get(\"GCP_PROJECT\"))\n",
    "\n",
    "save.outputs.standard_paths: |\n",
    "  Path(\"data/interim\").mkdir(parents=True, exist_ok=True)\n",
    "  Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "  df.to_csv(\"data/interim/telco_cleaned.csv\", index=False)\n",
    "\n",
    "# =========================\n",
    "# Data Quality (ingest)\n",
    "# =========================\n",
    "qa.null_audit: |\n",
    "  na = df.isna().sum().sort_values(ascending=False)\n",
    "  print(\"Missing by column:\")\n",
    "  print(na[na>0])\n",
    "\n",
    "qa.dup_pkey_check: |\n",
    "  dups = df[\"customerID\"].value_counts()\n",
    "  print(\"Dup primary keys:\", (dups > 1).sum())\n",
    "\n",
    "qa.pandera.schema_telco: |\n",
    "  import pandera as pa\n",
    "  from pandera import Column, Check\n",
    "  TelcoSchema = pa.DataFrameSchema({\n",
    "      \"customerID\": Column(str, nullable=False),\n",
    "      \"tenure\": Column(int, Check.in_range(0, 84), nullable=False),\n",
    "      \"MonthlyCharges\": Column(float, Check.in_range(0, 200), nullable=False),\n",
    "      \"TotalCharges\": Column(object, nullable=True),  # coerced later\n",
    "      \"Churn\": Column(str, Check.isin([\"Yes\",\"No\"]), nullable=False),\n",
    "  })\n",
    "  TelcoSchema.validate(df, lazy=True)\n",
    "\n",
    "qa.business_rule_totalcharges_tenure: |\n",
    "  bad = df[df[\"tenure\"].fillna(0) > 0].copy()\n",
    "  bad = bad[np.abs(pd.to_numeric(bad[\"TotalCharges\"], errors=\"coerce\") - bad[\"tenure\"] * bad[\"MonthlyCharges\"]) > 10]\n",
    "  print(f\"Rows outside $10 tolerance: {len(bad)}\")\n",
    "\n",
    "# =========================\n",
    "# Cleaning / Imputation\n",
    "# =========================\n",
    "clean.totalcharges_fix: |\n",
    "  df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"].astype(str).str.strip(), errors=\"coerce\")\n",
    "  mask0 = df[\"TotalCharges\"].isna() & (df[\"tenure\"].fillna(0) == 0)\n",
    "  df.loc[mask0, \"TotalCharges\"] = 0.0\n",
    "  mask_other = df[\"TotalCharges\"].isna() & df[\"tenure\"].notna() & df[\"MonthlyCharges\"].notna()\n",
    "  df.loc[mask_other, \"TotalCharges\"] = df.loc[mask_other, \"tenure\"] * df.loc[mask_other, \"MonthlyCharges\"]\n",
    "  df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(\"float64\")\n",
    "\n",
    "clean.category_normalize: |\n",
    "  yesno = [\"Partner\",\"Dependents\",\"PhoneService\",\"PaperlessBilling\",\"Churn\"]\n",
    "  for c in yesno:\n",
    "      df[c] = df[c].str.strip().str.title()\n",
    "  df[\"MultipleLines\"]   = df[\"MultipleLines\"].str.strip().str.replace(\"No phone service\",\"No Phone Service\", regex=False)\n",
    "  internet_cols = [\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\"]\n",
    "  for c in internet_cols:\n",
    "      df[c] = df[c].str.strip().str.replace(\"No internet service\",\"No Internet Service\", regex=False)\n",
    "\n",
    "# =========================\n",
    "# EDA\n",
    "# =========================\n",
    "eda.quick_overview: |\n",
    "  print(df.shape)\n",
    "  display(df.head())\n",
    "  print(df.describe(include=\"all\").T)\n",
    "\n",
    "eda.target_balance: |\n",
    "  ax = (df[\"Churn\"].value_counts(normalize=True)*100).plot.bar()\n",
    "  ax.set_title(\"Churn class balance (%)\"); plt.show()\n",
    "\n",
    "eda.corr_heatmap_numeric: |\n",
    "  num = df.select_dtypes(include=[\"number\"])\n",
    "  sns.heatmap(num.corr(numeric_only=True), annot=False, linewidths=.5)\n",
    "  plt.title(\"Numeric correlations\"); plt.show()\n",
    "\n",
    "eda.churn_rate_by_col_template: |\n",
    "  col = \"Contract\"  # <- change me\n",
    "  rate = (df.groupby(col)[\"Churn\"].apply(lambda s: (s==\"Yes\").mean()).sort_values()*100)\n",
    "  print(rate.round(2))\n",
    "\n",
    "# =========================\n",
    "# Feature Engineering\n",
    "# =========================\n",
    "feat.tenure_buckets: |\n",
    "  bins = [-1, 0, 6, 12, 24, 48, 84, 999]\n",
    "  labels = [\"0\",\"1-6\",\"7-12\",\"13-24\",\"25-48\",\"49-84\",\"85+\"]\n",
    "  df[\"tenure_bucket\"] = pd.cut(df[\"tenure\"], bins=bins, labels=labels)\n",
    "\n",
    "feat.boolean_target: |\n",
    "  df[\"y\"] = (df[\"Churn\"].str.upper() == \"YES\").astype(int)\n",
    "\n",
    "feat.split_train_test: |\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  target = \"y\"\n",
    "  y = df[target]\n",
    "  feature_drop = [\"customerID\",\"Churn\",\"y\"]\n",
    "  X = df.drop(columns=feature_drop, errors=\"ignore\")\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# =========================\n",
    "# Modeling ‚Äì shared utilities\n",
    "# =========================\n",
    "model.columns_splitter: |\n",
    "  num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "  cat_cols = X_train.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "  from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "  from sklearn.compose import ColumnTransformer\n",
    "  pre = ColumnTransformer(\n",
    "      transformers=[\n",
    "          (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "          (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "      ],\n",
    "      remainder=\"drop\"\n",
    "  )\n",
    "\n",
    "model.metrics_helpers: |\n",
    "  from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report, confusion_matrix\n",
    "  def print_metrics(y_true, prob, thr=0.5):\n",
    "      y_pred = (prob >= thr).astype(int)\n",
    "      print(f\"ROC AUC: {roc_auc_score(y_true, prob):.4f}\")\n",
    "      print(f\"PR AUC : {average_precision_score(y_true, prob):.4f}\")\n",
    "      print(f\"F1     : {f1_score(y_true, y_pred):.4f}\")\n",
    "      print(confusion_matrix(y_true, y_pred))\n",
    "      print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "# =========================\n",
    "# Baseline model (LogReg)\n",
    "# =========================\n",
    "model.baseline_logreg: |\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.pipeline import Pipeline\n",
    "  clf = Pipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"lr\", LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None))\n",
    "  ])\n",
    "  clf.fit(X_train, y_train)\n",
    "  prob = clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "# =========================\n",
    "# Tree models (XGBoost / CatBoost)\n",
    "# =========================\n",
    "model.xgboost_cv: |\n",
    "  import xgboost as xgb\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.pipeline import Pipeline\n",
    "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "  clf = Pipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"xgb\", xgb.XGBClassifier(\n",
    "          n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "          subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n",
    "          tree_method=\"hist\", reg_lambda=1.0, n_jobs=-1\n",
    "      ))\n",
    "  ])\n",
    "  aucs = []\n",
    "  for tr, va in skf.split(X_train, y_train):\n",
    "      clf.fit(X_train.iloc[tr], y_train.iloc[tr])\n",
    "      p = clf.predict_proba(X_train.iloc[va])[:,1]\n",
    "      aucs.append(roc_auc_score(y_train.iloc[va], p))\n",
    "  print(\"CV ROC AUC:\", np.mean(aucs).round(4), \"+/-\", np.std(aucs).round(4))\n",
    "  clf.fit(X_train, y_train)\n",
    "  prob = clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "model.catboost_simple: |\n",
    "  from catboost import CatBoostClassifier\n",
    "  # For CatBoost you can pass categorical indices directly (works best on raw categories)\n",
    "  Xc = df.drop(columns=[\"customerID\",\"Churn\",\"y\"], errors=\"ignore\")\n",
    "  cat_idx = np.where(Xc.dtypes == \"object\")[0].tolist()\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  Xtr, Xte, ytr, yte = train_test_split(Xc, df[\"y\"], test_size=0.2, stratify=df[\"y\"], random_state=42)\n",
    "  cat = CatBoostClassifier(\n",
    "      depth=6, iterations=1500, learning_rate=0.03, loss_function=\"Logloss\",\n",
    "      eval_metric=\"AUC\", verbose=200, random_seed=42, auto_class_weights=\"Balanced\"\n",
    "  )\n",
    "  cat.fit(Xtr, ytr, cat_features=cat_idx, eval_set=(Xte, yte), use_best_model=True)\n",
    "  prob = cat.predict_proba(Xte)[:,1]\n",
    "  print_metrics(yte, prob)\n",
    "\n",
    "# =========================\n",
    "# Imbalance strategies (optional)\n",
    "# =========================\n",
    "imb.smote_pipeline_xgb: |\n",
    "  from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "  from imblearn.over_sampling import SMOTE\n",
    "  import xgboost as xgb\n",
    "  imb_clf = ImbPipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"smote\", SMOTE(random_state=42)),\n",
    "      (\"xgb\", xgb.XGBClassifier(\n",
    "          n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "          subsample=0.9, colsample_bytree=0.9, eval_metric=\"logloss\", n_jobs=-1\n",
    "      ))\n",
    "  ])\n",
    "  imb_clf.fit(X_train, y_train)\n",
    "  prob = imb_clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "# =========================\n",
    "# Threshold tuning & calibration\n",
    "# =========================\n",
    "eval.threshold_opt_pr: |\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "  pr, rc, thr = precision_recall_curve(y_test, prob)\n",
    "  f = 2*pr*rc/(pr+rc+1e-9)\n",
    "  best = np.nanargmax(f)\n",
    "  print(\"Best F1 threshold:\", thr[best].round(4), \"F1:\", f[best].round(4))\n",
    "\n",
    "eval.calibration_plot: |\n",
    "  from sklearn.calibration import calibration_curve\n",
    "  prob_true, prob_pred = calibration_curve(y_test, prob, n_bins=10)\n",
    "  plt.plot(prob_pred, prob_true, marker=\"o\"); plt.plot([0,1],[0,1],\"--\")\n",
    "  plt.title(\"Calibration\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\"); plt.show()\n",
    "\n",
    "# =========================\n",
    "# SHAP explainability\n",
    "# =========================\n",
    "exp.shap_tree: |\n",
    "  import shap\n",
    "  shap.initjs()\n",
    "  # Works with tree-based models like XGBoost/CatBoost\n",
    "  booster = clf.named_steps[\"xgb\"] if \"xgb\" in dict(clf.steps) else None\n",
    "  if booster is None:\n",
    "      raise RuntimeError(\"This snippet expects a fitted Pipeline with step 'xgb'.\")\n",
    "  Xs = clf.named_steps[\"pre\"].transform(X_test)\n",
    "  explainer = shap.TreeExplainer(booster)\n",
    "  shap_values = explainer.shap_values(Xs)\n",
    "  shap.summary_plot(shap_values, Xs)\n",
    "\n",
    "exp.shap_linear: |\n",
    "  import shap\n",
    "  linear = clf.named_steps.get(\"lr\", None)\n",
    "  if linear is None:\n",
    "      raise RuntimeError(\"This snippet expects a fitted Pipeline with step 'lr'.\")\n",
    "  Xs = clf.named_steps[\"pre\"].transform(X_test)\n",
    "  explainer = shap.LinearExplainer(linear, Xs, feature_dependence=\"independent\")\n",
    "  shap_values = explainer.shap_values(Xs)\n",
    "  shap.summary_plot(shap_values, Xs)\n",
    "\n",
    "# =========================\n",
    "# MLflow tracking (optional)\n",
    "# =========================\n",
    "mlflow.start_run_and_log: |\n",
    "  import mlflow, mlflow.sklearn\n",
    "  mlflow.set_experiment(\"telco-churn\")\n",
    "  with mlflow.start_run(run_name=\"xgb_baseline\"):\n",
    "      mlflow.log_params({\"n_estimators\":600,\"max_depth\":6,\"lr\":0.05})\n",
    "      mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, prob))\n",
    "      mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "# =========================\n",
    "# SQL (analysis & sanity)\n",
    "# =========================\n",
    "sql.churn_rate_by_segment: |\n",
    "  SELECT Contract,\n",
    "         ROUND(100 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_pct,\n",
    "         COUNT(*) AS n\n",
    "  FROM analytics.telco_customers\n",
    "  GROUP BY Contract\n",
    "  ORDER BY churn_pct DESC;\n",
    "\n",
    "sql.monthly_retention: |\n",
    "  -- Requires start_date and end_date fields for each subscription\n",
    "  WITH months AS (\n",
    "    SELECT DATE_TRUNC(d, MONTH) AS month\n",
    "    FROM UNNEST(GENERATE_DATE_ARRAY('2017-01-01','2018-12-31', INTERVAL 1 DAY)) AS d\n",
    "  ),\n",
    "  active AS (\n",
    "    SELECT m.month,\n",
    "           COUNTIF(t.start_date <= m.month AND (t.end_date IS NULL OR t.end_date >= m.month)) AS active_customers\n",
    "    FROM months m CROSS JOIN analytics.telco_subscriptions t\n",
    "    GROUP BY m.month\n",
    "  )\n",
    "  SELECT * FROM active ORDER BY month;\n",
    "\n",
    "# =========================\n",
    "# dbt scaffolds\n",
    "# =========================\n",
    "dbt.model_scaffold: |\n",
    "  {{ config(materialized='table') }}\n",
    "  WITH src AS (\n",
    "    SELECT * FROM {{ ref('stg_telco_customers') }}\n",
    "  )\n",
    "  SELECT * FROM src;\n",
    "\n",
    "dbt.schema_tests_min: |\n",
    "  version: 2\n",
    "  models:\n",
    "    - name: stg_telco_customers\n",
    "      columns:\n",
    "        - name: customerID\n",
    "          tests: [not_null, unique]\n",
    "        - name: tenure\n",
    "          tests:\n",
    "            - dbt_expectations.expect_column_values_to_be_between:\n",
    "                min_value: 0\n",
    "                max_value: 84\n",
    "        - name: Churn\n",
    "          tests:\n",
    "            - accepted_values:\n",
    "                values: ['Yes','No']\n",
    "      tests:\n",
    "        - dbt_utils.expression_is_true:\n",
    "            expression: \"case when tenure = 0 then coalesce(TotalCharges,0) = 0 else true end\"\n",
    "        - dbt_utils.expression_is_true:\n",
    "            expression: \"abs(coalesce(TotalCharges,0) - coalesce(tenure,0)*coalesce(MonthlyCharges,0)) <= 10\"\n",
    "            where: \"tenure > 0\"\n",
    "\n",
    "dbt.freshness_recency: |\n",
    "  models:\n",
    "    - name: stg_telco_customers\n",
    "      tests:\n",
    "        - dbt_utils.recency:\n",
    "            field: ingested_at\n",
    "            datepart: hour\n",
    "            interval: 24\n",
    "\n",
    "# =========================\n",
    "# Reporting / export\n",
    "# =========================\n",
    "report.top_drivers_summary: |\n",
    "  # After SHAP: simple text summary\n",
    "  import numpy as np\n",
    "  def top_features(shap_vals, k=10):\n",
    "      imp = np.abs(shap_vals).mean(axis=0)\n",
    "      idx = np.argsort(imp)[::-1][:k]\n",
    "      return list(zip(idx, imp[idx]))\n",
    "  print(\"Top features by mean |SHAP| (indices):\")\n",
    "  print(top_features(shap_values, 10))\n",
    "\n",
    "export.predictions_csv: |\n",
    "  out = X_test.copy()\n",
    "  out[\"prob_churn\"] = prob\n",
    "  out[\"y_true\"] = y_test.values\n",
    "  out.to_csv(\"reports/predictions.csv\", index=False)\n",
    "  print(\"Saved reports/predictions.csv\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quick start flow (what to run in a fresh notebook)\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "\n",
    "# snipples nb.header.imports\n",
    "# snipples ingest.csv.read_telco\n",
    "# snipples qa.null_audit\n",
    "# snipples clean.totalcharges_fix\n",
    "# snipples clean.category_normalize\n",
    "# snipples feat.boolean_target\n",
    "# snipples feat.tenure_buckets\n",
    "# snipples feat.split_train_test\n",
    "# snipples model.columns_splitter\n",
    "# snipples model.metrics_helpers\n",
    "# snipples model.baseline_logreg       # => baseline\n",
    "# snipples model.xgboost_cv            # => stronger model\n",
    "# snipples eval.threshold_opt_pr\n",
    "# snipples exp.shap_tree               # if using the XGB pipeline\n",
    "# snipples export.predictions_csv\n",
    "```\n",
    "\n",
    "# Generate **workspace snippets** (VS Code/Windsurf JSON) \n",
    "\n",
    "    Here are two drop-in artifacts for Windsurf/VS Code:\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Workspace snippets\n",
    "\n",
    "Save as **`.vscode/snippets.code-snippets`** in your repo.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Snipples: expand line\": {\n",
    "    \"prefix\": \"snipples\",\n",
    "    \"description\": \"Insert a # snipples directive (works in Jupyter cells via snipples.py)\",\n",
    "    \"body\": [\"# snipples ${1:namespace.snippet_name}\"]\n",
    "  },\n",
    "\n",
    "  \"Imports: data analyst notebook header\": {\n",
    "    \"prefix\": \"py-imports-notebook\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Standard imports for analysis notebooks\",\n",
    "    \"body\": [\n",
    "      \"import os, sys, math, json, textwrap, warnings\",\n",
    "      \"import numpy as np\",\n",
    "      \"import pandas as pd\",\n",
    "      \"import matplotlib.pyplot as plt\",\n",
    "      \"import seaborn as sns\",\n",
    "      \"from pathlib import Path\",\n",
    "      \"warnings.filterwarnings('ignore')\",\n",
    "      \"pd.set_option('display.max_columns', 100)\",\n",
    "      \"plt.rcParams['figure.figsize'] = (10,5)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"IO: read Telco CSV with dtypes\": {\n",
    "    \"prefix\": \"py-read-telco\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Read IBM Telco Churn CSV with explicit dtypes\",\n",
    "    \"body\": [\n",
    "      \"dtype_map = {\",\n",
    "      \"  'customerID':'string','gender':'string','SeniorCitizen':'Int64','Partner':'string','Dependents':'string',\",\n",
    "      \"  'tenure':'Int64','PhoneService':'string','MultipleLines':'string','InternetService':'string','OnlineSecurity':'string',\",\n",
    "      \"  'OnlineBackup':'string','DeviceProtection':'string','TechSupport':'string','StreamingTV':'string','StreamingMovies':'string',\",\n",
    "      \"  'Contract':'string','PaperlessBilling':'string','PaymentMethod':'string','MonthlyCharges':'float64','TotalCharges':'string','Churn':'string'\",\n",
    "      \"}\",\n",
    "      \"df = pd.read_csv('${1:data/raw/Telco-Customer-Churn.csv}', dtype=dtype_map)\",\n",
    "      \"print(df.shape)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Clean: TotalCharges fix (tenure==0 => 0 else tenure*MonthlyCharges)\": {\n",
    "    \"prefix\": \"py-clean-totalcharges\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Coerce TotalCharges to numeric and impute correctly\",\n",
    "    \"body\": [\n",
    "      \"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].astype(str).str.strip(), errors='coerce')\",\n",
    "      \"mask0 = df['TotalCharges'].isna() & (df['tenure'].fillna(0) == 0)\",\n",
    "      \"df.loc[mask0, 'TotalCharges'] = 0.0\",\n",
    "      \"mask_other = df['TotalCharges'].isna() & df['tenure'].notna() & df['MonthlyCharges'].notna()\",\n",
    "      \"df.loc[mask_other, 'TotalCharges'] = df.loc[mask_other, 'tenure'] * df.loc[mask_other, 'MonthlyCharges']\",\n",
    "      \"df['TotalCharges'] = df['TotalCharges'].astype('float64')\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"EDA: churn class balance plot\": {\n",
    "    \"prefix\": \"py-eda-balance\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Bar chart of churn class balance\",\n",
    "    \"body\": [\n",
    "      \"ax = (df['Churn'].value_counts(normalize=true)*100).plot.bar()\",\n",
    "      \"ax.set_title('Churn class balance (%)'); plt.show()\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Feature: target + tenure buckets\": {\n",
    "    \"prefix\": \"py-feat-target-buckets\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Create y and tenure_bucket features\",\n",
    "    \"body\": [\n",
    "      \"df['y'] = (df['Churn'].str.upper() == 'YES').astype(int)\",\n",
    "      \"bins = [-1,0,6,12,24,48,84,999]\",\n",
    "      \"labels = ['0','1-6','7-12','13-24','25-48','49-84','85+']\",\n",
    "      \"df['tenure_bucket'] = pd.cut(df['tenure'], bins=bins, labels=labels)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Split: train/test\": {\n",
    "    \"prefix\": \"py-split\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Train/test split with drop of ID/label columns\",\n",
    "    \"body\": [\n",
    "      \"from sklearn.model_selection import train_test_split\",\n",
    "      \"target = 'y'\",\n",
    "      \"y = df[target]\",\n",
    "      \"X = df.drop(columns=['customerID','Churn','y'], errors='ignore')\",\n",
    "      \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Preprocess: ColumnTransformer (num scale + OHE cat)\": {\n",
    "    \"prefix\": \"py-pre-coltx\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"StandardScaler for numeric and OneHotEncoder for categories\",\n",
    "    \"body\": [\n",
    "      \"num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\",\n",
    "      \"cat_cols = X_train.select_dtypes(exclude=['number']).columns.tolist()\",\n",
    "      \"from sklearn.preprocessing import OneHotEncoder, StandardScaler\",\n",
    "      \"from sklearn.compose import ColumnTransformer\",\n",
    "      \"pre = ColumnTransformer([\",\n",
    "      \"  ('num', StandardScaler(with_mean=false), num_cols),\",\n",
    "      \"  ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=true), cat_cols)\",\n",
    "      \"])\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Model: Logistic Regression baseline\": {\n",
    "    \"prefix\": \"py-model-logreg\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Baseline classifier pipeline + quick metrics\",\n",
    "    \"body\": [\n",
    "      \"from sklearn.pipeline import Pipeline\",\n",
    "      \"from sklearn.linear_model import LogisticRegression\",\n",
    "      \"from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report, confusion_matrix\",\n",
    "      \"def print_metrics(y_true, prob, thr=0.5):\",\n",
    "      \"  y_pred = (prob >= thr).astype(int)\",\n",
    "      \"  print(f'ROC AUC: {roc_auc_score(y_true, prob):.4f}')\",\n",
    "      \"  print(f'PR AUC : {average_precision_score(y_true, prob):.4f}')\",\n",
    "      \"  print(f'F1     : {f1_score(y_true, y_pred):.4f}')\",\n",
    "      \"  print(confusion_matrix(y_true, y_pred))\",\n",
    "      \"  print(classification_report(y_true, y_pred, digits=3))\",\n",
    "      \"clf = Pipeline([('pre', pre), ('lr', LogisticRegression(max_iter=200, class_weight='balanced'))])\",\n",
    "      \"clf.fit(X_train, y_train)\",\n",
    "      \"prob = clf.predict_proba(X_test)[:,1]\",\n",
    "      \"print_metrics(y_test, prob)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Model: XGBoost pipeline (hist)\": {\n",
    "    \"prefix\": \"py-model-xgb\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"XGBoost classifier with ColumnTransformer preprocessing\",\n",
    "    \"body\": [\n",
    "      \"import xgboost as xgb\",\n",
    "      \"from sklearn.pipeline import Pipeline\",\n",
    "      \"from sklearn.metrics import roc_auc_score\",\n",
    "      \"clf = Pipeline([\",\n",
    "      \"  ('pre', pre),\",\n",
    "      \"  ('xgb', xgb.XGBClassifier(n_estimators=600, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, eval_metric='logloss', tree_method='hist', n_jobs=-1))\",\n",
    "      \"])\",\n",
    "      \"clf.fit(X_train, y_train)\",\n",
    "      \"prob = clf.predict_proba(X_test)[:,1]\",\n",
    "      \"print_metrics(y_test, prob)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Explain: SHAP (tree models)\": {\n",
    "    \"prefix\": \"py-shap-tree\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"SHAP summary for tree-based model within a Pipeline\",\n",
    "    \"body\": [\n",
    "      \"import shap\",\n",
    "      \"shap.initjs()\",\n",
    "      \"booster = clf.named_steps.get('xgb')\",\n",
    "      \"Xs = clf.named_steps['pre'].transform(X_test)\",\n",
    "      \"explainer = shap.TreeExplainer(booster)\",\n",
    "      \"shap_values = explainer.shap_values(Xs)\",\n",
    "      \"shap.summary_plot(shap_values, Xs)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Quality: Pandera mini schema\": {\n",
    "    \"prefix\": \"py-pandera-telco\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Quick Pandera schema to validate key columns\",\n",
    "    \"body\": [\n",
    "      \"import pandera as pa\",\n",
    "      \"from pandera import Column, Check\",\n",
    "      \"TelcoSchema = pa.DataFrameSchema({\",\n",
    "      \"  'customerID': Column(str, nullable=false),\",\n",
    "      \"  'tenure': Column(int, Check.in_range(0,84), nullable=false),\",\n",
    "      \"  'MonthlyCharges': Column(float, Check.in_range(0,200), nullable=false),\",\n",
    "      \"  'TotalCharges': Column(float, nullable=false),\",\n",
    "      \"  'Churn': Column(str, Check.isin(['Yes','No']), nullable=false)\",\n",
    "      \"})\",\n",
    "      \"TelcoSchema.validate(df, lazy=true)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"dbt: model scaffold\": {\n",
    "    \"prefix\": \"dbt-model\",\n",
    "    \"scope\": \"sql\",\n",
    "    \"description\": \"dbt table model scaffold\",\n",
    "    \"body\": [\n",
    "      \"{{ config(materialized='table') }}\",\n",
    "      \"WITH src AS (\",\n",
    "      \"  SELECT * FROM {{ ref('${1:stg_telco_customers}') }}\",\n",
    "      \")\",\n",
    "      \"SELECT * FROM src;\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"SQL: churn by segment\": {\n",
    "    \"prefix\": \"sql-churn-seg\",\n",
    "    \"scope\": \"sql\",\n",
    "    \"description\": \"Segment churn rates by Contract\",\n",
    "    \"body\": [\n",
    "      \"SELECT Contract,\",\n",
    "      \"       ROUND(100 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_pct,\",\n",
    "      \"       COUNT(*) AS n\",\n",
    "      \"FROM ${1:analytics.telco_customers}\",\n",
    "      \"GROUP BY Contract\",\n",
    "      \"ORDER BY churn_pct DESC;\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "># Tiny **Makefile** to lint/test/run your dbt & notebooks?\n",
    "\n",
    "# 2) Makefile\n",
    "\n",
    "Save as **`Makefile`** at the repo root. It‚Äôs opinionated but tidy for AE/ML + dbt.\n",
    "\n",
    "```make\n",
    "# -------- Config --------\n",
    "PY          ?= python3\n",
    "VENV        ?= .venv\n",
    "ACTIVATE    = . $(VENV)/bin/activate\n",
    "REQS        ?= requirements.txt\n",
    "\n",
    "DBT_TARGET  ?= dev\n",
    "DBT_PROJ    ?= .           # path to dbt project (has dbt_project.yml)\n",
    "RAW_CSV     ?= data/raw/Telco-Customer-Churn.csv\n",
    "\n",
    "# -------- Helpers --------\n",
    ".PHONY: help\n",
    "help:\n",
    "\t@echo \"Common targets:\"\n",
    "\t@echo \"  make venv           # create venv and install requirements\"\n",
    "\t@echo \"  make install        # install/upgrade packages in venv\"\n",
    "\t@echo \"  make lint           # ruff lint + format check\"\n",
    "\t@echo \"  make fmt            # ruff format\"\n",
    "\t@echo \"  make test           # run pytest\"\n",
    "\t@echo \"  make nb-run         # run notebooks with papermill (paramizable)\"\n",
    "\t@echo \"  make dbt-deps/run/test/freshness  # dbt workflow\"\n",
    "\t@echo \"  make qa-quick       # quick CSV sanity checks (row count, nulls, dup IDs)\"\n",
    "\t@echo \"  make clean          # remove build artifacts\"\n",
    "\n",
    "# -------- Environment --------\n",
    "$(VENV):\n",
    "\t$(PY) -m venv $(VENV)\n",
    "\t$(ACTIVATE) && python -m pip install --upgrade pip\n",
    "\n",
    ".PHONY: venv\n",
    "venv: $(VENV) install\n",
    "\n",
    ".PHONY: install\n",
    "install:\n",
    "\t@if [ -f \"$(REQS)\" ]; then \\\n",
    "\t\t$(ACTIVATE) && pip install -r $(REQS); \\\n",
    "\telse \\\n",
    "\t\techo \"No requirements.txt found; installing a sane default set...\"; \\\n",
    "\t\t$(ACTIVATE) && pip install pandas numpy scikit-learn imbalanced-learn xgboost catboost shap mlflow matplotlib seaborn pandera great-expectations jupyter papermill ruff pytest dbt-bigquery pandas-gbq google-cloud-bigquery pyarrow; \\\n",
    "\tfi\n",
    "\n",
    "# -------- Lint & Test --------\n",
    ".PHONY: lint\n",
    "lint:\n",
    "\t$(ACTIVATE) && ruff check .\n",
    "\n",
    ".PHONY: fmt\n",
    "fmt:\n",
    "\t$(ACTIVATE) && ruff format .\n",
    "\n",
    ".PHONY: test\n",
    "test:\n",
    "\t$(ACTIVATE) && pytest -q\n",
    "\n",
    "# -------- Notebooks (papermill) --------\n",
    "# Usage: make nb-run NB=notebooks/01_eda.ipynb OUT=reports/01_eda.out.ipynb\n",
    "NB ?= notebooks/01_eda.ipynb\n",
    "OUT ?= reports/01_eda.out.ipynb\n",
    ".PHONY: nb-run\n",
    "nb-run:\n",
    "\t$(ACTIVATE) && papermill $(NB) $(OUT)\n",
    "\n",
    "# -------- Quick CSV QA (no dbt) --------\n",
    ".PHONY: qa-quick\n",
    "qa-quick:\n",
    "\t@echo \"QA: $(RAW_CSV)\"\n",
    "\t$(ACTIVATE) && python - <<'PY'\n",
    "import pandas as pd, sys\n",
    "path = \"$(RAW_CSV)\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Nulls (top 10):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "if \"customerID\" in df.columns:\n",
    "    dup = (df[\"customerID\"].value_counts() > 1).sum()\n",
    "    print(\"Duplicate customerID:\", dup)\n",
    "PY\n",
    "\n",
    "# -------- dbt workflow --------\n",
    ".PHONY: dbt-deps\n",
    "dbt-deps:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt deps\n",
    "\n",
    ".PHONY: dbt-run\n",
    "dbt-run:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt run --target $(DBT_TARGET)\n",
    "\n",
    ".PHONY: dbt-test\n",
    "dbt-test:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt test --target $(DBT_TARGET)\n",
    "\n",
    ".PHONY: dbt-freshness\n",
    "dbt-freshness:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt source freshness --target $(DBT_TARGET)\n",
    "\n",
    "# -------- Clean --------\n",
    ".PHONY: clean\n",
    "clean:\n",
    "\trm -rf $(VENV) .pytest_cache .ruff_cache **/__pycache__\n",
    "\tfind . -name \"*.ipynb_checkpoints\" -type d -prune -exec rm -rf {} +\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to wire this up fast\n",
    "\n",
    "1. **Put your extension + snippets in place**\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.snipples\n",
    "# (Paste your large ~/.snipples/snippets.yaml we created earlier)\n",
    "# Put snipples.py somewhere on PYTHONPATH, or `pip install -e .` if you package it.\n",
    "```\n",
    "\n",
    "In notebooks:\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "# then type:  # snipples model.xgboost_cv   (Tab to browse)\n",
    "```\n",
    "\n",
    "2. **Add the workspace snippets**\n",
    "\n",
    "* Create `.vscode/snippets.code-snippets` and paste the JSON above.\n",
    "\n",
    "3. **Makefile**\n",
    "\n",
    "```bash\n",
    "make venv\n",
    "make qa-quick\n",
    "make dbt-deps dbt-run dbt-test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "># **generate a tiny `requirements.txt` and a `tasks.json` (Windsurf/VS Code tasks that wrap `make` targets so you can ‚åò‚áßB to run ‚ÄúDBT: test‚Äù)**\n",
    "\n",
    "\n",
    "\n",
    "># CLAUDE SUGGESTION: CODE LIBRARY\n",
    "\n",
    "Great question! Code organization and storage strategy is crucial for professional development and portfolio presentation. Here are the most appropriate approaches for different aspects of your projects:\n",
    "\n",
    "## üóÇÔ∏è **Repository Structure & Organization**\n",
    "\n",
    "### **Primary Code Storage: GitHub**\n",
    "```\n",
    "your-username/\n",
    "‚îú‚îÄ‚îÄ telco-churn-analysis-level-0/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ README.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ telco.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ methodology.md\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ findings.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tests/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_analysis.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ figures/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ reports/\n",
    "```\n",
    "\n",
    "### **Multi-Level Portfolio Structure**\n",
    "```\n",
    "data-analytics-journey/\n",
    "‚îú‚îÄ‚îÄ README.md (Portfolio overview)\n",
    "‚îú‚îÄ‚îÄ level-00-bare-bones/\n",
    "‚îú‚îÄ‚îÄ level-01-structured-eda/\n",
    "‚îú‚îÄ‚îÄ level-02-data-cleaning/\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ level-10-enterprise/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ learning-progression.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ technologies-mastered.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ business-impact.md\n",
    "‚îî‚îÄ‚îÄ portfolio-assets/\n",
    "    ‚îú‚îÄ‚îÄ presentations/\n",
    "    ‚îú‚îÄ‚îÄ publications/\n",
    "    ‚îî‚îÄ‚îÄ certifications/\n",
    "```\n",
    "\n",
    "## üíª **Code Workflow Storage Options**\n",
    "\n",
    "### **1. GitHub (Primary - Essential)**\n",
    "**Best for:**\n",
    "- Version control and collaboration\n",
    "- Portfolio presentation to employers\n",
    "- Open source contributions\n",
    "- CI/CD integration\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "‚îú‚îÄ‚îÄ .github/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ workflows/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ci.yml\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ISSUE_TEMPLATE/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îî‚îÄ‚îÄ examples/\n",
    "```\n",
    "\n",
    "### **2. Jupyter Notebooks (Development)**\n",
    "**Best for:**\n",
    "- Exploratory analysis documentation\n",
    "- Step-by-step methodology explanation\n",
    "- Interactive presentations\n",
    "- Educational content\n",
    "\n",
    "**Organization:**\n",
    "```\n",
    "notebooks/\n",
    "‚îú‚îÄ‚îÄ 01-data-exploration.ipynb\n",
    "‚îú‚îÄ‚îÄ 02-data-cleaning.ipynb\n",
    "‚îú‚îÄ‚îÄ 03-feature-engineering.ipynb\n",
    "‚îú‚îÄ‚îÄ 04-modeling.ipynb\n",
    "‚îî‚îÄ‚îÄ 05-results-analysis.ipynb\n",
    "```\n",
    "\n",
    "### **3. Professional Platforms**\n",
    "\n",
    "#### **Kaggle Notebooks**\n",
    "```python\n",
    "# Kaggle-optimized structure\n",
    "\"\"\"\n",
    "TELCO CUSTOMER CHURN - LEVEL 0 ANALYSIS\n",
    "=======================================\n",
    "Author: Your Name\n",
    "Created: Date\n",
    "Updated: Date\n",
    "\n",
    "Table of Contents:\n",
    "1. Introduction & Business Problem\n",
    "2. Data Loading & Initial Exploration  \n",
    "3. Data Quality Assessment\n",
    "4. Key Insights Discovery\n",
    "5. Business Recommendations\n",
    "6. Technical Implementation\n",
    "7. Next Steps & Learning Journey\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../input/telco-customer-churn/telco.csv')\n",
    "```\n",
    "\n",
    "#### **Google Colab (Shareable)**\n",
    "```python\n",
    "# Mount Google Drive for data persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# Professional notebook structure with clear sections\n",
    "```\n",
    "\n",
    "## üèóÔ∏è **Advanced Code Organization (Level 5+)**\n",
    "\n",
    "### **Modular Python Package Structure**\n",
    "```\n",
    "telco_churn_analysis/\n",
    "‚îú‚îÄ‚îÄ setup.py\n",
    "‚îú‚îÄ‚îÄ pyproject.toml\n",
    "‚îú‚îÄ‚îÄ telco_churn/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engineering.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selection.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ visualization/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plots.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ helpers.py\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îú‚îÄ‚îÄ examples/\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "```\n",
    "\n",
    "### **Configuration Management**\n",
    "```python\n",
    "# config.py\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR: Path = Path(\"data\")\n",
    "    RAW_DATA_FILE: str = \"telco.csv\"\n",
    "    PROCESSED_DATA_FILE: str = \"telco_processed.csv\"\n",
    "    \n",
    "    # Model parameters\n",
    "    TEST_SIZE: float = 0.2\n",
    "    RANDOM_STATE: int = 42\n",
    "    \n",
    "    # Output paths\n",
    "    RESULTS_DIR: Path = Path(\"results\")\n",
    "    FIGURES_DIR: Path = Path(\"results/figures\")\n",
    "    REPORTS_DIR: Path = Path(\"results/reports\")\n",
    "```\n",
    "\n",
    "## üìä **Documentation & Workflow Storage**\n",
    "\n",
    "### **README.md Template**\n",
    "```markdown\n",
    "# Level X: Telco Customer Churn Analysis\n",
    "\n",
    "## Quick Start\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python src/analysis.py\n",
    "```\n",
    "\n",
    "## Project Structure\n",
    "- `src/`: Core analysis code\n",
    "- `notebooks/`: Jupyter exploration\n",
    "- `data/`: Dataset files  \n",
    "- `results/`: Output artifacts\n",
    "- `tests/`: Unit tests\n",
    "- `docs/`: Documentation\n",
    "\n",
    "## Key Findings\n",
    "- Finding 1: Contract type drives churn\n",
    "- Finding 2: Payment method indicates engagement  \n",
    "- Finding 3: Premium services show paradox\n",
    "\n",
    "## Business Impact\n",
    "- $2.4M annual revenue at risk identified\n",
    "- 3 strategic initiatives recommended\n",
    "- Clear ROI projections provided\n",
    "\n",
    "## Technologies Used\n",
    "- Python, Pandas, NumPy\n",
    "- Matplotlib, Seaborn\n",
    "- Scikit-learn (for advanced levels)\n",
    "\n",
    "## Next Steps\n",
    "See Level X+1 for progression...\n",
    "```\n",
    "\n",
    "### **Code Documentation Standards**\n",
    "```python\n",
    "def analyze_churn_by_contract(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Analyze churn rates by contract type.\n",
    "    \n",
    "    This function calculates churn rates for each contract type,\n",
    "    revealing the primary business insight of this analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Customer dataset with 'Contract' and 'Churn' columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Churn rates by contract type, sorted descending\n",
    "        \n",
    "    Example:\n",
    "        >>> churn_rates = analyze_churn_by_contract(df)\n",
    "        >>> print(churn_rates)\n",
    "        Month-to-month    0.427\n",
    "        One year          0.113  \n",
    "        Two year          0.028\n",
    "    \n",
    "    Business Impact:\n",
    "        This analysis reveals that month-to-month customers churn at 15x \n",
    "        the rate of two-year contract customers, representing the single \n",
    "        most actionable insight for retention strategy.\n",
    "    \"\"\"\n",
    "    return df.groupby('Contract')['Churn'].apply(\n",
    "        lambda x: (x == 'Yes').mean()\n",
    "    ).sort_values(ascending=False)\n",
    "```\n",
    "\n",
    "## üîß **Workflow Management Tools**\n",
    "\n",
    "### **For Individual Projects (Levels 0-3)**\n",
    "- **Primary**: GitHub repository per level\n",
    "- **Development**: Jupyter notebooks + VS Code\n",
    "- **Documentation**: Markdown files + README\n",
    "- **Sharing**: Kaggle notebooks for community\n",
    "\n",
    "### **For Advanced Projects (Levels 4-7)**\n",
    "- **Code**: Modular Python packages\n",
    "- **Environment**: Poetry/Conda for dependency management\n",
    "- **Testing**: pytest framework\n",
    "- **Documentation**: Sphinx for API docs\n",
    "- **Workflow**: GitHub Actions for CI/CD\n",
    "\n",
    "### **For Enterprise Projects (Levels 8-10)**\n",
    "- **Infrastructure**: Infrastructure as Code (Terraform)\n",
    "- **Containerization**: Docker + Kubernetes\n",
    "- **Orchestration**: Apache Airflow\n",
    "- **Monitoring**: Logging and metrics collection\n",
    "- **Deployment**: Blue-green deployment strategies\n",
    "\n",
    "## üí° **Best Practices by Level**\n",
    "\n",
    "### **Level 0-2: Foundation**\n",
    "```python\n",
    "# Simple, clear, well-commented code\n",
    "import pandas as pd\n",
    "\n",
    "# Load data with error handling\n",
    "try:\n",
    "    df = pd.read_csv('data/telco.csv')\n",
    "    print(f\"‚úÖ Data loaded: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found\")\n",
    "    exit(1)\n",
    "\n",
    "# Clear business-focused analysis\n",
    "contract_churn = df.groupby('Contract')['Churn'].mean()\n",
    "print(\"üìä Churn by Contract Type:\")\n",
    "print(contract_churn.sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "### **Level 5-7: Professional**\n",
    "```python\n",
    "# Type hints, docstrings, error handling\n",
    "from typing import Tuple, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ChurnAnalyzer:\n",
    "    \"\"\"Professional customer churn analysis class.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.df = None\n",
    "        \n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and validate customer data.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\"Data loaded successfully: {self.df.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {e}\")\n",
    "            raise\n",
    "```\n",
    "\n",
    "### **Level 8-10: Enterprise**\n",
    "```python\n",
    "# Production-ready with monitoring\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import structlog\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "@dataclass\n",
    "class PredictionMetrics:\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    latency_ms: float\n",
    "\n",
    "class ChurnPredictor(ABC):\n",
    "    \"\"\"Abstract base class for churn prediction models.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def predict(self, customer_data: Dict) -> Tuple[float, PredictionMetrics]:\n",
    "        \"\"\"Predict churn probability with performance metrics.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "## üìà **Storage Strategy Recommendations**\n",
    "\n",
    "### **For Learning Journey Documentation**\n",
    "1. **GitHub**: Primary portfolio repository\n",
    "2. **Kaggle**: Public notebooks for community engagement  \n",
    "3. **Personal Blog**: Medium/LinkedIn articles\n",
    "4. **Cloud Storage**: AWS S3/Google Cloud for large datasets\n",
    "5. **Local Development**: VS Code + Jupyter for daily work\n",
    "\n",
    "### **For Professional Presentation**\n",
    "1. **Clean GitHub repos** with professional README files\n",
    "2. **Comprehensive documentation** explaining methodology\n",
    "3. **Business-focused presentations** with clear ROI\n",
    "4. **Code quality standards** with proper testing\n",
    "5. **Progression documentation** showing skill development\n",
    "\n",
    "The key is to maintain **consistency across platforms** while optimizing each for its specific audience and purpose. Your Level 0 should be simple and accessible, while your Level 10 should demonstrate enterprise-scale thinking and implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6c293",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "__init__.py is one of those little Python things that feels mysterious at first.\n",
    "\n",
    "What __init__.py is\n",
    "=====================\n",
    "\n",
    "It‚Äôs a special file that tells Python:\n",
    "‚ÄúThis folder should be treated as a package.‚Äù\n",
    "\n",
    "Without it, Python may not recognize your src/ directory as importable.\n",
    "\n",
    "In practice, having an __init__.py means you can do:\n",
    "\n",
    "from src import utils\n",
    "\n",
    "\n",
    "instead of hacking sys.path.\n",
    "\n",
    "What goes inside?\n",
    "==================\n",
    "\n",
    "For your Level 2 project, you can keep it super simple:\n",
    "\n",
    "# __init__.py is one of those little Python things that feels mysterious at first.\n",
    "\n",
    "üì¶ What __init__.py is\n",
    "\n",
    "It‚Äôs a special file that tells Python:\n",
    "‚ÄúThis folder should be treated as a package.‚Äù\n",
    "\n",
    "Without it, Python may not recognize your src/ directory as importable.\n",
    "\n",
    "In practice, having an __init__.py means you can do:\n",
    "\n",
    "from src import utils\n",
    "\n",
    "\n",
    "instead of hacking sys.path.\n",
    "\n",
    "üõ† What goes inside?\n",
    "\n",
    "For your Level 2 project, you can keep it super simple:\n",
    "\n",
    "# /src/__init__.py\n",
    "\"\"\"\n",
    "src package for Telco Churn Level 2 project.\n",
    "\n",
    "Contains:\n",
    "- utils: data loading, validation, EDA helpers\n",
    "\"\"\"\n",
    "\n",
    "from . import utils\n",
    "\n",
    "\n",
    "This way, when you import src, you automatically expose src.utils.\n",
    "\n",
    "ü§î Do you need it?\n",
    "\n",
    "Python 3.3+ technically supports ‚Äúimplicit namespace packages‚Äù without __init__.py.\n",
    "\n",
    "But most teams (and tools like pytest, linters, IDEs) still expect it.\n",
    "\n",
    "So it‚Äôs best practice to always include one, even if it‚Äôs empty.\n",
    "\n",
    "‚úÖ For your project\n",
    "\n",
    "In your repo:\n",
    "\n",
    "Telco-L2/\n",
    "‚îú‚îÄ src/\n",
    "‚îÇ  ‚îú‚îÄ __init__.py   ‚Üê just add this file (can even be empty)\n",
    "‚îÇ  ‚îî‚îÄ utils.py\n",
    "\n",
    "\n",
    "This will make your src folder behave like a proper Python package.\n",
    "\n",
    "Do you want me to show you the two import styles (monolithic vs. package-style with __init__.py) side by side, so you see how this little file changes how you call utils from notebooks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de63fa",
   "metadata": {},
   "source": [
    "You're building a **personal code library** - this is exactly what professional data scientists do. Here are several effective approaches:## üéØ **My Recommendation: Start Simple with Markdown Files**\n",
    "\n",
    "Create a folder structure like this:\n",
    "```\n",
    "my_code_library/\n",
    "‚îú‚îÄ‚îÄ missing_values.md\n",
    "‚îú‚îÄ‚îÄ data_types.md\n",
    "‚îú‚îÄ‚îÄ anomaly_detection.md\n",
    "‚îú‚îÄ‚îÄ data_cleaning.md\n",
    "‚îî‚îÄ‚îÄ quick_exploration.md\n",
    "```\n",
    "\n",
    "## üìù **Why Markdown Files Work Best:**\n",
    "\n",
    "1. **Easy to search** - Ctrl+F to find what you need\n",
    "2. **Copy-paste ready** - Code blocks ready to use  \n",
    "3. **Context included** - Notes on when/why to use each method\n",
    "4. **GitHub friendly** - Looks professional in your portfolio\n",
    "\n",
    "## üí° **Example: `missing_values.md`**\n",
    "\n",
    "```markdown\n",
    "# Missing Values Toolkit\n",
    "\n",
    "## Quick Count (Most Used)\n",
    "```python\n",
    "df.isna().sum().sum()  # 18 characters, fastest\n",
    "```\n",
    "\n",
    "## Show Details When Needed  \n",
    "```python\n",
    "df.isnull().sum()[df.isnull().sum() > 0]  # Only non-zero columns\n",
    "```\n",
    "\n",
    "## When to Use Each:\n",
    "- **Quick count**: Initial assessment, just need total\n",
    "- **Detailed view**: When you find missing values and need to know where\n",
    "```\n",
    "\n",
    "## üöÄ **Start Your Library Today:**\n",
    "\n",
    "1. **Create the folder structure**\n",
    "2. **Start with one file** - `missing_values.md`\n",
    "3. **Add your variations** from today's session\n",
    "4. **Include context** - when to use each method\n",
    "5. **Build as you learn** - add new categories in future levels\n",
    "\n",
    "## üìà **Benefits for Your Goals:**\n",
    "\n",
    "- **Goal #2**: Documents your learning progression\n",
    "- **Goal #4**: Creates your personal reference library\n",
    "- **Portfolio value**: Shows systematic thinking to hiring managers\n",
    "\n",
    "This approach scales perfectly - start simple, add complexity as you grow!\n",
    "\n",
    "Here are several ways to check for empty values across the entire dataset:\n",
    "\n",
    "## **Quick One-Liners for Different Types of Empty:**\n",
    "\n",
    "### **Check NaN/Null values:**\n",
    "```python\n",
    "df.isnull().sum().sum()  # Total count\n",
    "df.isnull().sum()        # By column\n",
    "```\n",
    "\n",
    "### **Check empty strings (''):**\n",
    "```python\n",
    "(df == '').sum().sum()   # Total count\n",
    "(df == '').sum()         # By column\n",
    "```\n",
    "\n",
    "### **Check empty spaces (' '):**\n",
    "```python\n",
    "(df == ' ').sum().sum()  # Total count\n",
    "(df == ' ').sum()        # By column\n",
    "```\n",
    "\n",
    "### **Ultimate one-liner for all types:**\n",
    "```python\n",
    "print(f\"NaN: {df.isnull().sum().sum()}, Empty: {(df == '').sum().sum()}, Spaces: {(df == ' ').sum().sum()}\")\n",
    "```\n",
    "\n",
    "## **Most Comprehensive Check:**\n",
    "\n",
    "```python\n",
    "# Check every column for all types of empty values\n",
    "for col in df.columns:\n",
    "    nan_count = df[col].isnull().sum()\n",
    "    empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "    space_count = (df[col] == ' ').sum() if df[col].dtype == 'object' else 0\n",
    "    \n",
    "    total = nan_count + empty_count + space_count\n",
    "    if total > 0:\n",
    "        print(f\"{col}: {total} empty values\")\n",
    "```\n",
    "\n",
    "## **Show Actual Problem Rows:**\n",
    "\n",
    "```python\n",
    "# Find and display rows with any empty values\n",
    "problem_rows = df[df.isnull().any(axis=1) | (df == '').any(axis=1) | (df == ' ').any(axis=1)]\n",
    "print(f\"Rows with empty values: {len(problem_rows)}\")\n",
    "if len(problem_rows) > 0:\n",
    "    print(problem_rows[['customerID']].head())  # Show first few problem customer IDs\n",
    "```\n",
    "\n",
    "**After your TotalCharges fix, you should see:**\n",
    "- **NaN: 0**\n",
    "- **Empty strings: 0** \n",
    "- **Spaces: 0**\n",
    "\n",
    "If you see any non-zero counts, that tells you where you still have empty value issues!\n",
    "\n",
    "---\n",
    "\n",
    "># **#2: Keep a snippet library handy across projects/workspaces**\n",
    "\n",
    "    Here are three ‚Äúprofessional and portable‚Äù ways to keep a snippet library handy across projects/workspaces on your Mac‚Äîordered from most editor-native to most universal. Pick one or combine B+C for a killer setup.\n",
    "\n",
    "# A) VS Code user snippets + built-in Sync (simple, editor-native)\n",
    "\n",
    "**Best if you primarily use VS Code.**\n",
    "\n",
    "1. Open Command Palette ‚Üí ‚ÄúPreferences: Configure User Snippets‚Äù.\n",
    "2. Create language or global snippets. Example (`python.json`):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Py docstring\": {\n",
    "    \"prefix\": \"pydoc\",\n",
    "    \"body\": [\n",
    "      \"\\\"\\\"\\\"${1:Summary}\",\n",
    "      \"\",\n",
    "      \"Args:\",\n",
    "      \"    ${2:param}: ${3:desc}\",\n",
    "      \"\",\n",
    "      \"Returns:\",\n",
    "      \"    ${4:type}: ${5:desc}\",\n",
    "      \"\\\"\\\"\\\"\"\n",
    "    ],\n",
    "    \"description\": \"Docstring scaffold\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Turn on **Settings Sync** (Account icon ‚Üí Turn On Sync). Your snippets follow you on any machine you sign into VS Code with.\n",
    "\n",
    "**Pro tip (shared across repos):** keep your snippet JSON files in a Git repo and **symlink** them into VS Code‚Äôs snippets folder so you can version-control them:\n",
    "\n",
    "```bash\n",
    "# Paths (macOS)\n",
    "VS_SNIPS=\"$HOME/Library/Application Support/Code/User/snippets\"\n",
    "mkdir -p ~/snippets/vscode\n",
    "ln -s ~/snippets/vscode/python.json \"$VS_SNIPS/python.json\"\n",
    "```\n",
    "\n",
    "# B) Git + Markdown snippets + fzf CLI (portable, editor-agnostic)\n",
    "\n",
    "**Best if you hop between tools (VS Code, Windsurf, terminals, notebooks).**\n",
    "\n",
    "1. Make a repo:\n",
    "\n",
    "```\n",
    "~/snippets/\n",
    "  python/\n",
    "  sql/\n",
    "  dbt/\n",
    "  shell/\n",
    "  README.md\n",
    "```\n",
    "\n",
    "Each snippet is a small `.md` with a clear title and a fenced code block.\n",
    "\n",
    "2. Install tools (free):\n",
    "\n",
    "```bash\n",
    "brew install fzf ripgrep\n",
    "```\n",
    "\n",
    "3. Add a tiny **`snip`** helper to your `~/.zshrc`:\n",
    "\n",
    "````zsh\n",
    "snip() {\n",
    "  local file\n",
    "  file=$(rg -l --hidden --glob \"!*.git/*\" \"${*:-.}\" \"$HOME/snippets\" | fzf --prompt=\"Snip> \") || return\n",
    "  # copy the code block contents to clipboard\n",
    "  awk '/^```/{f=!f; next} f' \"$file\" | pbcopy\n",
    "  echo \"üìã Copied from: $file\"\n",
    "}\n",
    "````\n",
    "\n",
    "Usage:\n",
    "\n",
    "````bash\n",
    "snip pandas groupby\n",
    "# copies the code inside the first ``` block from the chosen file\n",
    "# ‚åò+V (paste) in any editor/terminal\n",
    "````\n",
    "\n",
    "**Why this rocks:** works everywhere (terminal, notebooks, different editors), versioned with Git, searchable via fuzzy find. You can keep private snippets in a private repo.\n",
    "\n",
    "# C) Text expander (Espanso) for instant boilerplate (cross-app)\n",
    "\n",
    "**Great for short, frequently typed patterns.** Free & open-source.\n",
    "\n",
    "```bash\n",
    "brew install espanso\n",
    "espanso start\n",
    "espanso edit\n",
    "```\n",
    "\n",
    "Add triggers in `default.yml`:\n",
    "\n",
    "```yaml\n",
    "matches:\n",
    "  - trigger: \":sqlsel\"\n",
    "    replace: |\n",
    "      SELECT ${1:*}\n",
    "      FROM ${2:table}\n",
    "      WHERE ${3:cond};\n",
    "    vars:\n",
    "      - name: cursor\n",
    "        type: cursor\n",
    "  - trigger: \":dbtmodel\"\n",
    "    replace: |\n",
    "      {{ config(materialized='table') }}\n",
    "      WITH src AS (\n",
    "          SELECT * FROM {{ ref('${1:stg_table}') }}\n",
    "      )\n",
    "      SELECT * FROM src;\n",
    "    propagate_case: true\n",
    "filters:\n",
    "  apps:\n",
    "    only: [\"com.microsoft.VSCode\"]  # limit expansions to VS Code\n",
    "```\n",
    "\n",
    "Now typing `:sqlsel` in VS Code expands into a SQL skeleton with tabstops‚Äîwithout touching your editor‚Äôs snippet system.\n",
    "\n",
    "---\n",
    "\n",
    "## What I recommend for you\n",
    "\n",
    "Given you‚Äôre on macOS + zsh and bounce across projects:\n",
    "\n",
    "* **Primary:** VS Code user snippets + Sync (A) for language-aware tab-trigger snippets.\n",
    "* **Universal:** Git + `snip` (B) so you can search/paste any snippet in *any* workspace, terminal, or notebook.\n",
    "* **Boilerplate:** Espanso (C) for lightning-fast expansions you want everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## Starter pack you can paste in today\n",
    "\n",
    "**Repo skeleton**\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/snippets/{python,sql,dbt,shell}\n",
    "git init ~/snippets\n",
    "```\n",
    "\n",
    "**Example files**\n",
    "`~/snippets/sql/select-where.md`\n",
    "\n",
    "````md\n",
    "# Select with WHERE & limit\n",
    "```sql\n",
    "SELECT ${1:cols}\n",
    "FROM ${2:schema.table}\n",
    "WHERE ${3:cond}\n",
    "LIMIT ${4:100};\n",
    "````\n",
    "\n",
    "````\n",
    "\n",
    "`~/snippets/python/pandas-null-check.md`\n",
    "```md\n",
    "# Null audit (top offenders)\n",
    "```python\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(na[na>0].head(10))\n",
    "````\n",
    "\n",
    "````\n",
    "\n",
    "**VS Code snippet (dbt model)**\n",
    "`~/snippets/vscode/sql.json`\n",
    "```json\n",
    "{\n",
    "  \"dbt model\": {\n",
    "    \"prefix\": \"dbtmodel\",\n",
    "    \"body\": [\n",
    "      \"{{ config(materialized='table') }}\",\n",
    "      \"WITH src AS (\",\n",
    "      \"  SELECT * FROM {{ ref('${1:stg_table}') }}\",\n",
    "      \")\",\n",
    "      \"SELECT * FROM src;\"\n",
    "    ],\n",
    "    \"description\": \"dbt model scaffold\"\n",
    "  }\n",
    "}\n",
    "````\n",
    "\n",
    "Then symlink it as shown above.\n",
    "\n",
    "---\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "* **Secrets:** never store tokens/keys in snippets; use placeholders like `${SECRET_NAME}`.\n",
    "* **Licensing:** if you paste from blogs/StackOverflow, keep a note/link at the top of the snippet file.\n",
    "* **Naming:** use a consistent prefix (e.g., `:sql*`, `:py*`) for Espanso triggers to avoid accidental expansion.\n",
    "\n",
    ">If you want, tell me your top 10 snippet needs (Python/pandas, SQL/dbt, shell, GCP/AWS), and I‚Äôll generate the files to drop into `~/snippets/` plus the matching VS Code/Espanso entries.\n",
    "\n",
    "---\n",
    "\n",
    "># **#3: Windsurf with VS Code‚Äìstyle snippets**\n",
    "\n",
    "Yep‚ÄîWindsurf plays nicely with **VS Code‚Äìstyle snippets**, plus you can layer on a git repo and a text expander for portability. Here‚Äôs a clean setup that works across projects/workspaces.\n",
    "\n",
    "# 1) Built-in (VS Code‚Äìstyle) user & workspace snippets\n",
    "\n",
    "* Open the **Command Palette ‚Üí ‚ÄúPreferences: Configure User Snippets.‚Äù** Create a language file (e.g., `python.json`) or a **global** snippets file. Snippet format is the same as VS Code‚Äôs JSON (prefix/body/description). ([Arsturn][1], [Visual Studio Code][2])\n",
    "* Bringing snippets from VS Code? On first run Windsurf lets you **import settings from VS Code/Cursor**, which includes snippets. ([DEV Community][3])\n",
    "* For repo-scoped snippets, add a workspace file (e.g., `.vscode/snippets.code-snippets`) so they travel with the project in Git. (Same mechanism as VS Code.) ([Stack Overflow][4])\n",
    "\n",
    "Example snippet body (works in Windsurf since it‚Äôs VS Code syntax):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Docstring scaffold\": {\n",
    "    \"prefix\": \"pydoc\",\n",
    "    \"body\": [\n",
    "      \"\\\"\\\"\\\"${1:Summary}\",\n",
    "      \"\",\n",
    "      \"Args:\",\n",
    "      \"    ${2:param}: ${3:desc}\",\n",
    "      \"\",\n",
    "      \"Returns:\",\n",
    "      \"    ${4:type}: ${5:desc}\",\n",
    "      \"\\\"\\\"\\\"\"\n",
    "    ],\n",
    "    \"description\": \"Python docstring\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "# 2) Manage snippets with extensions (Open VSX)\n",
    "\n",
    "Windsurf uses the **Open VSX** marketplace, so install snippet helpers from there in the Extensions view:\n",
    "\n",
    "* **Snippets Manager** ‚Äì simple UI to create/edit snippets. ([open-vsx.org][5])\n",
    "* **Snippets Viewer** ‚Äì browse built-in, extension, and user snippets in a tree view. ([open-vsx.org][6])\n",
    "* **Snippets Ranger** ‚Äì create/select snippet files quickly. ([open-vsx.org][7])\n",
    "  (If you‚Äôre used to VS Code‚Äôs marketplace, note the store difference‚ÄîWindsurf surfaces Open VSX by default.) ([Reddit][8])\n",
    "\n",
    "# 3) Keep them portable with Git (+ optional symlink)\n",
    "\n",
    "* Put all snippet JSON (and markdown examples) in `~/snippets/` under Git.\n",
    "* In Windsurf, open one of your snippet files and copy its on-disk path; then **symlink** your repo files to that location so edits are versioned automatically (works on Mac/Linux).\n",
    "  This avoids guessing paths and keeps snippets consistent across workspaces.\n",
    "\n",
    "# 4) Universal quick-paste option (works in any app)\n",
    "\n",
    "If you want snippets outside the editor too:\n",
    "\n",
    "* **Espanso** (free text expander): type `:sqlsel` ‚Üí expands to a SQL skeleton anywhere.\n",
    "* Or a tiny **fzf** picker in your terminal that copies code blocks from `~/snippets/*.md` to clipboard (great inside Windsurf‚Äôs integrated terminal).\n",
    "\n",
    "# 5) Bonus: make snippets ‚Äúsearchable‚Äù by the AI\n",
    "\n",
    "If you keep a **Google Doc** of your favorite patterns, Windsurf‚Äôs **Knowledge Base / docs context** can ingest docs for retrieval in chat. It‚Äôs not an inserter like snippets, but handy for ‚Äúremind me of the dbt model scaffold‚Äù prompts. ([Windsurf Docs][9])\n",
    "\n",
    "---\n",
    "\n",
    "## A setup I‚Äôd recommend for you\n",
    "\n",
    "1. **User & workspace snippets** for tab-trigger speed. ([Arsturn][1], [Visual Studio Code][2])\n",
    "2. **Git repo** at `~/snippets/` + **symlink** to Windsurf‚Äôs snippet files so changes sync across workspaces.\n",
    "3. Install **Snippets Manager** (UI), plus **Snippets Viewer** (browse). ([open-vsx.org][5])\n",
    "4. Add **Espanso** for cross-app boilerplate (optional).\n",
    "5. (Nice-to-have) A **Google Doc** of patterns connected to Windsurf‚Äôs knowledge base for quick AI recall. ([Windsurf Docs][9])\n",
    "\n",
    "If you want, tell me your top 10 snippets (Python/pandas, SQL/dbt, shell/GCP) and I‚Äôll generate the JSON files + a starter `~/snippets/` repo structure to drop in.\n",
    "\n",
    "[1]: https://www.arsturn.com/blog/creating-custom-code-snippets-in-windsurf-for-reusability \"Mastering Code Snippets in Windsurf: Reusability Techniques\"\n",
    "[2]: https://code.visualstudio.com/docs/editing/userdefinedsnippets?utm_source=chatgpt.com \"Snippets in Visual Studio Code\"\n",
    "[3]: https://dev.to/proflead/this-ai-ide-can-code-for-you-windsurf-ai-full-tutorial-4p94 \"This AI IDE Can Code For You ‚Äì Windsurf AI Full Tutorial - DEV Community\"\n",
    "[4]: https://stackoverflow.com/questions/29995863/how-to-add-custom-code-snippets-in-vscode?utm_source=chatgpt.com \"How to add custom code snippets in VSCode?\"\n",
    "[5]: https://open-vsx.org/extension/zjffun/snippetsmanager?utm_source=chatgpt.com \"Snippets Manager\"\n",
    "[6]: https://open-vsx.org/extension/RandomFractalsInc/snippets-viewer?utm_source=chatgpt.com \"Snippets Viewer\"\n",
    "[7]: https://open-vsx.org/extension/robole/snippets-ranger?utm_source=chatgpt.com \"Snippets Ranger\"\n",
    "[8]: https://www.reddit.com/r/RooCode/comments/1i6ns94/new_users_discovery_of_roocode_on_vscode/?utm_source=chatgpt.com \"New users' discovery of RooCode on vscode marketplace ...\"\n",
    "[9]: https://docs.windsurf.com/context-awareness/overview?utm_source=chatgpt.com \"Overview - Windsurf Docs\"\n",
    "\n",
    "---\n",
    "\n",
    "># **#4: create a custome library that gives me suggestions in a jupyter cell or python file when I type # to call the library then call a specific snippet by name**\n",
    "\n",
    "Yes‚Äîyou can make a tiny IPython/Jupyter extension that:\n",
    "\n",
    "* lets you write a **comment directive** like `# snip <name>` which gets **expanded into code** before execution, and\n",
    "* gives you **Tab-completion suggestions** for snippet names after you type `# snip ` in a Jupyter cell (works in JupyterLab/Notebook and VS Code‚Äôs Jupyter).\n",
    "\n",
    "Below is a minimal, drop-in ‚Äúsnippet library‚Äù you can keep in `~/.snippets/`. It uses a YAML file for your snippets, a transformer to expand `# snip ...` lines, and a custom completer to suggest names.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Snippet storage (YAML)\n",
    "\n",
    "`~/.snippets/snippets.yaml`\n",
    "\n",
    "```yaml\n",
    "pandas.null_audit: |\n",
    "  na = df.isna().sum().sort_values(ascending=False)\n",
    "  print(na[na>0].head(10))\n",
    "sql.select_where: |\n",
    "  SELECT ${1:cols}\n",
    "  FROM ${2:schema.table}\n",
    "  WHERE ${3:cond}\n",
    "  LIMIT ${4:100};\n",
    "dbt.model_scaffold: |\n",
    "  {{ config(materialized='table') }}\n",
    "  WITH src AS (\n",
    "    SELECT * FROM {{ ref('${1:stg_table}') }}\n",
    "  )\n",
    "  SELECT * FROM src;\n",
    "```\n",
    "\n",
    "> You can keep multiple YAML files under `~/.snippets/`‚Äîthe loader will merge them.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) The extension (`snipline.py`)\n",
    "\n",
    "Put this file anywhere on your PYTHONPATH (e.g., `~/snipline/snipline.py`) and `pip install pyyaml` if you don‚Äôt have it.\n",
    "\n",
    "```python\n",
    "# snipline.py\n",
    "from __future__ import annotations\n",
    "import os, re, glob, time, yaml, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from IPython.core.magic import Magics, magics_class, line_magic\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "SNIP_DIR = Path(os.environ.get(\"SNIP_DIR\", \"~/.snippets\")).expanduser()\n",
    "\n",
    "class SnipStore:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.dir = Path(directory)\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._mtimes: Dict[str, float] = {}\n",
    "        self.reload()\n",
    "\n",
    "    def reload(self):\n",
    "        self.cache.clear()\n",
    "        self._mtimes.clear()\n",
    "        for yml in glob.glob(str(self.dir / \"*.y*ml\")):\n",
    "            with open(yml, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f) or {}\n",
    "            for k, v in data.items():\n",
    "                self.cache[str(k)] = str(v)\n",
    "            self._mtimes[yml] = os.path.getmtime(yml)\n",
    "\n",
    "    def maybe_reload(self):\n",
    "        for yml, old in list(self._mtimes.items()):\n",
    "            if os.path.exists(yml) and os.path.getmtime(yml) != old:\n",
    "                self.reload()\n",
    "                break\n",
    "\n",
    "    def names(self, prefix: str = \"\"):\n",
    "        self.maybe_reload()\n",
    "        return sorted([k for k in self.cache if k.startswith(prefix)])\n",
    "\n",
    "    def get(self, name: str) -> Optional[str]:\n",
    "        self.maybe_reload()\n",
    "        return self.cache.get(name)\n",
    "\n",
    "STORE = SnipStore(SNIP_DIR)\n",
    "\n",
    "# --- Input transformer: replace lines like \"# snip <name>\" with the snippet text\n",
    "_SNIP_LINE = re.compile(r\"^[ \\t]*#\\s*snip\\s+([A-Za-z0-9_.\\-/]+)[ \\t]*$\", re.MULTILINE)\n",
    "\n",
    "def _expand_snips(cell: str) -> str:\n",
    "    def repl(m):\n",
    "        name = m.group(1)\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            # Leave a visible marker if missing\n",
    "            return f\"# [snip: '{name}' not found]\"\n",
    "        # Preserve indentation of the directive line\n",
    "        indent = len(m.string[:m.start()].splitlines()[-1]) - len(m.string[:m.start()].splitlines()[-1].lstrip())\n",
    "        ind = \" \" * max(indent, 0)\n",
    "        expanded = textwrap.indent(body.rstrip(\"\\n\"), ind)\n",
    "        return expanded\n",
    "    return _SNIP_LINE.sub(repl, cell)\n",
    "\n",
    "# --- Tab completion for \"# snip \" or \"%snip \" prefixes\n",
    "def _snip_completer(self, event):\n",
    "    # Works for both comments and magic usage\n",
    "    line = getattr(event, \"line\", \"\")\n",
    "    # capture partial name user is typing\n",
    "    m = re.search(r\"(#\\s*snip|%snip)\\s+([A-Za-z0-9_.\\-/]*)$\", line[:getattr(event, \"cursor_position\", len(line))])\n",
    "    if not m:\n",
    "        return []\n",
    "    prefix = m.group(2)\n",
    "    return STORE.names(prefix)\n",
    "\n",
    "@magics_class\n",
    "class SnipMagics(Magics):\n",
    "    @line_magic(\"snip\")\n",
    "    def snip(self, line):\n",
    "        \"\"\"Usage: %snip <name>  -> prints the snippet body\n",
    "        With 'insert' in classic Notebook, it replaces the current cell.\"\"\"\n",
    "        name = line.strip()\n",
    "        if not name:\n",
    "            print(\"\\n\".join(STORE.names()))\n",
    "            return\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            print(f\"[snip: '{name}' not found]\")\n",
    "            return\n",
    "        # Print for copy/paste\n",
    "        print(body)\n",
    "        # Optional: try to replace current cell in classic Notebook UI\n",
    "        try:\n",
    "            js = Javascript(\"\"\"\n",
    "                if (typeof Jupyter !== 'undefined') {\n",
    "                  var cell = Jupyter.notebook.get_selected_cell();\n",
    "                  cell.set_text(atob('%s'));\n",
    "                }\n",
    "            \"\"\" % body.encode(\"utf-8\").hex())\n",
    "            display(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    # Expand \"# snip ...\" lines before execution\n",
    "    ip.input_transformers_post.append(_expand_snips)\n",
    "    # Add completer (works in Jupyter and VS Code's Jupyter)\n",
    "    try:\n",
    "        ip.set_hook('complete_command', _snip_completer, re_key='.*')\n",
    "    except Exception:\n",
    "        # Fallback: older IPython versions\n",
    "        completer = getattr(ip, 'Completer', None)\n",
    "        if completer and hasattr(completer, 'matchers'):\n",
    "            completer.matchers.insert(0, lambda self, text: [])\n",
    "    ip.register_magics(SnipMagics)\n",
    "\n",
    "def unload_ipython_extension(ip):\n",
    "    # Best-effort cleanup; new sessions usually just restart\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Use it in Jupyter/VS Code‚Äôs Jupyter\n",
    "\n",
    "In a cell:\n",
    "\n",
    "```python\n",
    "%load_ext snipline   # run once per session (or auto-load, see below)\n",
    "```\n",
    "\n",
    "Now you can:\n",
    "\n",
    "* Type: `# snip pandas.null_audit` then press **Tab** after `# snip ` to see snippet **name suggestions**.\n",
    "* Run the cell: that line is **replaced** with the snippet‚Äôs code before execution.\n",
    "* Or: `%snip pandas.null_audit` to print/insert the snippet body.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# snip pandas.null_audit\n",
    "```\n",
    "\n",
    "‚Ä¶expands to:\n",
    "\n",
    "```python\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(na[na>0].head(10))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Auto-load every session (optional)\n",
    "\n",
    "Create (or edit) `~/.ipython/profile_default/ipython_config.py`:\n",
    "\n",
    "```python\n",
    "c = get_config()\n",
    "c.InteractiveShellApp.extensions = ['snipline']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes & limits\n",
    "\n",
    "* **Where suggestions work:** in **Jupyter contexts** (classic, Lab, or VS Code‚Äôs Jupyter) because completion comes from the kernel. In a plain `.py` file editor view, completion is handled by the editor‚Äôs language server, so you won‚Äôt get the `# snip` suggestions there. For `.py` editing, pair this with **editor snippets** (VS Code/Windsurf) or a text expander (Espanso).\n",
    "* **Variables/placeholders:** You can embed `${1:placeholder}` in snippets for editor tabstops (useful when you paste into VS Code). The transformer leaves those as-is; your editor‚Äôs snippet engine will handle them if present.\n",
    "* **Versioning:** Keep `~/.snippets/` under Git so you can sync across machines. The loader auto-reloads when YAML files change.\n",
    "\n",
    "If you want, tell me your top 10 snippet names and I‚Äôll generate the YAML ready to drop into `~/.snippets/snippets.yaml`.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    Heck yes‚Äîlet‚Äôs make **snipples** your cross-workspace snippet engine. Below are two files:\n",
    "\n",
    "1. **`snipples.py`** ‚Äì an IPython/Jupyter extension so you can type `# snipples <name>` in a cell and it expands before execution (with tab-complete for names).\n",
    "2. **`~/.snipples/snippets.yaml`** ‚Äì a chunky library covering the Telco Churn project end-to-end (ingest ‚Üí EDA ‚Üí cleaning ‚Üí features ‚Üí ML ‚Üí explainability ‚Üí dbt/SQL ‚Üí ops).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) `snipples.py` (drop anywhere on your PYTHONPATH)\n",
    "\n",
    "```python\n",
    "# snipples.py\n",
    "from __future__ import annotations\n",
    "import os, re, glob, yaml, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from IPython.core.magic import Magics, magics_class, line_magic\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "SNIPS_DIR = Path(os.environ.get(\"SNIPPLES_DIR\", \"~/.snipples\")).expanduser()\n",
    "\n",
    "class SnipStore:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.dir = Path(directory)\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._mtimes: Dict[str, float] = {}\n",
    "        self.reload()\n",
    "\n",
    "    def reload(self):\n",
    "        self.cache.clear()\n",
    "        self._mtimes.clear()\n",
    "        self.dir.mkdir(parents=True, exist_ok=True)\n",
    "        for yml in sorted(self.dir.glob(\"*.y*ml\")):\n",
    "            with open(yml, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f) or {}\n",
    "            for k, v in data.items():\n",
    "                self.cache[str(k)] = str(v)\n",
    "            self._mtimes[str(yml)] = yml.stat().st_mtime\n",
    "\n",
    "    def maybe_reload(self):\n",
    "        for yml, old in list(self._mtimes.items()):\n",
    "            p = Path(yml)\n",
    "            if p.exists() and p.stat().st_mtime != old:\n",
    "                self.reload()\n",
    "                break\n",
    "\n",
    "    def names(self, prefix: str = \"\"):\n",
    "        self.maybe_reload()\n",
    "        return sorted([k for k in self.cache if k.startswith(prefix)])\n",
    "\n",
    "    def get(self, name: str) -> Optional[str]:\n",
    "        self.maybe_reload()\n",
    "        return self.cache.get(name)\n",
    "\n",
    "STORE = SnipStore(SNIPS_DIR)\n",
    "\n",
    "_SNIPLINE = re.compile(r\"^[ \\t]*#\\s*snipples\\s+([A-Za-z0-9_.\\-/]+)[ \\t]*$\", re.MULTILINE)\n",
    "\n",
    "def _expand_snips(cell: str) -> str:\n",
    "    def repl(m):\n",
    "        name = m.group(1)\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            return f\"# [snipples: '{name}' not found]\"\n",
    "        # Preserve indentation level of the directive line\n",
    "        line = cell[:m.start()].splitlines()[-1] if cell[:m.start()].splitlines() else \"\"\n",
    "        indent = len(line) - len(line.lstrip(\" \"))\n",
    "        expanded = textwrap.indent(body.rstrip(\"\\n\"), \" \" * max(indent, 0))\n",
    "        return expanded\n",
    "    return _SNIPLINE.sub(repl, cell)\n",
    "\n",
    "def _snipples_completer(self, event):\n",
    "    line = getattr(event, \"line\", \"\")\n",
    "    cur = getattr(event, \"cursor_position\", len(line))\n",
    "    m = re.search(r\"(#\\s*snipples|%snipples)\\s+([A-Za-z0-9_.\\-/]*)$\", line[:cur])\n",
    "    if not m:\n",
    "        return []\n",
    "    return STORE.names(m.group(2))\n",
    "\n",
    "@magics_class\n",
    "class SnipplesMagics(Magics):\n",
    "    @line_magic(\"snipples\")\n",
    "    def snipples(self, line):\n",
    "        \"\"\"%snipples <name>  -> prints/optionally inserts the snippet body\"\"\"\n",
    "        name = line.strip()\n",
    "        if not name:\n",
    "            print(\"\\n\".join(STORE.names()))\n",
    "            return\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            print(f\"[snipples: '{name}' not found]\")\n",
    "            return\n",
    "        print(body)\n",
    "        try:\n",
    "            js = Javascript(\"\"\"\n",
    "                if (typeof Jupyter !== 'undefined') {\n",
    "                  var cell = Jupyter.notebook.get_selected_cell();\n",
    "                  cell.set_text(atob('%s'));\n",
    "                }\n",
    "            \"\"\" % body.encode(\"utf-8\").hex())\n",
    "            display(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    ip.input_transformers_post.append(_expand_snips)\n",
    "    try:\n",
    "        ip.set_hook('complete_command', _snipples_completer, re_key='.*')\n",
    "    except Exception:\n",
    "        pass\n",
    "    ip.register_magics(SnipplesMagics)\n",
    "\n",
    "def unload_ipython_extension(ip):\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Use:** in a Jupyter cell run once per session:\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "# then type:  # snipples telco.qa.null_audit   (Tab to see suggestions)\n",
    "```\n",
    "\n",
    "To autoload every time, add to `~/.ipython/profile_default/ipython_config.py`:\n",
    "\n",
    "```python\n",
    "c = get_config()\n",
    "c.InteractiveShellApp.extensions = ['snipples']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) `~/.snipples/snippets.yaml` (Telco Churn end-to-end)\n",
    "\n",
    "Create the folder and file:\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.snipples\n",
    "# paste the following into ~/.snipples/snippets.yaml\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# =========================\n",
    "# Project scaffolding / setup\n",
    "# =========================\n",
    "proj.readme.telco: |\n",
    "  # Telco Churn ‚Äî Analytics & ML\n",
    "  ## Stack\n",
    "  - Python 3.11, pandas, scikit-learn, xgboost, catboost, imbalanced-learn, shap\n",
    "  - BigQuery + dbt (analytics)\n",
    "  - Great Expectations or Pandera (ingest DQ)\n",
    "  - MLflow (experiment tracking)\n",
    "\n",
    "  ## Structure\n",
    "  .\n",
    "  ‚îú‚îÄ data/{raw,interim,processed}\n",
    "  ‚îú‚îÄ notebooks/\n",
    "  ‚îú‚îÄ src/telco/...\n",
    "  ‚îú‚îÄ models/ (dbt)\n",
    "  ‚îî‚îÄ reports/\n",
    "\n",
    "  ## Targets\n",
    "  - Clean EDA + feature marts\n",
    "  - Baseline + tree models\n",
    "  - Explainability (SHAP)\n",
    "  - Reproducible pipelines + tests\n",
    "\n",
    "env.venv.setup: |\n",
    "  python3 -m venv .venv\n",
    "  source .venv/bin/activate\n",
    "  python -m pip install --upgrade pip\n",
    "  pip install pandas numpy scikit-learn imbalanced-learn xgboost catboost shap mlflow matplotlib seaborn pandera great-expectations python-dotenv pandas-gbq google-cloud-bigquery pyarrow\n",
    "\n",
    "nb.header.imports: |\n",
    "  import os, sys, math, json, textwrap, warnings\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  from pathlib import Path\n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "  pd.set_option(\"display.max_columns\", 100)\n",
    "  plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "# =========================\n",
    "# Ingest / IO\n",
    "# =========================\n",
    "ingest.csv.read_telco: |\n",
    "  dtype_map = {\n",
    "      \"customerID\": \"string\",\n",
    "      \"gender\": \"string\",\n",
    "      \"SeniorCitizen\": \"Int64\",\n",
    "      \"Partner\": \"string\",\n",
    "      \"Dependents\": \"string\",\n",
    "      \"tenure\": \"Int64\",\n",
    "      \"PhoneService\": \"string\",\n",
    "      \"MultipleLines\": \"string\",\n",
    "      \"InternetService\": \"string\",\n",
    "      \"OnlineSecurity\": \"string\",\n",
    "      \"OnlineBackup\": \"string\",\n",
    "      \"DeviceProtection\": \"string\",\n",
    "      \"TechSupport\": \"string\",\n",
    "      \"StreamingTV\": \"string\",\n",
    "      \"StreamingMovies\": \"string\",\n",
    "      \"Contract\": \"string\",\n",
    "      \"PaperlessBilling\": \"string\",\n",
    "      \"PaymentMethod\": \"string\",\n",
    "      \"MonthlyCharges\": \"float64\",\n",
    "      \"TotalCharges\": \"string\",   # coercion later\n",
    "      \"Churn\": \"string\"\n",
    "  }\n",
    "  df = pd.read_csv(\"data/raw/Telco-Customer-Churn.csv\", dtype=dtype_map)\n",
    "  print(df.shape)\n",
    "\n",
    "ingest.bigquery.read_table: |\n",
    "  # pip install pandas-gbq google-cloud-bigquery\n",
    "  from pandas_gbq import read_gbq\n",
    "  df = read_gbq(\"\"\"\n",
    "      SELECT * FROM `PROJECT.DATASET.telco_customers`\n",
    "  \"\"\", project_id=os.environ.get(\"GCP_PROJECT\"))\n",
    "\n",
    "save.outputs.standard_paths: |\n",
    "  Path(\"data/interim\").mkdir(parents=True, exist_ok=True)\n",
    "  Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "  df.to_csv(\"data/interim/telco_cleaned.csv\", index=False)\n",
    "\n",
    "# =========================\n",
    "# Data Quality (ingest)\n",
    "# =========================\n",
    "qa.null_audit: |\n",
    "  na = df.isna().sum().sort_values(ascending=False)\n",
    "  print(\"Missing by column:\")\n",
    "  print(na[na>0])\n",
    "\n",
    "qa.dup_pkey_check: |\n",
    "  dups = df[\"customerID\"].value_counts()\n",
    "  print(\"Dup primary keys:\", (dups > 1).sum())\n",
    "\n",
    "qa.pandera.schema_telco: |\n",
    "  import pandera as pa\n",
    "  from pandera import Column, Check\n",
    "  TelcoSchema = pa.DataFrameSchema({\n",
    "      \"customerID\": Column(str, nullable=False),\n",
    "      \"tenure\": Column(int, Check.in_range(0, 84), nullable=False),\n",
    "      \"MonthlyCharges\": Column(float, Check.in_range(0, 200), nullable=False),\n",
    "      \"TotalCharges\": Column(object, nullable=True),  # coerced later\n",
    "      \"Churn\": Column(str, Check.isin([\"Yes\",\"No\"]), nullable=False),\n",
    "  })\n",
    "  TelcoSchema.validate(df, lazy=True)\n",
    "\n",
    "qa.business_rule_totalcharges_tenure: |\n",
    "  bad = df[df[\"tenure\"].fillna(0) > 0].copy()\n",
    "  bad = bad[np.abs(pd.to_numeric(bad[\"TotalCharges\"], errors=\"coerce\") - bad[\"tenure\"] * bad[\"MonthlyCharges\"]) > 10]\n",
    "  print(f\"Rows outside $10 tolerance: {len(bad)}\")\n",
    "\n",
    "# =========================\n",
    "# Cleaning / Imputation\n",
    "# =========================\n",
    "clean.totalcharges_fix: |\n",
    "  df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"].astype(str).str.strip(), errors=\"coerce\")\n",
    "  mask0 = df[\"TotalCharges\"].isna() & (df[\"tenure\"].fillna(0) == 0)\n",
    "  df.loc[mask0, \"TotalCharges\"] = 0.0\n",
    "  mask_other = df[\"TotalCharges\"].isna() & df[\"tenure\"].notna() & df[\"MonthlyCharges\"].notna()\n",
    "  df.loc[mask_other, \"TotalCharges\"] = df.loc[mask_other, \"tenure\"] * df.loc[mask_other, \"MonthlyCharges\"]\n",
    "  df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(\"float64\")\n",
    "\n",
    "clean.category_normalize: |\n",
    "  yesno = [\"Partner\",\"Dependents\",\"PhoneService\",\"PaperlessBilling\",\"Churn\"]\n",
    "  for c in yesno:\n",
    "      df[c] = df[c].str.strip().str.title()\n",
    "  df[\"MultipleLines\"]   = df[\"MultipleLines\"].str.strip().str.replace(\"No phone service\",\"No Phone Service\", regex=False)\n",
    "  internet_cols = [\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\"]\n",
    "  for c in internet_cols:\n",
    "      df[c] = df[c].str.strip().str.replace(\"No internet service\",\"No Internet Service\", regex=False)\n",
    "\n",
    "# =========================\n",
    "# EDA\n",
    "# =========================\n",
    "eda.quick_overview: |\n",
    "  print(df.shape)\n",
    "  display(df.head())\n",
    "  print(df.describe(include=\"all\").T)\n",
    "\n",
    "eda.target_balance: |\n",
    "  ax = (df[\"Churn\"].value_counts(normalize=True)*100).plot.bar()\n",
    "  ax.set_title(\"Churn class balance (%)\"); plt.show()\n",
    "\n",
    "eda.corr_heatmap_numeric: |\n",
    "  num = df.select_dtypes(include=[\"number\"])\n",
    "  sns.heatmap(num.corr(numeric_only=True), annot=False, linewidths=.5)\n",
    "  plt.title(\"Numeric correlations\"); plt.show()\n",
    "\n",
    "eda.churn_rate_by_col_template: |\n",
    "  col = \"Contract\"  # <- change me\n",
    "  rate = (df.groupby(col)[\"Churn\"].apply(lambda s: (s==\"Yes\").mean()).sort_values()*100)\n",
    "  print(rate.round(2))\n",
    "\n",
    "# =========================\n",
    "# Feature Engineering\n",
    "# =========================\n",
    "feat.tenure_buckets: |\n",
    "  bins = [-1, 0, 6, 12, 24, 48, 84, 999]\n",
    "  labels = [\"0\",\"1-6\",\"7-12\",\"13-24\",\"25-48\",\"49-84\",\"85+\"]\n",
    "  df[\"tenure_bucket\"] = pd.cut(df[\"tenure\"], bins=bins, labels=labels)\n",
    "\n",
    "feat.boolean_target: |\n",
    "  df[\"y\"] = (df[\"Churn\"].str.upper() == \"YES\").astype(int)\n",
    "\n",
    "feat.split_train_test: |\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  target = \"y\"\n",
    "  y = df[target]\n",
    "  feature_drop = [\"customerID\",\"Churn\",\"y\"]\n",
    "  X = df.drop(columns=feature_drop, errors=\"ignore\")\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# =========================\n",
    "# Modeling ‚Äì shared utilities\n",
    "# =========================\n",
    "model.columns_splitter: |\n",
    "  num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "  cat_cols = X_train.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "  from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "  from sklearn.compose import ColumnTransformer\n",
    "  pre = ColumnTransformer(\n",
    "      transformers=[\n",
    "          (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "          (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "      ],\n",
    "      remainder=\"drop\"\n",
    "  )\n",
    "\n",
    "model.metrics_helpers: |\n",
    "  from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report, confusion_matrix\n",
    "  def print_metrics(y_true, prob, thr=0.5):\n",
    "      y_pred = (prob >= thr).astype(int)\n",
    "      print(f\"ROC AUC: {roc_auc_score(y_true, prob):.4f}\")\n",
    "      print(f\"PR AUC : {average_precision_score(y_true, prob):.4f}\")\n",
    "      print(f\"F1     : {f1_score(y_true, y_pred):.4f}\")\n",
    "      print(confusion_matrix(y_true, y_pred))\n",
    "      print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "# =========================\n",
    "# Baseline model (LogReg)\n",
    "# =========================\n",
    "model.baseline_logreg: |\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.pipeline import Pipeline\n",
    "  clf = Pipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"lr\", LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None))\n",
    "  ])\n",
    "  clf.fit(X_train, y_train)\n",
    "  prob = clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "# =========================\n",
    "# Tree models (XGBoost / CatBoost)\n",
    "# =========================\n",
    "model.xgboost_cv: |\n",
    "  import xgboost as xgb\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.pipeline import Pipeline\n",
    "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "  clf = Pipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"xgb\", xgb.XGBClassifier(\n",
    "          n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "          subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n",
    "          tree_method=\"hist\", reg_lambda=1.0, n_jobs=-1\n",
    "      ))\n",
    "  ])\n",
    "  aucs = []\n",
    "  for tr, va in skf.split(X_train, y_train):\n",
    "      clf.fit(X_train.iloc[tr], y_train.iloc[tr])\n",
    "      p = clf.predict_proba(X_train.iloc[va])[:,1]\n",
    "      aucs.append(roc_auc_score(y_train.iloc[va], p))\n",
    "  print(\"CV ROC AUC:\", np.mean(aucs).round(4), \"+/-\", np.std(aucs).round(4))\n",
    "  clf.fit(X_train, y_train)\n",
    "  prob = clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "model.catboost_simple: |\n",
    "  from catboost import CatBoostClassifier\n",
    "  # For CatBoost you can pass categorical indices directly (works best on raw categories)\n",
    "  Xc = df.drop(columns=[\"customerID\",\"Churn\",\"y\"], errors=\"ignore\")\n",
    "  cat_idx = np.where(Xc.dtypes == \"object\")[0].tolist()\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  Xtr, Xte, ytr, yte = train_test_split(Xc, df[\"y\"], test_size=0.2, stratify=df[\"y\"], random_state=42)\n",
    "  cat = CatBoostClassifier(\n",
    "      depth=6, iterations=1500, learning_rate=0.03, loss_function=\"Logloss\",\n",
    "      eval_metric=\"AUC\", verbose=200, random_seed=42, auto_class_weights=\"Balanced\"\n",
    "  )\n",
    "  cat.fit(Xtr, ytr, cat_features=cat_idx, eval_set=(Xte, yte), use_best_model=True)\n",
    "  prob = cat.predict_proba(Xte)[:,1]\n",
    "  print_metrics(yte, prob)\n",
    "\n",
    "# =========================\n",
    "# Imbalance strategies (optional)\n",
    "# =========================\n",
    "imb.smote_pipeline_xgb: |\n",
    "  from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "  from imblearn.over_sampling import SMOTE\n",
    "  import xgboost as xgb\n",
    "  imb_clf = ImbPipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"smote\", SMOTE(random_state=42)),\n",
    "      (\"xgb\", xgb.XGBClassifier(\n",
    "          n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "          subsample=0.9, colsample_bytree=0.9, eval_metric=\"logloss\", n_jobs=-1\n",
    "      ))\n",
    "  ])\n",
    "  imb_clf.fit(X_train, y_train)\n",
    "  prob = imb_clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "# =========================\n",
    "# Threshold tuning & calibration\n",
    "# =========================\n",
    "eval.threshold_opt_pr: |\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "  pr, rc, thr = precision_recall_curve(y_test, prob)\n",
    "  f = 2*pr*rc/(pr+rc+1e-9)\n",
    "  best = np.nanargmax(f)\n",
    "  print(\"Best F1 threshold:\", thr[best].round(4), \"F1:\", f[best].round(4))\n",
    "\n",
    "eval.calibration_plot: |\n",
    "  from sklearn.calibration import calibration_curve\n",
    "  prob_true, prob_pred = calibration_curve(y_test, prob, n_bins=10)\n",
    "  plt.plot(prob_pred, prob_true, marker=\"o\"); plt.plot([0,1],[0,1],\"--\")\n",
    "  plt.title(\"Calibration\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\"); plt.show()\n",
    "\n",
    "# =========================\n",
    "# SHAP explainability\n",
    "# =========================\n",
    "exp.shap_tree: |\n",
    "  import shap\n",
    "  shap.initjs()\n",
    "  # Works with tree-based models like XGBoost/CatBoost\n",
    "  booster = clf.named_steps[\"xgb\"] if \"xgb\" in dict(clf.steps) else None\n",
    "  if booster is None:\n",
    "      raise RuntimeError(\"This snippet expects a fitted Pipeline with step 'xgb'.\")\n",
    "  Xs = clf.named_steps[\"pre\"].transform(X_test)\n",
    "  explainer = shap.TreeExplainer(booster)\n",
    "  shap_values = explainer.shap_values(Xs)\n",
    "  shap.summary_plot(shap_values, Xs)\n",
    "\n",
    "exp.shap_linear: |\n",
    "  import shap\n",
    "  linear = clf.named_steps.get(\"lr\", None)\n",
    "  if linear is None:\n",
    "      raise RuntimeError(\"This snippet expects a fitted Pipeline with step 'lr'.\")\n",
    "  Xs = clf.named_steps[\"pre\"].transform(X_test)\n",
    "  explainer = shap.LinearExplainer(linear, Xs, feature_dependence=\"independent\")\n",
    "  shap_values = explainer.shap_values(Xs)\n",
    "  shap.summary_plot(shap_values, Xs)\n",
    "\n",
    "# =========================\n",
    "# MLflow tracking (optional)\n",
    "# =========================\n",
    "mlflow.start_run_and_log: |\n",
    "  import mlflow, mlflow.sklearn\n",
    "  mlflow.set_experiment(\"telco-churn\")\n",
    "  with mlflow.start_run(run_name=\"xgb_baseline\"):\n",
    "      mlflow.log_params({\"n_estimators\":600,\"max_depth\":6,\"lr\":0.05})\n",
    "      mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, prob))\n",
    "      mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "# =========================\n",
    "# SQL (analysis & sanity)\n",
    "# =========================\n",
    "sql.churn_rate_by_segment: |\n",
    "  SELECT Contract,\n",
    "         ROUND(100 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_pct,\n",
    "         COUNT(*) AS n\n",
    "  FROM analytics.telco_customers\n",
    "  GROUP BY Contract\n",
    "  ORDER BY churn_pct DESC;\n",
    "\n",
    "sql.monthly_retention: |\n",
    "  -- Requires start_date and end_date fields for each subscription\n",
    "  WITH months AS (\n",
    "    SELECT DATE_TRUNC(d, MONTH) AS month\n",
    "    FROM UNNEST(GENERATE_DATE_ARRAY('2017-01-01','2018-12-31', INTERVAL 1 DAY)) AS d\n",
    "  ),\n",
    "  active AS (\n",
    "    SELECT m.month,\n",
    "           COUNTIF(t.start_date <= m.month AND (t.end_date IS NULL OR t.end_date >= m.month)) AS active_customers\n",
    "    FROM months m CROSS JOIN analytics.telco_subscriptions t\n",
    "    GROUP BY m.month\n",
    "  )\n",
    "  SELECT * FROM active ORDER BY month;\n",
    "\n",
    "# =========================\n",
    "# dbt scaffolds\n",
    "# =========================\n",
    "dbt.model_scaffold: |\n",
    "  {{ config(materialized='table') }}\n",
    "  WITH src AS (\n",
    "    SELECT * FROM {{ ref('stg_telco_customers') }}\n",
    "  )\n",
    "  SELECT * FROM src;\n",
    "\n",
    "dbt.schema_tests_min: |\n",
    "  version: 2\n",
    "  models:\n",
    "    - name: stg_telco_customers\n",
    "      columns:\n",
    "        - name: customerID\n",
    "          tests: [not_null, unique]\n",
    "        - name: tenure\n",
    "          tests:\n",
    "            - dbt_expectations.expect_column_values_to_be_between:\n",
    "                min_value: 0\n",
    "                max_value: 84\n",
    "        - name: Churn\n",
    "          tests:\n",
    "            - accepted_values:\n",
    "                values: ['Yes','No']\n",
    "      tests:\n",
    "        - dbt_utils.expression_is_true:\n",
    "            expression: \"case when tenure = 0 then coalesce(TotalCharges,0) = 0 else true end\"\n",
    "        - dbt_utils.expression_is_true:\n",
    "            expression: \"abs(coalesce(TotalCharges,0) - coalesce(tenure,0)*coalesce(MonthlyCharges,0)) <= 10\"\n",
    "            where: \"tenure > 0\"\n",
    "\n",
    "dbt.freshness_recency: |\n",
    "  models:\n",
    "    - name: stg_telco_customers\n",
    "      tests:\n",
    "        - dbt_utils.recency:\n",
    "            field: ingested_at\n",
    "            datepart: hour\n",
    "            interval: 24\n",
    "\n",
    "# =========================\n",
    "# Reporting / export\n",
    "# =========================\n",
    "report.top_drivers_summary: |\n",
    "  # After SHAP: simple text summary\n",
    "  import numpy as np\n",
    "  def top_features(shap_vals, k=10):\n",
    "      imp = np.abs(shap_vals).mean(axis=0)\n",
    "      idx = np.argsort(imp)[::-1][:k]\n",
    "      return list(zip(idx, imp[idx]))\n",
    "  print(\"Top features by mean |SHAP| (indices):\")\n",
    "  print(top_features(shap_values, 10))\n",
    "\n",
    "export.predictions_csv: |\n",
    "  out = X_test.copy()\n",
    "  out[\"prob_churn\"] = prob\n",
    "  out[\"y_true\"] = y_test.values\n",
    "  out.to_csv(\"reports/predictions.csv\", index=False)\n",
    "  print(\"Saved reports/predictions.csv\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quick start flow (what to run in a fresh notebook)\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "\n",
    "# snipples nb.header.imports\n",
    "# snipples ingest.csv.read_telco\n",
    "# snipples qa.null_audit\n",
    "# snipples clean.totalcharges_fix\n",
    "# snipples clean.category_normalize\n",
    "# snipples feat.boolean_target\n",
    "# snipples feat.tenure_buckets\n",
    "# snipples feat.split_train_test\n",
    "# snipples model.columns_splitter\n",
    "# snipples model.metrics_helpers\n",
    "# snipples model.baseline_logreg       # => baseline\n",
    "# snipples model.xgboost_cv            # => stronger model\n",
    "# snipples eval.threshold_opt_pr\n",
    "# snipples exp.shap_tree               # if using the XGB pipeline\n",
    "# snipples export.predictions_csv\n",
    "```\n",
    "\n",
    "# Generate **workspace snippets** (VS Code/Windsurf JSON) \n",
    "\n",
    "    Here are two drop-in artifacts for Windsurf/VS Code:\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Workspace snippets\n",
    "\n",
    "Save as **`.vscode/snippets.code-snippets`** in your repo.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Snipples: expand line\": {\n",
    "    \"prefix\": \"snipples\",\n",
    "    \"description\": \"Insert a # snipples directive (works in Jupyter cells via snipples.py)\",\n",
    "    \"body\": [\"# snipples ${1:namespace.snippet_name}\"]\n",
    "  },\n",
    "\n",
    "  \"Imports: data analyst notebook header\": {\n",
    "    \"prefix\": \"py-imports-notebook\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Standard imports for analysis notebooks\",\n",
    "    \"body\": [\n",
    "      \"import os, sys, math, json, textwrap, warnings\",\n",
    "      \"import numpy as np\",\n",
    "      \"import pandas as pd\",\n",
    "      \"import matplotlib.pyplot as plt\",\n",
    "      \"import seaborn as sns\",\n",
    "      \"from pathlib import Path\",\n",
    "      \"warnings.filterwarnings('ignore')\",\n",
    "      \"pd.set_option('display.max_columns', 100)\",\n",
    "      \"plt.rcParams['figure.figsize'] = (10,5)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"IO: read Telco CSV with dtypes\": {\n",
    "    \"prefix\": \"py-read-telco\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Read IBM Telco Churn CSV with explicit dtypes\",\n",
    "    \"body\": [\n",
    "      \"dtype_map = {\",\n",
    "      \"  'customerID':'string','gender':'string','SeniorCitizen':'Int64','Partner':'string','Dependents':'string',\",\n",
    "      \"  'tenure':'Int64','PhoneService':'string','MultipleLines':'string','InternetService':'string','OnlineSecurity':'string',\",\n",
    "      \"  'OnlineBackup':'string','DeviceProtection':'string','TechSupport':'string','StreamingTV':'string','StreamingMovies':'string',\",\n",
    "      \"  'Contract':'string','PaperlessBilling':'string','PaymentMethod':'string','MonthlyCharges':'float64','TotalCharges':'string','Churn':'string'\",\n",
    "      \"}\",\n",
    "      \"df = pd.read_csv('${1:data/raw/Telco-Customer-Churn.csv}', dtype=dtype_map)\",\n",
    "      \"print(df.shape)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Clean: TotalCharges fix (tenure==0 => 0 else tenure*MonthlyCharges)\": {\n",
    "    \"prefix\": \"py-clean-totalcharges\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Coerce TotalCharges to numeric and impute correctly\",\n",
    "    \"body\": [\n",
    "      \"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].astype(str).str.strip(), errors='coerce')\",\n",
    "      \"mask0 = df['TotalCharges'].isna() & (df['tenure'].fillna(0) == 0)\",\n",
    "      \"df.loc[mask0, 'TotalCharges'] = 0.0\",\n",
    "      \"mask_other = df['TotalCharges'].isna() & df['tenure'].notna() & df['MonthlyCharges'].notna()\",\n",
    "      \"df.loc[mask_other, 'TotalCharges'] = df.loc[mask_other, 'tenure'] * df.loc[mask_other, 'MonthlyCharges']\",\n",
    "      \"df['TotalCharges'] = df['TotalCharges'].astype('float64')\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"EDA: churn class balance plot\": {\n",
    "    \"prefix\": \"py-eda-balance\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Bar chart of churn class balance\",\n",
    "    \"body\": [\n",
    "      \"ax = (df['Churn'].value_counts(normalize=true)*100).plot.bar()\",\n",
    "      \"ax.set_title('Churn class balance (%)'); plt.show()\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Feature: target + tenure buckets\": {\n",
    "    \"prefix\": \"py-feat-target-buckets\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Create y and tenure_bucket features\",\n",
    "    \"body\": [\n",
    "      \"df['y'] = (df['Churn'].str.upper() == 'YES').astype(int)\",\n",
    "      \"bins = [-1,0,6,12,24,48,84,999]\",\n",
    "      \"labels = ['0','1-6','7-12','13-24','25-48','49-84','85+']\",\n",
    "      \"df['tenure_bucket'] = pd.cut(df['tenure'], bins=bins, labels=labels)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Split: train/test\": {\n",
    "    \"prefix\": \"py-split\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Train/test split with drop of ID/label columns\",\n",
    "    \"body\": [\n",
    "      \"from sklearn.model_selection import train_test_split\",\n",
    "      \"target = 'y'\",\n",
    "      \"y = df[target]\",\n",
    "      \"X = df.drop(columns=['customerID','Churn','y'], errors='ignore')\",\n",
    "      \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Preprocess: ColumnTransformer (num scale + OHE cat)\": {\n",
    "    \"prefix\": \"py-pre-coltx\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"StandardScaler for numeric and OneHotEncoder for categories\",\n",
    "    \"body\": [\n",
    "      \"num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\",\n",
    "      \"cat_cols = X_train.select_dtypes(exclude=['number']).columns.tolist()\",\n",
    "      \"from sklearn.preprocessing import OneHotEncoder, StandardScaler\",\n",
    "      \"from sklearn.compose import ColumnTransformer\",\n",
    "      \"pre = ColumnTransformer([\",\n",
    "      \"  ('num', StandardScaler(with_mean=false), num_cols),\",\n",
    "      \"  ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=true), cat_cols)\",\n",
    "      \"])\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Model: Logistic Regression baseline\": {\n",
    "    \"prefix\": \"py-model-logreg\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Baseline classifier pipeline + quick metrics\",\n",
    "    \"body\": [\n",
    "      \"from sklearn.pipeline import Pipeline\",\n",
    "      \"from sklearn.linear_model import LogisticRegression\",\n",
    "      \"from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report, confusion_matrix\",\n",
    "      \"def print_metrics(y_true, prob, thr=0.5):\",\n",
    "      \"  y_pred = (prob >= thr).astype(int)\",\n",
    "      \"  print(f'ROC AUC: {roc_auc_score(y_true, prob):.4f}')\",\n",
    "      \"  print(f'PR AUC : {average_precision_score(y_true, prob):.4f}')\",\n",
    "      \"  print(f'F1     : {f1_score(y_true, y_pred):.4f}')\",\n",
    "      \"  print(confusion_matrix(y_true, y_pred))\",\n",
    "      \"  print(classification_report(y_true, y_pred, digits=3))\",\n",
    "      \"clf = Pipeline([('pre', pre), ('lr', LogisticRegression(max_iter=200, class_weight='balanced'))])\",\n",
    "      \"clf.fit(X_train, y_train)\",\n",
    "      \"prob = clf.predict_proba(X_test)[:,1]\",\n",
    "      \"print_metrics(y_test, prob)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Model: XGBoost pipeline (hist)\": {\n",
    "    \"prefix\": \"py-model-xgb\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"XGBoost classifier with ColumnTransformer preprocessing\",\n",
    "    \"body\": [\n",
    "      \"import xgboost as xgb\",\n",
    "      \"from sklearn.pipeline import Pipeline\",\n",
    "      \"from sklearn.metrics import roc_auc_score\",\n",
    "      \"clf = Pipeline([\",\n",
    "      \"  ('pre', pre),\",\n",
    "      \"  ('xgb', xgb.XGBClassifier(n_estimators=600, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, eval_metric='logloss', tree_method='hist', n_jobs=-1))\",\n",
    "      \"])\",\n",
    "      \"clf.fit(X_train, y_train)\",\n",
    "      \"prob = clf.predict_proba(X_test)[:,1]\",\n",
    "      \"print_metrics(y_test, prob)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Explain: SHAP (tree models)\": {\n",
    "    \"prefix\": \"py-shap-tree\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"SHAP summary for tree-based model within a Pipeline\",\n",
    "    \"body\": [\n",
    "      \"import shap\",\n",
    "      \"shap.initjs()\",\n",
    "      \"booster = clf.named_steps.get('xgb')\",\n",
    "      \"Xs = clf.named_steps['pre'].transform(X_test)\",\n",
    "      \"explainer = shap.TreeExplainer(booster)\",\n",
    "      \"shap_values = explainer.shap_values(Xs)\",\n",
    "      \"shap.summary_plot(shap_values, Xs)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Quality: Pandera mini schema\": {\n",
    "    \"prefix\": \"py-pandera-telco\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Quick Pandera schema to validate key columns\",\n",
    "    \"body\": [\n",
    "      \"import pandera as pa\",\n",
    "      \"from pandera import Column, Check\",\n",
    "      \"TelcoSchema = pa.DataFrameSchema({\",\n",
    "      \"  'customerID': Column(str, nullable=false),\",\n",
    "      \"  'tenure': Column(int, Check.in_range(0,84), nullable=false),\",\n",
    "      \"  'MonthlyCharges': Column(float, Check.in_range(0,200), nullable=false),\",\n",
    "      \"  'TotalCharges': Column(float, nullable=false),\",\n",
    "      \"  'Churn': Column(str, Check.isin(['Yes','No']), nullable=false)\",\n",
    "      \"})\",\n",
    "      \"TelcoSchema.validate(df, lazy=true)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"dbt: model scaffold\": {\n",
    "    \"prefix\": \"dbt-model\",\n",
    "    \"scope\": \"sql\",\n",
    "    \"description\": \"dbt table model scaffold\",\n",
    "    \"body\": [\n",
    "      \"{{ config(materialized='table') }}\",\n",
    "      \"WITH src AS (\",\n",
    "      \"  SELECT * FROM {{ ref('${1:stg_telco_customers}') }}\",\n",
    "      \")\",\n",
    "      \"SELECT * FROM src;\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"SQL: churn by segment\": {\n",
    "    \"prefix\": \"sql-churn-seg\",\n",
    "    \"scope\": \"sql\",\n",
    "    \"description\": \"Segment churn rates by Contract\",\n",
    "    \"body\": [\n",
    "      \"SELECT Contract,\",\n",
    "      \"       ROUND(100 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_pct,\",\n",
    "      \"       COUNT(*) AS n\",\n",
    "      \"FROM ${1:analytics.telco_customers}\",\n",
    "      \"GROUP BY Contract\",\n",
    "      \"ORDER BY churn_pct DESC;\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "># Tiny **Makefile** to lint/test/run your dbt & notebooks?\n",
    "\n",
    "# 2) Makefile\n",
    "\n",
    "Save as **`Makefile`** at the repo root. It‚Äôs opinionated but tidy for AE/ML + dbt.\n",
    "\n",
    "```make\n",
    "# -------- Config --------\n",
    "PY          ?= python3\n",
    "VENV        ?= .venv\n",
    "ACTIVATE    = . $(VENV)/bin/activate\n",
    "REQS        ?= requirements.txt\n",
    "\n",
    "DBT_TARGET  ?= dev\n",
    "DBT_PROJ    ?= .           # path to dbt project (has dbt_project.yml)\n",
    "RAW_CSV     ?= data/raw/Telco-Customer-Churn.csv\n",
    "\n",
    "# -------- Helpers --------\n",
    ".PHONY: help\n",
    "help:\n",
    "\t@echo \"Common targets:\"\n",
    "\t@echo \"  make venv           # create venv and install requirements\"\n",
    "\t@echo \"  make install        # install/upgrade packages in venv\"\n",
    "\t@echo \"  make lint           # ruff lint + format check\"\n",
    "\t@echo \"  make fmt            # ruff format\"\n",
    "\t@echo \"  make test           # run pytest\"\n",
    "\t@echo \"  make nb-run         # run notebooks with papermill (paramizable)\"\n",
    "\t@echo \"  make dbt-deps/run/test/freshness  # dbt workflow\"\n",
    "\t@echo \"  make qa-quick       # quick CSV sanity checks (row count, nulls, dup IDs)\"\n",
    "\t@echo \"  make clean          # remove build artifacts\"\n",
    "\n",
    "# -------- Environment --------\n",
    "$(VENV):\n",
    "\t$(PY) -m venv $(VENV)\n",
    "\t$(ACTIVATE) && python -m pip install --upgrade pip\n",
    "\n",
    ".PHONY: venv\n",
    "venv: $(VENV) install\n",
    "\n",
    ".PHONY: install\n",
    "install:\n",
    "\t@if [ -f \"$(REQS)\" ]; then \\\n",
    "\t\t$(ACTIVATE) && pip install -r $(REQS); \\\n",
    "\telse \\\n",
    "\t\techo \"No requirements.txt found; installing a sane default set...\"; \\\n",
    "\t\t$(ACTIVATE) && pip install pandas numpy scikit-learn imbalanced-learn xgboost catboost shap mlflow matplotlib seaborn pandera great-expectations jupyter papermill ruff pytest dbt-bigquery pandas-gbq google-cloud-bigquery pyarrow; \\\n",
    "\tfi\n",
    "\n",
    "# -------- Lint & Test --------\n",
    ".PHONY: lint\n",
    "lint:\n",
    "\t$(ACTIVATE) && ruff check .\n",
    "\n",
    ".PHONY: fmt\n",
    "fmt:\n",
    "\t$(ACTIVATE) && ruff format .\n",
    "\n",
    ".PHONY: test\n",
    "test:\n",
    "\t$(ACTIVATE) && pytest -q\n",
    "\n",
    "# -------- Notebooks (papermill) --------\n",
    "# Usage: make nb-run NB=notebooks/01_eda.ipynb OUT=reports/01_eda.out.ipynb\n",
    "NB ?= notebooks/01_eda.ipynb\n",
    "OUT ?= reports/01_eda.out.ipynb\n",
    ".PHONY: nb-run\n",
    "nb-run:\n",
    "\t$(ACTIVATE) && papermill $(NB) $(OUT)\n",
    "\n",
    "# -------- Quick CSV QA (no dbt) --------\n",
    ".PHONY: qa-quick\n",
    "qa-quick:\n",
    "\t@echo \"QA: $(RAW_CSV)\"\n",
    "\t$(ACTIVATE) && python - <<'PY'\n",
    "import pandas as pd, sys\n",
    "path = \"$(RAW_CSV)\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Nulls (top 10):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "if \"customerID\" in df.columns:\n",
    "    dup = (df[\"customerID\"].value_counts() > 1).sum()\n",
    "    print(\"Duplicate customerID:\", dup)\n",
    "PY\n",
    "\n",
    "# -------- dbt workflow --------\n",
    ".PHONY: dbt-deps\n",
    "dbt-deps:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt deps\n",
    "\n",
    ".PHONY: dbt-run\n",
    "dbt-run:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt run --target $(DBT_TARGET)\n",
    "\n",
    ".PHONY: dbt-test\n",
    "dbt-test:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt test --target $(DBT_TARGET)\n",
    "\n",
    ".PHONY: dbt-freshness\n",
    "dbt-freshness:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt source freshness --target $(DBT_TARGET)\n",
    "\n",
    "# -------- Clean --------\n",
    ".PHONY: clean\n",
    "clean:\n",
    "\trm -rf $(VENV) .pytest_cache .ruff_cache **/__pycache__\n",
    "\tfind . -name \"*.ipynb_checkpoints\" -type d -prune -exec rm -rf {} +\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to wire this up fast\n",
    "\n",
    "1. **Put your extension + snippets in place**\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.snipples\n",
    "# (Paste your large ~/.snipples/snippets.yaml we created earlier)\n",
    "# Put snipples.py somewhere on PYTHONPATH, or `pip install -e .` if you package it.\n",
    "```\n",
    "\n",
    "In notebooks:\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "# then type:  # snipples model.xgboost_cv   (Tab to browse)\n",
    "```\n",
    "\n",
    "2. **Add the workspace snippets**\n",
    "\n",
    "* Create `.vscode/snippets.code-snippets` and paste the JSON above.\n",
    "\n",
    "3. **Makefile**\n",
    "\n",
    "```bash\n",
    "make venv\n",
    "make qa-quick\n",
    "make dbt-deps dbt-run dbt-test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "># **generate a tiny `requirements.txt` and a `tasks.json` (Windsurf/VS Code tasks that wrap `make` targets so you can ‚åò‚áßB to run ‚ÄúDBT: test‚Äù)**\n",
    "\n",
    "\n",
    "\n",
    "># CLAUDE SUGGESTION: CODE LIBRARY\n",
    "\n",
    "Great question! Code organization and storage strategy is crucial for professional development and portfolio presentation. Here are the most appropriate approaches for different aspects of your projects:\n",
    "\n",
    "## üóÇÔ∏è **Repository Structure & Organization**\n",
    "\n",
    "### **Primary Code Storage: GitHub**\n",
    "```\n",
    "your-username/\n",
    "‚îú‚îÄ‚îÄ telco-churn-analysis-level-0/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ README.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ telco.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ methodology.md\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ findings.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tests/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_analysis.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ figures/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ reports/\n",
    "```\n",
    "\n",
    "### **Multi-Level Portfolio Structure**\n",
    "```\n",
    "data-analytics-journey/\n",
    "‚îú‚îÄ‚îÄ README.md (Portfolio overview)\n",
    "‚îú‚îÄ‚îÄ level-00-bare-bones/\n",
    "‚îú‚îÄ‚îÄ level-01-structured-eda/\n",
    "‚îú‚îÄ‚îÄ level-02-data-cleaning/\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ level-10-enterprise/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ learning-progression.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ technologies-mastered.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ business-impact.md\n",
    "‚îî‚îÄ‚îÄ portfolio-assets/\n",
    "    ‚îú‚îÄ‚îÄ presentations/\n",
    "    ‚îú‚îÄ‚îÄ publications/\n",
    "    ‚îî‚îÄ‚îÄ certifications/\n",
    "```\n",
    "\n",
    "## üíª **Code Workflow Storage Options**\n",
    "\n",
    "### **1. GitHub (Primary - Essential)**\n",
    "**Best for:**\n",
    "- Version control and collaboration\n",
    "- Portfolio presentation to employers\n",
    "- Open source contributions\n",
    "- CI/CD integration\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "‚îú‚îÄ‚îÄ .github/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ workflows/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ci.yml\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ISSUE_TEMPLATE/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îî‚îÄ‚îÄ examples/\n",
    "```\n",
    "\n",
    "### **2. Jupyter Notebooks (Development)**\n",
    "**Best for:**\n",
    "- Exploratory analysis documentation\n",
    "- Step-by-step methodology explanation\n",
    "- Interactive presentations\n",
    "- Educational content\n",
    "\n",
    "**Organization:**\n",
    "```\n",
    "notebooks/\n",
    "‚îú‚îÄ‚îÄ 01-data-exploration.ipynb\n",
    "‚îú‚îÄ‚îÄ 02-data-cleaning.ipynb\n",
    "‚îú‚îÄ‚îÄ 03-feature-engineering.ipynb\n",
    "‚îú‚îÄ‚îÄ 04-modeling.ipynb\n",
    "‚îî‚îÄ‚îÄ 05-results-analysis.ipynb\n",
    "```\n",
    "\n",
    "### **3. Professional Platforms**\n",
    "\n",
    "#### **Kaggle Notebooks**\n",
    "```python\n",
    "# Kaggle-optimized structure\n",
    "\"\"\"\n",
    "TELCO CUSTOMER CHURN - LEVEL 0 ANALYSIS\n",
    "=======================================\n",
    "Author: Your Name\n",
    "Created: Date\n",
    "Updated: Date\n",
    "\n",
    "Table of Contents:\n",
    "1. Introduction & Business Problem\n",
    "2. Data Loading & Initial Exploration  \n",
    "3. Data Quality Assessment\n",
    "4. Key Insights Discovery\n",
    "5. Business Recommendations\n",
    "6. Technical Implementation\n",
    "7. Next Steps & Learning Journey\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../input/telco-customer-churn/telco.csv')\n",
    "```\n",
    "\n",
    "#### **Google Colab (Shareable)**\n",
    "```python\n",
    "# Mount Google Drive for data persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# Professional notebook structure with clear sections\n",
    "```\n",
    "\n",
    "## üèóÔ∏è **Advanced Code Organization (Level 5+)**\n",
    "\n",
    "### **Modular Python Package Structure**\n",
    "```\n",
    "telco_churn_analysis/\n",
    "‚îú‚îÄ‚îÄ setup.py\n",
    "‚îú‚îÄ‚îÄ pyproject.toml\n",
    "‚îú‚îÄ‚îÄ telco_churn/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engineering.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selection.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ visualization/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plots.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ helpers.py\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îú‚îÄ‚îÄ examples/\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "```\n",
    "\n",
    "### **Configuration Management**\n",
    "```python\n",
    "# config.py\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR: Path = Path(\"data\")\n",
    "    RAW_DATA_FILE: str = \"telco.csv\"\n",
    "    PROCESSED_DATA_FILE: str = \"telco_processed.csv\"\n",
    "    \n",
    "    # Model parameters\n",
    "    TEST_SIZE: float = 0.2\n",
    "    RANDOM_STATE: int = 42\n",
    "    \n",
    "    # Output paths\n",
    "    RESULTS_DIR: Path = Path(\"results\")\n",
    "    FIGURES_DIR: Path = Path(\"results/figures\")\n",
    "    REPORTS_DIR: Path = Path(\"results/reports\")\n",
    "```\n",
    "\n",
    "## üìä **Documentation & Workflow Storage**\n",
    "\n",
    "### **README.md Template**\n",
    "```markdown\n",
    "# Level X: Telco Customer Churn Analysis\n",
    "\n",
    "## Quick Start\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python src/analysis.py\n",
    "```\n",
    "\n",
    "## Project Structure\n",
    "- `src/`: Core analysis code\n",
    "- `notebooks/`: Jupyter exploration\n",
    "- `data/`: Dataset files  \n",
    "- `results/`: Output artifacts\n",
    "- `tests/`: Unit tests\n",
    "- `docs/`: Documentation\n",
    "\n",
    "## Key Findings\n",
    "- Finding 1: Contract type drives churn\n",
    "- Finding 2: Payment method indicates engagement  \n",
    "- Finding 3: Premium services show paradox\n",
    "\n",
    "## Business Impact\n",
    "- $2.4M annual revenue at risk identified\n",
    "- 3 strategic initiatives recommended\n",
    "- Clear ROI projections provided\n",
    "\n",
    "## Technologies Used\n",
    "- Python, Pandas, NumPy\n",
    "- Matplotlib, Seaborn\n",
    "- Scikit-learn (for advanced levels)\n",
    "\n",
    "## Next Steps\n",
    "See Level X+1 for progression...\n",
    "```\n",
    "\n",
    "### **Code Documentation Standards**\n",
    "```python\n",
    "def analyze_churn_by_contract(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Analyze churn rates by contract type.\n",
    "    \n",
    "    This function calculates churn rates for each contract type,\n",
    "    revealing the primary business insight of this analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Customer dataset with 'Contract' and 'Churn' columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Churn rates by contract type, sorted descending\n",
    "        \n",
    "    Example:\n",
    "        >>> churn_rates = analyze_churn_by_contract(df)\n",
    "        >>> print(churn_rates)\n",
    "        Month-to-month    0.427\n",
    "        One year          0.113  \n",
    "        Two year          0.028\n",
    "    \n",
    "    Business Impact:\n",
    "        This analysis reveals that month-to-month customers churn at 15x \n",
    "        the rate of two-year contract customers, representing the single \n",
    "        most actionable insight for retention strategy.\n",
    "    \"\"\"\n",
    "    return df.groupby('Contract')['Churn'].apply(\n",
    "        lambda x: (x == 'Yes').mean()\n",
    "    ).sort_values(ascending=False)\n",
    "```\n",
    "\n",
    "## üîß **Workflow Management Tools**\n",
    "\n",
    "### **For Individual Projects (Levels 0-3)**\n",
    "- **Primary**: GitHub repository per level\n",
    "- **Development**: Jupyter notebooks + VS Code\n",
    "- **Documentation**: Markdown files + README\n",
    "- **Sharing**: Kaggle notebooks for community\n",
    "\n",
    "### **For Advanced Projects (Levels 4-7)**\n",
    "- **Code**: Modular Python packages\n",
    "- **Environment**: Poetry/Conda for dependency management\n",
    "- **Testing**: pytest framework\n",
    "- **Documentation**: Sphinx for API docs\n",
    "- **Workflow**: GitHub Actions for CI/CD\n",
    "\n",
    "### **For Enterprise Projects (Levels 8-10)**\n",
    "- **Infrastructure**: Infrastructure as Code (Terraform)\n",
    "- **Containerization**: Docker + Kubernetes\n",
    "- **Orchestration**: Apache Airflow\n",
    "- **Monitoring**: Logging and metrics collection\n",
    "- **Deployment**: Blue-green deployment strategies\n",
    "\n",
    "## üí° **Best Practices by Level**\n",
    "\n",
    "### **Level 0-2: Foundation**\n",
    "```python\n",
    "# Simple, clear, well-commented code\n",
    "import pandas as pd\n",
    "\n",
    "# Load data with error handling\n",
    "try:\n",
    "    df = pd.read_csv('data/telco.csv')\n",
    "    print(f\"‚úÖ Data loaded: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found\")\n",
    "    exit(1)\n",
    "\n",
    "# Clear business-focused analysis\n",
    "contract_churn = df.groupby('Contract')['Churn'].mean()\n",
    "print(\"üìä Churn by Contract Type:\")\n",
    "print(contract_churn.sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "### **Level 5-7: Professional**\n",
    "```python\n",
    "# Type hints, docstrings, error handling\n",
    "from typing import Tuple, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ChurnAnalyzer:\n",
    "    \"\"\"Professional customer churn analysis class.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.df = None\n",
    "        \n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and validate customer data.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\"Data loaded successfully: {self.df.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {e}\")\n",
    "            raise\n",
    "```\n",
    "\n",
    "### **Level 8-10: Enterprise**\n",
    "```python\n",
    "# Production-ready with monitoring\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import structlog\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "@dataclass\n",
    "class PredictionMetrics:\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    latency_ms: float\n",
    "\n",
    "class ChurnPredictor(ABC):\n",
    "    \"\"\"Abstract base class for churn prediction models.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def predict(self, customer_data: Dict) -> Tuple[float, PredictionMetrics]:\n",
    "        \"\"\"Predict churn probability with performance metrics.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "## üìà **Storage Strategy Recommendations**\n",
    "\n",
    "### **For Learning Journey Documentation**\n",
    "1. **GitHub**: Primary portfolio repository\n",
    "2. **Kaggle**: Public notebooks for community engagement  \n",
    "3. **Personal Blog**: Medium/LinkedIn articles\n",
    "4. **Cloud Storage**: AWS S3/Google Cloud for large datasets\n",
    "5. **Local Development**: VS Code + Jupyter for daily work\n",
    "\n",
    "### **For Professional Presentation**\n",
    "1. **Clean GitHub repos** with professional README files\n",
    "2. **Comprehensive documentation** explaining methodology\n",
    "3. **Business-focused presentations** with clear ROI\n",
    "4. **Code quality standards** with proper testing\n",
    "5. **Progression documentation** showing skill development\n",
    "\n",
    "The key is to maintain **consistency across platforms** while optimizing each for its specific audience and purpose. Your Level 0 should be simple and accessible, while your Level 10 should demonstrate enterprise-scale thinking and implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33590611",
   "metadata": {},
   "source": [
    "You're building a **personal code library** - this is exactly what professional data scientists do. Here are several effective approaches:## üéØ **My Recommendation: Start Simple with Markdown Files**\n",
    "\n",
    "Create a folder structure like this:\n",
    "```\n",
    "my_code_library/\n",
    "‚îú‚îÄ‚îÄ missing_values.md\n",
    "‚îú‚îÄ‚îÄ data_types.md\n",
    "‚îú‚îÄ‚îÄ anomaly_detection.md\n",
    "‚îú‚îÄ‚îÄ data_cleaning.md\n",
    "‚îî‚îÄ‚îÄ quick_exploration.md\n",
    "```\n",
    "\n",
    "## üìù **Why Markdown Files Work Best:**\n",
    "\n",
    "1. **Easy to search** - Ctrl+F to find what you need\n",
    "2. **Copy-paste ready** - Code blocks ready to use  \n",
    "3. **Context included** - Notes on when/why to use each method\n",
    "4. **GitHub friendly** - Looks professional in your portfolio\n",
    "\n",
    "## üí° **Example: `missing_values.md`**\n",
    "\n",
    "```markdown\n",
    "# Missing Values Toolkit\n",
    "\n",
    "## Quick Count (Most Used)\n",
    "```python\n",
    "df.isna().sum().sum()  # 18 characters, fastest\n",
    "```\n",
    "\n",
    "## Show Details When Needed  \n",
    "```python\n",
    "df.isnull().sum()[df.isnull().sum() > 0]  # Only non-zero columns\n",
    "```\n",
    "\n",
    "## When to Use Each:\n",
    "- **Quick count**: Initial assessment, just need total\n",
    "- **Detailed view**: When you find missing values and need to know where\n",
    "```\n",
    "\n",
    "## üöÄ **Start Your Library Today:**\n",
    "\n",
    "1. **Create the folder structure**\n",
    "2. **Start with one file** - `missing_values.md`\n",
    "3. **Add your variations** from today's session\n",
    "4. **Include context** - when to use each method\n",
    "5. **Build as you learn** - add new categories in future levels\n",
    "\n",
    "## üìà **Benefits for Your Goals:**\n",
    "\n",
    "- **Goal #2**: Documents your learning progression\n",
    "- **Goal #4**: Creates your personal reference library\n",
    "- **Portfolio value**: Shows systematic thinking to hiring managers\n",
    "\n",
    "This approach scales perfectly - start simple, add complexity as you grow!\n",
    "\n",
    "Here are several ways to check for empty values across the entire dataset:\n",
    "\n",
    "## **Quick One-Liners for Different Types of Empty:**\n",
    "\n",
    "### **Check NaN/Null values:**\n",
    "```python\n",
    "df.isnull().sum().sum()  # Total count\n",
    "df.isnull().sum()        # By column\n",
    "```\n",
    "\n",
    "### **Check empty strings (''):**\n",
    "```python\n",
    "(df == '').sum().sum()   # Total count\n",
    "(df == '').sum()         # By column\n",
    "```\n",
    "\n",
    "### **Check empty spaces (' '):**\n",
    "```python\n",
    "(df == ' ').sum().sum()  # Total count\n",
    "(df == ' ').sum()        # By column\n",
    "```\n",
    "\n",
    "### **Ultimate one-liner for all types:**\n",
    "```python\n",
    "print(f\"NaN: {df.isnull().sum().sum()}, Empty: {(df == '').sum().sum()}, Spaces: {(df == ' ').sum().sum()}\")\n",
    "```\n",
    "\n",
    "## **Most Comprehensive Check:**\n",
    "\n",
    "```python\n",
    "# Check every column for all types of empty values\n",
    "for col in df.columns:\n",
    "    nan_count = df[col].isnull().sum()\n",
    "    empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "    space_count = (df[col] == ' ').sum() if df[col].dtype == 'object' else 0\n",
    "    \n",
    "    total = nan_count + empty_count + space_count\n",
    "    if total > 0:\n",
    "        print(f\"{col}: {total} empty values\")\n",
    "```\n",
    "\n",
    "## **Show Actual Problem Rows:**\n",
    "\n",
    "```python\n",
    "# Find and display rows with any empty values\n",
    "problem_rows = df[df.isnull().any(axis=1) | (df == '').any(axis=1) | (df == ' ').any(axis=1)]\n",
    "print(f\"Rows with empty values: {len(problem_rows)}\")\n",
    "if len(problem_rows) > 0:\n",
    "    print(problem_rows[['customerID']].head())  # Show first few problem customer IDs\n",
    "```\n",
    "\n",
    "**After your TotalCharges fix, you should see:**\n",
    "- **NaN: 0**\n",
    "- **Empty strings: 0** \n",
    "- **Spaces: 0**\n",
    "\n",
    "If you see any non-zero counts, that tells you where you still have empty value issues!\n",
    "\n",
    "---\n",
    "\n",
    "># **#2: Keep a snippet library handy across projects/workspaces**\n",
    "\n",
    "    Here are three ‚Äúprofessional and portable‚Äù ways to keep a snippet library handy across projects/workspaces on your Mac‚Äîordered from most editor-native to most universal. Pick one or combine B+C for a killer setup.\n",
    "\n",
    "# A) VS Code user snippets + built-in Sync (simple, editor-native)\n",
    "\n",
    "**Best if you primarily use VS Code.**\n",
    "\n",
    "1. Open Command Palette ‚Üí ‚ÄúPreferences: Configure User Snippets‚Äù.\n",
    "2. Create language or global snippets. Example (`python.json`):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Py docstring\": {\n",
    "    \"prefix\": \"pydoc\",\n",
    "    \"body\": [\n",
    "      \"\\\"\\\"\\\"${1:Summary}\",\n",
    "      \"\",\n",
    "      \"Args:\",\n",
    "      \"    ${2:param}: ${3:desc}\",\n",
    "      \"\",\n",
    "      \"Returns:\",\n",
    "      \"    ${4:type}: ${5:desc}\",\n",
    "      \"\\\"\\\"\\\"\"\n",
    "    ],\n",
    "    \"description\": \"Docstring scaffold\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Turn on **Settings Sync** (Account icon ‚Üí Turn On Sync). Your snippets follow you on any machine you sign into VS Code with.\n",
    "\n",
    "**Pro tip (shared across repos):** keep your snippet JSON files in a Git repo and **symlink** them into VS Code‚Äôs snippets folder so you can version-control them:\n",
    "\n",
    "```bash\n",
    "# Paths (macOS)\n",
    "VS_SNIPS=\"$HOME/Library/Application Support/Code/User/snippets\"\n",
    "mkdir -p ~/snippets/vscode\n",
    "ln -s ~/snippets/vscode/python.json \"$VS_SNIPS/python.json\"\n",
    "```\n",
    "\n",
    "# B) Git + Markdown snippets + fzf CLI (portable, editor-agnostic)\n",
    "\n",
    "**Best if you hop between tools (VS Code, Windsurf, terminals, notebooks).**\n",
    "\n",
    "1. Make a repo:\n",
    "\n",
    "```\n",
    "~/snippets/\n",
    "  python/\n",
    "  sql/\n",
    "  dbt/\n",
    "  shell/\n",
    "  README.md\n",
    "```\n",
    "\n",
    "Each snippet is a small `.md` with a clear title and a fenced code block.\n",
    "\n",
    "2. Install tools (free):\n",
    "\n",
    "```bash\n",
    "brew install fzf ripgrep\n",
    "```\n",
    "\n",
    "3. Add a tiny **`snip`** helper to your `~/.zshrc`:\n",
    "\n",
    "````zsh\n",
    "snip() {\n",
    "  local file\n",
    "  file=$(rg -l --hidden --glob \"!*.git/*\" \"${*:-.}\" \"$HOME/snippets\" | fzf --prompt=\"Snip> \") || return\n",
    "  # copy the code block contents to clipboard\n",
    "  awk '/^```/{f=!f; next} f' \"$file\" | pbcopy\n",
    "  echo \"üìã Copied from: $file\"\n",
    "}\n",
    "````\n",
    "\n",
    "Usage:\n",
    "\n",
    "````bash\n",
    "snip pandas groupby\n",
    "# copies the code inside the first ``` block from the chosen file\n",
    "# ‚åò+V (paste) in any editor/terminal\n",
    "````\n",
    "\n",
    "**Why this rocks:** works everywhere (terminal, notebooks, different editors), versioned with Git, searchable via fuzzy find. You can keep private snippets in a private repo.\n",
    "\n",
    "# C) Text expander (Espanso) for instant boilerplate (cross-app)\n",
    "\n",
    "**Great for short, frequently typed patterns.** Free & open-source.\n",
    "\n",
    "```bash\n",
    "brew install espanso\n",
    "espanso start\n",
    "espanso edit\n",
    "```\n",
    "\n",
    "Add triggers in `default.yml`:\n",
    "\n",
    "```yaml\n",
    "matches:\n",
    "  - trigger: \":sqlsel\"\n",
    "    replace: |\n",
    "      SELECT ${1:*}\n",
    "      FROM ${2:table}\n",
    "      WHERE ${3:cond};\n",
    "    vars:\n",
    "      - name: cursor\n",
    "        type: cursor\n",
    "  - trigger: \":dbtmodel\"\n",
    "    replace: |\n",
    "      {{ config(materialized='table') }}\n",
    "      WITH src AS (\n",
    "          SELECT * FROM {{ ref('${1:stg_table}') }}\n",
    "      )\n",
    "      SELECT * FROM src;\n",
    "    propagate_case: true\n",
    "filters:\n",
    "  apps:\n",
    "    only: [\"com.microsoft.VSCode\"]  # limit expansions to VS Code\n",
    "```\n",
    "\n",
    "Now typing `:sqlsel` in VS Code expands into a SQL skeleton with tabstops‚Äîwithout touching your editor‚Äôs snippet system.\n",
    "\n",
    "---\n",
    "\n",
    "## What I recommend for you\n",
    "\n",
    "Given you‚Äôre on macOS + zsh and bounce across projects:\n",
    "\n",
    "* **Primary:** VS Code user snippets + Sync (A) for language-aware tab-trigger snippets.\n",
    "* **Universal:** Git + `snip` (B) so you can search/paste any snippet in *any* workspace, terminal, or notebook.\n",
    "* **Boilerplate:** Espanso (C) for lightning-fast expansions you want everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## Starter pack you can paste in today\n",
    "\n",
    "**Repo skeleton**\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/snippets/{python,sql,dbt,shell}\n",
    "git init ~/snippets\n",
    "```\n",
    "\n",
    "**Example files**\n",
    "`~/snippets/sql/select-where.md`\n",
    "\n",
    "````md\n",
    "# Select with WHERE & limit\n",
    "```sql\n",
    "SELECT ${1:cols}\n",
    "FROM ${2:schema.table}\n",
    "WHERE ${3:cond}\n",
    "LIMIT ${4:100};\n",
    "````\n",
    "\n",
    "````\n",
    "\n",
    "`~/snippets/python/pandas-null-check.md`\n",
    "```md\n",
    "# Null audit (top offenders)\n",
    "```python\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(na[na>0].head(10))\n",
    "````\n",
    "\n",
    "````\n",
    "\n",
    "**VS Code snippet (dbt model)**\n",
    "`~/snippets/vscode/sql.json`\n",
    "```json\n",
    "{\n",
    "  \"dbt model\": {\n",
    "    \"prefix\": \"dbtmodel\",\n",
    "    \"body\": [\n",
    "      \"{{ config(materialized='table') }}\",\n",
    "      \"WITH src AS (\",\n",
    "      \"  SELECT * FROM {{ ref('${1:stg_table}') }}\",\n",
    "      \")\",\n",
    "      \"SELECT * FROM src;\"\n",
    "    ],\n",
    "    \"description\": \"dbt model scaffold\"\n",
    "  }\n",
    "}\n",
    "````\n",
    "\n",
    "Then symlink it as shown above.\n",
    "\n",
    "---\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "* **Secrets:** never store tokens/keys in snippets; use placeholders like `${SECRET_NAME}`.\n",
    "* **Licensing:** if you paste from blogs/StackOverflow, keep a note/link at the top of the snippet file.\n",
    "* **Naming:** use a consistent prefix (e.g., `:sql*`, `:py*`) for Espanso triggers to avoid accidental expansion.\n",
    "\n",
    ">If you want, tell me your top 10 snippet needs (Python/pandas, SQL/dbt, shell, GCP/AWS), and I‚Äôll generate the files to drop into `~/snippets/` plus the matching VS Code/Espanso entries.\n",
    "\n",
    "---\n",
    "\n",
    "># **#3: Windsurf with VS Code‚Äìstyle snippets**\n",
    "\n",
    "Yep‚ÄîWindsurf plays nicely with **VS Code‚Äìstyle snippets**, plus you can layer on a git repo and a text expander for portability. Here‚Äôs a clean setup that works across projects/workspaces.\n",
    "\n",
    "# 1) Built-in (VS Code‚Äìstyle) user & workspace snippets\n",
    "\n",
    "* Open the **Command Palette ‚Üí ‚ÄúPreferences: Configure User Snippets.‚Äù** Create a language file (e.g., `python.json`) or a **global** snippets file. Snippet format is the same as VS Code‚Äôs JSON (prefix/body/description). ([Arsturn][1], [Visual Studio Code][2])\n",
    "* Bringing snippets from VS Code? On first run Windsurf lets you **import settings from VS Code/Cursor**, which includes snippets. ([DEV Community][3])\n",
    "* For repo-scoped snippets, add a workspace file (e.g., `.vscode/snippets.code-snippets`) so they travel with the project in Git. (Same mechanism as VS Code.) ([Stack Overflow][4])\n",
    "\n",
    "Example snippet body (works in Windsurf since it‚Äôs VS Code syntax):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Docstring scaffold\": {\n",
    "    \"prefix\": \"pydoc\",\n",
    "    \"body\": [\n",
    "      \"\\\"\\\"\\\"${1:Summary}\",\n",
    "      \"\",\n",
    "      \"Args:\",\n",
    "      \"    ${2:param}: ${3:desc}\",\n",
    "      \"\",\n",
    "      \"Returns:\",\n",
    "      \"    ${4:type}: ${5:desc}\",\n",
    "      \"\\\"\\\"\\\"\"\n",
    "    ],\n",
    "    \"description\": \"Python docstring\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "# 2) Manage snippets with extensions (Open VSX)\n",
    "\n",
    "Windsurf uses the **Open VSX** marketplace, so install snippet helpers from there in the Extensions view:\n",
    "\n",
    "* **Snippets Manager** ‚Äì simple UI to create/edit snippets. ([open-vsx.org][5])\n",
    "* **Snippets Viewer** ‚Äì browse built-in, extension, and user snippets in a tree view. ([open-vsx.org][6])\n",
    "* **Snippets Ranger** ‚Äì create/select snippet files quickly. ([open-vsx.org][7])\n",
    "  (If you‚Äôre used to VS Code‚Äôs marketplace, note the store difference‚ÄîWindsurf surfaces Open VSX by default.) ([Reddit][8])\n",
    "\n",
    "# 3) Keep them portable with Git (+ optional symlink)\n",
    "\n",
    "* Put all snippet JSON (and markdown examples) in `~/snippets/` under Git.\n",
    "* In Windsurf, open one of your snippet files and copy its on-disk path; then **symlink** your repo files to that location so edits are versioned automatically (works on Mac/Linux).\n",
    "  This avoids guessing paths and keeps snippets consistent across workspaces.\n",
    "\n",
    "# 4) Universal quick-paste option (works in any app)\n",
    "\n",
    "If you want snippets outside the editor too:\n",
    "\n",
    "* **Espanso** (free text expander): type `:sqlsel` ‚Üí expands to a SQL skeleton anywhere.\n",
    "* Or a tiny **fzf** picker in your terminal that copies code blocks from `~/snippets/*.md` to clipboard (great inside Windsurf‚Äôs integrated terminal).\n",
    "\n",
    "# 5) Bonus: make snippets ‚Äúsearchable‚Äù by the AI\n",
    "\n",
    "If you keep a **Google Doc** of your favorite patterns, Windsurf‚Äôs **Knowledge Base / docs context** can ingest docs for retrieval in chat. It‚Äôs not an inserter like snippets, but handy for ‚Äúremind me of the dbt model scaffold‚Äù prompts. ([Windsurf Docs][9])\n",
    "\n",
    "---\n",
    "\n",
    "## A setup I‚Äôd recommend for you\n",
    "\n",
    "1. **User & workspace snippets** for tab-trigger speed. ([Arsturn][1], [Visual Studio Code][2])\n",
    "2. **Git repo** at `~/snippets/` + **symlink** to Windsurf‚Äôs snippet files so changes sync across workspaces.\n",
    "3. Install **Snippets Manager** (UI), plus **Snippets Viewer** (browse). ([open-vsx.org][5])\n",
    "4. Add **Espanso** for cross-app boilerplate (optional).\n",
    "5. (Nice-to-have) A **Google Doc** of patterns connected to Windsurf‚Äôs knowledge base for quick AI recall. ([Windsurf Docs][9])\n",
    "\n",
    "If you want, tell me your top 10 snippets (Python/pandas, SQL/dbt, shell/GCP) and I‚Äôll generate the JSON files + a starter `~/snippets/` repo structure to drop in.\n",
    "\n",
    "[1]: https://www.arsturn.com/blog/creating-custom-code-snippets-in-windsurf-for-reusability \"Mastering Code Snippets in Windsurf: Reusability Techniques\"\n",
    "[2]: https://code.visualstudio.com/docs/editing/userdefinedsnippets?utm_source=chatgpt.com \"Snippets in Visual Studio Code\"\n",
    "[3]: https://dev.to/proflead/this-ai-ide-can-code-for-you-windsurf-ai-full-tutorial-4p94 \"This AI IDE Can Code For You ‚Äì Windsurf AI Full Tutorial - DEV Community\"\n",
    "[4]: https://stackoverflow.com/questions/29995863/how-to-add-custom-code-snippets-in-vscode?utm_source=chatgpt.com \"How to add custom code snippets in VSCode?\"\n",
    "[5]: https://open-vsx.org/extension/zjffun/snippetsmanager?utm_source=chatgpt.com \"Snippets Manager\"\n",
    "[6]: https://open-vsx.org/extension/RandomFractalsInc/snippets-viewer?utm_source=chatgpt.com \"Snippets Viewer\"\n",
    "[7]: https://open-vsx.org/extension/robole/snippets-ranger?utm_source=chatgpt.com \"Snippets Ranger\"\n",
    "[8]: https://www.reddit.com/r/RooCode/comments/1i6ns94/new_users_discovery_of_roocode_on_vscode/?utm_source=chatgpt.com \"New users' discovery of RooCode on vscode marketplace ...\"\n",
    "[9]: https://docs.windsurf.com/context-awareness/overview?utm_source=chatgpt.com \"Overview - Windsurf Docs\"\n",
    "\n",
    "---\n",
    "\n",
    "># **#4: create a custome library that gives me suggestions in a jupyter cell or python file when I type # to call the library then call a specific snippet by name**\n",
    "\n",
    "Yes‚Äîyou can make a tiny IPython/Jupyter extension that:\n",
    "\n",
    "* lets you write a **comment directive** like `# snip <name>` which gets **expanded into code** before execution, and\n",
    "* gives you **Tab-completion suggestions** for snippet names after you type `# snip ` in a Jupyter cell (works in JupyterLab/Notebook and VS Code‚Äôs Jupyter).\n",
    "\n",
    "Below is a minimal, drop-in ‚Äúsnippet library‚Äù you can keep in `~/.snippets/`. It uses a YAML file for your snippets, a transformer to expand `# snip ...` lines, and a custom completer to suggest names.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Snippet storage (YAML)\n",
    "\n",
    "`~/.snippets/snippets.yaml`\n",
    "\n",
    "```yaml\n",
    "pandas.null_audit: |\n",
    "  na = df.isna().sum().sort_values(ascending=False)\n",
    "  print(na[na>0].head(10))\n",
    "sql.select_where: |\n",
    "  SELECT ${1:cols}\n",
    "  FROM ${2:schema.table}\n",
    "  WHERE ${3:cond}\n",
    "  LIMIT ${4:100};\n",
    "dbt.model_scaffold: |\n",
    "  {{ config(materialized='table') }}\n",
    "  WITH src AS (\n",
    "    SELECT * FROM {{ ref('${1:stg_table}') }}\n",
    "  )\n",
    "  SELECT * FROM src;\n",
    "```\n",
    "\n",
    "> You can keep multiple YAML files under `~/.snippets/`‚Äîthe loader will merge them.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) The extension (`snipline.py`)\n",
    "\n",
    "Put this file anywhere on your PYTHONPATH (e.g., `~/snipline/snipline.py`) and `pip install pyyaml` if you don‚Äôt have it.\n",
    "\n",
    "```python\n",
    "# snipline.py\n",
    "from __future__ import annotations\n",
    "import os, re, glob, time, yaml, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from IPython.core.magic import Magics, magics_class, line_magic\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "SNIP_DIR = Path(os.environ.get(\"SNIP_DIR\", \"~/.snippets\")).expanduser()\n",
    "\n",
    "class SnipStore:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.dir = Path(directory)\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._mtimes: Dict[str, float] = {}\n",
    "        self.reload()\n",
    "\n",
    "    def reload(self):\n",
    "        self.cache.clear()\n",
    "        self._mtimes.clear()\n",
    "        for yml in glob.glob(str(self.dir / \"*.y*ml\")):\n",
    "            with open(yml, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f) or {}\n",
    "            for k, v in data.items():\n",
    "                self.cache[str(k)] = str(v)\n",
    "            self._mtimes[yml] = os.path.getmtime(yml)\n",
    "\n",
    "    def maybe_reload(self):\n",
    "        for yml, old in list(self._mtimes.items()):\n",
    "            if os.path.exists(yml) and os.path.getmtime(yml) != old:\n",
    "                self.reload()\n",
    "                break\n",
    "\n",
    "    def names(self, prefix: str = \"\"):\n",
    "        self.maybe_reload()\n",
    "        return sorted([k for k in self.cache if k.startswith(prefix)])\n",
    "\n",
    "    def get(self, name: str) -> Optional[str]:\n",
    "        self.maybe_reload()\n",
    "        return self.cache.get(name)\n",
    "\n",
    "STORE = SnipStore(SNIP_DIR)\n",
    "\n",
    "# --- Input transformer: replace lines like \"# snip <name>\" with the snippet text\n",
    "_SNIP_LINE = re.compile(r\"^[ \\t]*#\\s*snip\\s+([A-Za-z0-9_.\\-/]+)[ \\t]*$\", re.MULTILINE)\n",
    "\n",
    "def _expand_snips(cell: str) -> str:\n",
    "    def repl(m):\n",
    "        name = m.group(1)\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            # Leave a visible marker if missing\n",
    "            return f\"# [snip: '{name}' not found]\"\n",
    "        # Preserve indentation of the directive line\n",
    "        indent = len(m.string[:m.start()].splitlines()[-1]) - len(m.string[:m.start()].splitlines()[-1].lstrip())\n",
    "        ind = \" \" * max(indent, 0)\n",
    "        expanded = textwrap.indent(body.rstrip(\"\\n\"), ind)\n",
    "        return expanded\n",
    "    return _SNIP_LINE.sub(repl, cell)\n",
    "\n",
    "# --- Tab completion for \"# snip \" or \"%snip \" prefixes\n",
    "def _snip_completer(self, event):\n",
    "    # Works for both comments and magic usage\n",
    "    line = getattr(event, \"line\", \"\")\n",
    "    # capture partial name user is typing\n",
    "    m = re.search(r\"(#\\s*snip|%snip)\\s+([A-Za-z0-9_.\\-/]*)$\", line[:getattr(event, \"cursor_position\", len(line))])\n",
    "    if not m:\n",
    "        return []\n",
    "    prefix = m.group(2)\n",
    "    return STORE.names(prefix)\n",
    "\n",
    "@magics_class\n",
    "class SnipMagics(Magics):\n",
    "    @line_magic(\"snip\")\n",
    "    def snip(self, line):\n",
    "        \"\"\"Usage: %snip <name>  -> prints the snippet body\n",
    "        With 'insert' in classic Notebook, it replaces the current cell.\"\"\"\n",
    "        name = line.strip()\n",
    "        if not name:\n",
    "            print(\"\\n\".join(STORE.names()))\n",
    "            return\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            print(f\"[snip: '{name}' not found]\")\n",
    "            return\n",
    "        # Print for copy/paste\n",
    "        print(body)\n",
    "        # Optional: try to replace current cell in classic Notebook UI\n",
    "        try:\n",
    "            js = Javascript(\"\"\"\n",
    "                if (typeof Jupyter !== 'undefined') {\n",
    "                  var cell = Jupyter.notebook.get_selected_cell();\n",
    "                  cell.set_text(atob('%s'));\n",
    "                }\n",
    "            \"\"\" % body.encode(\"utf-8\").hex())\n",
    "            display(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    # Expand \"# snip ...\" lines before execution\n",
    "    ip.input_transformers_post.append(_expand_snips)\n",
    "    # Add completer (works in Jupyter and VS Code's Jupyter)\n",
    "    try:\n",
    "        ip.set_hook('complete_command', _snip_completer, re_key='.*')\n",
    "    except Exception:\n",
    "        # Fallback: older IPython versions\n",
    "        completer = getattr(ip, 'Completer', None)\n",
    "        if completer and hasattr(completer, 'matchers'):\n",
    "            completer.matchers.insert(0, lambda self, text: [])\n",
    "    ip.register_magics(SnipMagics)\n",
    "\n",
    "def unload_ipython_extension(ip):\n",
    "    # Best-effort cleanup; new sessions usually just restart\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Use it in Jupyter/VS Code‚Äôs Jupyter\n",
    "\n",
    "In a cell:\n",
    "\n",
    "```python\n",
    "%load_ext snipline   # run once per session (or auto-load, see below)\n",
    "```\n",
    "\n",
    "Now you can:\n",
    "\n",
    "* Type: `# snip pandas.null_audit` then press **Tab** after `# snip ` to see snippet **name suggestions**.\n",
    "* Run the cell: that line is **replaced** with the snippet‚Äôs code before execution.\n",
    "* Or: `%snip pandas.null_audit` to print/insert the snippet body.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# snip pandas.null_audit\n",
    "```\n",
    "\n",
    "‚Ä¶expands to:\n",
    "\n",
    "```python\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(na[na>0].head(10))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Auto-load every session (optional)\n",
    "\n",
    "Create (or edit) `~/.ipython/profile_default/ipython_config.py`:\n",
    "\n",
    "```python\n",
    "c = get_config()\n",
    "c.InteractiveShellApp.extensions = ['snipline']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes & limits\n",
    "\n",
    "* **Where suggestions work:** in **Jupyter contexts** (classic, Lab, or VS Code‚Äôs Jupyter) because completion comes from the kernel. In a plain `.py` file editor view, completion is handled by the editor‚Äôs language server, so you won‚Äôt get the `# snip` suggestions there. For `.py` editing, pair this with **editor snippets** (VS Code/Windsurf) or a text expander (Espanso).\n",
    "* **Variables/placeholders:** You can embed `${1:placeholder}` in snippets for editor tabstops (useful when you paste into VS Code). The transformer leaves those as-is; your editor‚Äôs snippet engine will handle them if present.\n",
    "* **Versioning:** Keep `~/.snippets/` under Git so you can sync across machines. The loader auto-reloads when YAML files change.\n",
    "\n",
    "If you want, tell me your top 10 snippet names and I‚Äôll generate the YAML ready to drop into `~/.snippets/snippets.yaml`.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    Heck yes‚Äîlet‚Äôs make **snipples** your cross-workspace snippet engine. Below are two files:\n",
    "\n",
    "1. **`snipples.py`** ‚Äì an IPython/Jupyter extension so you can type `# snipples <name>` in a cell and it expands before execution (with tab-complete for names).\n",
    "2. **`~/.snipples/snippets.yaml`** ‚Äì a chunky library covering the Telco Churn project end-to-end (ingest ‚Üí EDA ‚Üí cleaning ‚Üí features ‚Üí ML ‚Üí explainability ‚Üí dbt/SQL ‚Üí ops).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) `snipples.py` (drop anywhere on your PYTHONPATH)\n",
    "\n",
    "```python\n",
    "# snipples.py\n",
    "from __future__ import annotations\n",
    "import os, re, glob, yaml, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from IPython.core.magic import Magics, magics_class, line_magic\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "SNIPS_DIR = Path(os.environ.get(\"SNIPPLES_DIR\", \"~/.snipples\")).expanduser()\n",
    "\n",
    "class SnipStore:\n",
    "    def __init__(self, directory: Path):\n",
    "        self.dir = Path(directory)\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._mtimes: Dict[str, float] = {}\n",
    "        self.reload()\n",
    "\n",
    "    def reload(self):\n",
    "        self.cache.clear()\n",
    "        self._mtimes.clear()\n",
    "        self.dir.mkdir(parents=True, exist_ok=True)\n",
    "        for yml in sorted(self.dir.glob(\"*.y*ml\")):\n",
    "            with open(yml, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f) or {}\n",
    "            for k, v in data.items():\n",
    "                self.cache[str(k)] = str(v)\n",
    "            self._mtimes[str(yml)] = yml.stat().st_mtime\n",
    "\n",
    "    def maybe_reload(self):\n",
    "        for yml, old in list(self._mtimes.items()):\n",
    "            p = Path(yml)\n",
    "            if p.exists() and p.stat().st_mtime != old:\n",
    "                self.reload()\n",
    "                break\n",
    "\n",
    "    def names(self, prefix: str = \"\"):\n",
    "        self.maybe_reload()\n",
    "        return sorted([k for k in self.cache if k.startswith(prefix)])\n",
    "\n",
    "    def get(self, name: str) -> Optional[str]:\n",
    "        self.maybe_reload()\n",
    "        return self.cache.get(name)\n",
    "\n",
    "STORE = SnipStore(SNIPS_DIR)\n",
    "\n",
    "_SNIPLINE = re.compile(r\"^[ \\t]*#\\s*snipples\\s+([A-Za-z0-9_.\\-/]+)[ \\t]*$\", re.MULTILINE)\n",
    "\n",
    "def _expand_snips(cell: str) -> str:\n",
    "    def repl(m):\n",
    "        name = m.group(1)\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            return f\"# [snipples: '{name}' not found]\"\n",
    "        # Preserve indentation level of the directive line\n",
    "        line = cell[:m.start()].splitlines()[-1] if cell[:m.start()].splitlines() else \"\"\n",
    "        indent = len(line) - len(line.lstrip(\" \"))\n",
    "        expanded = textwrap.indent(body.rstrip(\"\\n\"), \" \" * max(indent, 0))\n",
    "        return expanded\n",
    "    return _SNIPLINE.sub(repl, cell)\n",
    "\n",
    "def _snipples_completer(self, event):\n",
    "    line = getattr(event, \"line\", \"\")\n",
    "    cur = getattr(event, \"cursor_position\", len(line))\n",
    "    m = re.search(r\"(#\\s*snipples|%snipples)\\s+([A-Za-z0-9_.\\-/]*)$\", line[:cur])\n",
    "    if not m:\n",
    "        return []\n",
    "    return STORE.names(m.group(2))\n",
    "\n",
    "@magics_class\n",
    "class SnipplesMagics(Magics):\n",
    "    @line_magic(\"snipples\")\n",
    "    def snipples(self, line):\n",
    "        \"\"\"%snipples <name>  -> prints/optionally inserts the snippet body\"\"\"\n",
    "        name = line.strip()\n",
    "        if not name:\n",
    "            print(\"\\n\".join(STORE.names()))\n",
    "            return\n",
    "        body = STORE.get(name)\n",
    "        if body is None:\n",
    "            print(f\"[snipples: '{name}' not found]\")\n",
    "            return\n",
    "        print(body)\n",
    "        try:\n",
    "            js = Javascript(\"\"\"\n",
    "                if (typeof Jupyter !== 'undefined') {\n",
    "                  var cell = Jupyter.notebook.get_selected_cell();\n",
    "                  cell.set_text(atob('%s'));\n",
    "                }\n",
    "            \"\"\" % body.encode(\"utf-8\").hex())\n",
    "            display(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    ip.input_transformers_post.append(_expand_snips)\n",
    "    try:\n",
    "        ip.set_hook('complete_command', _snipples_completer, re_key='.*')\n",
    "    except Exception:\n",
    "        pass\n",
    "    ip.register_magics(SnipplesMagics)\n",
    "\n",
    "def unload_ipython_extension(ip):\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Use:** in a Jupyter cell run once per session:\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "# then type:  # snipples telco.qa.null_audit   (Tab to see suggestions)\n",
    "```\n",
    "\n",
    "To autoload every time, add to `~/.ipython/profile_default/ipython_config.py`:\n",
    "\n",
    "```python\n",
    "c = get_config()\n",
    "c.InteractiveShellApp.extensions = ['snipples']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) `~/.snipples/snippets.yaml` (Telco Churn end-to-end)\n",
    "\n",
    "Create the folder and file:\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.snipples\n",
    "# paste the following into ~/.snipples/snippets.yaml\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# =========================\n",
    "# Project scaffolding / setup\n",
    "# =========================\n",
    "proj.readme.telco: |\n",
    "  # Telco Churn ‚Äî Analytics & ML\n",
    "  ## Stack\n",
    "  - Python 3.11, pandas, scikit-learn, xgboost, catboost, imbalanced-learn, shap\n",
    "  - BigQuery + dbt (analytics)\n",
    "  - Great Expectations or Pandera (ingest DQ)\n",
    "  - MLflow (experiment tracking)\n",
    "\n",
    "  ## Structure\n",
    "  .\n",
    "  ‚îú‚îÄ data/{raw,interim,processed}\n",
    "  ‚îú‚îÄ notebooks/\n",
    "  ‚îú‚îÄ src/telco/...\n",
    "  ‚îú‚îÄ models/ (dbt)\n",
    "  ‚îî‚îÄ reports/\n",
    "\n",
    "  ## Targets\n",
    "  - Clean EDA + feature marts\n",
    "  - Baseline + tree models\n",
    "  - Explainability (SHAP)\n",
    "  - Reproducible pipelines + tests\n",
    "\n",
    "env.venv.setup: |\n",
    "  python3 -m venv .venv\n",
    "  source .venv/bin/activate\n",
    "  python -m pip install --upgrade pip\n",
    "  pip install pandas numpy scikit-learn imbalanced-learn xgboost catboost shap mlflow matplotlib seaborn pandera great-expectations python-dotenv pandas-gbq google-cloud-bigquery pyarrow\n",
    "\n",
    "nb.header.imports: |\n",
    "  import os, sys, math, json, textwrap, warnings\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  from pathlib import Path\n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "  pd.set_option(\"display.max_columns\", 100)\n",
    "  plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "# =========================\n",
    "# Ingest / IO\n",
    "# =========================\n",
    "ingest.csv.read_telco: |\n",
    "  dtype_map = {\n",
    "      \"customerID\": \"string\",\n",
    "      \"gender\": \"string\",\n",
    "      \"SeniorCitizen\": \"Int64\",\n",
    "      \"Partner\": \"string\",\n",
    "      \"Dependents\": \"string\",\n",
    "      \"tenure\": \"Int64\",\n",
    "      \"PhoneService\": \"string\",\n",
    "      \"MultipleLines\": \"string\",\n",
    "      \"InternetService\": \"string\",\n",
    "      \"OnlineSecurity\": \"string\",\n",
    "      \"OnlineBackup\": \"string\",\n",
    "      \"DeviceProtection\": \"string\",\n",
    "      \"TechSupport\": \"string\",\n",
    "      \"StreamingTV\": \"string\",\n",
    "      \"StreamingMovies\": \"string\",\n",
    "      \"Contract\": \"string\",\n",
    "      \"PaperlessBilling\": \"string\",\n",
    "      \"PaymentMethod\": \"string\",\n",
    "      \"MonthlyCharges\": \"float64\",\n",
    "      \"TotalCharges\": \"string\",   # coercion later\n",
    "      \"Churn\": \"string\"\n",
    "  }\n",
    "  df = pd.read_csv(\"data/raw/Telco-Customer-Churn.csv\", dtype=dtype_map)\n",
    "  print(df.shape)\n",
    "\n",
    "ingest.bigquery.read_table: |\n",
    "  # pip install pandas-gbq google-cloud-bigquery\n",
    "  from pandas_gbq import read_gbq\n",
    "  df = read_gbq(\"\"\"\n",
    "      SELECT * FROM `PROJECT.DATASET.telco_customers`\n",
    "  \"\"\", project_id=os.environ.get(\"GCP_PROJECT\"))\n",
    "\n",
    "save.outputs.standard_paths: |\n",
    "  Path(\"data/interim\").mkdir(parents=True, exist_ok=True)\n",
    "  Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "  df.to_csv(\"data/interim/telco_cleaned.csv\", index=False)\n",
    "\n",
    "# =========================\n",
    "# Data Quality (ingest)\n",
    "# =========================\n",
    "qa.null_audit: |\n",
    "  na = df.isna().sum().sort_values(ascending=False)\n",
    "  print(\"Missing by column:\")\n",
    "  print(na[na>0])\n",
    "\n",
    "qa.dup_pkey_check: |\n",
    "  dups = df[\"customerID\"].value_counts()\n",
    "  print(\"Dup primary keys:\", (dups > 1).sum())\n",
    "\n",
    "qa.pandera.schema_telco: |\n",
    "  import pandera as pa\n",
    "  from pandera import Column, Check\n",
    "  TelcoSchema = pa.DataFrameSchema({\n",
    "      \"customerID\": Column(str, nullable=False),\n",
    "      \"tenure\": Column(int, Check.in_range(0, 84), nullable=False),\n",
    "      \"MonthlyCharges\": Column(float, Check.in_range(0, 200), nullable=False),\n",
    "      \"TotalCharges\": Column(object, nullable=True),  # coerced later\n",
    "      \"Churn\": Column(str, Check.isin([\"Yes\",\"No\"]), nullable=False),\n",
    "  })\n",
    "  TelcoSchema.validate(df, lazy=True)\n",
    "\n",
    "qa.business_rule_totalcharges_tenure: |\n",
    "  bad = df[df[\"tenure\"].fillna(0) > 0].copy()\n",
    "  bad = bad[np.abs(pd.to_numeric(bad[\"TotalCharges\"], errors=\"coerce\") - bad[\"tenure\"] * bad[\"MonthlyCharges\"]) > 10]\n",
    "  print(f\"Rows outside $10 tolerance: {len(bad)}\")\n",
    "\n",
    "# =========================\n",
    "# Cleaning / Imputation\n",
    "# =========================\n",
    "clean.totalcharges_fix: |\n",
    "  df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"].astype(str).str.strip(), errors=\"coerce\")\n",
    "  mask0 = df[\"TotalCharges\"].isna() & (df[\"tenure\"].fillna(0) == 0)\n",
    "  df.loc[mask0, \"TotalCharges\"] = 0.0\n",
    "  mask_other = df[\"TotalCharges\"].isna() & df[\"tenure\"].notna() & df[\"MonthlyCharges\"].notna()\n",
    "  df.loc[mask_other, \"TotalCharges\"] = df.loc[mask_other, \"tenure\"] * df.loc[mask_other, \"MonthlyCharges\"]\n",
    "  df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(\"float64\")\n",
    "\n",
    "clean.category_normalize: |\n",
    "  yesno = [\"Partner\",\"Dependents\",\"PhoneService\",\"PaperlessBilling\",\"Churn\"]\n",
    "  for c in yesno:\n",
    "      df[c] = df[c].str.strip().str.title()\n",
    "  df[\"MultipleLines\"]   = df[\"MultipleLines\"].str.strip().str.replace(\"No phone service\",\"No Phone Service\", regex=False)\n",
    "  internet_cols = [\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\"]\n",
    "  for c in internet_cols:\n",
    "      df[c] = df[c].str.strip().str.replace(\"No internet service\",\"No Internet Service\", regex=False)\n",
    "\n",
    "# =========================\n",
    "# EDA\n",
    "# =========================\n",
    "eda.quick_overview: |\n",
    "  print(df.shape)\n",
    "  display(df.head())\n",
    "  print(df.describe(include=\"all\").T)\n",
    "\n",
    "eda.target_balance: |\n",
    "  ax = (df[\"Churn\"].value_counts(normalize=True)*100).plot.bar()\n",
    "  ax.set_title(\"Churn class balance (%)\"); plt.show()\n",
    "\n",
    "eda.corr_heatmap_numeric: |\n",
    "  num = df.select_dtypes(include=[\"number\"])\n",
    "  sns.heatmap(num.corr(numeric_only=True), annot=False, linewidths=.5)\n",
    "  plt.title(\"Numeric correlations\"); plt.show()\n",
    "\n",
    "eda.churn_rate_by_col_template: |\n",
    "  col = \"Contract\"  # <- change me\n",
    "  rate = (df.groupby(col)[\"Churn\"].apply(lambda s: (s==\"Yes\").mean()).sort_values()*100)\n",
    "  print(rate.round(2))\n",
    "\n",
    "# =========================\n",
    "# Feature Engineering\n",
    "# =========================\n",
    "feat.tenure_buckets: |\n",
    "  bins = [-1, 0, 6, 12, 24, 48, 84, 999]\n",
    "  labels = [\"0\",\"1-6\",\"7-12\",\"13-24\",\"25-48\",\"49-84\",\"85+\"]\n",
    "  df[\"tenure_bucket\"] = pd.cut(df[\"tenure\"], bins=bins, labels=labels)\n",
    "\n",
    "feat.boolean_target: |\n",
    "  df[\"y\"] = (df[\"Churn\"].str.upper() == \"YES\").astype(int)\n",
    "\n",
    "feat.split_train_test: |\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  target = \"y\"\n",
    "  y = df[target]\n",
    "  feature_drop = [\"customerID\",\"Churn\",\"y\"]\n",
    "  X = df.drop(columns=feature_drop, errors=\"ignore\")\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# =========================\n",
    "# Modeling ‚Äì shared utilities\n",
    "# =========================\n",
    "model.columns_splitter: |\n",
    "  num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "  cat_cols = X_train.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "  from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "  from sklearn.compose import ColumnTransformer\n",
    "  pre = ColumnTransformer(\n",
    "      transformers=[\n",
    "          (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "          (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "      ],\n",
    "      remainder=\"drop\"\n",
    "  )\n",
    "\n",
    "model.metrics_helpers: |\n",
    "  from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report, confusion_matrix\n",
    "  def print_metrics(y_true, prob, thr=0.5):\n",
    "      y_pred = (prob >= thr).astype(int)\n",
    "      print(f\"ROC AUC: {roc_auc_score(y_true, prob):.4f}\")\n",
    "      print(f\"PR AUC : {average_precision_score(y_true, prob):.4f}\")\n",
    "      print(f\"F1     : {f1_score(y_true, y_pred):.4f}\")\n",
    "      print(confusion_matrix(y_true, y_pred))\n",
    "      print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "# =========================\n",
    "# Baseline model (LogReg)\n",
    "# =========================\n",
    "model.baseline_logreg: |\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.pipeline import Pipeline\n",
    "  clf = Pipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"lr\", LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None))\n",
    "  ])\n",
    "  clf.fit(X_train, y_train)\n",
    "  prob = clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "# =========================\n",
    "# Tree models (XGBoost / CatBoost)\n",
    "# =========================\n",
    "model.xgboost_cv: |\n",
    "  import xgboost as xgb\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.pipeline import Pipeline\n",
    "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "  clf = Pipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"xgb\", xgb.XGBClassifier(\n",
    "          n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "          subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n",
    "          tree_method=\"hist\", reg_lambda=1.0, n_jobs=-1\n",
    "      ))\n",
    "  ])\n",
    "  aucs = []\n",
    "  for tr, va in skf.split(X_train, y_train):\n",
    "      clf.fit(X_train.iloc[tr], y_train.iloc[tr])\n",
    "      p = clf.predict_proba(X_train.iloc[va])[:,1]\n",
    "      aucs.append(roc_auc_score(y_train.iloc[va], p))\n",
    "  print(\"CV ROC AUC:\", np.mean(aucs).round(4), \"+/-\", np.std(aucs).round(4))\n",
    "  clf.fit(X_train, y_train)\n",
    "  prob = clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "model.catboost_simple: |\n",
    "  from catboost import CatBoostClassifier\n",
    "  # For CatBoost you can pass categorical indices directly (works best on raw categories)\n",
    "  Xc = df.drop(columns=[\"customerID\",\"Churn\",\"y\"], errors=\"ignore\")\n",
    "  cat_idx = np.where(Xc.dtypes == \"object\")[0].tolist()\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  Xtr, Xte, ytr, yte = train_test_split(Xc, df[\"y\"], test_size=0.2, stratify=df[\"y\"], random_state=42)\n",
    "  cat = CatBoostClassifier(\n",
    "      depth=6, iterations=1500, learning_rate=0.03, loss_function=\"Logloss\",\n",
    "      eval_metric=\"AUC\", verbose=200, random_seed=42, auto_class_weights=\"Balanced\"\n",
    "  )\n",
    "  cat.fit(Xtr, ytr, cat_features=cat_idx, eval_set=(Xte, yte), use_best_model=True)\n",
    "  prob = cat.predict_proba(Xte)[:,1]\n",
    "  print_metrics(yte, prob)\n",
    "\n",
    "# =========================\n",
    "# Imbalance strategies (optional)\n",
    "# =========================\n",
    "imb.smote_pipeline_xgb: |\n",
    "  from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "  from imblearn.over_sampling import SMOTE\n",
    "  import xgboost as xgb\n",
    "  imb_clf = ImbPipeline(steps=[\n",
    "      (\"pre\", pre),\n",
    "      (\"smote\", SMOTE(random_state=42)),\n",
    "      (\"xgb\", xgb.XGBClassifier(\n",
    "          n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "          subsample=0.9, colsample_bytree=0.9, eval_metric=\"logloss\", n_jobs=-1\n",
    "      ))\n",
    "  ])\n",
    "  imb_clf.fit(X_train, y_train)\n",
    "  prob = imb_clf.predict_proba(X_test)[:,1]\n",
    "  print_metrics(y_test, prob)\n",
    "\n",
    "# =========================\n",
    "# Threshold tuning & calibration\n",
    "# =========================\n",
    "eval.threshold_opt_pr: |\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "  pr, rc, thr = precision_recall_curve(y_test, prob)\n",
    "  f = 2*pr*rc/(pr+rc+1e-9)\n",
    "  best = np.nanargmax(f)\n",
    "  print(\"Best F1 threshold:\", thr[best].round(4), \"F1:\", f[best].round(4))\n",
    "\n",
    "eval.calibration_plot: |\n",
    "  from sklearn.calibration import calibration_curve\n",
    "  prob_true, prob_pred = calibration_curve(y_test, prob, n_bins=10)\n",
    "  plt.plot(prob_pred, prob_true, marker=\"o\"); plt.plot([0,1],[0,1],\"--\")\n",
    "  plt.title(\"Calibration\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\"); plt.show()\n",
    "\n",
    "# =========================\n",
    "# SHAP explainability\n",
    "# =========================\n",
    "exp.shap_tree: |\n",
    "  import shap\n",
    "  shap.initjs()\n",
    "  # Works with tree-based models like XGBoost/CatBoost\n",
    "  booster = clf.named_steps[\"xgb\"] if \"xgb\" in dict(clf.steps) else None\n",
    "  if booster is None:\n",
    "      raise RuntimeError(\"This snippet expects a fitted Pipeline with step 'xgb'.\")\n",
    "  Xs = clf.named_steps[\"pre\"].transform(X_test)\n",
    "  explainer = shap.TreeExplainer(booster)\n",
    "  shap_values = explainer.shap_values(Xs)\n",
    "  shap.summary_plot(shap_values, Xs)\n",
    "\n",
    "exp.shap_linear: |\n",
    "  import shap\n",
    "  linear = clf.named_steps.get(\"lr\", None)\n",
    "  if linear is None:\n",
    "      raise RuntimeError(\"This snippet expects a fitted Pipeline with step 'lr'.\")\n",
    "  Xs = clf.named_steps[\"pre\"].transform(X_test)\n",
    "  explainer = shap.LinearExplainer(linear, Xs, feature_dependence=\"independent\")\n",
    "  shap_values = explainer.shap_values(Xs)\n",
    "  shap.summary_plot(shap_values, Xs)\n",
    "\n",
    "# =========================\n",
    "# MLflow tracking (optional)\n",
    "# =========================\n",
    "mlflow.start_run_and_log: |\n",
    "  import mlflow, mlflow.sklearn\n",
    "  mlflow.set_experiment(\"telco-churn\")\n",
    "  with mlflow.start_run(run_name=\"xgb_baseline\"):\n",
    "      mlflow.log_params({\"n_estimators\":600,\"max_depth\":6,\"lr\":0.05})\n",
    "      mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, prob))\n",
    "      mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "# =========================\n",
    "# SQL (analysis & sanity)\n",
    "# =========================\n",
    "sql.churn_rate_by_segment: |\n",
    "  SELECT Contract,\n",
    "         ROUND(100 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_pct,\n",
    "         COUNT(*) AS n\n",
    "  FROM analytics.telco_customers\n",
    "  GROUP BY Contract\n",
    "  ORDER BY churn_pct DESC;\n",
    "\n",
    "sql.monthly_retention: |\n",
    "  -- Requires start_date and end_date fields for each subscription\n",
    "  WITH months AS (\n",
    "    SELECT DATE_TRUNC(d, MONTH) AS month\n",
    "    FROM UNNEST(GENERATE_DATE_ARRAY('2017-01-01','2018-12-31', INTERVAL 1 DAY)) AS d\n",
    "  ),\n",
    "  active AS (\n",
    "    SELECT m.month,\n",
    "           COUNTIF(t.start_date <= m.month AND (t.end_date IS NULL OR t.end_date >= m.month)) AS active_customers\n",
    "    FROM months m CROSS JOIN analytics.telco_subscriptions t\n",
    "    GROUP BY m.month\n",
    "  )\n",
    "  SELECT * FROM active ORDER BY month;\n",
    "\n",
    "# =========================\n",
    "# dbt scaffolds\n",
    "# =========================\n",
    "dbt.model_scaffold: |\n",
    "  {{ config(materialized='table') }}\n",
    "  WITH src AS (\n",
    "    SELECT * FROM {{ ref('stg_telco_customers') }}\n",
    "  )\n",
    "  SELECT * FROM src;\n",
    "\n",
    "dbt.schema_tests_min: |\n",
    "  version: 2\n",
    "  models:\n",
    "    - name: stg_telco_customers\n",
    "      columns:\n",
    "        - name: customerID\n",
    "          tests: [not_null, unique]\n",
    "        - name: tenure\n",
    "          tests:\n",
    "            - dbt_expectations.expect_column_values_to_be_between:\n",
    "                min_value: 0\n",
    "                max_value: 84\n",
    "        - name: Churn\n",
    "          tests:\n",
    "            - accepted_values:\n",
    "                values: ['Yes','No']\n",
    "      tests:\n",
    "        - dbt_utils.expression_is_true:\n",
    "            expression: \"case when tenure = 0 then coalesce(TotalCharges,0) = 0 else true end\"\n",
    "        - dbt_utils.expression_is_true:\n",
    "            expression: \"abs(coalesce(TotalCharges,0) - coalesce(tenure,0)*coalesce(MonthlyCharges,0)) <= 10\"\n",
    "            where: \"tenure > 0\"\n",
    "\n",
    "dbt.freshness_recency: |\n",
    "  models:\n",
    "    - name: stg_telco_customers\n",
    "      tests:\n",
    "        - dbt_utils.recency:\n",
    "            field: ingested_at\n",
    "            datepart: hour\n",
    "            interval: 24\n",
    "\n",
    "# =========================\n",
    "# Reporting / export\n",
    "# =========================\n",
    "report.top_drivers_summary: |\n",
    "  # After SHAP: simple text summary\n",
    "  import numpy as np\n",
    "  def top_features(shap_vals, k=10):\n",
    "      imp = np.abs(shap_vals).mean(axis=0)\n",
    "      idx = np.argsort(imp)[::-1][:k]\n",
    "      return list(zip(idx, imp[idx]))\n",
    "  print(\"Top features by mean |SHAP| (indices):\")\n",
    "  print(top_features(shap_values, 10))\n",
    "\n",
    "export.predictions_csv: |\n",
    "  out = X_test.copy()\n",
    "  out[\"prob_churn\"] = prob\n",
    "  out[\"y_true\"] = y_test.values\n",
    "  out.to_csv(\"reports/predictions.csv\", index=False)\n",
    "  print(\"Saved reports/predictions.csv\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quick start flow (what to run in a fresh notebook)\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "\n",
    "# snipples nb.header.imports\n",
    "# snipples ingest.csv.read_telco\n",
    "# snipples qa.null_audit\n",
    "# snipples clean.totalcharges_fix\n",
    "# snipples clean.category_normalize\n",
    "# snipples feat.boolean_target\n",
    "# snipples feat.tenure_buckets\n",
    "# snipples feat.split_train_test\n",
    "# snipples model.columns_splitter\n",
    "# snipples model.metrics_helpers\n",
    "# snipples model.baseline_logreg       # => baseline\n",
    "# snipples model.xgboost_cv            # => stronger model\n",
    "# snipples eval.threshold_opt_pr\n",
    "# snipples exp.shap_tree               # if using the XGB pipeline\n",
    "# snipples export.predictions_csv\n",
    "```\n",
    "\n",
    "# Generate **workspace snippets** (VS Code/Windsurf JSON) \n",
    "\n",
    "    Here are two drop-in artifacts for Windsurf/VS Code:\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Workspace snippets\n",
    "\n",
    "Save as **`.vscode/snippets.code-snippets`** in your repo.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Snipples: expand line\": {\n",
    "    \"prefix\": \"snipples\",\n",
    "    \"description\": \"Insert a # snipples directive (works in Jupyter cells via snipples.py)\",\n",
    "    \"body\": [\"# snipples ${1:namespace.snippet_name}\"]\n",
    "  },\n",
    "\n",
    "  \"Imports: data analyst notebook header\": {\n",
    "    \"prefix\": \"py-imports-notebook\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Standard imports for analysis notebooks\",\n",
    "    \"body\": [\n",
    "      \"import os, sys, math, json, textwrap, warnings\",\n",
    "      \"import numpy as np\",\n",
    "      \"import pandas as pd\",\n",
    "      \"import matplotlib.pyplot as plt\",\n",
    "      \"import seaborn as sns\",\n",
    "      \"from pathlib import Path\",\n",
    "      \"warnings.filterwarnings('ignore')\",\n",
    "      \"pd.set_option('display.max_columns', 100)\",\n",
    "      \"plt.rcParams['figure.figsize'] = (10,5)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"IO: read Telco CSV with dtypes\": {\n",
    "    \"prefix\": \"py-read-telco\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Read IBM Telco Churn CSV with explicit dtypes\",\n",
    "    \"body\": [\n",
    "      \"dtype_map = {\",\n",
    "      \"  'customerID':'string','gender':'string','SeniorCitizen':'Int64','Partner':'string','Dependents':'string',\",\n",
    "      \"  'tenure':'Int64','PhoneService':'string','MultipleLines':'string','InternetService':'string','OnlineSecurity':'string',\",\n",
    "      \"  'OnlineBackup':'string','DeviceProtection':'string','TechSupport':'string','StreamingTV':'string','StreamingMovies':'string',\",\n",
    "      \"  'Contract':'string','PaperlessBilling':'string','PaymentMethod':'string','MonthlyCharges':'float64','TotalCharges':'string','Churn':'string'\",\n",
    "      \"}\",\n",
    "      \"df = pd.read_csv('${1:data/raw/Telco-Customer-Churn.csv}', dtype=dtype_map)\",\n",
    "      \"print(df.shape)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Clean: TotalCharges fix (tenure==0 => 0 else tenure*MonthlyCharges)\": {\n",
    "    \"prefix\": \"py-clean-totalcharges\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Coerce TotalCharges to numeric and impute correctly\",\n",
    "    \"body\": [\n",
    "      \"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].astype(str).str.strip(), errors='coerce')\",\n",
    "      \"mask0 = df['TotalCharges'].isna() & (df['tenure'].fillna(0) == 0)\",\n",
    "      \"df.loc[mask0, 'TotalCharges'] = 0.0\",\n",
    "      \"mask_other = df['TotalCharges'].isna() & df['tenure'].notna() & df['MonthlyCharges'].notna()\",\n",
    "      \"df.loc[mask_other, 'TotalCharges'] = df.loc[mask_other, 'tenure'] * df.loc[mask_other, 'MonthlyCharges']\",\n",
    "      \"df['TotalCharges'] = df['TotalCharges'].astype('float64')\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"EDA: churn class balance plot\": {\n",
    "    \"prefix\": \"py-eda-balance\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Bar chart of churn class balance\",\n",
    "    \"body\": [\n",
    "      \"ax = (df['Churn'].value_counts(normalize=true)*100).plot.bar()\",\n",
    "      \"ax.set_title('Churn class balance (%)'); plt.show()\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Feature: target + tenure buckets\": {\n",
    "    \"prefix\": \"py-feat-target-buckets\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Create y and tenure_bucket features\",\n",
    "    \"body\": [\n",
    "      \"df['y'] = (df['Churn'].str.upper() == 'YES').astype(int)\",\n",
    "      \"bins = [-1,0,6,12,24,48,84,999]\",\n",
    "      \"labels = ['0','1-6','7-12','13-24','25-48','49-84','85+']\",\n",
    "      \"df['tenure_bucket'] = pd.cut(df['tenure'], bins=bins, labels=labels)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Split: train/test\": {\n",
    "    \"prefix\": \"py-split\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Train/test split with drop of ID/label columns\",\n",
    "    \"body\": [\n",
    "      \"from sklearn.model_selection import train_test_split\",\n",
    "      \"target = 'y'\",\n",
    "      \"y = df[target]\",\n",
    "      \"X = df.drop(columns=['customerID','Churn','y'], errors='ignore')\",\n",
    "      \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Preprocess: ColumnTransformer (num scale + OHE cat)\": {\n",
    "    \"prefix\": \"py-pre-coltx\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"StandardScaler for numeric and OneHotEncoder for categories\",\n",
    "    \"body\": [\n",
    "      \"num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\",\n",
    "      \"cat_cols = X_train.select_dtypes(exclude=['number']).columns.tolist()\",\n",
    "      \"from sklearn.preprocessing import OneHotEncoder, StandardScaler\",\n",
    "      \"from sklearn.compose import ColumnTransformer\",\n",
    "      \"pre = ColumnTransformer([\",\n",
    "      \"  ('num', StandardScaler(with_mean=false), num_cols),\",\n",
    "      \"  ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=true), cat_cols)\",\n",
    "      \"])\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Model: Logistic Regression baseline\": {\n",
    "    \"prefix\": \"py-model-logreg\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Baseline classifier pipeline + quick metrics\",\n",
    "    \"body\": [\n",
    "      \"from sklearn.pipeline import Pipeline\",\n",
    "      \"from sklearn.linear_model import LogisticRegression\",\n",
    "      \"from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report, confusion_matrix\",\n",
    "      \"def print_metrics(y_true, prob, thr=0.5):\",\n",
    "      \"  y_pred = (prob >= thr).astype(int)\",\n",
    "      \"  print(f'ROC AUC: {roc_auc_score(y_true, prob):.4f}')\",\n",
    "      \"  print(f'PR AUC : {average_precision_score(y_true, prob):.4f}')\",\n",
    "      \"  print(f'F1     : {f1_score(y_true, y_pred):.4f}')\",\n",
    "      \"  print(confusion_matrix(y_true, y_pred))\",\n",
    "      \"  print(classification_report(y_true, y_pred, digits=3))\",\n",
    "      \"clf = Pipeline([('pre', pre), ('lr', LogisticRegression(max_iter=200, class_weight='balanced'))])\",\n",
    "      \"clf.fit(X_train, y_train)\",\n",
    "      \"prob = clf.predict_proba(X_test)[:,1]\",\n",
    "      \"print_metrics(y_test, prob)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Model: XGBoost pipeline (hist)\": {\n",
    "    \"prefix\": \"py-model-xgb\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"XGBoost classifier with ColumnTransformer preprocessing\",\n",
    "    \"body\": [\n",
    "      \"import xgboost as xgb\",\n",
    "      \"from sklearn.pipeline import Pipeline\",\n",
    "      \"from sklearn.metrics import roc_auc_score\",\n",
    "      \"clf = Pipeline([\",\n",
    "      \"  ('pre', pre),\",\n",
    "      \"  ('xgb', xgb.XGBClassifier(n_estimators=600, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, eval_metric='logloss', tree_method='hist', n_jobs=-1))\",\n",
    "      \"])\",\n",
    "      \"clf.fit(X_train, y_train)\",\n",
    "      \"prob = clf.predict_proba(X_test)[:,1]\",\n",
    "      \"print_metrics(y_test, prob)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Explain: SHAP (tree models)\": {\n",
    "    \"prefix\": \"py-shap-tree\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"SHAP summary for tree-based model within a Pipeline\",\n",
    "    \"body\": [\n",
    "      \"import shap\",\n",
    "      \"shap.initjs()\",\n",
    "      \"booster = clf.named_steps.get('xgb')\",\n",
    "      \"Xs = clf.named_steps['pre'].transform(X_test)\",\n",
    "      \"explainer = shap.TreeExplainer(booster)\",\n",
    "      \"shap_values = explainer.shap_values(Xs)\",\n",
    "      \"shap.summary_plot(shap_values, Xs)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Quality: Pandera mini schema\": {\n",
    "    \"prefix\": \"py-pandera-telco\",\n",
    "    \"scope\": \"python\",\n",
    "    \"description\": \"Quick Pandera schema to validate key columns\",\n",
    "    \"body\": [\n",
    "      \"import pandera as pa\",\n",
    "      \"from pandera import Column, Check\",\n",
    "      \"TelcoSchema = pa.DataFrameSchema({\",\n",
    "      \"  'customerID': Column(str, nullable=false),\",\n",
    "      \"  'tenure': Column(int, Check.in_range(0,84), nullable=false),\",\n",
    "      \"  'MonthlyCharges': Column(float, Check.in_range(0,200), nullable=false),\",\n",
    "      \"  'TotalCharges': Column(float, nullable=false),\",\n",
    "      \"  'Churn': Column(str, Check.isin(['Yes','No']), nullable=false)\",\n",
    "      \"})\",\n",
    "      \"TelcoSchema.validate(df, lazy=true)\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"dbt: model scaffold\": {\n",
    "    \"prefix\": \"dbt-model\",\n",
    "    \"scope\": \"sql\",\n",
    "    \"description\": \"dbt table model scaffold\",\n",
    "    \"body\": [\n",
    "      \"{{ config(materialized='table') }}\",\n",
    "      \"WITH src AS (\",\n",
    "      \"  SELECT * FROM {{ ref('${1:stg_telco_customers}') }}\",\n",
    "      \")\",\n",
    "      \"SELECT * FROM src;\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"SQL: churn by segment\": {\n",
    "    \"prefix\": \"sql-churn-seg\",\n",
    "    \"scope\": \"sql\",\n",
    "    \"description\": \"Segment churn rates by Contract\",\n",
    "    \"body\": [\n",
    "      \"SELECT Contract,\",\n",
    "      \"       ROUND(100 * AVG(CASE WHEN Churn='Yes' THEN 1 ELSE 0 END), 2) AS churn_pct,\",\n",
    "      \"       COUNT(*) AS n\",\n",
    "      \"FROM ${1:analytics.telco_customers}\",\n",
    "      \"GROUP BY Contract\",\n",
    "      \"ORDER BY churn_pct DESC;\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "># Tiny **Makefile** to lint/test/run your dbt & notebooks?\n",
    "\n",
    "# 2) Makefile\n",
    "\n",
    "Save as **`Makefile`** at the repo root. It‚Äôs opinionated but tidy for AE/ML + dbt.\n",
    "\n",
    "```make\n",
    "# -------- Config --------\n",
    "PY          ?= python3\n",
    "VENV        ?= .venv\n",
    "ACTIVATE    = . $(VENV)/bin/activate\n",
    "REQS        ?= requirements.txt\n",
    "\n",
    "DBT_TARGET  ?= dev\n",
    "DBT_PROJ    ?= .           # path to dbt project (has dbt_project.yml)\n",
    "RAW_CSV     ?= data/raw/Telco-Customer-Churn.csv\n",
    "\n",
    "# -------- Helpers --------\n",
    ".PHONY: help\n",
    "help:\n",
    "\t@echo \"Common targets:\"\n",
    "\t@echo \"  make venv           # create venv and install requirements\"\n",
    "\t@echo \"  make install        # install/upgrade packages in venv\"\n",
    "\t@echo \"  make lint           # ruff lint + format check\"\n",
    "\t@echo \"  make fmt            # ruff format\"\n",
    "\t@echo \"  make test           # run pytest\"\n",
    "\t@echo \"  make nb-run         # run notebooks with papermill (paramizable)\"\n",
    "\t@echo \"  make dbt-deps/run/test/freshness  # dbt workflow\"\n",
    "\t@echo \"  make qa-quick       # quick CSV sanity checks (row count, nulls, dup IDs)\"\n",
    "\t@echo \"  make clean          # remove build artifacts\"\n",
    "\n",
    "# -------- Environment --------\n",
    "$(VENV):\n",
    "\t$(PY) -m venv $(VENV)\n",
    "\t$(ACTIVATE) && python -m pip install --upgrade pip\n",
    "\n",
    ".PHONY: venv\n",
    "venv: $(VENV) install\n",
    "\n",
    ".PHONY: install\n",
    "install:\n",
    "\t@if [ -f \"$(REQS)\" ]; then \\\n",
    "\t\t$(ACTIVATE) && pip install -r $(REQS); \\\n",
    "\telse \\\n",
    "\t\techo \"No requirements.txt found; installing a sane default set...\"; \\\n",
    "\t\t$(ACTIVATE) && pip install pandas numpy scikit-learn imbalanced-learn xgboost catboost shap mlflow matplotlib seaborn pandera great-expectations jupyter papermill ruff pytest dbt-bigquery pandas-gbq google-cloud-bigquery pyarrow; \\\n",
    "\tfi\n",
    "\n",
    "# -------- Lint & Test --------\n",
    ".PHONY: lint\n",
    "lint:\n",
    "\t$(ACTIVATE) && ruff check .\n",
    "\n",
    ".PHONY: fmt\n",
    "fmt:\n",
    "\t$(ACTIVATE) && ruff format .\n",
    "\n",
    ".PHONY: test\n",
    "test:\n",
    "\t$(ACTIVATE) && pytest -q\n",
    "\n",
    "# -------- Notebooks (papermill) --------\n",
    "# Usage: make nb-run NB=notebooks/01_eda.ipynb OUT=reports/01_eda.out.ipynb\n",
    "NB ?= notebooks/01_eda.ipynb\n",
    "OUT ?= reports/01_eda.out.ipynb\n",
    ".PHONY: nb-run\n",
    "nb-run:\n",
    "\t$(ACTIVATE) && papermill $(NB) $(OUT)\n",
    "\n",
    "# -------- Quick CSV QA (no dbt) --------\n",
    ".PHONY: qa-quick\n",
    "qa-quick:\n",
    "\t@echo \"QA: $(RAW_CSV)\"\n",
    "\t$(ACTIVATE) && python - <<'PY'\n",
    "import pandas as pd, sys\n",
    "path = \"$(RAW_CSV)\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Nulls (top 10):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "if \"customerID\" in df.columns:\n",
    "    dup = (df[\"customerID\"].value_counts() > 1).sum()\n",
    "    print(\"Duplicate customerID:\", dup)\n",
    "PY\n",
    "\n",
    "# -------- dbt workflow --------\n",
    ".PHONY: dbt-deps\n",
    "dbt-deps:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt deps\n",
    "\n",
    ".PHONY: dbt-run\n",
    "dbt-run:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt run --target $(DBT_TARGET)\n",
    "\n",
    ".PHONY: dbt-test\n",
    "dbt-test:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt test --target $(DBT_TARGET)\n",
    "\n",
    ".PHONY: dbt-freshness\n",
    "dbt-freshness:\n",
    "\t$(ACTIVATE) && cd $(DBT_PROJ) && dbt source freshness --target $(DBT_TARGET)\n",
    "\n",
    "# -------- Clean --------\n",
    ".PHONY: clean\n",
    "clean:\n",
    "\trm -rf $(VENV) .pytest_cache .ruff_cache **/__pycache__\n",
    "\tfind . -name \"*.ipynb_checkpoints\" -type d -prune -exec rm -rf {} +\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to wire this up fast\n",
    "\n",
    "1. **Put your extension + snippets in place**\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.snipples\n",
    "# (Paste your large ~/.snipples/snippets.yaml we created earlier)\n",
    "# Put snipples.py somewhere on PYTHONPATH, or `pip install -e .` if you package it.\n",
    "```\n",
    "\n",
    "In notebooks:\n",
    "\n",
    "```python\n",
    "%load_ext snipples\n",
    "# then type:  # snipples model.xgboost_cv   (Tab to browse)\n",
    "```\n",
    "\n",
    "2. **Add the workspace snippets**\n",
    "\n",
    "* Create `.vscode/snippets.code-snippets` and paste the JSON above.\n",
    "\n",
    "3. **Makefile**\n",
    "\n",
    "```bash\n",
    "make venv\n",
    "make qa-quick\n",
    "make dbt-deps dbt-run dbt-test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "># **generate a tiny `requirements.txt` and a `tasks.json` (Windsurf/VS Code tasks that wrap `make` targets so you can ‚åò‚áßB to run ‚ÄúDBT: test‚Äù)**\n",
    "\n",
    "\n",
    "\n",
    "># CLAUDE SUGGESTION: CODE LIBRARY\n",
    "\n",
    "Great question! Code organization and storage strategy is crucial for professional development and portfolio presentation. Here are the most appropriate approaches for different aspects of your projects:\n",
    "\n",
    "## üóÇÔ∏è **Repository Structure & Organization**\n",
    "\n",
    "### **Primary Code Storage: GitHub**\n",
    "```\n",
    "your-username/\n",
    "‚îú‚îÄ‚îÄ telco-churn-analysis-level-0/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ README.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ telco.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ methodology.md\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ findings.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tests/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_analysis.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ figures/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ reports/\n",
    "```\n",
    "\n",
    "### **Multi-Level Portfolio Structure**\n",
    "```\n",
    "data-analytics-journey/\n",
    "‚îú‚îÄ‚îÄ README.md (Portfolio overview)\n",
    "‚îú‚îÄ‚îÄ level-00-bare-bones/\n",
    "‚îú‚îÄ‚îÄ level-01-structured-eda/\n",
    "‚îú‚îÄ‚îÄ level-02-data-cleaning/\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ level-10-enterprise/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ learning-progression.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ technologies-mastered.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ business-impact.md\n",
    "‚îî‚îÄ‚îÄ portfolio-assets/\n",
    "    ‚îú‚îÄ‚îÄ presentations/\n",
    "    ‚îú‚îÄ‚îÄ publications/\n",
    "    ‚îî‚îÄ‚îÄ certifications/\n",
    "```\n",
    "\n",
    "## üíª **Code Workflow Storage Options**\n",
    "\n",
    "### **1. GitHub (Primary - Essential)**\n",
    "**Best for:**\n",
    "- Version control and collaboration\n",
    "- Portfolio presentation to employers\n",
    "- Open source contributions\n",
    "- CI/CD integration\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "‚îú‚îÄ‚îÄ .github/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ workflows/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ci.yml\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ISSUE_TEMPLATE/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îî‚îÄ‚îÄ examples/\n",
    "```\n",
    "\n",
    "### **2. Jupyter Notebooks (Development)**\n",
    "**Best for:**\n",
    "- Exploratory analysis documentation\n",
    "- Step-by-step methodology explanation\n",
    "- Interactive presentations\n",
    "- Educational content\n",
    "\n",
    "**Organization:**\n",
    "```\n",
    "notebooks/\n",
    "‚îú‚îÄ‚îÄ 01-data-exploration.ipynb\n",
    "‚îú‚îÄ‚îÄ 02-data-cleaning.ipynb\n",
    "‚îú‚îÄ‚îÄ 03-feature-engineering.ipynb\n",
    "‚îú‚îÄ‚îÄ 04-modeling.ipynb\n",
    "‚îî‚îÄ‚îÄ 05-results-analysis.ipynb\n",
    "```\n",
    "\n",
    "### **3. Professional Platforms**\n",
    "\n",
    "#### **Kaggle Notebooks**\n",
    "```python\n",
    "# Kaggle-optimized structure\n",
    "\"\"\"\n",
    "TELCO CUSTOMER CHURN - LEVEL 0 ANALYSIS\n",
    "=======================================\n",
    "Author: Your Name\n",
    "Created: Date\n",
    "Updated: Date\n",
    "\n",
    "Table of Contents:\n",
    "1. Introduction & Business Problem\n",
    "2. Data Loading & Initial Exploration  \n",
    "3. Data Quality Assessment\n",
    "4. Key Insights Discovery\n",
    "5. Business Recommendations\n",
    "6. Technical Implementation\n",
    "7. Next Steps & Learning Journey\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../input/telco-customer-churn/telco.csv')\n",
    "```\n",
    "\n",
    "#### **Google Colab (Shareable)**\n",
    "```python\n",
    "# Mount Google Drive for data persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# Professional notebook structure with clear sections\n",
    "```\n",
    "\n",
    "## üèóÔ∏è **Advanced Code Organization (Level 5+)**\n",
    "\n",
    "### **Modular Python Package Structure**\n",
    "```\n",
    "telco_churn_analysis/\n",
    "‚îú‚îÄ‚îÄ setup.py\n",
    "‚îú‚îÄ‚îÄ pyproject.toml\n",
    "‚îú‚îÄ‚îÄ telco_churn/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engineering.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selection.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ visualization/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plots.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ helpers.py\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îú‚îÄ‚îÄ examples/\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "```\n",
    "\n",
    "### **Configuration Management**\n",
    "```python\n",
    "# config.py\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR: Path = Path(\"data\")\n",
    "    RAW_DATA_FILE: str = \"telco.csv\"\n",
    "    PROCESSED_DATA_FILE: str = \"telco_processed.csv\"\n",
    "    \n",
    "    # Model parameters\n",
    "    TEST_SIZE: float = 0.2\n",
    "    RANDOM_STATE: int = 42\n",
    "    \n",
    "    # Output paths\n",
    "    RESULTS_DIR: Path = Path(\"results\")\n",
    "    FIGURES_DIR: Path = Path(\"results/figures\")\n",
    "    REPORTS_DIR: Path = Path(\"results/reports\")\n",
    "```\n",
    "\n",
    "## üìä **Documentation & Workflow Storage**\n",
    "\n",
    "### **README.md Template**\n",
    "```markdown\n",
    "# Level X: Telco Customer Churn Analysis\n",
    "\n",
    "## Quick Start\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python src/analysis.py\n",
    "```\n",
    "\n",
    "## Project Structure\n",
    "- `src/`: Core analysis code\n",
    "- `notebooks/`: Jupyter exploration\n",
    "- `data/`: Dataset files  \n",
    "- `results/`: Output artifacts\n",
    "- `tests/`: Unit tests\n",
    "- `docs/`: Documentation\n",
    "\n",
    "## Key Findings\n",
    "- Finding 1: Contract type drives churn\n",
    "- Finding 2: Payment method indicates engagement  \n",
    "- Finding 3: Premium services show paradox\n",
    "\n",
    "## Business Impact\n",
    "- $2.4M annual revenue at risk identified\n",
    "- 3 strategic initiatives recommended\n",
    "- Clear ROI projections provided\n",
    "\n",
    "## Technologies Used\n",
    "- Python, Pandas, NumPy\n",
    "- Matplotlib, Seaborn\n",
    "- Scikit-learn (for advanced levels)\n",
    "\n",
    "## Next Steps\n",
    "See Level X+1 for progression...\n",
    "```\n",
    "\n",
    "### **Code Documentation Standards**\n",
    "```python\n",
    "def analyze_churn_by_contract(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Analyze churn rates by contract type.\n",
    "    \n",
    "    This function calculates churn rates for each contract type,\n",
    "    revealing the primary business insight of this analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Customer dataset with 'Contract' and 'Churn' columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Churn rates by contract type, sorted descending\n",
    "        \n",
    "    Example:\n",
    "        >>> churn_rates = analyze_churn_by_contract(df)\n",
    "        >>> print(churn_rates)\n",
    "        Month-to-month    0.427\n",
    "        One year          0.113  \n",
    "        Two year          0.028\n",
    "    \n",
    "    Business Impact:\n",
    "        This analysis reveals that month-to-month customers churn at 15x \n",
    "        the rate of two-year contract customers, representing the single \n",
    "        most actionable insight for retention strategy.\n",
    "    \"\"\"\n",
    "    return df.groupby('Contract')['Churn'].apply(\n",
    "        lambda x: (x == 'Yes').mean()\n",
    "    ).sort_values(ascending=False)\n",
    "```\n",
    "\n",
    "## üîß **Workflow Management Tools**\n",
    "\n",
    "### **For Individual Projects (Levels 0-3)**\n",
    "- **Primary**: GitHub repository per level\n",
    "- **Development**: Jupyter notebooks + VS Code\n",
    "- **Documentation**: Markdown files + README\n",
    "- **Sharing**: Kaggle notebooks for community\n",
    "\n",
    "### **For Advanced Projects (Levels 4-7)**\n",
    "- **Code**: Modular Python packages\n",
    "- **Environment**: Poetry/Conda for dependency management\n",
    "- **Testing**: pytest framework\n",
    "- **Documentation**: Sphinx for API docs\n",
    "- **Workflow**: GitHub Actions for CI/CD\n",
    "\n",
    "### **For Enterprise Projects (Levels 8-10)**\n",
    "- **Infrastructure**: Infrastructure as Code (Terraform)\n",
    "- **Containerization**: Docker + Kubernetes\n",
    "- **Orchestration**: Apache Airflow\n",
    "- **Monitoring**: Logging and metrics collection\n",
    "- **Deployment**: Blue-green deployment strategies\n",
    "\n",
    "## üí° **Best Practices by Level**\n",
    "\n",
    "### **Level 0-2: Foundation**\n",
    "```python\n",
    "# Simple, clear, well-commented code\n",
    "import pandas as pd\n",
    "\n",
    "# Load data with error handling\n",
    "try:\n",
    "    df = pd.read_csv('data/telco.csv')\n",
    "    print(f\"‚úÖ Data loaded: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found\")\n",
    "    exit(1)\n",
    "\n",
    "# Clear business-focused analysis\n",
    "contract_churn = df.groupby('Contract')['Churn'].mean()\n",
    "print(\"üìä Churn by Contract Type:\")\n",
    "print(contract_churn.sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "### **Level 5-7: Professional**\n",
    "```python\n",
    "# Type hints, docstrings, error handling\n",
    "from typing import Tuple, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ChurnAnalyzer:\n",
    "    \"\"\"Professional customer churn analysis class.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.df = None\n",
    "        \n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and validate customer data.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\"Data loaded successfully: {self.df.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {e}\")\n",
    "            raise\n",
    "```\n",
    "\n",
    "### **Level 8-10: Enterprise**\n",
    "```python\n",
    "# Production-ready with monitoring\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import structlog\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "@dataclass\n",
    "class PredictionMetrics:\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    latency_ms: float\n",
    "\n",
    "class ChurnPredictor(ABC):\n",
    "    \"\"\"Abstract base class for churn prediction models.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def predict(self, customer_data: Dict) -> Tuple[float, PredictionMetrics]:\n",
    "        \"\"\"Predict churn probability with performance metrics.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "## üìà **Storage Strategy Recommendations**\n",
    "\n",
    "### **For Learning Journey Documentation**\n",
    "1. **GitHub**: Primary portfolio repository\n",
    "2. **Kaggle**: Public notebooks for community engagement  \n",
    "3. **Personal Blog**: Medium/LinkedIn articles\n",
    "4. **Cloud Storage**: AWS S3/Google Cloud for large datasets\n",
    "5. **Local Development**: VS Code + Jupyter for daily work\n",
    "\n",
    "### **For Professional Presentation**\n",
    "1. **Clean GitHub repos** with professional README files\n",
    "2. **Comprehensive documentation** explaining methodology\n",
    "3. **Business-focused presentations** with clear ROI\n",
    "4. **Code quality standards** with proper testing\n",
    "5. **Progression documentation** showing skill development\n",
    "\n",
    "The key is to maintain **consistency across platforms** while optimizing each for its specific audience and purpose. Your Level 0 should be simple and accessible, while your Level 10 should demonstrate enterprise-scale thinking and implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3921ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e17eb2",
   "metadata": {},
   "source": [
    "Here‚Äôs a **professional, production-grade** way to do data quality (DQ) that hiring managers expect to see. It‚Äôs tool-agnostic but I‚Äôll show dbt / SQL / Python snippets you can drop in.\n",
    "\n",
    "# 1) Write a data contract (single source of truth)\n",
    "\n",
    "For each dataset/table define: owner, schema (types, nullability), keys, ranges, enums, referential links, freshness SLA, row-level filters, and severity rules (P0=fail pipeline, P1=alert only).\n",
    "\n",
    "```yaml\n",
    "# data_contract.yml (excerpt)\n",
    "table: analytics.telco_customers\n",
    "owner: data@company.com\n",
    "freshness_sla_minutes: 60\n",
    "columns:\n",
    "  customerID: {type: STRING, unique: true, nullable: false}\n",
    "  tenure:     {type: INT64, nullable: false, range: [0, 120]}\n",
    "  MonthlyCharges: {type: NUMERIC, nullable: false, range: [0, 500]}\n",
    "  TotalCharges:   {type: NUMERIC, nullable: false}\n",
    "rules:\n",
    "  - name: fk_contract_type\n",
    "    description: contractType must exist in dim_contracts\n",
    "    severity: P0\n",
    "```\n",
    "\n",
    "# 2) Test across the full pipeline (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "\n",
    "* **Ingest/Bronze:** schema, required columns, type coercion, row count sanity, duplicate primary keys.\n",
    "* **Transform/Silver:** referential integrity, accepted values, business rules (e.g., `TotalCharges ‚âà tenure*MonthlyCharges` tolerance), distribution drift checks.\n",
    "* **Publish/Gold:** KPI reconciliation vs previous day, freshness, completeness of key segments.\n",
    "\n",
    "# 3) Standard test catalog (cover the dimensions)\n",
    "\n",
    "* **Completeness:** `NOT NULL`, required % filled.\n",
    "* **Validity:** types, ranges, regexes, enumerations.\n",
    "* **Uniqueness:** primary keys, composite keys.\n",
    "* **Consistency/Integrity:** FKs, cross-column rules.\n",
    "* **Timeliness/Freshness:** max timestamp vs now.\n",
    "* **Accuracy/Reconciliation:** totals vs source-of-record.\n",
    "* **Drift/Anomaly:** z-score/IQR/seasonal anomaly on row\\_count, null\\_rate, mean/median.\n",
    "\n",
    "# 4) Implement with your stack\n",
    "\n",
    "### dbt (fast, declarative tests)\n",
    "\n",
    "```yaml\n",
    "# models/telco/schema.yml\n",
    "version: 2\n",
    "models:\n",
    "  - name: stg_telco_customers\n",
    "    tests:\n",
    "      - dbt_utils.unique_combination_of_columns:\n",
    "          combination_of_columns: [customerID]\n",
    "      - dbt_utils.not_null_proportion:\n",
    "          column_name: tenure\n",
    "          at_least: 0.999\n",
    "    columns:\n",
    "      - name: contractType\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              values: ['Month-to-month','One year','Two year']\n",
    "      - name: plan_id\n",
    "        tests:\n",
    "          - relationships:\n",
    "              to: ref('dim_contracts')\n",
    "              field: plan_id\n",
    "```\n",
    "\n",
    "### SQL spot checks (works anywhere)\n",
    "\n",
    "```sql\n",
    "-- P0: duplicate primary keys\n",
    "SELECT customerID, COUNT(*) c\n",
    "FROM analytics.telco_customers\n",
    "GROUP BY 1 HAVING c > 1;\n",
    "\n",
    "-- Freshness (expect < 60 min)\n",
    "SELECT TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(updated_at), MINUTE) AS minutes_late\n",
    "FROM analytics.telco_customers;\n",
    "\n",
    "-- Business rule tolerance\n",
    "SELECT COUNT(*) AS bad_rows\n",
    "FROM analytics.telco_customers\n",
    "WHERE tenure > 0\n",
    "  AND ABS(TotalCharges - tenure*MonthlyCharges) > 10;  -- $10 tolerance\n",
    "```\n",
    "\n",
    "### Python contracts (schema in code; great for ingestion)\n",
    "\n",
    "**Pandera** (lightweight, free):\n",
    "\n",
    "```python\n",
    "import pandera as pa\n",
    "from pandera import Column, Check\n",
    "\n",
    "TelcoSchema = pa.DataFrameSchema({\n",
    "  \"customerID\": Column(str, nullable=False, unique=True),\n",
    "  \"tenure\": Column(int, Check.in_range(0, 120), nullable=False),\n",
    "  \"MonthlyCharges\": Column(float, Check.in_range(0, 500), nullable=False),\n",
    "  \"TotalCharges\": Column(float, nullable=False),\n",
    "})\n",
    "TelcoSchema.validate(df, lazy=True)\n",
    "```\n",
    "\n",
    "**Great Expectations** (rich profiling + stores):\n",
    "\n",
    "```python\n",
    "validator.expect_column_values_to_not_be_null(\"customerID\")\n",
    "validator.expect_column_values_to_be_unique(\"customerID\")\n",
    "validator.expect_column_values_to_be_between(\"tenure\", 0, 120)\n",
    "validator.expect_column_values_to_be_in_set(\"contractType\",\n",
    "    [\"Month-to-month\",\"One year\",\"Two year\"])\n",
    "```\n",
    "\n",
    "# 5) Gate the pipeline & alert\n",
    "\n",
    "* **Fail fast on P0** (stop the job, quarantine bad rows to an error table).\n",
    "* **Alert on P1/P2** (Slack/Email with row samples and a run link).\n",
    "* **Auto-ticket** for repeated breaches. Keep **run\\_history** of test metrics.\n",
    "\n",
    "# 6) Monitor, not just test\n",
    "\n",
    "Track time-series of: row\\_count, null\\_rate per column, distinct\\_count, mean/median, min/max. Use simple anomaly rules (EWMA or seasonal z-scores) to catch silent schema/content drift. Put them on a small dashboard.\n",
    "\n",
    "# 7) CI/CD integration (professional touch)\n",
    "\n",
    "* **Pre-merge:** run unit SQL, dbt tests, and sample validations (on a 1‚Äì5% subset).\n",
    "* **Post-merge:** full dataset tests in the orchestrator (Airflow/Prefect/dbt Cloud).\n",
    "* **Data diffs** on key tables (e.g., elementary-dbt or data-diff) to show what changed.\n",
    "\n",
    "# 8) Document & iterate\n",
    "\n",
    "Every failed test should have: owner, impact, remediation, and a decision (tighten, relax, or deprecate). Review thresholds quarterly with stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal ‚Äúday-one‚Äù setup for your Telco project (BigQuery + dbt)\n",
    "\n",
    "1. Write `data_contract.yml`.\n",
    "2. Add dbt tests (unique/not\\_null/accepted\\_values/relationships) in `schema.yml`.\n",
    "3. Add 3 SQL checks (freshness, dup keys, business rule tolerance) to a nightly job.\n",
    "4. Add Pandera (or GE) on ingest to catch bad CSVs early.\n",
    "5. Wire Slack alerts + P0 gating.\n",
    "\n",
    "If you want, I can generate the **contract + dbt tests** tailored to your current Telco column names so you can paste them in and run today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bba98",
   "metadata": {},
   "source": [
    "Awesome‚Äîhere‚Äôs a ready-to-run **data contract** and **dbt tests** for the classic IBM Telco Churn columns. I‚Äôll assume your final model is named `stg_telco_customers` (rename if yours is different).\n",
    "\n",
    "---\n",
    "\n",
    "### `packages.yml`\n",
    "\n",
    "```yaml\n",
    "packages:\n",
    "  - package: dbt-labs/dbt_utils\n",
    "    version: [\">=1.1.1\", \"<2.0.0\"]\n",
    "  - package: calogica/dbt_expectations\n",
    "    version: [\">=0.10.3\", \"<1.0.0\"]\n",
    "```\n",
    "\n",
    "Run once:\n",
    "\n",
    "```bash\n",
    "dbt deps\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `data_contracts/telco_customers_contract.yml`  *(living doc; human + CI reference)*\n",
    "\n",
    "```yaml\n",
    "table: stg_telco_customers\n",
    "owner: you@example.com\n",
    "freshness_sla_minutes: 1440  # dataset is static; adjust if you refresh\n",
    "primary_key: customerID\n",
    "\n",
    "columns:\n",
    "  customerID:      {type: STRING, nullable: false, unique: true}\n",
    "  gender:          {type: STRING, nullable: false, enum: [\"Male\",\"Female\"]}\n",
    "  SeniorCitizen:   {type: INT64,  nullable: false, enum: [0,1]}\n",
    "  Partner:         {type: STRING, nullable: false, enum: [\"Yes\",\"No\"]}\n",
    "  Dependents:      {type: STRING, nullable: false, enum: [\"Yes\",\"No\"]}\n",
    "  tenure:          {type: INT64,  nullable: false, range: [0, 84]}\n",
    "  PhoneService:    {type: STRING, nullable: false, enum: [\"Yes\",\"No\"]}\n",
    "  MultipleLines:   {type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No phone service\"]}\n",
    "  InternetService: {type: STRING, nullable: false, enum: [\"DSL\",\"Fiber optic\",\"No\"]}\n",
    "  OnlineSecurity:  {type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No internet service\"]}\n",
    "  OnlineBackup:    {type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No internet service\"]}\n",
    "  DeviceProtection:{type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No internet service\"]}\n",
    "  TechSupport:     {type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No internet service\"]}\n",
    "  StreamingTV:     {type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No internet service\"]}\n",
    "  StreamingMovies: {type: STRING, nullable: false, enum: [\"Yes\",\"No\",\"No internet service\"]}\n",
    "  Contract:        {type: STRING, nullable: false, enum: [\"Month-to-month\",\"One year\",\"Two year\"]}\n",
    "  PaperlessBilling:{type: STRING, nullable: false, enum: [\"Yes\",\"No\"]}\n",
    "  PaymentMethod:   {type: STRING, nullable: false, enum: [\"Electronic check\",\"Mailed check\",\"Bank transfer (automatic)\",\"Credit card (automatic)\"]}\n",
    "  MonthlyCharges:  {type: FLOAT64, nullable: false, range: [0, 200]}\n",
    "  TotalCharges:    {type: FLOAT64, nullable: false, min: 0}\n",
    "  Churn:           {type: STRING, nullable: false, enum: [\"Yes\",\"No\"]}\n",
    "\n",
    "rules:\n",
    "  - name: total_vs_tenure_rate_tolerance\n",
    "    description: |-\n",
    "      For tenure > 0, TotalCharges should be close to tenure * MonthlyCharges.\n",
    "      Allow $10 tolerance for pro-rations/fees.\n",
    "    severity: P1\n",
    "  - name: zero_total_when_tenure_zero\n",
    "    description: For tenure == 0, TotalCharges must be 0.\n",
    "    severity: P0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `models/telco/schema.yml`  *(dbt tests for `stg_telco_customers`)*\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: stg_telco_customers\n",
    "    config:\n",
    "      # Optional: if your warehouse supports dbt contracts, enforce types at build time.\n",
    "      # BigQuery/Snowflake work well. Remove if you don't want hard enforcement.\n",
    "      contract:\n",
    "        enforced: true\n",
    "    columns:\n",
    "      - name: customerID\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "\n",
    "      - name: gender\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              values: ['Male','Female']\n",
    "\n",
    "      - name: SeniorCitizen\n",
    "        data_type: int64\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              values: [0,1]\n",
    "\n",
    "      - name: Partner\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No']}\n",
    "\n",
    "      - name: Dependents\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No']}\n",
    "\n",
    "      - name: tenure\n",
    "        data_type: int64\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_expectations.expect_column_values_to_be_between:\n",
    "              min_value: 0\n",
    "              max_value: 84\n",
    "              strictly: false\n",
    "\n",
    "      - name: PhoneService\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No']}\n",
    "\n",
    "      - name: MultipleLines\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No phone service']}\n",
    "\n",
    "      - name: InternetService\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['DSL','Fiber optic','No']}\n",
    "\n",
    "      - name: OnlineSecurity\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No internet service']}\n",
    "\n",
    "      - name: OnlineBackup\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No internet service']}\n",
    "\n",
    "      - name: DeviceProtection\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No internet service']}\n",
    "\n",
    "      - name: TechSupport\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No internet service']}\n",
    "\n",
    "      - name: StreamingTV\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No internet service']}\n",
    "\n",
    "      - name: StreamingMovies\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No','No internet service']}\n",
    "\n",
    "      - name: Contract\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Month-to-month','One year','Two year']}\n",
    "\n",
    "      - name: PaperlessBilling\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No']}\n",
    "\n",
    "      - name: PaymentMethod\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              values: ['Electronic check','Mailed check','Bank transfer (automatic)','Credit card (automatic)']\n",
    "\n",
    "      - name: MonthlyCharges\n",
    "        data_type: float64\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_expectations.expect_column_values_to_be_between:\n",
    "              min_value: 0\n",
    "              max_value: 200\n",
    "              strictly: false\n",
    "\n",
    "      - name: TotalCharges\n",
    "        data_type: float64\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_expectations.expect_column_values_to_be_between:\n",
    "              min_value: 0\n",
    "              max_value: 10000   # generous upper bound\n",
    "              strictly: false\n",
    "\n",
    "      - name: Churn\n",
    "        data_type: string\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values: {values: ['Yes','No']}\n",
    "\n",
    "    tests:\n",
    "      # P0: tenure==0 => TotalCharges must be 0\n",
    "      - dbt_utils.expression_is_true:\n",
    "          expression: \"case when tenure = 0 then coalesce(TotalCharges,0) = 0 else true end\"\n",
    "\n",
    "      # P1: business-rule tolerance for tenure>0\n",
    "      - dbt_utils.expression_is_true:\n",
    "          expression: \"abs(coalesce(TotalCharges,0) - coalesce(tenure,0) * coalesce(MonthlyCharges,0)) <= 10\"\n",
    "          where: \"tenure > 0\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How to run\n",
    "\n",
    "```bash\n",
    "# From your dbt project root\n",
    "dbt deps\n",
    "dbt test -s stg_telco_customers\n",
    "```\n",
    "\n",
    "If your model name isn‚Äôt `stg_telco_customers`, just replace it in both files. Want me to generate the same for your **Gold** table (post-imputation/feature-engineered version) or add a couple of **freshness and row-count anomaly** checks?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121b942",
   "metadata": {},
   "source": [
    "># **freshness and row-count anomaly checks**\n",
    "\n",
    "here are drop-in **freshness** and **row-count anomaly** checks you can add to your Telco dbt project. I‚Äôll show three layers: (A) model-level freshness, (B) source freshness, and (C) row-count anomaly (static bounds + statistical anomalies).\n",
    "\n",
    "\n",
    "### 0) Add the package (one time)\n",
    "\n",
    "```yaml\n",
    "# packages.yml\n",
    "packages:\n",
    "  - package: dbt-labs/dbt_utils\n",
    "    version: [\">=1.1.1\", \"<2.0.0\"]\n",
    "  - package: calogica/dbt_expectations\n",
    "    version: [\">=0.10.3\", \"<1.0.0\"]\n",
    "  - package: elementary-data/elementary\n",
    "    version: [\">=0.14.0\", \"<1.0.0\"]\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```bash\n",
    "dbt deps\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Model-level freshness (dbt\\_utils)\n",
    "\n",
    "Assumes your staged model `stg_telco_customers` has a timestamp column (e.g., `ingested_at`). If you don‚Äôt have one, add it in your staging SQL (e.g., `CURRENT_TIMESTAMP()` at load).\n",
    "\n",
    "```yaml\n",
    "# models/telco/schema.yml (append under the same model block)\n",
    "models:\n",
    "  - name: stg_telco_customers\n",
    "    tests:\n",
    "      # Data should be no older than 24 hours\n",
    "      - dbt_utils.recency:\n",
    "          field: ingested_at\n",
    "          datepart: hour\n",
    "          interval: 24\n",
    "```\n",
    "\n",
    "This asserts the **max(ingested\\_at)** is within the last 24 hours. ([GitHub][1], [Elementary Data][2])\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Source freshness (built-in dbt ‚Äúsource freshness‚Äù)\n",
    "\n",
    "If you ingest from a raw/source table, define it with a `loaded_at_field` and SLAs:\n",
    "\n",
    "```yaml\n",
    "# models/telco/sources.yml\n",
    "version: 2\n",
    "sources:\n",
    "  - name: telco_raw\n",
    "    database: your_db\n",
    "    schema: your_schema\n",
    "    tables:\n",
    "      - name: telco_customers_raw\n",
    "        loaded_at_field: _ingested_at        # or your load timestamp col\n",
    "        freshness:\n",
    "          warn_after: {count: 1, period: day}\n",
    "          error_after: {count: 2, period: day}\n",
    "```\n",
    "\n",
    "Run it with:\n",
    "\n",
    "```bash\n",
    "dbt source freshness -s source:telco_raw\n",
    "```\n",
    "\n",
    "This is the standard way to gate stale sources in CI. ([Datafold][3])\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Row-count anomaly checks\n",
    "\n",
    "#### 3a) Simple guardrail (static bounds)\n",
    "\n",
    "Because the classic Telco CSV is \\~7,043 rows, put a conservative envelope around it (adjust if you‚Äôre appending/refreshing):\n",
    "\n",
    "```yaml\n",
    "# models/telco/schema.yml\n",
    "models:\n",
    "  - name: stg_telco_customers\n",
    "    tests:\n",
    "      - dbt_expectations.expect_table_row_count_to_be_between:\n",
    "          min_value: 6500\n",
    "          max_value: 8000\n",
    "```\n",
    "\n",
    "Great for catching truncation/duplication in a single run. ([Elementary Data][4], [Airbyte][5])\n",
    "\n",
    "#### 3b) Statistical anomalies over time (Elementary)\n",
    "\n",
    "Elementary learns typical row counts and flags spikes/drops using historical windows.\n",
    "\n",
    "```yaml\n",
    "# models/telco/schema.yml\n",
    "models:\n",
    "  - name: stg_telco_customers\n",
    "    tests:\n",
    "      - elementary.volume_anomalies:\n",
    "          timestamp_column: ingested_at      # if omitted, counts total rows per run\n",
    "          time_bucket:                       # daily buckets\n",
    "            period: day\n",
    "            count: 1\n",
    "          training_period:                   # learn from last 28 days\n",
    "            period: day\n",
    "            count: 28\n",
    "          detection_period:                  # check last 2 completed days\n",
    "            period: day\n",
    "            count: 2\n",
    "          anomaly_sensitivity: 3             # ~3 standard deviations\n",
    "          anomaly_direction: both            # flag drops and spikes\n",
    "          fail_on_zero: true                 # fail if volume suddenly hits zero\n",
    "```\n",
    "\n",
    "Elementary stores historical metrics in your warehouse and compares recent buckets to expected ranges‚Äîso you catch subtle changes, not just hard breaches. ([Elementary Data][6])\n",
    "\n",
    "> Optional: You can also add **freshness anomalies** the same way to detect delayed updates over time:\n",
    "\n",
    "```yaml\n",
    "- elementary.freshness_anomalies:\n",
    "    timestamp_column: ingested_at\n",
    "    time_bucket: { period: day, count: 1 }\n",
    "    training_period: { period: day, count: 28 }\n",
    "    detection_period:{ period: day, count: 2 }\n",
    "```\n",
    "\n",
    "([Elementary Data][7], [Elementary Data][8])\n",
    "\n",
    "---\n",
    "\n",
    "### How to run (summary)\n",
    "\n",
    "```bash\n",
    "dbt deps\n",
    "dbt test -s stg_telco_customers           # runs schema tests + Elementary tests\n",
    "dbt source freshness -s source:telco_raw  # runs source freshness\n",
    "```\n",
    "\n",
    "If you want, I can also add **null-rate/enum drift** anomaly checks (Elementary‚Äôs column/dimension anomalies) for fields like `InternetService`, `Contract`, and `Churn`. ([Elementary Data][9])\n",
    "\n",
    "[1]: https://github.com/dbt-labs/dbt-utils?utm_source=chatgpt.com \"dbt-labs/dbt-utils: Utility functions for dbt projects.\"\n",
    "[2]: https://www.elementary-data.com/dbt-tests/recency?utm_source=chatgpt.com \"dbt-utils.recency | Elementary dbt test hub\"\n",
    "[3]: https://www.datafold.com/blog/dbt-source-freshness?utm_source=chatgpt.com \"How to use dbt source freshness tests to detect stale data\"\n",
    "[4]: https://www.elementary-data.com/dbt-tests/expect-table-row-count-to-be-between?utm_source=chatgpt.com \"dbt test: expect_table_row_count_to_be_between\"\n",
    "[5]: https://airbyte.com/blog/testing-with-dbt-expectations?utm_source=chatgpt.com \"Testing Data Pipelines with dbt-expectations: A Beginner's ...\"\n",
    "[6]: https://docs.elementary-data.com/data-tests/anomaly-detection-tests/volume-anomalies?utm_source=chatgpt.com \"volume_anomalies - Elementary Data\"\n",
    "[7]: https://www.elementary-data.com/dbt-tests/freshness-anomalies?utm_source=chatgpt.com \"dbt test: freshness_anomalies\"\n",
    "[8]: https://docs.elementary-data.com/data-tests/anomaly-detection-tests/freshness-anomalies?utm_source=chatgpt.com \"freshness_anomalies - Elementary Data\"\n",
    "[9]: https://docs.elementary-data.com/data-tests/introduction?utm_source=chatgpt.com \"Elementary Data Tests\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
