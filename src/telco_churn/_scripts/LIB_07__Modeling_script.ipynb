{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc0fe5e",
   "metadata": {},
   "source": [
    "# Notebook: 03_baseline_model.ipynb\n",
    "## Building and Evaluating the Baseline Model\n",
    "\n",
    "```python\n",
    "# Cell 1: Setup and Import Custom Package\n",
    "\"\"\"\n",
    "Level 3 - Baseline Model Development\n",
    "=====================================\n",
    "This notebook uses our newly created telco_analysis package to build\n",
    "a baseline logistic regression model.\n",
    "\n",
    "Key Goals:\n",
    "1. Use extracted functions from our package\n",
    "2. Build complete sklearn pipeline\n",
    "3. Establish baseline performance metrics\n",
    "4. Save model for future use\n",
    "\n",
    "This represents our transition from monolithic notebooks to modular code!\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path so we can import our package\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom telco_analysis package - this is new for Level 3!\n",
    "from telco_analysis import (\n",
    "    load_data,\n",
    "    clean_data, \n",
    "    create_features,\n",
    "    split_data,\n",
    "    train_baseline,\n",
    "    evaluate_model,\n",
    "    save_pipeline\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ“ Custom telco_analysis package imported successfully!\")\n",
    "print(\"âœ“ All libraries loaded\")\n",
    "print(\"\\nThis notebook demonstrates the shift from repetitive code to reusable functions.\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 2: Load and Prepare Data Using Our Functions\n",
    "\"\"\"\n",
    "Using our extracted functions instead of repeating code\n",
    "Before Level 3: 20+ lines of data loading code\n",
    "After Level 3: 3 function calls\n",
    "\"\"\"\n",
    "\n",
    "print(\"Step 1: Loading data using our custom function...\")\n",
    "df = load_data('../data/raw/telco_customer_churn.csv')\n",
    "\n",
    "print(\"\\nStep 2: Cleaning data...\")\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "print(\"\\nStep 3: Creating engineered features...\")\n",
    "df_features = create_features(df_clean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Preparation Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final shape: {df_features.shape}\")\n",
    "print(f\"New features added: AvgChargesPerMonth, HasInternet, HasPhone, IsNewCustomer, IsElectronicCheck, IsMonthToMonth\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 3: Prepare Data for Modeling\n",
    "\"\"\"\n",
    "Convert data to modeling format and identify feature types\n",
    "\"\"\"\n",
    "\n",
    "from telco_analysis.preprocessing import prepare_for_modeling\n",
    "\n",
    "# Prepare features and target\n",
    "df_model, numeric_cols, categorical_cols = prepare_for_modeling(df_features)\n",
    "\n",
    "print(\"Data prepared for modeling:\")\n",
    "print(f\"  Total features: {len(df_model.columns) - 1}\")  # Minus target\n",
    "print(f\"  Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"  Target variable: Churn (0/1)\")\n",
    "\n",
    "print(\"\\nNumeric features:\")\n",
    "print(f\"  {', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''}\")\n",
    "\n",
    "print(\"\\nCategorical features:\")\n",
    "print(f\"  {', '.join(categorical_cols[:5])}{'...' if len(categorical_cols) > 5 else ''}\")\n",
    "\n",
    "# Quick target distribution check\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_model['Churn'].value_counts(normalize=True).to_dict())\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 4: Create Train-Test Split\n",
    "\"\"\"\n",
    "Split data with stratification to maintain class balance\n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating train-test split...\")\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    df_model,\n",
    "    target='Churn',\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train-Test Split Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set: {len(X_train):,} samples ({len(X_train)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"Test set:     {len(X_test):,} samples ({len(X_test)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution maintained:\")\n",
    "print(f\"  Overall churn rate:  {df_model['Churn'].mean():.2%}\")\n",
    "print(f\"  Train churn rate:    {y_train.mean():.2%}\")\n",
    "print(f\"  Test churn rate:     {y_test.mean():.2%}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 5: Train Baseline Logistic Regression Model\n",
    "\"\"\"\n",
    "Build and train our baseline model using the pipeline approach\n",
    "Why Logistic Regression for baseline?\n",
    "- Simple and interpretable\n",
    "- Fast training\n",
    "- Provides probability estimates\n",
    "- Good starting point for binary classification\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training baseline logistic regression model...\")\n",
    "print(\"This creates a full sklearn pipeline with:\")\n",
    "print(\"  1. Preprocessing (scaling + encoding)\")\n",
    "print(\"  2. Logistic Regression classifier\")\n",
    "print()\n",
    "\n",
    "# Train model using our function\n",
    "pipeline = train_baseline(X_train, y_train, numeric_cols, categorical_cols)\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "print(pipeline)\n",
    "\n",
    "# Get predictions for training set to check for overfitting\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "print(f\"\\nTraining accuracy: {train_score:.3f}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 6: Evaluate Model Performance\n",
    "\"\"\"\n",
    "Calculate comprehensive metrics for our baseline model\n",
    "\"\"\"\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "metrics = evaluate_model(pipeline, X_test, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a nice summary table\n",
    "metrics_df = pd.DataFrame([\n",
    "    ['Accuracy', metrics['accuracy'], '0.79-0.81', metrics['accuracy'] >= 0.79],\n",
    "    ['Precision', metrics['precision'], '0.65-0.70', metrics['precision'] >= 0.65],\n",
    "    ['Recall', metrics['recall'], '0.50-0.55', metrics['recall'] >= 0.50],\n",
    "    ['F1-Score', metrics['f1'], '0.58-0.62', metrics['f1'] >= 0.58],\n",
    "    ['ROC-AUC', metrics['roc_auc'], '0.84-0.86', metrics['roc_auc'] >= 0.84],\n",
    "], columns=['Metric', 'Score', 'Expected Range', 'Meets Baseline'])\n",
    "\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nâœ“ Baseline model successfully trained and evaluated!\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 7: Detailed Classification Report\n",
    "\"\"\"\n",
    "Get detailed per-class metrics\n",
    "\"\"\"\n",
    "\n",
    "# Get predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Balanced Accuracy: {balanced_acc:.3f}\")\n",
    "print(f\"  Matthews Correlation Coefficient: {mcc:.3f}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 8: Confusion Matrix Visualization\n",
    "\"\"\"\n",
    "Visualize model's classification performance\n",
    "\"\"\"\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            ax=ax1)\n",
    "ax1.set_title('Confusion Matrix - Counts')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_xlabel('Predicted')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            ax=ax2)\n",
    "ax2.set_title('Confusion Matrix - Percentages')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_xlabel('Predicted')\n",
    "\n",
    "plt.suptitle('Baseline Model - Confusion Matrix Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('../outputs/figures/03_confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"  True Negatives (Correctly predicted No Churn):  {tn:,}\")\n",
    "print(f\"  False Positives (Incorrectly predicted Churn):  {fp:,}\")\n",
    "print(f\"  False Negatives (Missed actual Churners):       {fn:,}\")\n",
    "print(f\"  True Positives (Correctly predicted Churn):     {tp:,}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 9: ROC Curve and AUC\n",
    "\"\"\"\n",
    "Evaluate model's ability to discriminate between classes\n",
    "\"\"\"\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'Baseline Model (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Baseline Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "plt.annotate(f'AUC = {roc_auc:.3f}', \n",
    "            xy=(0.5, 0.5), xytext=(0.6, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', alpha=0.5),\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('../outputs/figures/03_roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - AUC of {roc_auc:.3f} indicates {'good' if roc_auc > 0.8 else 'moderate'} discrimination ability\")\n",
    "print(f\"  - Model is {(roc_auc - 0.5) / 0.5 * 100:.1f}% better than random guessing\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 10: Precision-Recall Curve\n",
    "\"\"\"\n",
    "Important for imbalanced datasets like ours\n",
    "\"\"\"\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Baseline (random classifier) performance\n",
    "baseline_precision = y_test.mean()\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='#2ecc71', lw=2,\n",
    "         label=f'Baseline Model (AP = {pr_auc:.3f})')\n",
    "plt.axhline(y=baseline_precision, color='r', linestyle='--', \n",
    "            label=f'Random Classifier (AP = {baseline_precision:.3f})')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve - Baseline Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('../outputs/figures/03_precision_recall_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision Score: {pr_auc:.3f}\")\n",
    "print(f\"Baseline (random) Score: {baseline_precision:.3f}\")\n",
    "print(f\"\\nModel is {pr_auc/baseline_precision:.1f}x better than random classifier\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 11: Feature Importance Analysis\n",
    "\"\"\"\n",
    "Understand what features drive the model's predictions\n",
    "\"\"\"\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "def get_feature_names_from_pipeline(pipeline):\n",
    "    \"\"\"Extract feature names from the preprocessing pipeline.\"\"\"\n",
    "    # Get the preprocessor\n",
    "    preprocessor = pipeline.named_steps['preprocessor']\n",
    "    \n",
    "    # Get numeric feature names\n",
    "    numeric_features = numeric_cols\n",
    "    \n",
    "    # Get categorical feature names after encoding\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']\n",
    "    categorical_features = cat_encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = list(numeric_features) + list(categorical_features)\n",
    "    return all_features\n",
    "\n",
    "# Get feature names and coefficients\n",
    "feature_names = get_feature_names_from_pipeline(pipeline)\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Plot top 15 most important features\n",
    "top_15 = importance_df.head(15).sort_values('coefficient')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red' if x < 0 else 'green' for x in top_15['coefficient']]\n",
    "plt.barh(range(len(top_15)), top_15['coefficient'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_15)), top_15['feature'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "red_patch = Patch(color='red', alpha=0.7, label='Increases Churn Probability')\n",
    "green_patch = Patch(color='green', alpha=0.7, label='Decreases Churn Probability')\n",
    "plt.legend(handles=[red_patch, green_patch], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/03_feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Features Increasing Churn Probability:\")\n",
    "for idx, row in importance_df[importance_df['coefficient'] > 0].head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['coefficient']:.3f}\")\n",
    "\n",
    "print(\"\\nTop 5 Features Decreasing Churn Probability:\")\n",
    "for idx, row in importance_df[importance_df['coefficient'] < 0].head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['coefficient']:.3f}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 12: Prediction Distribution Analysis\n",
    "\"\"\"\n",
    "Analyze the distribution of prediction probabilities\n",
    "\"\"\"\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Overall distribution\n",
    "axes[0].hist(y_proba, bins=30, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of All Predictions')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Distribution by actual class\n",
    "axes[1].hist(y_proba[y_test==0], bins=30, alpha=0.5, label='No Churn (Actual)', color='green')\n",
    "axes[1].hist(y_proba[y_test==1], bins=30, alpha=0.5, label='Churn (Actual)', color='red')\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Predictions by Actual Class')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Calibration plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "fraction_pos, mean_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "axes[2].plot(mean_pred, fraction_pos, 's-', label='Baseline Model', color='#e74c3c')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "axes[2].set_xlabel('Mean Predicted Probability')\n",
    "axes[2].set_ylabel('Fraction of Positives')\n",
    "axes[2].set_title('Calibration Plot')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction Probability Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/03_prediction_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Prediction Statistics:\")\n",
    "print(f\"  Mean probability: {y_proba.mean():.3f}\")\n",
    "print(f\"  Std deviation: {y_proba.std():.3f}\")\n",
    "print(f\"  Min probability: {y_proba.min():.3f}\")\n",
    "print(f\"  Max probability: {y_proba.max():.3f}\")\n",
    "print(f\"\\n  Predictions > 0.5: {(y_proba > 0.5).sum()} ({(y_proba > 0.5).mean():.1%})\")\n",
    "print(f\"  Predictions < 0.5: {(y_proba < 0.5).sum()} ({(y_proba < 0.5).mean():.1%})\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 13: Save Model and Metrics\n",
    "\"\"\"\n",
    "Persist our trained model and performance metrics\n",
    "\"\"\"\n",
    "\n",
    "print(\"Saving model artifacts...\")\n",
    "\n",
    "# Save the pipeline\n",
    "save_pipeline(\n",
    "    pipeline, \n",
    "    metrics,\n",
    "    model_path='../models/baseline_logistic.pkl',\n",
    "    metrics_path='../models/model_metrics.json'\n",
    ")\n",
    "\n",
    "# Create additional metadata\n",
    "metadata = {\n",
    "    'model_type': 'Logistic Regression',\n",
    "    'pipeline_steps': [\n",
    "        'StandardScaler for numeric features',\n",
    "        'OneHotEncoder for categorical features',\n",
    "        'LogisticRegression classifier'\n",
    "    ],\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'numeric_features': numeric_cols,\n",
    "    'categorical_features': categorical_cols,\n",
    "    'performance': metrics\n",
    "}\n",
    "\n",
    "# Save extended metadata\n",
    "with open('../models/model_metadata_extended.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved to: models/baseline_logistic.pkl\")\n",
    "print(f\"âœ“ Metrics saved to: models/model_metrics.json\")\n",
    "print(f\"âœ“ Extended metadata saved to: models/model_metadata_extended.json\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 14: Test Model Loading and Prediction\n",
    "\"\"\"\n",
    "Verify that we can reload and use the saved model\n",
    "\"\"\"\n",
    "\n",
    "from telco_analysis.modeling import load_pipeline\n",
    "\n",
    "print(\"Testing model loading and prediction...\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_pipeline = load_pipeline('../models/baseline_logistic.pkl')\n",
    "\n",
    "# Make predictions on a few test samples\n",
    "sample_size = 5\n",
    "X_sample = X_test.iloc[:sample_size]\n",
    "y_sample = y_test.iloc[:sample_size]\n",
    "\n",
    "# Get predictions\n",
    "predictions = loaded_pipeline.predict(X_sample)\n",
    "probabilities = loaded_pipeline.predict_proba(X_sample)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_sample.values,\n",
    "    'Predicted': predictions,\n",
    "    'Prob_No_Churn': probabilities[:, 0],\n",
    "    'Prob_Churn': probabilities[:, 1]\n",
    "})\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nâœ“ Model successfully loaded and making predictions!\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 15: Generate Final Report\n",
    "\"\"\"\n",
    "Create a summary report of our baseline model development\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "Project: Telco Customer Churn Prediction\n",
    "Level: 3 - Baseline Model with Modular Code\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "DATA SUMMARY:\n",
    "- Total samples: {len(df_model):,}\n",
    "- Features: {len(X_train.columns)}\n",
    "- Target: Binary (Churn: Yes/No)\n",
    "- Class distribution: {y_train.value_counts(normalize=True).to_dict()}\n",
    "\n",
    "MODEL ARCHITECTURE:\n",
    "- Preprocessing: StandardScaler + OneHotEncoder\n",
    "- Algorithm: Logistic Regression\n",
    "- Pipeline: Sklearn Pipeline with preprocessing\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "- Accuracy:  {metrics['accuracy']:.3f}\n",
    "- Precision: {metrics['precision']:.3f}\n",
    "- Recall:    {metrics['recall']:.3f}\n",
    "- F1-Score:  {metrics['f1']:.3f}\n",
    "- ROC-AUC:   {metrics['roc_auc']:.3f}\n",
    "\n",
    "KEY INSIGHTS:\n",
    "1. Model achieves {metrics['roc_auc']:.3f} ROC-AUC, indicating good discrimination\n",
    "2. Recall of {metrics['recall']:.3f} means we catch ~{metrics['recall']*100:.0f}% of actual churners\n",
    "3. Top churn indicators: Contract type, Payment method, Tenure\n",
    "\n",
    "CODE IMPROVEMENTS (Level 3 Achievement):\n",
    "âœ“ Extracted repetitive code into reusable functions\n",
    "âœ“ Created telco_analysis package with modular structure\n",
    "âœ“ Implemented complete sklearn pipeline\n",
    "âœ“ Established reproducible workflow\n",
    "\n",
    "NEXT STEPS (Level 4):\n",
    "- Feature selection and engineering optimization\n",
    "- Cross-validation for robust evaluation\n",
    "- Model comparison (Random Forest, XGBoost)\n",
    "- Hyperparameter tuning\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save report to file\n",
    "with open('../outputs/reports/baseline_model_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nâœ“ Report saved to: outputs/reports/baseline_model_summary.txt\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Cell 16: Level 3 Completion Checklist\n",
    "\"\"\"\n",
    "Verify that all Level 3 objectives have been met\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LEVEL 3 COMPLETION CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = {\n",
    "    \"Code Organization\": {\n",
    "        \"Created src/telco_analysis package\": True,\n",
    "        \"Extracted functions from notebooks\": True,\n",
    "        \"Implemented proper imports\": True,\n",
    "        \"Functions have docstrings\": True\n",
    "    },\n",
    "    \"Baseline Model\": {\n",
    "        \"Built sklearn pipeline\": True,\n",
    "        \"Preprocessing handles mixed types\": True,\n",
    "        \"Trained logistic regression\": True,\n",
    "        \"Evaluated performance\": True\n",
    "    },\n",
    "    \"Model Persistence\": {\n",
    "        \"Saved trained pipeline\": True,\n",
    "        \"Saved performance metrics\": True,\n",
    "        \"Can reload and predict\": True\n",
    "    },\n",
    "    \"Visualizations\": {\n",
    "        \"Confusion matrix\": True,\n",
    "        \"ROC curve\": True,\n",
    "        \"Precision-Recall curve\": True,\n",
    "        \"Feature importance\": True,\n",
    "        \"Prediction distribution\": True\n",
    "    },\n",
    "    \"Documentation\": {\n",
    "        \"Code is well-commented\": True,\n",
    "        \"Decisions are explained\": True,\n",
    "        \"Results are interpreted\": True,\n",
    "        \"Report generated\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, items in checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item, status in items.items():\n",
    "        status_symbol = \"âœ“\" if status else \"âœ—\"\n",
    "        print(f\"  {status_symbol} {item}\")\n",
    "\n",
    "all_complete = all(all(items.values()) for items in checklist.values())\n",
    "if all_complete:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ‰ CONGRATULATIONS! Level 3 Complete! ðŸŽ‰\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nYou have successfully:\")\n",
    "    print(\"1. Transitioned from monolithic to modular code\")\n",
    "    print(\"2. Created your first Python package\")\n",
    "    print(\"3. Built a complete ML pipeline\")\n",
    "    print(\"4. Established baseline performance metrics\")\n",
    "    print(\"\\nReady to proceed to Level 4: Advanced Model Development!\")\n",
    "else:\n",
    "    print(\"\\nâš  Some items are incomplete. Review and complete before proceeding.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28449dc4",
   "metadata": {},
   "source": [
    "># Q: Should I need to clean the data again?\n",
    "\n",
    "Great question! This is an important architectural decision for Level 3. Let me explain the best approach:\n",
    "\n",
    "## ðŸŽ¯ **For Level 3: Use RAW Data**\n",
    "\n",
    "You should load from the **original raw CSV** file, not the processed parquet. Here's why:\n",
    "\n",
    "### **Why Start from Raw Data in Level 3:**\n",
    "\n",
    "1. **Demonstrates Your Functions Work End-to-End**\n",
    "   ```python\n",
    "   # This shows the complete pipeline:\n",
    "   df = load_data('../data/raw/telco_customer_churn.csv')  # RAW\n",
    "   df_clean = clean_data(df)                               # Your cleaning function\n",
    "   df_features = create_features(df_clean)                 # Your feature function\n",
    "   ```\n",
    "   This proves your functions can handle the entire workflow.\n",
    "\n",
    "2. **Tests Your Data Pipeline**\n",
    "   - Verifies `clean_data()` handles the TotalCharges issue\n",
    "   - Confirms `create_features()` works on cleaned data\n",
    "   - Ensures reproducibility from raw source\n",
    "\n",
    "3. **Learning Objective of Level 3**\n",
    "   - Level 3 is about extracting repetitive code into functions\n",
    "   - Using raw data shows these functions replace your previous manual steps\n",
    "   - Demonstrates the value of modularization\n",
    "\n",
    "### **Here's the Correct Flow:**\n",
    "\n",
    "```python\n",
    "# Cell 2: What you should have\n",
    "print(\"Step 1: Loading RAW data using our custom function...\")\n",
    "df = load_data('../data/raw/telco_customer_churn.csv')  # âœ… RAW data\n",
    "\n",
    "print(\"\\nStep 2: Cleaning data (handles TotalCharges, etc.)...\")\n",
    "df_clean = clean_data(df)  # âœ… Your function handles all cleaning\n",
    "\n",
    "print(\"\\nStep 3: Creating engineered features...\")\n",
    "df_features = create_features(df_clean)  # âœ… Your function adds features\n",
    "```\n",
    "\n",
    "### **When to Use Processed Data:**\n",
    "\n",
    "**Level 4-5**: You might start loading processed data for:\n",
    "- Cross-validation experiments\n",
    "- Hyperparameter tuning\n",
    "- Model comparison\n",
    "\n",
    "**Level 6+**: Definitely use processed data for:\n",
    "- Production pipelines\n",
    "- API endpoints\n",
    "- Real-time scoring\n",
    "\n",
    "### **If You Want to Support Both:**\n",
    "\n",
    "You could enhance your `load_data()` function to handle both:\n",
    "\n",
    "```python\n",
    "def load_data(filepath='data/raw/telco_customer_churn.csv', processed=False):\n",
    "    \"\"\"\n",
    "    Load telco churn data from raw or processed source.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to data file\n",
    "    processed : bool\n",
    "        If True, expects pre-processed parquet file\n",
    "        If False, expects raw CSV (default)\n",
    "    \"\"\"\n",
    "    if processed:\n",
    "        df = pd.read_parquet(filepath)\n",
    "        print(f\"Loaded processed data: {df.shape}\")\n",
    "    else:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded raw data: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "### **But for Level 3 Specifically:**\n",
    "\n",
    "```python\n",
    "# Your notebook should demonstrate the full pipeline\n",
    "# This is the learning objective - showing modular functions work together\n",
    "\n",
    "# âŒ Don't do this in Level 3:\n",
    "df = pd.read_parquet('/processed/telco_cleaned.parquet')\n",
    "# (skips your cleaning functions)\n",
    "\n",
    "# âœ… Do this in Level 3:\n",
    "df = load_data('../data/raw/telco_customer_churn.csv')\n",
    "df_clean = clean_data(df)\n",
    "df_features = create_features(df_clean)\n",
    "# (demonstrates your complete pipeline)\n",
    "```\n",
    "\n",
    "### **File Path Note:**\n",
    "\n",
    "Make sure your path is correct relative to your notebook location:\n",
    "```python\n",
    "# If your structure is:\n",
    "# telco_churn_level3/\n",
    "#   â”œâ”€â”€ notebooks/\n",
    "#   â”‚   â””â”€â”€ 03_baseline_model.ipynb  (you are here)\n",
    "#   â””â”€â”€ data/\n",
    "#       â””â”€â”€ raw/\n",
    "#           â””â”€â”€ telco_customer_churn.csv\n",
    "\n",
    "# Then use:\n",
    "df = load_data('../data/raw/telco_customer_churn.csv')\n",
    "```\n",
    "\n",
    "## ðŸ“‹ **Summary**\n",
    "\n",
    "**For Level 3**: Use **raw data** to demonstrate your functions work end-to-end\n",
    "**Why**: Shows the value of your modularization and tests your complete pipeline\n",
    "**Later levels**: Can start from processed data for efficiency\n",
    "\n",
    "This approach best demonstrates the Level 3 learning objective: transforming repetitive notebook code into reusable, modular functions that handle the complete workflow!\n",
    "\n",
    "---\n",
    "\n",
    "># Q: Yes, but we already cleaned it in the EDA notebook, didnt we?\n",
    "\n",
    "> (Windsurf Suggestion) No, we cleaned it in the EDA notebook, but we need to clean it again in the baseline model notebook. This is because the EDA notebook is not part of the baseline model notebook. The EDA notebook is just for exploration and analysis, while the baseline model notebook is for building and evaluating the model. \n",
    "\n",
    "You're absolutely right! This is a **crucial insight** about the Level 3 workflow. Let me clarify the approach:\n",
    "\n",
    "## ðŸŽ¯ **You Have Two Valid Options**\n",
    "\n",
    "### **Option 1: Demonstrate Full Pipeline (Current Approach)**\n",
    "```python\n",
    "# In 03_baseline_model.ipynb\n",
    "df = load_data('../data/raw/telco_customer_churn.csv')  # Start fresh\n",
    "df_clean = clean_data(df)  # Re-clean (even though done in 01_eda)\n",
    "df_features = create_features(df_clean)  # Add features\n",
    "```\n",
    "\n",
    "**Why this makes sense:**\n",
    "- Shows your functions work independently\n",
    "- Tests the complete pipeline\n",
    "- Demonstrates reproducibility\n",
    "- Someone could run JUST this notebook\n",
    "\n",
    "### **Option 2: Use Previously Processed Data (More Realistic)**\n",
    "```python\n",
    "# In 03_baseline_model.ipynb\n",
    "# Load the cleaned data from notebook 02\n",
    "df = pd.read_csv('../data/processed/data_with_features.csv')  # From notebook 02\n",
    "\n",
    "# Or if you saved after notebook 01:\n",
    "df_clean = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "df_features = create_features(df_clean)  # Only do the new work\n",
    "```\n",
    "\n",
    "**Why this makes sense:**\n",
    "- Reflects real workflow (don't repeat work)\n",
    "- Saves computation time\n",
    "- Shows progression through notebooks\n",
    "- More efficient\n",
    "\n",
    "## ðŸ’¡ **The Real Level 3 Pattern**\n",
    "\n",
    "You're highlighting an important point: **Level 3 is transitional**. Here's what's really happening:\n",
    "\n",
    "### **Current State (Level 3):**\n",
    "```python\n",
    "# Notebook 01: EDA\n",
    "df = pd.read_csv('raw_data.csv')\n",
    "# ... manual cleaning ...\n",
    "df.to_csv('processed/cleaned.csv')\n",
    "\n",
    "# Notebook 02: Features  \n",
    "df = pd.read_csv('processed/cleaned.csv')\n",
    "# ... manual feature engineering ...\n",
    "df.to_csv('processed/with_features.csv')\n",
    "\n",
    "# Notebook 03: Model (TWO APPROACHES)\n",
    "\n",
    "# Approach A: Use saved work\n",
    "df = pd.read_csv('processed/with_features.csv')  # Use previous work\n",
    "# Jump straight to modeling\n",
    "\n",
    "# Approach B: Test your new functions\n",
    "df = load_data('raw_data.csv')  # Start fresh to test functions\n",
    "df_clean = clean_data(df)  # Test your extracted function\n",
    "df_features = create_features(df_clean)  # Test this function too\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ **Recommended Approach for Level 3**\n",
    "\n",
    "Since Level 3 is about **learning function extraction**, I suggest a **hybrid approach**:\n",
    "\n",
    "```python\n",
    "# Cell 2: Modified approach\n",
    "\"\"\"\n",
    "Demonstrating our extracted functions work correctly\n",
    "by comparing with our previous manual work\n",
    "\"\"\"\n",
    "\n",
    "# Option A: Load and process fresh to test functions\n",
    "print(\"Testing complete pipeline with extracted functions...\")\n",
    "df_raw = load_data('../data/raw/telco_customer_churn.csv')\n",
    "df_clean_new = clean_data(df_raw)\n",
    "df_features_new = create_features(df_clean_new)\n",
    "\n",
    "# Option B: Load previously processed data for comparison\n",
    "print(\"\\nLoading previously processed data for validation...\")\n",
    "df_features_saved = pd.read_csv('../data/processed/data_with_features.csv')\n",
    "\n",
    "# Verify they match (proving your functions work)\n",
    "print(f\"\\nShapes match: {df_features_new.shape == df_features_saved.shape}\")\n",
    "print(f\"Columns match: {set(df_features_new.columns) == set(df_features_saved.columns)}\")\n",
    "\n",
    "# Use the newly processed data (to prove the pipeline works)\n",
    "df_features = df_features_new\n",
    "print(\"\\nâœ… Functions validated - using freshly processed data\")\n",
    "```\n",
    "\n",
    "## ðŸ“ **Or Even Simpler:**\n",
    "\n",
    "Add a parameter to control this:\n",
    "\n",
    "```python\n",
    "# Cell 1.5: Configuration\n",
    "USE_SAVED_DATA = False  # Set to True to skip reprocessing\n",
    "\n",
    "# Cell 2: Load Data\n",
    "if USE_SAVED_DATA:\n",
    "    print(\"Loading pre-processed data from notebook 02...\")\n",
    "    df_features = pd.read_csv('../data/processed/data_with_features.csv')\n",
    "    print(\"âœ… Loaded processed data, skipping cleaning/feature steps\")\n",
    "else:\n",
    "    print(\"Running full pipeline with extracted functions...\")\n",
    "    df = load_data('../data/raw/telco_customer_churn.csv')\n",
    "    df_clean = clean_data(df)\n",
    "    df_features = create_features(df_clean)\n",
    "    print(\"âœ… Full pipeline complete\")\n",
    "```\n",
    "\n",
    "## ðŸŽ“ **Learning Point**\n",
    "\n",
    "This confusion actually demonstrates growth! You're recognizing that:\n",
    "\n",
    "1. **Real projects don't repeat work unnecessarily**\n",
    "2. **There's tension between \"demonstrating functions\" and \"efficient workflow\"**\n",
    "3. **Level 3 is a bridge between notebook-style and production-style**\n",
    "\n",
    "**For your report/portfolio**, you could mention:\n",
    "> \"In Level 3, I implemented both approaches: testing the full pipeline from raw data to validate my extracted functions, and loading pre-processed data for efficiency. This mirrors real-world scenarios where development requires validation but production prioritizes efficiency.\"\n",
    "\n",
    "## âœ… **Final Recommendation**\n",
    "\n",
    "For **learning purposes** in Level 3: Run the full pipeline at least once to prove your functions work\n",
    "For **efficiency** after that: Use saved processed data\n",
    "\n",
    "This shows you understand both the importance of testing AND practical workflow efficiency!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fa2e4",
   "metadata": {},
   "source": [
    "# Notebook: 03_baseline_model.ipynb\n",
    "# Building and Evaluating the Baseline Model\n",
    "# =========================\n",
    "# Setup and Import Custom Package\n",
    "# =========================\n",
    "\n",
    "\"\"\"\n",
    "Level 3 - Baseline Model Development\n",
    "=====================================\n",
    "This notebook uses our modular telco_analysis package to build\n",
    "a baseline logistic regression model.\n",
    "\n",
    "Key Goals:\n",
    "1. Use extracted functions from our package\n",
    "2. Build complete sklearn pipeline\n",
    "3. Establish baseline performance metrics\n",
    "4. Save model for future use\n",
    "\n",
    "This represents our transition from monolithic notebooks to modular code!\n",
    "\"\"\"\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "\n",
    "# Add src to path so we can import our package\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom telco_analysis package - this is new for Level 3!\n",
    "from telco_analysis import (\n",
    "    load_data,\n",
    "    clean_data,\n",
    "    create_features,\n",
    "    split_data,\n",
    "    train_baseline,\n",
    "    evaluate_model,\n",
    "    save_pipeline\n",
    ")\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ“ Custom telco_analysis package imported successfully!\")\n",
    "print(\"âœ“ All libraries loaded\")\n",
    "print(\"\\nThis notebook demonstrates the shift from repetitive code to reusable functions.\")\n",
    "\n",
    "# ================================\n",
    "# Prepare Directories/Files\n",
    "# ================================\n",
    "\n",
    "\"\"\"\n",
    "Ensure project directories exist and data file is available.\n",
    "\"\"\"\n",
    "# Directories to create\n",
    "dirs = [\"../figures\", \"../reports\", \"../models\", \"../data/raw\", \"../data/processed\"]\n",
    "for path in dirs:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "print(\"âœ“ Verified/created all project directories\")\n",
    "\n",
    "# Define file paths\n",
    "data_primary = '../data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "source_file = \"../../resources/data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "# Copy file if not already in project folder\n",
    "if not os.path.exists(data_primary):\n",
    "    if os.path.exists(source_file):\n",
    "        shutil.copy(source_file, data_primary)\n",
    "        print(f\"âœ“ Copied data file to {data_primary}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Source file not found at {source_file}\")\n",
    "\n",
    "# Optional: list files in raw folder\n",
    "print(f\"\\nFiles in ../data/raw/: {os.listdir('../data/raw/')}\\n\")\n",
    "\n",
    "# ================================\n",
    "# Cell 3: Load, Clean, and Feature Engineer\n",
    "# ================================\n",
    "\"\"\"\n",
    "Load data using our modular functions, clean it, and create features.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Step 1: Loading data...\")\n",
    "df = load_data(data_primary)\n",
    "\n",
    "print(\"Step 2: Cleaning data...\")\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "print(\"Step 3: Creating engineered features...\")\n",
    "df_features = create_features(df_clean)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Preparation Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final shape: {df_features.shape}\")\n",
    "print(f\"New features added: AvgChargesPerMonth, HasInternet, HasPhone, IsNewCustomer, IsElectronicCheck, IsMonthToMonth\")\n",
    "\n",
    "# print(f\"\\nâœ“ Data ready: {df_features.shape}\")\n",
    "# print(\"Sample of new features:\")\n",
    "# print(df_features.head(3))\n",
    "\n",
    "# # Optional: quick sanity check\n",
    "# if (df_clean['tenure'] == 0).any():\n",
    "#     print(\"\\nNote: Some customers have tenure = 0\")\n",
    "# Inspect cleaned data\n",
    "df_clean.info()\n",
    "# Check TotalCharges = 0 where tenure == 0\n",
    "df_clean.loc[df_clean['tenure'] == 0, ['tenure', 'MonthlyCharges', 'TotalCharges']]\n",
    "# Cell 3: Prepare Data for Modeling\n",
    "\"\"\"\n",
    "Convert data to modeling format and identify feature types\n",
    "\"\"\"\n",
    "\n",
    "from telco_analysis.preprocessing import prepare_for_modeling\n",
    "\n",
    "# Prepare features and target\n",
    "df_model, numeric_cols, categorical_cols = prepare_for_modeling(df_features)\n",
    "\n",
    "print(\"Data prepared for modeling:\")\n",
    "print(f\"  Total features: {len(df_model.columns) - 1}\")  # Minus target\n",
    "print(f\"  Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"  Target variable: Churn (0/1)\")\n",
    "\n",
    "print(\"\\nNumeric features:\")\n",
    "print(f\"  {', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''}\")\n",
    "\n",
    "print(\"\\nCategorical features:\")\n",
    "print(f\"  {', '.join(categorical_cols[:5])}{'...' if len(categorical_cols) > 5 else ''}\")\n",
    "\n",
    "# Quick target distribution check\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_model['Churn'].value_counts(normalize=True).to_dict())\n",
    "\n",
    "# Cell 4: Create Train-Test Split\n",
    "\"\"\"\n",
    "Split data with stratification to maintain class balance\n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating train-test split...\")\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    df_model,\n",
    "    target='Churn',\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train-Test Split Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set: {len(X_train):,} samples ({len(X_train)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"Test set:     {len(X_test):,} samples ({len(X_test)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution maintained:\")\n",
    "print(f\"  Overall churn rate:  {df_model['Churn'].mean():.2%}\")\n",
    "print(f\"  Train churn rate:    {y_train.mean():.2%}\")\n",
    "print(f\"  Test churn rate:     {y_test.mean():.2%}\")\n",
    "\n",
    "# Cell 5: Train Baseline Logistic Regression Model\n",
    "\"\"\"\n",
    "Build and train our baseline model using the pipeline approach\n",
    "Why Logistic Regression for baseline?\n",
    "- Simple and interpretable\n",
    "- Fast training\n",
    "- Provides probability estimates\n",
    "- Good starting point for binary classification\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training baseline logistic regression model...\")\n",
    "print(\"This creates a full sklearn pipeline with:\")\n",
    "print(\"  1. Preprocessing (scaling + encoding)\")\n",
    "print(\"  2. Logistic Regression classifier\")\n",
    "print()\n",
    "\n",
    "# Train model using our function\n",
    "pipeline = train_baseline(X_train, y_train, numeric_cols, categorical_cols)\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "print(pipeline)\n",
    "\n",
    "# Get predictions for training set to check for overfitting\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "print(f\"\\nTraining accuracy: {train_score:.3f}\")\n",
    "\n",
    "\"\"\"\n",
    "Calculate comprehensive metrics for our baseline model\n",
    "Evaluate Model Performance\n",
    "\"\"\"\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "metrics = evaluate_model(pipeline, X_test, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a nice summary table\n",
    "metrics_df = pd.DataFrame([\n",
    "    ['Accuracy', metrics['accuracy'], '0.79-0.81', metrics['accuracy'] >= 0.79],\n",
    "    ['Precision', metrics['precision'], '0.65-0.70', metrics['precision'] >= 0.65],\n",
    "    ['Recall', metrics['recall'], '0.50-0.55', metrics['recall'] >= 0.50],\n",
    "    ['F1-Score', metrics['f1'], '0.58-0.62', metrics['f1'] >= 0.58],\n",
    "    ['ROC-AUC', metrics['roc_auc'], '0.84-0.86', metrics['roc_auc'] >= 0.84],\n",
    "], columns=['Metric', 'Score', 'Expected Range', 'Meets Baseline'])\n",
    "\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nâœ“ Baseline model successfully trained and evaluated!\")\n",
    "\n",
    "\"\"\"\n",
    "Detailed Classification Report\n",
    "Get detailed per-class metrics\n",
    "\"\"\"\n",
    "\n",
    "# Get predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Balanced Accuracy: {balanced_acc:.3f}\")\n",
    "print(f\"  Matthews Correlation Coefficient: {mcc:.3f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Confusion Matrix Visualization\n",
    "Visualize model's classification performance\n",
    "\"\"\"\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            ax=ax1)\n",
    "ax1.set_title('Confusion Matrix - Counts')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_xlabel('Predicted')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            ax=ax2)\n",
    "ax2.set_title('Confusion Matrix - Percentages')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_xlabel('Predicted')\n",
    "\n",
    "plt.suptitle('Baseline Model - Confusion Matrix Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('../figures/03_confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"  True Negatives (Correctly predicted No Churn):  {tn:,}\")\n",
    "print(f\"  False Positives (Incorrectly predicted Churn):  {fp:,}\")\n",
    "print(f\"  False Negatives (Missed actual Churners):       {fn:,}\")\n",
    "print(f\"  True Positives (Correctly predicted Churn):     {tp:,}\")\n",
    "\n",
    "\"\"\"\n",
    "Evaluate model's ability to discriminate between classes:\n",
    "ROC Curve and AUC\n",
    "\"\"\"\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'Baseline Model (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Baseline Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "plt.annotate(f'AUC = {roc_auc:.3f}', \n",
    "            xy=(0.5, 0.5), xytext=(0.6, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', alpha=0.5),\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('../figures/03_roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - AUC of {roc_auc:.3f} indicates {'good' if roc_auc > 0.8 else 'moderate'} discrimination ability\")\n",
    "print(f\"  - Model is {(roc_auc - 0.5) / 0.5 * 100:.1f}% better than random guessing\")\n",
    "\n",
    "\n",
    "# Cell 10: Precision-Recall Curve\n",
    "\"\"\"\n",
    "Important for imbalanced datasets like ours\n",
    "\"\"\"\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Baseline (random classifier) performance\n",
    "baseline_precision = y_test.mean()\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='#2ecc71', lw=2,\n",
    "         label=f'Baseline Model (AP = {pr_auc:.3f})')\n",
    "plt.axhline(y=baseline_precision, color='r', linestyle='--', \n",
    "            label=f'Random Classifier (AP = {baseline_precision:.3f})')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve - Baseline Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('../figures/03_precision_recall_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision Score: {pr_auc:.3f}\")\n",
    "print(f\"Baseline (random) Score: {baseline_precision:.3f}\")\n",
    "print(f\"\\nModel is {pr_auc/baseline_precision:.1f}x better than random classifier\")\n",
    "\n",
    "\"\"\"\n",
    "Feature Importance Analysis:\n",
    "Understand what features drive the model's predictions\n",
    "\"\"\"\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "def get_feature_names_from_pipeline(pipeline):\n",
    "    \"\"\"Extract feature names from the preprocessing pipeline.\"\"\"\n",
    "    # Get the preprocessor\n",
    "    preprocessor = pipeline.named_steps['preprocessor']\n",
    "    \n",
    "    # Get numeric feature names\n",
    "    numeric_features = numeric_cols\n",
    "    \n",
    "    # Get categorical feature names after encoding\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']\n",
    "    categorical_features = cat_encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = list(numeric_features) + list(categorical_features)\n",
    "    return all_features\n",
    "\n",
    "# Get feature names and coefficients\n",
    "feature_names = get_feature_names_from_pipeline(pipeline)\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Plot top 15 most important features\n",
    "top_15 = importance_df.head(15).sort_values('coefficient')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red' if x < 0 else 'green' for x in top_15['coefficient']]\n",
    "plt.barh(range(len(top_15)), top_15['coefficient'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_15)), top_15['feature'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "red_patch = Patch(color='red', alpha=0.7, label='Increases Churn Probability')\n",
    "green_patch = Patch(color='green', alpha=0.7, label='Decreases Churn Probability')\n",
    "plt.legend(handles=[red_patch, green_patch], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/03_feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Features Increasing Churn Probability:\")\n",
    "for idx, row in importance_df[importance_df['coefficient'] > 0].head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['coefficient']:.3f}\")\n",
    "\n",
    "print(\"\\nTop 5 Features Decreasing Churn Probability:\")\n",
    "for idx, row in importance_df[importance_df['coefficient'] < 0].head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['coefficient']:.3f}\")\n",
    "\n",
    "\"\"\"\n",
    "Prediction Distribution Analysis:\n",
    "Analyze the distribution of prediction probabilities\n",
    "\"\"\"\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Overall distribution\n",
    "axes[0].hist(y_proba, bins=30, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of All Predictions')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Distribution by actual class\n",
    "axes[1].hist(y_proba[y_test==0], bins=30, alpha=0.5, label='No Churn (Actual)', color='green')\n",
    "axes[1].hist(y_proba[y_test==1], bins=30, alpha=0.5, label='Churn (Actual)', color='red')\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Predictions by Actual Class')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Calibration plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "fraction_pos, mean_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "axes[2].plot(mean_pred, fraction_pos, 's-', label='Baseline Model', color='#e74c3c')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "axes[2].set_xlabel('Mean Predicted Probability')\n",
    "axes[2].set_ylabel('Fraction of Positives')\n",
    "axes[2].set_title('Calibration Plot')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction Probability Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/03_prediction_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Prediction Statistics:\")\n",
    "print(f\"  Mean probability: {y_proba.mean():.3f}\")\n",
    "print(f\"  Std deviation: {y_proba.std():.3f}\")\n",
    "print(f\"  Min probability: {y_proba.min():.3f}\")\n",
    "print(f\"  Max probability: {y_proba.max():.3f}\")\n",
    "print(f\"\\n  Predictions > 0.5: {(y_proba > 0.5).sum()} ({(y_proba > 0.5).mean():.1%})\")\n",
    "print(f\"  Predictions < 0.5: {(y_proba < 0.5).sum()} ({(y_proba < 0.5).mean():.1%})\")\n",
    "\n",
    "\"\"\"\n",
    "Save Model and Metrics:Persist our trained model and performance metrics\n",
    "\"\"\"\n",
    "\n",
    "print(\"Saving model artifacts...\")\n",
    "\n",
    "# Save the pipeline\n",
    "save_pipeline(\n",
    "    pipeline, \n",
    "    metrics,\n",
    "    model_path='../models/baseline_logistic.pkl',\n",
    "    metrics_path='../models/model_metrics.json'\n",
    ")\n",
    "\n",
    "# Create additional metadata\n",
    "metadata = {\n",
    "    'model_type': 'Logistic Regression',\n",
    "    'pipeline_steps': [\n",
    "        'StandardScaler for numeric features',\n",
    "        'OneHotEncoder for categorical features',\n",
    "        'LogisticRegression classifier'\n",
    "    ],\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'numeric_features': numeric_cols,\n",
    "    'categorical_features': categorical_cols,\n",
    "    'performance': metrics\n",
    "}\n",
    "\n",
    "# Save extended metadata\n",
    "with open('../models/model_metadata_extended.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved to: models/baseline_logistic.pkl\")\n",
    "print(f\"âœ“ Metrics saved to: models/model_metrics.json\")\n",
    "print(f\"âœ“ Extended metadata saved to: models/model_metadata_extended.json\")\n",
    "\n",
    "\"\"\"\n",
    "Test Model Loading and Prediction:\n",
    "Verify that we can reload and use the saved model\n",
    "\"\"\"\n",
    "\n",
    "from telco_analysis.modeling import load_pipeline\n",
    "\n",
    "print(\"Testing model loading and prediction...\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_pipeline = load_pipeline('../models/baseline_logistic.pkl')\n",
    "\n",
    "# Make predictions on a few test samples\n",
    "sample_size = 5\n",
    "X_sample = X_test.iloc[:sample_size]\n",
    "y_sample = y_test.iloc[:sample_size]\n",
    "\n",
    "# Get predictions\n",
    "predictions = loaded_pipeline.predict(X_sample)\n",
    "probabilities = loaded_pipeline.predict_proba(X_sample)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_sample.values,\n",
    "    'Predicted': predictions,\n",
    "    'Prob_No_Churn': probabilities[:, 0],\n",
    "    'Prob_Churn': probabilities[:, 1]\n",
    "})\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nâœ“ Model successfully loaded and making predictions!\")\n",
    "\n",
    "\"\"\"\n",
    "Generate Final Report:\n",
    "Create a summary report of our baseline model development\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "Project: Telco Customer Churn Prediction\n",
    "Level: 3 - Baseline Model with Modular Code\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "DATA SUMMARY:\n",
    "- Total samples: {len(df_model):,}\n",
    "- Features: {len(X_train.columns)}\n",
    "- Target: Binary (Churn: Yes/No)\n",
    "- Class distribution: {y_train.value_counts(normalize=True).to_dict()}\n",
    "\n",
    "MODEL ARCHITECTURE:\n",
    "- Preprocessing: StandardScaler + OneHotEncoder\n",
    "- Algorithm: Logistic Regression\n",
    "- Pipeline: Sklearn Pipeline with preprocessing\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "- Accuracy:  {metrics['accuracy']:.3f}\n",
    "- Precision: {metrics['precision']:.3f}\n",
    "- Recall:    {metrics['recall']:.3f}\n",
    "- F1-Score:  {metrics['f1']:.3f}\n",
    "- ROC-AUC:   {metrics['roc_auc']:.3f}\n",
    "\n",
    "KEY INSIGHTS:\n",
    "1. Model achieves {metrics['roc_auc']:.3f} ROC-AUC, indicating good discrimination\n",
    "2. Recall of {metrics['recall']:.3f} means we catch ~{metrics['recall']*100:.0f}% of actual churners\n",
    "3. Top churn indicators: Contract type, Payment method, Tenure\n",
    "\n",
    "CODE IMPROVEMENTS (Level 3 Achievement):\n",
    "âœ“ Extracted repetitive code into reusable functions\n",
    "âœ“ Created telco_analysis package with modular structure\n",
    "âœ“ Implemented complete sklearn pipeline\n",
    "âœ“ Established reproducible workflow\n",
    "\n",
    "NEXT STEPS (Level 4):\n",
    "- Feature selection and engineering optimization\n",
    "- Cross-validation for robust evaluation\n",
    "- Model comparison (Random Forest, XGBoost)\n",
    "- Hyperparameter tuning\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save report to file\n",
    "with open('../reports/baseline_model_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nâœ“ Report saved to: outputs/reports/baseline_model_summary.txt\")\n",
    "\n",
    "\n",
    "# Cell 16: Level 3 Completion Checklist\n",
    "\"\"\"\n",
    "Verify that all Level 3 objectives have been met\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LEVEL 3 COMPLETION CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = {\n",
    "    \"Code Organization\": {\n",
    "        \"Created src/telco_analysis package\": True,\n",
    "        \"Extracted functions from notebooks\": True,\n",
    "        \"Implemented proper imports\": True,\n",
    "        \"Functions have docstrings\": True\n",
    "    },\n",
    "    \"Baseline Model\": {\n",
    "        \"Built sklearn pipeline\": True,\n",
    "        \"Preprocessing handles mixed types\": True,\n",
    "        \"Trained logistic regression\": True,\n",
    "        \"Evaluated performance\": True\n",
    "    },\n",
    "    \"Model Persistence\": {\n",
    "        \"Saved trained pipeline\": True,\n",
    "        \"Saved performance metrics\": True,\n",
    "        \"Can reload and predict\": True\n",
    "    },\n",
    "    \"Visualizations\": {\n",
    "        \"Confusion matrix\": True,\n",
    "        \"ROC curve\": True,\n",
    "        \"Precision-Recall curve\": True,\n",
    "        \"Feature importance\": True,\n",
    "        \"Prediction distribution\": True\n",
    "    },\n",
    "    \"Documentation\": {\n",
    "        \"Code is well-commented\": True,\n",
    "        \"Decisions are explained\": True,\n",
    "        \"Results are interpreted\": True,\n",
    "        \"Report generated\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, items in checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item, status in items.items():\n",
    "        status_symbol = \"âœ“\" if status else \"âœ—\"\n",
    "        print(f\"  {status_symbol} {item}\")\n",
    "\n",
    "all_complete = all(all(items.values()) for items in checklist.values())\n",
    "if all_complete:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ‰ CONGRATULATIONS! Level 3 Complete! ðŸŽ‰\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nYou have successfully:\")\n",
    "    print(\"1. Transitioned from monolithic to modular code\")\n",
    "    print(\"2. Created your first Python package\")\n",
    "    print(\"3. Built a complete ML pipeline\")\n",
    "    print(\"4. Established baseline performance metrics\")\n",
    "    print(\"\\nReady to proceed to Level 4: Advanced Model Development!\")\n",
    "else:\n",
    "    print(\"\\nâš  Some items are incomplete. Review and complete before proceeding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb6bd7",
   "metadata": {},
   "source": [
    "# Automated Statistical Test & Model Selection Functions\n",
    "\n",
    "## Complete Python Framework for Choosing Statistical Tests and Models\n",
    "\n",
    "### **Part 1: Data Profiling Functions**\n",
    "\n",
    "First, we need to understand what kind of data we're dealing with:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def profile_dataset(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data profiling to inform test selection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Data characteristics\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        'n_samples': len(df),\n",
    "        'n_features': len(df.columns),\n",
    "        'numeric_features': [],\n",
    "        'categorical_features': [],\n",
    "        'binary_features': [],\n",
    "        'target_type': None,\n",
    "        'has_missing': df.isnull().any().any(),\n",
    "        'is_balanced': None,\n",
    "        'data_types': {}\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        # Classify column type\n",
    "        if dtype in ['int64', 'float64']:\n",
    "            if unique_count == 2:\n",
    "                profile['binary_features'].append(col)\n",
    "            else:\n",
    "                profile['numeric_features'].append(col)\n",
    "        elif dtype == 'object':\n",
    "            if unique_count == 2:\n",
    "                profile['binary_features'].append(col)\n",
    "            else:\n",
    "                profile['categorical_features'].append(col)\n",
    "        \n",
    "        profile['data_types'][col] = {\n",
    "            'dtype': str(dtype),\n",
    "            'unique_values': unique_count,\n",
    "            'missing_pct': df[col].isnull().mean() * 100\n",
    "        }\n",
    "    \n",
    "    return profile\n",
    "\n",
    "# Example usage with telco data\n",
    "profile = profile_dataset(df)\n",
    "print(f\"Dataset has {profile['n_samples']} samples with {profile['n_features']} features\")\n",
    "print(f\"Numeric: {len(profile['numeric_features'])}, Categorical: {len(profile['categorical_features'])}\")\n",
    "```\n",
    "\n",
    "### **Part 2: Problem Type Detection**\n",
    "\n",
    "```python\n",
    "def detect_problem_type(df, target_column=None):\n",
    "    \"\"\"\n",
    "    Automatically detect what type of problem you're solving.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset\n",
    "    target_column : str\n",
    "        Name of target column (if supervised learning)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Problem characteristics and recommendations\n",
    "    \"\"\"\n",
    "    problem = {\n",
    "        'type': None,\n",
    "        'subtype': None,\n",
    "        'recommended_approach': [],\n",
    "        'metrics': []\n",
    "    }\n",
    "    \n",
    "    # No target = Unsupervised\n",
    "    if target_column is None:\n",
    "        problem['type'] = 'unsupervised'\n",
    "        problem['recommended_approach'] = ['clustering', 'dimensionality_reduction']\n",
    "        problem['metrics'] = ['silhouette_score', 'inertia']\n",
    "        return problem\n",
    "    \n",
    "    # Check target characteristics\n",
    "    target = df[target_column]\n",
    "    unique_values = target.nunique()\n",
    "    dtype = target.dtype\n",
    "    \n",
    "    # Classification problems\n",
    "    if dtype == 'object' or unique_values < 10:\n",
    "        problem['type'] = 'classification'\n",
    "        \n",
    "        if unique_values == 2:\n",
    "            problem['subtype'] = 'binary_classification'\n",
    "            problem['metrics'] = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "            \n",
    "            # Check class balance\n",
    "            value_counts = target.value_counts()\n",
    "            minority_pct = value_counts.min() / len(target)\n",
    "            \n",
    "            if minority_pct < 0.3:\n",
    "                problem['subtype'] = 'imbalanced_binary_classification'\n",
    "                problem['metrics'] = ['precision', 'recall', 'f1', 'pr_auc', 'roc_auc']\n",
    "                problem['recommended_approach'].append('use SMOTE or class weights')\n",
    "        else:\n",
    "            problem['subtype'] = 'multiclass_classification'\n",
    "            problem['metrics'] = ['accuracy', 'macro_f1', 'weighted_f1', 'confusion_matrix']\n",
    "    \n",
    "    # Regression problems\n",
    "    elif dtype in ['int64', 'float64'] and unique_values > 10:\n",
    "        problem['type'] = 'regression'\n",
    "        problem['metrics'] = ['mae', 'mse', 'rmse', 'r2', 'mape']\n",
    "        \n",
    "        # Check for count data\n",
    "        if (target >= 0).all() and (target % 1 == 0).all():\n",
    "            problem['subtype'] = 'count_regression'\n",
    "            problem['recommended_approach'].append('consider Poisson regression')\n",
    "    \n",
    "    # Time series\n",
    "    if 'date' in df.columns or df.index.name == 'date':\n",
    "        problem['subtype'] = 'time_series_' + problem['type']\n",
    "        problem['recommended_approach'].append('use time-based splits')\n",
    "    \n",
    "    return problem\n",
    "\n",
    "# Example usage\n",
    "problem_info = detect_problem_type(df, target_column='Churn')\n",
    "print(f\"Problem type: {problem_info['subtype']}\")\n",
    "print(f\"Recommended metrics: {problem_info['metrics']}\")\n",
    "```\n",
    "\n",
    "### **Part 3: Statistical Test Selection**\n",
    "\n",
    "```python\n",
    "def select_statistical_tests(df, col1, col2=None, target=None):\n",
    "    \"\"\"\n",
    "    Automatically select appropriate statistical test based on data types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset\n",
    "    col1 : str\n",
    "        First column to analyze\n",
    "    col2 : str, optional\n",
    "        Second column for relationship analysis\n",
    "    target : str, optional\n",
    "        Target variable for supervised analysis\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Recommended tests and how to run them\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        'test': None,\n",
    "        'reason': None,\n",
    "        'code': None,\n",
    "        'interpretation': None\n",
    "    }\n",
    "    \n",
    "    # Single column analysis\n",
    "    if col2 is None and target is None:\n",
    "        if df[col1].dtype in ['int64', 'float64']:\n",
    "            # Normality test first\n",
    "            stat, p_value = stats.normaltest(df[col1].dropna())\n",
    "            \n",
    "            if p_value > 0.05:\n",
    "                recommendations['test'] = 'One-sample t-test'\n",
    "                recommendations['reason'] = 'Data appears normally distributed'\n",
    "                recommendations['code'] = \"\"\"\n",
    "from scipy import stats\n",
    "t_stat, p_value = stats.ttest_1samp(df['{}'], popmean=expected_mean)\n",
    "                \"\"\".format(col1)\n",
    "            else:\n",
    "                recommendations['test'] = 'Wilcoxon signed-rank test'\n",
    "                recommendations['reason'] = 'Data is not normally distributed'\n",
    "                recommendations['code'] = \"\"\"\n",
    "from scipy import stats\n",
    "stat, p_value = stats.wilcoxon(df['{}'] - expected_median)\n",
    "                \"\"\".format(col1)\n",
    "    \n",
    "    # Two column analysis\n",
    "    elif col2 is not None:\n",
    "        type1 = 'numeric' if df[col1].dtype in ['int64', 'float64'] else 'categorical'\n",
    "        type2 = 'numeric' if df[col2].dtype in ['int64', 'float64'] else 'categorical'\n",
    "        \n",
    "        # Both numeric\n",
    "        if type1 == 'numeric' and type2 == 'numeric':\n",
    "            recommendations['test'] = 'Correlation analysis'\n",
    "            recommendations['reason'] = 'Both variables are numeric'\n",
    "            recommendations['code'] = \"\"\"\n",
    "# Pearson correlation (linear relationships)\n",
    "pearson_corr, p_value = stats.pearsonr(df['{}'], df['{}'])\n",
    "\n",
    "# Spearman correlation (monotonic relationships)\n",
    "spearman_corr, p_value = stats.spearmanr(df['{}'], df['{}'])\n",
    "            \"\"\".format(col1, col2, col1, col2)\n",
    "            \n",
    "        # Both categorical\n",
    "        elif type1 == 'categorical' and type2 == 'categorical':\n",
    "            recommendations['test'] = 'Chi-square test'\n",
    "            recommendations['reason'] = 'Both variables are categorical'\n",
    "            recommendations['code'] = \"\"\"\n",
    "from scipy.stats import chi2_contingency\n",
    "contingency_table = pd.crosstab(df['{}'], df['{}'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "            \"\"\".format(col1, col2)\n",
    "            \n",
    "        # Mixed types\n",
    "        else:\n",
    "            cat_col = col1 if type1 == 'categorical' else col2\n",
    "            num_col = col1 if type1 == 'numeric' else col2\n",
    "            \n",
    "            if df[cat_col].nunique() == 2:\n",
    "                recommendations['test'] = 'Independent t-test'\n",
    "                recommendations['reason'] = 'Comparing numeric variable across 2 groups'\n",
    "                recommendations['code'] = \"\"\"\n",
    "group1 = df[df['{}'] == df['{}'].unique()[0]]['{}']\n",
    "group2 = df[df['{}'] == df['{}'].unique()[1]]['{}']\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "                \"\"\".format(cat_col, cat_col, num_col, cat_col, cat_col, num_col)\n",
    "            else:\n",
    "                recommendations['test'] = 'ANOVA'\n",
    "                recommendations['reason'] = 'Comparing numeric variable across multiple groups'\n",
    "                recommendations['code'] = \"\"\"\n",
    "from scipy.stats import f_oneway\n",
    "groups = [group['{}'].values for name, group in df.groupby('{}')]\n",
    "f_stat, p_value = f_oneway(*groups)\n",
    "                \"\"\".format(num_col, cat_col)\n",
    "    \n",
    "    # Add interpretation guide\n",
    "    recommendations['interpretation'] = \"\"\"\n",
    "    p-value < 0.05: Statistically significant relationship\n",
    "    p-value >= 0.05: No significant relationship found\n",
    "    Effect size matters more than p-value for practical significance\n",
    "    \"\"\"\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "test_rec = select_statistical_tests(df, 'MonthlyCharges', 'Contract')\n",
    "print(f\"Recommended test: {test_rec['test']}\")\n",
    "print(f\"Reason: {test_rec['reason']}\")\n",
    "print(f\"Code:\\n{test_rec['code']}\")\n",
    "```\n",
    "\n",
    "### **Part 4: Model Selection Based on Data**\n",
    "\n",
    "```python\n",
    "def recommend_models(df, target_column, profile=None):\n",
    "    \"\"\"\n",
    "    Recommend machine learning models based on data characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset\n",
    "    target_column : str\n",
    "        Target variable name\n",
    "    profile : dict, optional\n",
    "        Output from profile_dataset function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Model recommendations with reasoning\n",
    "    \"\"\"\n",
    "    if profile is None:\n",
    "        profile = profile_dataset(df)\n",
    "    \n",
    "    recommendations = {\n",
    "        'baseline': None,\n",
    "        'intermediate': [],\n",
    "        'advanced': [],\n",
    "        'reasoning': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    n_samples = profile['n_samples']\n",
    "    n_features = profile['n_features']\n",
    "    \n",
    "    # Detect problem type\n",
    "    problem = detect_problem_type(df, target_column)\n",
    "    \n",
    "    # Binary Classification (like your telco churn)\n",
    "    if problem['subtype'] == 'binary_classification':\n",
    "        recommendations['baseline'] = {\n",
    "            'model': 'LogisticRegression',\n",
    "            'reason': 'Simple, interpretable, fast, provides probabilities',\n",
    "            'code': \"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        recommendations['intermediate'] = [\n",
    "            {\n",
    "                'model': 'RandomForestClassifier',\n",
    "                'reason': 'Handles non-linear relationships, feature importance',\n",
    "                'when_to_use': n_samples > 1000\n",
    "            },\n",
    "            {\n",
    "                'model': 'GradientBoostingClassifier',\n",
    "                'reason': 'Often best performance, handles interactions',\n",
    "                'when_to_use': n_samples > 2000\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        recommendations['advanced'] = [\n",
    "            {\n",
    "                'model': 'XGBoost',\n",
    "                'reason': 'State-of-the-art performance',\n",
    "                'when_to_use': n_samples > 5000\n",
    "            },\n",
    "            {\n",
    "                'model': 'Neural Network',\n",
    "                'reason': 'Complex patterns, many features',\n",
    "                'when_to_use': n_samples > 10000 and n_features > 20\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    # Imbalanced Classification\n",
    "    elif problem['subtype'] == 'imbalanced_binary_classification':\n",
    "        recommendations['warnings'].append('Class imbalance detected!')\n",
    "        recommendations['baseline'] = {\n",
    "            'model': 'LogisticRegression with class_weight=\"balanced\"',\n",
    "            'reason': 'Handles imbalance through weighted loss',\n",
    "            'code': \"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "            \"\"\"\n",
    "        }\n",
    "    \n",
    "    # Regression\n",
    "    elif problem['type'] == 'regression':\n",
    "        recommendations['baseline'] = {\n",
    "            'model': 'LinearRegression',\n",
    "            'reason': 'Simple, interpretable, fast',\n",
    "            'code': \"\"\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Check for regularization needs\n",
    "        if n_features > n_samples / 10:\n",
    "            recommendations['warnings'].append('High feature-to-sample ratio: use regularization')\n",
    "            recommendations['intermediate'].append({\n",
    "                'model': 'Ridge/Lasso Regression',\n",
    "                'reason': 'Prevents overfitting with many features'\n",
    "            })\n",
    "    \n",
    "    # Sample size warnings\n",
    "    if n_samples < 100:\n",
    "        recommendations['warnings'].append('Very small dataset: simple models only')\n",
    "    elif n_samples < 1000:\n",
    "        recommendations['warnings'].append('Small dataset: avoid complex models')\n",
    "    \n",
    "    # Feature warnings\n",
    "    if profile['has_missing']:\n",
    "        recommendations['warnings'].append('Missing data: handle before modeling')\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "model_recs = recommend_models(df, 'Churn')\n",
    "print(f\"Baseline model: {model_recs['baseline']['model']}\")\n",
    "print(f\"Reason: {model_recs['baseline']['reason']}\")\n",
    "for warning in model_recs['warnings']:\n",
    "    print(f\"âš ï¸ {warning}\")\n",
    "```\n",
    "\n",
    "### **Part 5: Automated Metric Selection**\n",
    "\n",
    "```python\n",
    "def select_metrics(problem_type, business_context=None):\n",
    "    \"\"\"\n",
    "    Select appropriate metrics based on problem type and business needs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    problem_type : dict\n",
    "        Output from detect_problem_type\n",
    "    business_context : str, optional\n",
    "        One of: 'high_precision', 'high_recall', 'balanced'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Metrics to use with explanation\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'primary': [],\n",
    "        'secondary': [],\n",
    "        'visualization': [],\n",
    "        'business': [],\n",
    "        'code': {}\n",
    "    }\n",
    "    \n",
    "    if 'binary_classification' in problem_type['subtype']:\n",
    "        # Always include these\n",
    "        metrics['primary'] = ['roc_auc', 'precision', 'recall']\n",
    "        metrics['visualization'] = ['confusion_matrix', 'roc_curve']\n",
    "        \n",
    "        # Context-specific metrics\n",
    "        if business_context == 'high_precision':\n",
    "            metrics['primary'].insert(0, 'precision')\n",
    "            metrics['business'].append('minimize_false_positives')\n",
    "            metrics['code']['threshold'] = \"\"\"\n",
    "# Optimize threshold for precision\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "optimal_idx = np.argmax(precisions >= 0.9)  # Target 90% precision\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "            \"\"\"\n",
    "            \n",
    "        elif business_context == 'high_recall':\n",
    "            metrics['primary'].insert(0, 'recall')\n",
    "            metrics['business'].append('minimize_false_negatives')\n",
    "            metrics['code']['threshold'] = \"\"\"\n",
    "# Optimize threshold for recall\n",
    "optimal_threshold = np.percentile(y_proba, 25)  # Catch top 75% risk scores\n",
    "            \"\"\"\n",
    "            \n",
    "        else:  # balanced\n",
    "            metrics['primary'].insert(0, 'f1_score')\n",
    "            \n",
    "        # Imbalanced addition\n",
    "        if 'imbalanced' in problem_type['subtype']:\n",
    "            metrics['primary'].append('average_precision')\n",
    "            metrics['secondary'].append('balanced_accuracy')\n",
    "            \n",
    "    elif problem_type['type'] == 'regression':\n",
    "        metrics['primary'] = ['rmse', 'mae', 'r2']\n",
    "        metrics['visualization'] = ['residual_plot', 'prediction_vs_actual']\n",
    "        metrics['business'] = ['mean_absolute_percentage_error']\n",
    "    \n",
    "    # Add interpretation\n",
    "    metrics['interpretation'] = {\n",
    "        'roc_auc': '0.5=random, 1.0=perfect, >0.8=good',\n",
    "        'precision': '% of positive predictions that are correct',\n",
    "        'recall': '% of actual positives that are caught',\n",
    "        'f1_score': 'Harmonic mean of precision and recall',\n",
    "        'rmse': 'Lower is better, in same units as target'\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "metrics = select_metrics(problem_info, business_context='balanced')\n",
    "print(f\"Primary metrics: {metrics['primary']}\")\n",
    "print(f\"Visualizations: {metrics['visualization']}\")\n",
    "```\n",
    "\n",
    "### **Part 6: Master Function - Complete Auto-Selection**\n",
    "\n",
    "```python\n",
    "def auto_select_analysis(df, target=None, columns_to_analyze=None, business_context=None):\n",
    "    \"\"\"\n",
    "    Master function that automatically selects all appropriate analyses.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset\n",
    "    target : str, optional\n",
    "        Target column for supervised learning\n",
    "    columns_to_analyze : list, optional\n",
    "        Specific columns to analyze relationships\n",
    "    business_context : str, optional\n",
    "        Business priority (high_precision/high_recall/balanced)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Complete analysis recommendations\n",
    "    \"\"\"\n",
    "    analysis_plan = {\n",
    "        'data_profile': profile_dataset(df),\n",
    "        'problem_type': None,\n",
    "        'statistical_tests': [],\n",
    "        'recommended_models': None,\n",
    "        'metrics_to_use': None,\n",
    "        'preprocessing_steps': [],\n",
    "        'validation_strategy': None\n",
    "    }\n",
    "    \n",
    "    # Determine problem type\n",
    "    if target:\n",
    "        analysis_plan['problem_type'] = detect_problem_type(df, target)\n",
    "        analysis_plan['recommended_models'] = recommend_models(df, target)\n",
    "        analysis_plan['metrics_to_use'] = select_metrics(\n",
    "            analysis_plan['problem_type'], \n",
    "            business_context\n",
    "        )\n",
    "    \n",
    "    # Statistical tests for feature relationships\n",
    "    if columns_to_analyze:\n",
    "        for col in columns_to_analyze:\n",
    "            if target:\n",
    "                test = select_statistical_tests(df, col, col2=target)\n",
    "                analysis_plan['statistical_tests'].append({\n",
    "                    'feature': col,\n",
    "                    'vs': target,\n",
    "                    'test': test\n",
    "                })\n",
    "    \n",
    "    # Preprocessing recommendations\n",
    "    if analysis_plan['data_profile']['has_missing']:\n",
    "        analysis_plan['preprocessing_steps'].append('Handle missing values')\n",
    "    \n",
    "    if len(analysis_plan['data_profile']['categorical_features']) > 0:\n",
    "        analysis_plan['preprocessing_steps'].append('Encode categorical variables')\n",
    "    \n",
    "    if len(analysis_plan['data_profile']['numeric_features']) > 0:\n",
    "        analysis_plan['preprocessing_steps'].append('Scale numeric features')\n",
    "    \n",
    "    # Validation strategy\n",
    "    n_samples = analysis_plan['data_profile']['n_samples']\n",
    "    if n_samples < 1000:\n",
    "        analysis_plan['validation_strategy'] = 'Use k-fold cross-validation (k=5)'\n",
    "    else:\n",
    "        analysis_plan['validation_strategy'] = 'Use train/test split (80/20)'\n",
    "    \n",
    "    return analysis_plan\n",
    "\n",
    "# Complete example usage with telco data\n",
    "analysis = auto_select_analysis(\n",
    "    df=df,\n",
    "    target='Churn',\n",
    "    columns_to_analyze=['MonthlyCharges', 'Contract', 'tenure'],\n",
    "    business_context='balanced'\n",
    ")\n",
    "\n",
    "# Display complete recommendations\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOMATED ANALYSIS PLAN FOR YOUR DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Profile:\")\n",
    "print(f\"  Samples: {analysis['data_profile']['n_samples']}\")\n",
    "print(f\"  Features: {analysis['data_profile']['n_features']}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Problem Type:\")\n",
    "print(f\"  {analysis['problem_type']['subtype']}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Recommended Models:\")\n",
    "print(f\"  Baseline: {analysis['recommended_models']['baseline']['model']}\")\n",
    "print(f\"  Reason: {analysis['recommended_models']['baseline']['reason']}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Metrics to Track:\")\n",
    "for metric in analysis['metrics_to_use']['primary']:\n",
    "    print(f\"  - {metric}\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ Statistical Tests to Run:\")\n",
    "for test_info in analysis['statistical_tests']:\n",
    "    print(f\"  {test_info['feature']} vs {test_info['vs']}: {test_info['test']['test']}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Preprocessing Steps:\")\n",
    "for step in analysis['preprocessing_steps']:\n",
    "    print(f\"  - {step}\")\n",
    "\n",
    "print(f\"\\nâœ… Validation Strategy:\")\n",
    "print(f\"  {analysis['validation_strategy']}\")\n",
    "```\n",
    "\n",
    "### **Part 7: Practical Application Function**\n",
    "\n",
    "```python\n",
    "def generate_analysis_code(analysis_plan):\n",
    "    \"\"\"\n",
    "    Generate actual Python code based on analysis recommendations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_plan : dict\n",
    "        Output from auto_select_analysis\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Complete Python code to run the analysis\n",
    "    \"\"\"\n",
    "    code = \"\"\"\n",
    "# Auto-generated analysis code based on your data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "{}\n",
    "\n",
    "# Load and prepare data\n",
    "X = df.drop('{}', axis=1)\n",
    "y = df['{}']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model\n",
    "{}\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {{roc_auc_score(y_test, y_proba):.3f}}\")\n",
    "    \"\"\".format(\n",
    "        analysis_plan['recommended_models']['baseline']['code'],\n",
    "        analysis_plan['problem_type'].get('target', 'target'),\n",
    "        analysis_plan['problem_type'].get('target', 'target'),\n",
    "        \"model = LogisticRegression()\"  # Simplified for example\n",
    "    )\n",
    "    \n",
    "    return code\n",
    "\n",
    "# Generate code\n",
    "code = generate_analysis_code(analysis)\n",
    "print(\"Generated Code:\")\n",
    "print(code)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: How These Functions Help You\n",
    "\n",
    "1. **Profile your data** â†’ Understand what you're working with\n",
    "2. **Detect problem type** â†’ Classification vs regression, balanced vs imbalanced\n",
    "3. **Select statistical tests** â†’ Based on variable types and relationships\n",
    "4. **Choose models** â†’ Based on data size and complexity\n",
    "5. **Pick metrics** â†’ Based on problem type and business needs\n",
    "6. **Generate code** â†’ Get started immediately\n",
    "\n",
    "These functions remove the guesswork from statistical analysis and model selection, ensuring you use appropriate methods for your specific data and problem type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4935621",
   "metadata": {},
   "source": [
    "## Statistical Tests in the Telco Churn Model: Complete Walk-Through\n",
    "\n",
    "Let me guide you through each statistical test and metric used in your Level 3 project, explaining why we choose each one and how to interpret the results.\n",
    "\n",
    "## **Part 1: Why These Specific Metrics for Binary Classification**\n",
    "\n",
    "### **The Business Context**\n",
    "Your telco churn problem is a **binary classification** task:\n",
    "- **0** = Customer stays (No churn)\n",
    "- **1** = Customer leaves (Churn)\n",
    "\n",
    "With 26.5% churn rate, you have **class imbalance** - this affects which metrics matter most.\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Metrics and Their Interpretation**\n",
    "\n",
    "### **1. Accuracy**\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Your result: ~0.796 (79.6%)\n",
    "```\n",
    "\n",
    "**What it measures:** Overall correctness\n",
    "```\n",
    "Accuracy = (Correct Predictions) / (Total Predictions)\n",
    "         = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "**Why we use it:**\n",
    "- Simple to understand and explain to stakeholders\n",
    "- Good baseline metric\n",
    "\n",
    "**Why it's not enough:**\n",
    "- Can be misleading with imbalanced data\n",
    "- If you predicted \"no churn\" for everyone, you'd get 73.5% accuracy!\n",
    "\n",
    "**Your interpretation:**\n",
    "\"The model correctly predicts 79.6% of all customers, but this alone doesn't tell us if we're catching churners effectively.\"\n",
    "\n",
    "### **2. Precision**\n",
    "```python\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "# Your result: ~0.653 (65.3%)\n",
    "```\n",
    "\n",
    "**What it measures:** When you predict churn, how often are you right?\n",
    "```\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "          = Correct Churn Predictions / All Churn Predictions\n",
    "```\n",
    "\n",
    "**Business meaning:**\n",
    "- Of 100 customers you flag as \"will churn\", 65 actually will\n",
    "- 35 false alarms (customers you'll bother unnecessarily)\n",
    "\n",
    "**When precision matters most:**\n",
    "- When intervention is expensive (offering big discounts)\n",
    "- When false positives annoy customers\n",
    "\n",
    "**Your interpretation:**\n",
    "\"65.3% precision means our retention team won't waste too much time on customers who weren't actually leaving.\"\n",
    "\n",
    "### **3. Recall (Sensitivity)**\n",
    "```python\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Your result: ~0.547 (54.7%)\n",
    "```\n",
    "\n",
    "**What it measures:** Of all actual churners, how many do you catch?\n",
    "```\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "       = Caught Churners / All Actual Churners\n",
    "```\n",
    "\n",
    "**Business meaning:**\n",
    "- You catch 55% of customers who would leave\n",
    "- You miss 45% of churners\n",
    "\n",
    "**When recall matters most:**\n",
    "- When missing a churner is very costly\n",
    "- When you have budget for broad retention campaigns\n",
    "\n",
    "**Your interpretation:**\n",
    "\"We identify 54.7% of churners before they leave. While we miss some, this is much better than random guessing (26.5%).\"\n",
    "\n",
    "### **4. F1-Score**\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "# Your result: ~0.595 (59.5%)\n",
    "```\n",
    "\n",
    "**What it measures:** Harmonic mean of precision and recall\n",
    "```\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "**Why harmonic mean?**\n",
    "- Penalizes extreme imbalances\n",
    "- If either precision or recall is very low, F1 is low\n",
    "\n",
    "**When to use:**\n",
    "- When you need balance between precision and recall\n",
    "- Good single metric for model comparison\n",
    "\n",
    "**Your interpretation:**\n",
    "\"F1 of 0.595 shows reasonable balance - we're not sacrificing too much of either precision or recall.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 2: ROC-AUC - The Key Discriminator**\n",
    "\n",
    "### **5. ROC-AUC Score**\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Calculate\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probabilities\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "# Your result: ~0.842\n",
    "```\n",
    "\n",
    "**What it measures:** \n",
    "How well the model ranks customers by churn probability\n",
    "\n",
    "**Understanding the ROC Curve:**\n",
    "```python\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "```\n",
    "- **False Positive Rate (x-axis)**: Rate of false alarms at each threshold\n",
    "- **True Positive Rate (y-axis)**: Rate of caught churners at each threshold\n",
    "- **Each point**: Different probability threshold for classification\n",
    "\n",
    "**Why AUC = 0.842 is good:**\n",
    "- 0.5 = Random guessing (diagonal line)\n",
    "- 1.0 = Perfect (impossible in practice)\n",
    "- 0.842 = Strong discrimination ability\n",
    "\n",
    "**Business interpretation:**\n",
    "\"If we randomly pick one churner and one non-churner, the model will rank the churner higher 84.2% of the time.\"\n",
    "\n",
    "**Practical use:**\n",
    "```python\n",
    "# You can choose different thresholds based on business needs\n",
    "# High threshold (e.g., 0.7) = High precision, low recall\n",
    "# Low threshold (e.g., 0.3) = Low precision, high recall\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 3: Precision-Recall Curve - Better for Imbalanced Data**\n",
    "\n",
    "### **6. Average Precision Score**\n",
    "```python\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "# Calculate\n",
    "ap_score = average_precision_score(y_test, y_proba)\n",
    "# Your result: ~0.651\n",
    "\n",
    "# Baseline (random) would be:\n",
    "baseline = y_test.mean()  # 0.265 (your churn rate)\n",
    "```\n",
    "\n",
    "**Why PR curve over ROC for imbalanced data:**\n",
    "- ROC can be overly optimistic with imbalanced classes\n",
    "- PR curve focuses on the minority class (churners)\n",
    "\n",
    "**Interpretation:**\n",
    "\"Average precision of 0.651 vs baseline 0.265 means we're 2.45x better than random at identifying churners.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 4: Statistical Significance Tests**\n",
    "\n",
    "### **7. Confusion Matrix Analysis**\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Your typical results:\n",
    "# TN: ~1001 (correctly identified non-churners)\n",
    "# FP: ~35  (false alarms)\n",
    "# FN: ~168 (missed churners)\n",
    "# TP: ~205 (correctly identified churners)\n",
    "```\n",
    "\n",
    "**Statistical test for significance:**\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Test if predictions are better than random\n",
    "chi2, p_value, dof, expected = chi2_contingency(cm)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Model predictions are statistically significant (p < 0.05)\")\n",
    "```\n",
    "\n",
    "### **8. Calibration Test**\n",
    "```python\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Check if predicted probabilities match actual frequencies\n",
    "fraction_pos, mean_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "\n",
    "# Perfect calibration: fraction_pos == mean_pred\n",
    "# Plot these to see if your probabilities are reliable\n",
    "```\n",
    "\n",
    "**Why calibration matters:**\n",
    "- If model says \"70% chance of churn\", do 70% actually churn?\n",
    "- Important for risk-based decisions\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 5: Business Metrics Translation**\n",
    "\n",
    "### **Converting Statistical Metrics to Business Value**\n",
    "\n",
    "```python\n",
    "def calculate_business_metrics(cm, avg_customer_value=1500):\n",
    "    \"\"\"\n",
    "    Translate confusion matrix to business metrics\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Business calculations\n",
    "    saved_customers = tp  # Churners we caught\n",
    "    revenue_saved = saved_customers * avg_customer_value\n",
    "    \n",
    "    wasted_interventions = fp  # False alarms\n",
    "    intervention_cost = wasted_interventions * 50  # $50 per intervention\n",
    "    \n",
    "    missed_opportunities = fn  # Churners we missed\n",
    "    revenue_lost = missed_opportunities * avg_customer_value\n",
    "    \n",
    "    return {\n",
    "        'revenue_saved': revenue_saved,\n",
    "        'intervention_cost': intervention_cost,\n",
    "        'revenue_lost': revenue_lost,\n",
    "        'net_benefit': revenue_saved - intervention_cost\n",
    "    }\n",
    "\n",
    "# Apply to your results\n",
    "business_metrics = calculate_business_metrics(cm)\n",
    "print(f\"Potential monthly savings: ${business_metrics['net_benefit']:,.0f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Choosing the Right Metric for Decisions**\n",
    "\n",
    "### **Decision Framework**\n",
    "\n",
    "```python\n",
    "# Scenario 1: Limited retention budget\n",
    "# â†’ Optimize for PRECISION\n",
    "# â†’ Target only highest probability churners\n",
    "\n",
    "# Scenario 2: High customer lifetime value\n",
    "# â†’ Optimize for RECALL\n",
    "# â†’ Cast wider net to catch more churners\n",
    "\n",
    "# Scenario 3: Balanced approach\n",
    "# â†’ Optimize for F1-SCORE or ROC-AUC\n",
    "# â†’ Your current model choice\n",
    "```\n",
    "\n",
    "### **Practical Threshold Selection**\n",
    "\n",
    "```python\n",
    "# Instead of default 0.5 threshold, choose based on business needs\n",
    "def find_optimal_threshold(y_test, y_proba, metric='f1'):\n",
    "    \"\"\"\n",
    "    Find threshold that maximizes chosen metric\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba > thresh).astype(int)\n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_test, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_test, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    return thresholds[best_idx], scores[best_idx]\n",
    "\n",
    "optimal_thresh, best_score = find_optimal_threshold(y_test, y_proba)\n",
    "print(f\"Optimal threshold: {optimal_thresh:.2f} with F1: {best_score:.3f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary: Your Model's Story in Statistics**\n",
    "\n",
    "Your baseline model tells this statistical story:\n",
    "\n",
    "1. **Overall Performance** (Accuracy: 79.6%) - Decent but not the full picture\n",
    "2. **Business Precision** (65.3%) - 2 out of 3 flagged customers will actually churn\n",
    "3. **Coverage** (Recall: 54.7%) - Catches half of all churners\n",
    "4. **Discrimination** (ROC-AUC: 0.842) - Excellent at ranking customers by risk\n",
    "5. **Practical Value** (AP: 0.651 vs 0.265 baseline) - 2.5x better than random\n",
    "\n",
    "The key insight: Your model successfully identifies high-risk customers while maintaining reasonable precision, making it valuable for targeted retention campaigns. The 0.842 ROC-AUC indicates strong fundamental pattern recognition that can be refined in Level 4 with better features and models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
