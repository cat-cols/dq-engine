{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06a5c96",
   "metadata": {},
   "source": [
    "# üìö Modularization\n",
    "># 00_Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de90602",
   "metadata": {},
   "source": [
    "Short answer: **A is the best starting point** for a disciplined Telco project‚Äîit's explicit about project roots and scales well.\n",
    "**B** is too fragile (relative to wherever you run it).\n",
    "**C** is great for *bootstrapping folders + copying data*, but its `BASE_DIR = Path.cwd().resolve().parent` is brittle and the copy step may hide mistakes.\n",
    "\n",
    "### Why A wins\n",
    "\n",
    "* Uses a **stable project root** (via `__file__`), not just ‚Äúwhere I launched the notebook‚Äù.\n",
    "* Keeps paths centralized (`BASE_DIR / ...`)‚Äîclean, testable, refactor-friendly.\n",
    "* Easy to reuse across modules and scripts.\n",
    "\n",
    "### Nice upgrade: A + a bit of C (create dirs, validate file)\n",
    "\n",
    "Use A‚Äôs root handling, add safe directory creation and a clear check for the CSV:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "def get_base_dir() -> Path:\n",
    "    # Works both in scripts (via __file__) and in notebooks (fallback to CWD)\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent.parent\n",
    "    except NameError:\n",
    "        # In notebooks: put this file under project_root/notebooks/ to keep same base\n",
    "        return Path.cwd().resolve().parent  # adjust if your notebook sits at project_root\n",
    "\n",
    "BASE_DIR = get_base_dir()\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DATA_RAW = DATA_DIR / \"raw\"\n",
    "DATA_PROCESSED = DATA_DIR / \"processed\"\n",
    "OUTPUTS = BASE_DIR / \"outputs\"\n",
    "FIGURES = OUTPUTS / \"figures\"\n",
    "REPORTS = OUTPUTS / \"reports\"\n",
    "MODELS = BASE_DIR / \"models\"\n",
    "\n",
    "# Ensure expected folders exist\n",
    "for p in [DATA_RAW, DATA_PROCESSED, FIGURES, REPORTS, MODELS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TELCO_CSV = DATA_RAW / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "if TELCO_CSV.exists():\n",
    "    print(f\"‚úÖ Dataset found: {TELCO_CSV}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Missing dataset. Place it here:\\n   {TELCO_CSV}\")\n",
    "```\n",
    "\n",
    "**Usage tip:** keep this in `src/telco_utils/paths.py` and import it everywhere.\n",
    "If you also want an optional one-time ‚Äúcopy from downloads‚Äù step, do it in a **separate** bootstrap script to avoid silently masking missing-data bugs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77daf6a8",
   "metadata": {},
   "source": [
    "# telco_paths.py  (you can store this under /Level_3/scripts/)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def get_telco_paths():\n",
    "    current_path = Path.cwd().resolve()\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        if parent.name == \"Telco\":\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"‚ùå Could not find 'Telco' above {current_path}\")\n",
    "\n",
    "    DATA_ROOT = PROJECT_ROOT / \"resources\" / \"data\"\n",
    "    DATA_RAW_DIR = DATA_ROOT / \"raw\"\n",
    "    DATA_PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "\n",
    "    LEVEL_DIR = PROJECT_ROOT / \"Level_3\"\n",
    "    FIGURES = LEVEL_DIR / \"figures\"\n",
    "    REPORTS = LEVEL_DIR / \"reports\"\n",
    "    MODELS = LEVEL_DIR / \"models\"\n",
    "    OUTPUTS = LEVEL_DIR / \"outputs\"\n",
    "\n",
    "    for d in [DATA_RAW_DIR, DATA_PROCESSED_DIR, FIGURES, REPORTS, MODELS, OUTPUTS]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    DATA_RAW_FILE = DATA_RAW_DIR / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "    return {\n",
    "        \"PROJECT_ROOT\": PROJECT_ROOT,\n",
    "        \"DATA_RAW_FILE\": DATA_RAW_FILE,\n",
    "        \"DATA_PROCESSED_DIR\": DATA_PROCESSED_DIR,\n",
    "        \"FIGURES\": FIGURES,\n",
    "        \"REPORTS\": REPORTS,\n",
    "        \"MODELS\": MODELS,\n",
    "        \"OUTPUTS\": OUTPUTS\n",
    "    }\n",
    "# Then in 01_EDA.ipynb\n",
    "from scripts.telco_paths import get_telco_paths\n",
    "import pandas as pd\n",
    "\n",
    "paths = get_telco_paths()\n",
    "df = pd.read_csv(paths[\"DATA_RAW_FILE\"])\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {len(df):,} rows √ó {len(df.columns)} columns\")\n",
    "display(df.head())\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030f44d",
   "metadata": {},
   "source": [
    "Perfect ‚Äî here‚Äôs exactly what you asked for:\n",
    "\n",
    "1Ô∏è‚É£ A **functionless version** of your `telco_paths.py` (for modular use)\n",
    "2Ô∏è‚É£ A **ready-to-run cell** for your `01_EDA.ipynb` to safely and automatically load your Telco dataset\n",
    "\n",
    "Both follow your Level-3 structure conventions and are fully self-contained üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **telco_paths.py** (Functionless, stored in `/Level_3/scripts/`)\n",
    "\n",
    "```python\n",
    "# ==========================================================\n",
    "# üìÇ Telco Project Path Setup (functionless version)\n",
    "# ==========================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Auto-detect /Telco project root ---\n",
    "current_path = Path.cwd().resolve()\n",
    "\n",
    "for parent in [current_path] + list(current_path.parents):\n",
    "    if parent.name == \"Telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(f\"‚ùå Could not find 'Telco' above: {current_path}\")\n",
    "\n",
    "# --- 2. Define shared data directories ---\n",
    "DATA_ROOT = PROJECT_ROOT / \"resources\" / \"data\"\n",
    "DATA_RAW_DIR = DATA_ROOT / \"raw\"\n",
    "DATA_PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "DATA_RAW_FILE = DATA_RAW_DIR / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "# --- 3. Define Level 3 output directories ---\n",
    "LEVEL_DIR = PROJECT_ROOT / \"Level_3\"\n",
    "FIGURES = LEVEL_DIR / \"figures\"\n",
    "REPORTS = LEVEL_DIR / \"reports\"\n",
    "MODELS = LEVEL_DIR / \"models\"\n",
    "OUTPUTS = LEVEL_DIR / \"outputs\"\n",
    "\n",
    "# --- 4. Create missing directories (safe) ---\n",
    "for d in [DATA_RAW_DIR, DATA_PROCESSED_DIR, FIGURES, REPORTS, MODELS, OUTPUTS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 5. Optional dataset fallback ---\n",
    "if not DATA_RAW_FILE.exists():\n",
    "    alt_files = list(DATA_RAW_DIR.glob(\"*Telco*Churn*.csv\"))\n",
    "    if alt_files:\n",
    "        DATA_RAW_FILE = alt_files[0]\n",
    "        print(f\"‚ö†Ô∏è Default file not found, using detected file:\\n   {DATA_RAW_FILE}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Could not find any Telco dataset in:\\n   {DATA_RAW_DIR}\"\n",
    "        )\n",
    "\n",
    "# --- 6. Print confirmation ---\n",
    "print(\"‚úÖ Telco directory structure ready.\")\n",
    "print(f\"Project root:       {PROJECT_ROOT}\")\n",
    "print(f\"Raw dataset:        {DATA_RAW_FILE}\")\n",
    "print(f\"Processed data dir: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"Figures dir:        {FIGURES}\")\n",
    "print(f\"Reports dir:        {REPORTS}\")\n",
    "print(f\"Models dir:         {MODELS}\")\n",
    "print(f\"Outputs dir:        {OUTPUTS}\")\n",
    "```\n",
    "\n",
    "This script can be imported in any notebook via:\n",
    "\n",
    "```python\n",
    "%run ../scripts/telco_paths.py\n",
    "```\n",
    "\n",
    "It will automatically set the variables\n",
    "`PROJECT_ROOT`, `DATA_RAW_FILE`, `FIGURES`, etc. in your notebook‚Äôs global scope.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Now for 01_EDA.ipynb ‚Äî Functionless Data Load Script**\n",
    "\n",
    "Place this cell near the top of your EDA notebook (right after your environment imports):\n",
    "\n",
    "```python\n",
    "# ==========================================================\n",
    "# üß≠ 01_EDA Notebook: Dataset Load & Verification\n",
    "# ==========================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Auto-detect Telco root ---\n",
    "current_path = Path.cwd().resolve()\n",
    "for parent in [current_path] + list(current_path.parents):\n",
    "    if parent.name == \"Telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(f\"‚ùå Could not find 'Telco' above: {current_path}\")\n",
    "\n",
    "# --- Define key data paths ---\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"resources\" / \"data\" / \"raw\"\n",
    "DATA_RAW_FILE = DATA_RAW_DIR / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "# --- Verify dataset presence ---\n",
    "if not DATA_RAW_FILE.exists():\n",
    "    alt_files = list(DATA_RAW_DIR.glob(\"*Telco*Churn*.csv\"))\n",
    "    if alt_files:\n",
    "        DATA_RAW_FILE = alt_files[0]\n",
    "        print(f\"‚ö†Ô∏è Using detected dataset file:\\n   {DATA_RAW_FILE}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"‚ùå Missing dataset. Expected in:\\n   {DATA_RAW_DIR}\")\n",
    "\n",
    "# --- Load dataset ---\n",
    "df = pd.read_csv(DATA_RAW_FILE)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully:\")\n",
    "print(f\"   ‚Üí {len(df):,} rows √ó {len(df.columns)} columns\")\n",
    "print(f\"   Source: {DATA_RAW_FILE}\")\n",
    "\n",
    "# --- Preview dataset ---\n",
    "display(df.head(3))\n",
    "display(df.tail(3))\n",
    "df.info()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **How This Works**\n",
    "\n",
    "* **No functions** ‚Äî executes top-down, populating all variables in your notebook‚Äôs global scope.\n",
    "* Works from **any subfolder** inside `/Telco` (it walks upward until it finds the root).\n",
    "* Automatically detects your dataset name if it differs slightly (fallback via `glob(\"*Telco*Churn*.csv\")`).\n",
    "* Prints helpful diagnostics (row/column count, dataset location, and structure).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show the **next cell** you can add right after this (to create `df_raw`, `df_eda`, and your EDA-ready copies safely)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a458bdc",
   "metadata": {},
   "source": [
    "Here‚Äôs the **clean, professional answer** you‚Äôll want to follow for your Telco project structure:\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Situation\n",
    "\n",
    "You‚Äôve already done all the heavy lifting in **`00_Setup.ipynb`**:\n",
    "\n",
    "* Established project root (`PROJECT_ROOT`)\n",
    "* Defined all data directories (`DATA_RAW_DIR`, `DATA_PROCESSED_DIR`, etc.)\n",
    "* Possibly saved your cleaned dataset to `resources/data/processed/`\n",
    "* Imported foundational libraries (e.g., `pandas`, `pathlib`, etc.)\n",
    "\n",
    "Now in **`01_EDA.ipynb`**, your goal is to:\n",
    "\n",
    "> Load only what you *need to analyze*, not reconfigure the whole project.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Best Practice Approach\n",
    "\n",
    "### **1. Keep notebooks independent**\n",
    "\n",
    "Even though `00_Setup` created the directories, `01_EDA` should still *define its own lightweight imports and path logic*, so it can run standalone in your portfolio.\n",
    "\n",
    "That means:\n",
    "\n",
    "* Import libraries again\n",
    "* Redefine or import your path setup logic\n",
    "* Load dataset directly from the processed directory\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Recommended EDA Notebook Start**\n",
    "\n",
    "```python\n",
    "# ==========================================================\n",
    "# üìä 01_EDA ‚Äî Exploratory Data Analysis\n",
    "# ==========================================================\n",
    "\n",
    "# --- 1. Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 2. Locate Project Root (same logic as 00_Setup) ---\n",
    "current_path = Path.cwd().resolve()\n",
    "for parent in [current_path] + list(current_path.parents):\n",
    "    if parent.name == \"Telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå Could not find 'Telco' directory.\")\n",
    "\n",
    "# --- 3. Define Data Paths ---\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"resources\" / \"data\" / \"processed\"\n",
    "dataset_path = DATA_PROCESSED_DIR / \"telco_clean.csv\"\n",
    "\n",
    "# --- 4. Load Dataset ---\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded dataset: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Why This Is Ideal**\n",
    "\n",
    "| Reason                            | Benefit                                                             |\n",
    "| --------------------------------- | ------------------------------------------------------------------- |\n",
    "| üîÅ Independent execution          | Anyone can open `01_EDA.ipynb` directly and reproduce your analysis |\n",
    "| üß± Reusable path logic            | Same logic as `00_Setup`, no hardcoded paths                        |\n",
    "| üß© Clean imports                  | Each notebook shows exactly which libraries it uses                 |\n",
    "| üì¶ No dependency on kernel memory | You don‚Äôt need to ‚ÄúRun All‚Äù from the start for it to work           |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Optional ‚Äî Centralize the Setup**\n",
    "\n",
    "If you find this pattern repeating in multiple notebooks, move your setup logic to a **utility file** like:\n",
    "\n",
    "```bash\n",
    "Telco/\n",
    "‚îî‚îÄ‚îÄ src/\n",
    "    ‚îî‚îÄ‚îÄ utils/\n",
    "        ‚îî‚îÄ‚îÄ paths.py\n",
    "```\n",
    "\n",
    "**`paths.py`**\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_paths():\n",
    "    current_path = Path.cwd().resolve()\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        if parent.name == \"Telco\":\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"‚ùå Could not find 'Telco' directory.\")\n",
    "\n",
    "    DATA_PROCESSED = PROJECT_ROOT / \"resources\" / \"data\" / \"processed\"\n",
    "    return {\"project_root\": PROJECT_ROOT, \"data_processed\": DATA_PROCESSED}\n",
    "```\n",
    "\n",
    "Then in your **`01_EDA.ipynb`**:\n",
    "\n",
    "```python\n",
    "from src.utils.paths import get_project_paths\n",
    "import pandas as pd\n",
    "\n",
    "paths = get_project_paths()\n",
    "df = pd.read_csv(paths[\"data_processed\"] / \"telco_clean.csv\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What *Not* to Do**\n",
    "\n",
    "üö´ Don‚Äôt rely on variables like `PROJECT_ROOT` or `df` from the previous notebook ‚Äî they disappear if you restart your kernel.\n",
    "üö´ Don‚Äôt re-run setup cells that create directories or rewrite files ‚Äî that belongs only in `00_Setup`.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show you a **clean header markdown cell + first two code cells** (ready to paste at the top of `01_EDA.ipynb`) that make it look fully polished and consistent with your `00_Setup` style?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a70f9",
   "metadata": {},
   "source": [
    "# 02_Data_Validation_and_Cleaning.ipynb\n",
    "\n",
    "For **Level 3**, keep it **monolithic + notebook-only** so you can see every step, then refactor at Level 4.\n",
    "\n",
    "Here‚Äôs a tidy way to do it now (no functions), plus a clean migration path later.\n",
    "\n",
    "# What to do now (Level 3 ‚Äî monolithic notebook)\n",
    "\n",
    "Use **one notebook cell per step**, no defs, no imports reuse across modules. Keep it linear and explicit.\n",
    "\n",
    "1. **Imports & paths (inline constants)**\n",
    "\n",
    "```python\n",
    "import json, re, hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, yaml\n",
    "\n",
    "SCHEMA_YAML = Path(\"configs/schema.yaml\")\n",
    "CONFIG_YAML = Path(\"configs/config.yaml\")\n",
    "\n",
    "with open(SCHEMA_YAML) as f: SCHEMA = yaml.safe_load(f)\n",
    "with open(CONFIG_YAML) as f: CFG = yaml.safe_load(f)\n",
    "\n",
    "RAW = Path(CFG[\"data\"][\"raw_path\"])\n",
    "CLEAN = Path(CFG[\"data\"][\"processed_path\"])\n",
    "REPORTS = Path(CFG[\"data\"][\"reports_dir\"]); REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "ISSUE_LOG = Path(CFG[\"validation\"][\"issue_log_path\"])\n",
    "VALID_JSON = Path(CFG[\"exports\"][\"validation_summary\"])\n",
    "BASELINE_JSON = Path(CFG[\"exports\"][\"baseline_stats\"])\n",
    "ENV_JSON = Path(CFG[\"exports\"][\"environment_snapshot\"])\n",
    "```\n",
    "\n",
    "2. **Load data (before/after)**\n",
    "\n",
    "```python\n",
    "df_before = pd.read_csv(RAW)\n",
    "df_after = pd.read_csv(CLEAN) if CLEAN.exists() else None  # may be None on first run\n",
    "```\n",
    "\n",
    "3. **Schema & dtype checks (inline)**\n",
    "\n",
    "```python\n",
    "expected = SCHEMA[\"expected_columns\"]\n",
    "required = set(SCHEMA[\"required_columns\"])\n",
    "present = set(df_before.columns)\n",
    "\n",
    "missing_required = list(required - present)\n",
    "unexpected_cols = list(present - set(expected.keys()))\n",
    "\n",
    "dfv = df_before.copy()\n",
    "for col, dt in expected.items():\n",
    "    if col in dfv and dt in (\"float64\",\"float32\",\"int64\",\"int32\"):\n",
    "        dfv[col] = pd.to_numeric(dfv[col], errors=\"coerce\")\n",
    "    elif col in dfv and dt in (\"string\",\"category\"):\n",
    "        dfv[col] = dfv[col].astype(\"string\")\n",
    "```\n",
    "\n",
    "4. **Missing & empties (inline)**\n",
    "\n",
    "```python\n",
    "def _empty_like(s):\n",
    "    s = s.astype(\"string\")\n",
    "    return int(s.isna().sum() + (s==\"\").sum() + (s==\" \").sum())\n",
    "\n",
    "allow_missing = set(SCHEMA.get(\"allow_missing\", []))\n",
    "missing_counts = {c: _empty_like(dfv[c]) for c in dfv.columns}\n",
    "```\n",
    "\n",
    "5. **PK, target, leakage, numeric bounds & outliers (inline)**\n",
    "\n",
    "```python\n",
    "issues = []\n",
    "def log_issue(idx, col, rule, sev, details): issues.append(\n",
    "    {\"id\":idx, \"column\":col, \"rule\":rule, \"severity\":sev, \"details\":details}\n",
    ")\n",
    "\n",
    "pk = SCHEMA.get(\"primary_key\"); target = SCHEMA.get(\"target_column\")\n",
    "exp_cats = SCHEMA.get(\"expected_categories\", {})\n",
    "bounds = SCHEMA.get(\"numeric_bounds\", {})\n",
    "z_thr = float(CFG[\"outlier_detection\"][\"zscore_threshold\"])\n",
    "iqr_k = float(CFG[\"outlier_detection\"][\"iqr_multiplier\"])\n",
    "hi_card = int(CFG[\"categorical\"][\"high_cardinality_threshold\"])\n",
    "\n",
    "# PK\n",
    "if pk and pk in dfv:\n",
    "    if dfv[pk].isna().any():\n",
    "        log_issue(\"ALL\", pk, \"pk_nulls\", \"high\", f\"count={int(dfv[pk].isna().sum())}\")\n",
    "    dup = int(dfv.duplicated(pk).sum())\n",
    "    if dup: log_issue(\"ALL\", pk, \"pk_duplicates\", \"high\", f\"count={dup}\")\n",
    "\n",
    "# Target + leakage\n",
    "if target in dfv and exp_cats.get(target):\n",
    "    bad = dfv[~dfv[target].isin(exp_cats[target])][target].dropna().unique().tolist()\n",
    "    if bad: log_issue(\"ALL\", target, \"invalid_target_labels\", \"high\", f\"{bad}\")\n",
    "leak_cols = [c for c in dfv.columns if re.search(r\"(churn|cancel|termination|disconnect)\", c, re.I) and c!=target]\n",
    "\n",
    "# Numeric bounds + outliers (log only)\n",
    "for c, b in bounds.items():\n",
    "    if c not in dfv: continue\n",
    "    s = pd.to_numeric(dfv[c], errors=\"coerce\")\n",
    "    below = int((s < b.get(\"min\", -np.inf)).sum()); above = int((s > b.get(\"max\", np.inf)).sum())\n",
    "    if below or above: log_issue(\"ALL\", c, \"bounds_violation\", \"medium\", f\"below={below}, above={above}, bounds={b}\")\n",
    "    mu, sd = s.mean(), s.std(ddof=0)\n",
    "    if sd and np.isfinite(sd) and ((s-mu).abs()>z_thr*sd).any():\n",
    "        log_issue(\"ALL\", c, \"zscore_outliers\", \"low\", f\"z_thr={z_thr}\")\n",
    "    q1,q3 = s.quantile(0.25), s.quantile(0.75); iqr=q3-q1\n",
    "    if pd.notna(iqr) and iqr>0:\n",
    "        lo,hi = q1-iqr_k*iqr, q3+iqr_k*iqr\n",
    "        if ((s<lo)|(s>hi)).any(): log_issue(\"ALL\", c, \"iqr_outliers\", \"low\", f\"iqr_k={iqr_k}\")\n",
    "```\n",
    "\n",
    "6. **Categorical hygiene & logical check (inline)**\n",
    "\n",
    "```python\n",
    "# Unexpected categories\n",
    "for c, allowed in exp_cats.items():\n",
    "    if c in dfv:\n",
    "        uniq = set(pd.Series(dfv[c], dtype=\"string\").dropna().unique())\n",
    "        bad = sorted(list(uniq - set(allowed)))\n",
    "        if bad: log_issue(\"ALL\", c, \"unexpected_categories\", \"medium\", f\"{bad}\")\n",
    "\n",
    "# High cardinality\n",
    "for c in dfv.select_dtypes(include=[\"object\",\"string\"]).columns:\n",
    "    if dfv[c].nunique(dropna=True) > hi_card:\n",
    "        log_issue(\"ALL\", c, \"high_cardinality\", \"low\", f\"nunique={int(dfv[c].nunique())}\")\n",
    "\n",
    "# tenure vs TotalCharges\n",
    "if {\"tenure\",\"TotalCharges\"}.issubset(dfv.columns):\n",
    "    t = pd.to_numeric(dfv[\"tenure\"], errors=\"coerce\")\n",
    "    tc = pd.to_numeric(dfv[\"TotalCharges\"], errors=\"coerce\")\n",
    "    bad = int(((t>0) & ((tc<=0)|tc.isna())).sum())\n",
    "    if bad: log_issue(\"ALL\", \"tenure|TotalCharges\", \"logical_inconsistency\", \"medium\", f\"count={bad}\")\n",
    "```\n",
    "\n",
    "7. **Issue log + baseline stats (inline)**\n",
    "\n",
    "```python\n",
    "pd.DataFrame(issues).to_csv(ISSUE_LOG, index=False)\n",
    "\n",
    "def baseline_stats(df):\n",
    "    out={\"numeric\":{}, \"categorical\":{}}\n",
    "    for c in df.columns:\n",
    "        s=df[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            out[\"numeric\"][c]={ \"count\": int(s.count()), \"mean\": float(s.mean()),\n",
    "                                \"std\": float(s.std(ddof=0)), \"min\": float(s.min()),\n",
    "                                \"p50\": float(s.quantile(0.5)), \"max\": float(s.max()) }\n",
    "        else:\n",
    "            vc = s.astype(\"string\").value_counts(dropna=True).head(20).to_dict()\n",
    "            out[\"categorical\"][c] = {\"top_values\": {str(k): int(v) for k,v in vc.items()}}\n",
    "    return out\n",
    "\n",
    "with open(BASELINE_JSON, \"w\") as f:\n",
    "    json.dump(baseline_stats(dfv), f, indent=2)\n",
    "```\n",
    "\n",
    "8. **DQ score + summary JSON (inline)**\n",
    "\n",
    "```python\n",
    "def _hash(path):\n",
    "    if not Path(path).exists(): return \"\"\n",
    "    h=hashlib.sha256()\n",
    "    with open(path,\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1<<20), b\"\"): h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "base_checks=[]\n",
    "base_checks.append(len(missing_required)==0)\n",
    "if pk and pk in dfv: base_checks.append(dfv[pk].notna().all() and dfv[pk].nunique(dropna=False)==len(dfv))\n",
    "dq_score = round(100.0 * (sum(base_checks)/len(base_checks) if base_checks else 1.0), 1)\n",
    "\n",
    "summary = {\n",
    "  \"dataset\": {\n",
    "    \"raw_path\": str(RAW), \"clean_path\": str(CLEAN) if CLEAN.exists() else \"\",\n",
    "    \"rows_before\": int(len(df_before)),\n",
    "    \"rows_after\": int(len(df_after)) if isinstance(df_after, pd.DataFrame) else None,\n",
    "    \"columns\": list(df_before.columns)\n",
    "  },\n",
    "  \"schema\": {\"missing_required_columns\": missing_required, \"unexpected_columns\": unexpected_cols,\n",
    "             \"dtype_expected\": SCHEMA[\"expected_columns\"]},\n",
    "  \"quality\": {\"dq_score_percent\": dq_score, \"missing_counts\": {k:int(v) for k,v in missing_counts.items()}},\n",
    "  \"rules\": {\"numeric_bounds\": bounds, \"expected_categories\": exp_cats, \"primary_key\": pk,\n",
    "            \"target_column\": target, \"leakage_name_hits\": leak_cols,\n",
    "            \"outlier_params\": {\"zscore_threshold\": z_thr, \"iqr_multiplier\": iqr_k}},\n",
    "  \"artifacts\": {\"issue_log_csv\": str(ISSUE_LOG), \"baseline_stats_json\": str(BASELINE_JSON),\n",
    "                \"environment_snapshot_json\": str(ENV_JSON)},\n",
    "  \"provenance\": {\"raw_sha256\": _hash(RAW), \"clean_sha256\": _hash(CLEAN) if CLEAN.exists() else \"\"}\n",
    "}\n",
    "with open(VALID_JSON, \"w\") as f: json.dump(summary, f, indent=2)\n",
    "print(f\"‚úÖ Wrote\\n- {VALID_JSON}\\n- {ISSUE_LOG}\\n- {BASELINE_JSON}\")\n",
    "```\n",
    "\n",
    "> This is **pure notebook**: sequential, no functions, no modules. It produces your `validation_summary.json`, `issue_log.csv`, and `baseline_stats.json`.\n",
    "\n",
    "---\n",
    "\n",
    "# How to migrate later (Level 4 ‚Äî modular)\n",
    "\n",
    "When you‚Äôre ready:\n",
    "\n",
    "* Move logic into `src/validation/report.py`:\n",
    "\n",
    "  * `load_yaml()`, `hash_file()`, `run_checks(df, schema, cfg)`, `write_issue_log()`, `write_summary()`\n",
    "* Replace notebook cells with:\n",
    "\n",
    "  ```python\n",
    "  from src.validation.report import generate_validation_summary\n",
    "  generate_validation_summary(schema_path=\"configs/schema.yaml\", config_path=\"configs/config.yaml\")\n",
    "  ```\n",
    "* Add a small CLI (`python -m src.validation.report --schema configs/schema.yaml --config configs/config.yaml`)\n",
    "* Add tests for rules (e.g., bounds violations, PK duplicates).\n",
    "\n",
    "If you want, I can produce the **exact file skeletons** and a minimal `generate_validation_summary()` function when you hit Level 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885c817",
   "metadata": {},
   "source": [
    "### Standard Investigation Routine:\n",
    "\n",
    "```python\n",
    "def investigate_column(df, col):\n",
    "    \"\"\"Standard data detective routine for any column\"\"\"\n",
    "    print(f\"=== INVESTIGATING {col} ===\")\n",
    "    print(f\"Data type: {df[col].dtype}\")\n",
    "    print(f\"Null count: {df[col].isnull().sum()}\")\n",
    "    print(f\"Unique values: {df[col].nunique()}\")\n",
    "    print(f\"Sample values: {df[col].head(5).tolist()}\")\n",
    "    \n",
    "    if df[col].dtype == 'object':\n",
    "        print(f\"All unique values: {df[col].unique()}\")\n",
    "        \n",
    "        # Check if it should be numeric\n",
    "        print(\"Testing numeric conversion...\")\n",
    "        converted = pd.to_numeric(df[col], errors='coerce')\n",
    "        conversion_issues = converted.isnull().sum() - df[col].isnull().sum()\n",
    "        if conversion_issues > 0:\n",
    "            print(f\"üö® {conversion_issues} values can't convert to numeric!\")\n",
    "            problematic = df[converted.isnull() & df[col].notnull()][col].unique()\n",
    "            print(f\"Problematic values: {problematic}\")\n",
    "\n",
    "# Use it on suspicious columns\n",
    "investigate_column(df, 'TotalCharges')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5d3f6",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_base_dir() -> Path:\n",
    "    # Works both in scripts (via __file__) and in notebooks (fallback to CWD)\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent.parent\n",
    "    except NameError:\n",
    "        # In notebooks: put this file under project_root/notebooks/ to keep same base\n",
    "        return Path.cwd().resolve().parent  # adjust if your notebook sits at project_root\n",
    "\n",
    "BASE_DIR = get_base_dir()\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DATA_RAW = DATA_DIR / \"raw\"\n",
    "DATA_PROCESSED = DATA_DIR / \"processed\"\n",
    "OUTPUTS = BASE_DIR / \"outputs\"\n",
    "FIGURES = OUTPUTS / \"figures\"\n",
    "REPORTS = OUTPUTS / \"reports\"\n",
    "MODELS = BASE_DIR / \"models\"\n",
    "\n",
    "# Ensure expected folders exist\n",
    "for p in [DATA_RAW, DATA_PROCESSED, FIGURES, REPORTS, MODELS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TELCO_CSV = DATA_RAW / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "if TELCO_CSV.exists():\n",
    "    print(f\"‚úÖ Dataset found: {TELCO_CSV}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Missing dataset. Place it here:\\n   {TELCO_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c1869",
   "metadata": {},
   "source": [
    "Short answer: **A is the best starting point** for a disciplined Telco project‚Äîit's explicit about project roots and scales well.\n",
    "**B** is too fragile (relative to wherever you run it).\n",
    "**C** is great for *bootstrapping folders + copying data*, but its `BASE_DIR = Path.cwd().resolve().parent` is brittle and the copy step may hide mistakes.\n",
    "\n",
    "### Why A wins\n",
    "\n",
    "* Uses a **stable project root** (via `__file__`), not just ‚Äúwhere I launched the notebook‚Äù.\n",
    "* Keeps paths centralized (`BASE_DIR / ...`)‚Äîclean, testable, refactor-friendly.\n",
    "* Easy to reuse across modules and scripts.\n",
    "\n",
    "### Nice upgrade: A + a bit of C (create dirs, validate file)\n",
    "\n",
    "Use A‚Äôs root handling, add safe directory creation and a clear check for the CSV:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "def get_base_dir() -> Path:\n",
    "    # Works both in scripts (via __file__) and in notebooks (fallback to CWD)\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent.parent\n",
    "    except NameError:\n",
    "        # In notebooks: put this file under project_root/notebooks/ to keep same base\n",
    "        return Path.cwd().resolve().parent  # adjust if your notebook sits at project_root\n",
    "\n",
    "BASE_DIR = get_base_dir()\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DATA_RAW = DATA_DIR / \"raw\"\n",
    "DATA_PROCESSED = DATA_DIR / \"processed\"\n",
    "OUTPUTS = BASE_DIR / \"outputs\"\n",
    "FIGURES = OUTPUTS / \"figures\"\n",
    "REPORTS = OUTPUTS / \"reports\"\n",
    "MODELS = BASE_DIR / \"models\"\n",
    "\n",
    "# Ensure expected folders exist\n",
    "for p in [DATA_RAW, DATA_PROCESSED, FIGURES, REPORTS, MODELS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TELCO_CSV = DATA_RAW / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "if TELCO_CSV.exists():\n",
    "    print(f\"‚úÖ Dataset found: {TELCO_CSV}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Missing dataset. Place it here:\\n   {TELCO_CSV}\")\n",
    "```\n",
    "\n",
    "**Usage tip:** keep this in `src/telco_utils/paths.py` and import it everywhere.\n",
    "If you also want an optional one-time ‚Äúcopy from downloads‚Äù step, do it in a **separate** bootstrap script to avoid silently masking missing-data bugs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0c586",
   "metadata": {},
   "source": [
    "\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils.py  # Start this immediately!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12bcda",
   "metadata": {},
   "source": [
    "0.2 Data Acquisition & First Look\n",
    "Load the CSV file\n",
    "Display first/last rows\n",
    "Check shape and size\n",
    "Identify column names\n",
    "Save working copy\n",
    "Extract first function immediately:\n",
    "# src/utils.py\n",
    "def load_telco_data(filepath):\n",
    "    \"\"\"Load and display basic info about telco data.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Loaded {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils.py\n",
    "def get_data_overview(df):\n",
    "    \"\"\"Generate comprehensive data overview.\"\"\"\n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'dtypes': df.dtypes.value_counts(),\n",
    "        'missing': df.isnull().sum(),\n",
    "        'memory': df.memory_usage().sum() / 1024**2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 Detect cardinality & low-frequency categories\n",
    "\n",
    "# Cardinality summary and dominance of top category (helps spot imbalance)\n",
    "def top_prop(s: pd.Series) -> float:\n",
    "    vc = s.value_counts(normalize=True)\n",
    "    return float(vc.iloc[0]) if not vc.empty else np.nan\n",
    "\n",
    "card_summary = pd.DataFrame({\n",
    "    \"unique_count\": df[cat_cols].nunique(),\n",
    "    \"top_level_prop\": df[cat_cols].apply(top_prop).round(3)\n",
    "}).sort_values([\"unique_count\", \"top_level_prop\"], ascending=[False, False])\n",
    "\n",
    "print(\"Categorical cardinality / dominance summary:\")\n",
    "display(card_summary)\n",
    "\n",
    "# Rare-level flags (e.g., levels with < 1% of rows)\n",
    "rare_threshold = 0.01\n",
    "rare_levels = {}\n",
    "for c in cat_cols:\n",
    "    prop = df[c].value_counts(normalize=True)\n",
    "    rare = prop[prop < rare_threshold]\n",
    "    if not rare.empty:\n",
    "        rare_levels[c] = rare.round(4)\n",
    "if rare_levels:\n",
    "    print(\"\\nLevels with <1% frequency (consider grouping into 'Other'):\")\n",
    "    for k, v in rare_levels.items():\n",
    "        print(f\"- {k}:\")\n",
    "        display(v.to_frame(\"proportion\"))\n",
    "else:\n",
    "    print(\"\\nNo rare levels (<1%) detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc56b14",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Analyze distribution of numerical features.\n",
    "Pattern: repetitive plotting code that can later be extracted to a function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8e2f2",
   "metadata": {},
   "source": [
    "```md\n",
    "`__file__` is a **special variable** that exists when Python runs a script (e.g., `python myscript.py`).\n",
    "In **Jupyter notebooks**, there‚Äôs no script file being executed ‚Äî so `__file__` is *undefined*, which causes the `NameError`.\n",
    "```\n",
    "\n",
    "```py\n",
    "# üìÇ Unified project paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Define the \"project root\" ‚Äî always the Telco folder\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[2]  # works in scripts\n",
    "# üëâ If you're inside a Jupyter notebook, use:\n",
    "# PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "\n",
    "# Step 2: Define shared data paths (always same regardless of Level)\n",
    "DATA_ROOT = PROJECT_ROOT / \"resources\" / \"data\"\n",
    "DATA_RAW = DATA_ROOT / \"raw\" / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "DATA_PROCESSED = DATA_ROOT / \"processed\"\n",
    "\n",
    "# Step 3: Confirm\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìÑ Raw dataset:  {DATA_RAW}\")\n",
    "print(f\"üìÇ Processed:    {DATA_PROCESSED}\")\n",
    "\n",
    "# Step 4: Safety check\n",
    "for p in [DATA_RAW.parent, DATA_PROCESSED]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATA_RAW.exists():\n",
    "    print(\"‚úÖ Dataset found.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Missing dataset: {DATA_RAW}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01c715",
   "metadata": {},
   "source": [
    "Awesome‚Äîhere‚Äôs a **Skill Level 3/10 Telco Churn mini-project** that starts in notebooks and then shows how to extract what you built into a tidy, modular codebase. Minimal math, strong guardrails, and clear copy-paste cells.\n",
    "\n",
    "---\n",
    "\n",
    "# Telco Churn (Skill 3/10)\n",
    "\n",
    "## What you‚Äôll build\n",
    "\n",
    "1. A simple churn classifier (Logistic Regression) using scikit-learn.\n",
    "2. Four small notebooks to go from data ‚Üí model ‚Üí evaluation ‚Üí batch inference.\n",
    "3. Then you‚Äôll refactor that workflow into a clean `src/` Python package + tiny CLIs.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick setup\n",
    "\n",
    "```bash\n",
    "# (zsh/macOS)\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "pip install pandas numpy scikit-learn matplotlib joblib pyyaml\n",
    "# (optional) seaborn\n",
    "```\n",
    "\n",
    "Put your CSV in `data/raw/` (e.g., `data/raw/telco.csv`).\n",
    "You should have typical columns like: `customerID`, `gender`, `SeniorCitizen`, `Partner`, `Dependents`, `tenure`, `PhoneService`, `MultipleLines`, `InternetService`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`, `Contract`, `PaperlessBilling`, `PaymentMethod`, `MonthlyCharges`, `TotalCharges`, `Churn`.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook 1 ‚Äî Load & EDA (`01_load_eda.ipynb`)\n",
    "\n",
    "### Cell 1: Imports & read\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/raw/telco.csv\")\n",
    "print(df.shape)\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### Cell 2: Basic quality checks\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "df.isna().sum().sort_values(ascending=False).head(10)\n",
    "df.describe(include='all').T.head(20)\n",
    "```\n",
    "\n",
    "### Cell 3: Light cleaning (Skill-3 friendly)\n",
    "\n",
    "```python\n",
    "# Fix TotalCharges: sometimes it's blank string; coerce to numeric\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Drop rows missing the label if any\n",
    "df = df.dropna(subset=['Churn']).copy()\n",
    "\n",
    "# Fill any remaining numeric NaNs with column medians\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Standardize target to 0/1\n",
    "df['Churn'] = (df['Churn'].astype(str).str.strip().str.lower() == 'yes').astype(int)\n",
    "\n",
    "# Drop non-predictive identifiers\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop(columns=['customerID'])\n",
    "\n",
    "df.to_csv(\"data/processed/telco_clean.csv\", index=False)\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### Cell 4: Quick plots (optional)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Churn'].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Churn distribution (0=No, 1=Yes)\")\n",
    "plt.show()\n",
    "\n",
    "if 'Contract' in df.columns:\n",
    "    (df.groupby('Contract')['Churn']\n",
    "      .mean()\n",
    "      .sort_values()\n",
    "      .plot(kind=\"bar\"))\n",
    "    plt.title(\"Mean churn rate by Contract\")\n",
    "    plt.ylabel(\"Churn rate\")\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook 2 ‚Äî Preprocess & Train (`02_preprocess_train.ipynb`)\n",
    "\n",
    "### Cell 1: Imports & split\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"data/processed/telco_clean.csv\")\n",
    "\n",
    "y = df['Churn']\n",
    "X = df.drop(columns=['Churn'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape\n",
    "```\n",
    "\n",
    "### Cell 2: Build preprocessing (numeric + categorical)\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "num_cols = X_train.select_dtypes(include='number').columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Cell 3: Model pipeline (LogReg)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Cell 4: Save artifacts\n",
    "\n",
    "```python\n",
    "import joblib, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(clf, \"models/churn_pipeline.joblib\")\n",
    "joblib.dump({\"num_cols\": num_cols, \"cat_cols\": cat_cols}, \"models/columns.joblib\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook 3 ‚Äî Evaluate (`03_evaluate.ipynb`)\n",
    "\n",
    "```python\n",
    "import joblib, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay, roc_auc_score\n",
    "\n",
    "clf = joblib.load(\"models/churn_pipeline.joblib\")\n",
    "df = pd.read_csv(\"data/processed/telco_clean.csv\")\n",
    "y = df['Churn']; X = df.drop(columns=['Churn'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"ROC Curve\"); plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook 4 ‚Äî Batch Inference (`04_inference_batch.ipynb`)\n",
    "\n",
    "```python\n",
    "import pandas as pd, joblib\n",
    "\n",
    "clf = joblib.load(\"models/churn_pipeline.joblib\")\n",
    "new_data = pd.read_csv(\"data/scoring/new_customers.csv\")  # your new records\n",
    "pred_proba = clf.predict_proba(new_data)[:,1]\n",
    "pred_label = (pred_proba >= 0.5).astype(int)\n",
    "\n",
    "scored = new_data.copy()\n",
    "scored['churn_proba'] = pred_proba\n",
    "scored['churn_pred']  = pred_label\n",
    "scored.to_csv(\"data/scoring/new_customers_scored.csv\", index=False)\n",
    "scored.head()\n",
    "```\n",
    "\n",
    "That‚Äôs the full notebook-first flow. Next: extract it into a small, modular codebase.\n",
    "\n",
    "---\n",
    "\n",
    "# Extracting to a Modular Codebase\n",
    "\n",
    "## Suggested structure\n",
    "\n",
    "```\n",
    "telco-churn/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ scoring/\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 01_load_eda.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocess_train.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 03_evaluate.ipynb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 04_inference_batch.ipynb\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ churn/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ data.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ features.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ model.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ evaluate.py\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ io.py\n",
    "‚îú‚îÄ‚îÄ scripts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ predict.py\n",
    "‚îú‚îÄ‚îÄ configs/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ config.yaml\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "## `configs/config.yaml` (single source of truth)\n",
    "\n",
    "```yaml\n",
    "paths:\n",
    "  raw: data/raw/telco.csv\n",
    "  processed: data/processed/telco_clean.csv\n",
    "  model: models/churn_pipeline.joblib\n",
    "  columns: models/columns.joblib\n",
    "\n",
    "training:\n",
    "  test_size: 0.2\n",
    "  random_state: 42\n",
    "  model:\n",
    "    type: logistic_regression\n",
    "    max_iter: 200\n",
    "\n",
    "inference:\n",
    "  threshold: 0.5\n",
    "```\n",
    "\n",
    "## `src/churn/io.py`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def write_csv(df: pd.DataFrame, path: str) -> None:\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def save_obj(obj, path: str) -> None:\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(obj, path)\n",
    "\n",
    "def load_obj(path: str):\n",
    "    return joblib.load(path)\n",
    "```\n",
    "\n",
    "## `src/churn/data.py`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def basic_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Label\n",
    "    df['Churn'] = (df['Churn'].astype(str).str.strip().str.lower() == 'yes').astype(int)\n",
    "    # Numerics\n",
    "    if 'TotalCharges' in df.columns:\n",
    "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    num_cols = df.select_dtypes(include='number').columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    # Drop IDs\n",
    "    if 'customerID' in df.columns:\n",
    "        df = df.drop(columns=['customerID'])\n",
    "    return df\n",
    "\n",
    "def split_xy(df: pd.DataFrame, target: str = 'Churn'):\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target])\n",
    "    return X, y\n",
    "```\n",
    "\n",
    "## `src/churn/features.py`\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def build_preprocess(X):\n",
    "    num_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "    cat_cols = X.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocess, num_cols, cat_cols\n",
    "```\n",
    "\n",
    "## `src/churn/model.py`\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def build_pipeline(preprocess, max_iter=200):\n",
    "    return Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=max_iter))\n",
    "    ])\n",
    "```\n",
    "\n",
    "## `src/churn/evaluate.py`\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "def evaluate(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    report = classification_report(y_test, y_pred, digits=3, output_dict=False)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    return {\"report\": report, \"roc_auc\": auc}\n",
    "```\n",
    "\n",
    "## `scripts/train.py`\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from churn.io import read_csv, write_csv, save_obj\n",
    "from churn.data import basic_clean, split_xy\n",
    "from churn.features import build_preprocess\n",
    "from churn.model import build_pipeline\n",
    "\n",
    "def main():\n",
    "    cfg = yaml.safe_load(open(\"configs/config.yaml\"))\n",
    "    df = read_csv(cfg[\"paths\"][\"raw\"])\n",
    "    df = basic_clean(df)\n",
    "    write_csv(df, cfg[\"paths\"][\"processed\"])\n",
    "\n",
    "    X, y = split_xy(df, target=\"Churn\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=cfg[\"training\"][\"test_size\"],\n",
    "        random_state=cfg[\"training\"][\"random_state\"],\n",
    "        stratify=y\n",
    "    )\n",
    "    preprocess, num_cols, cat_cols = build_preprocess(X_train)\n",
    "    clf = build_pipeline(preprocess, max_iter=cfg[\"training\"][\"model\"][\"max_iter\"])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    save_obj(clf, cfg[\"paths\"][\"model\"])\n",
    "    save_obj({\"num_cols\": num_cols, \"cat_cols\": cat_cols}, cfg[\"paths\"][\"columns\"])\n",
    "    print(\"Model trained and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "## `scripts/predict.py`\n",
    "\n",
    "```python\n",
    "import sys, yaml\n",
    "import pandas as pd\n",
    "from churn.io import load_obj\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python scripts/predict.py <input_csv>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    cfg = yaml.safe_load(open(\"configs/config.yaml\"))\n",
    "    input_csv = sys.argv[1]\n",
    "    df_new = pd.read_csv(input_csv)\n",
    "\n",
    "    clf = load_obj(cfg[\"paths\"][\"model\"])\n",
    "    proba = clf.predict_proba(df_new)[:,1]\n",
    "    pred  = (proba >= cfg[\"inference\"][\"threshold\"]).astype(int)\n",
    "    df_new[\"churn_proba\"] = proba\n",
    "    df_new[\"churn_pred\"] = pred\n",
    "    out = input_csv.replace(\".csv\", \"_scored.csv\")\n",
    "    df_new.to_csv(out, index=False)\n",
    "    print(f\"Scored: {out}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to move from notebooks ‚Üí modules (simple recipe)\n",
    "\n",
    "1. **Identify stable chunks** in your notebooks (loading/cleaning, preprocessing, model build, evaluation).\n",
    "2. **Copy each chunk** into a small function (see `data.py`, `features.py`, `model.py`, `evaluate.py`).\n",
    "3. **Replace notebook constants** (paths, test_size, thresholds) with a **`config.yaml`** and read them in your scripts.\n",
    "4. **Create tiny CLIs** (`scripts/train.py`, `scripts/predict.py`) that call those functions in order.\n",
    "5. Keep notebooks for **exploration and demos**, but run production tasks via `python scripts/train.py` and `python scripts/predict.py data/scoring/new_customers.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## Run it end-to-end\n",
    "\n",
    "```bash\n",
    "# Train\n",
    "python scripts/train.py\n",
    "\n",
    "# Score new data\n",
    "python scripts/predict.py data/scoring/new_customers.csv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What you learned (Level-3 takeaways)\n",
    "\n",
    "* A single **sklearn Pipeline** with `ColumnTransformer` is enough to ship a baseline.\n",
    "* **Config-first**: paths & knobs live in YAML, not scattered through notebooks.\n",
    "* **Separation of concerns**: `data.py` (clean), `features.py` (prep), `model.py` (build), `evaluate.py` (metrics), `io.py` (I/O), `scripts/` (entrypoints).\n",
    "\n",
    "If you want, I can tailor the code to your exact column names (or plug in class-weighting, simple threshold tuning, or a RandomForest baseline) ‚Äî but this is a clean, Skill-3 path you can run today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cc310",
   "metadata": {},
   "source": [
    "Excellent catch ‚Äî and that‚Äôs actually a *strategic* design choice for **Skill Level 3**, not an omission.\n",
    "\n",
    "Let me break it down clearly üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Goal of Skill Level 3\n",
    "\n",
    "This level is about **understanding the workflow**, not the modeling subtleties.\n",
    "You‚Äôre learning how to:\n",
    "\n",
    "1. Move from **notebook exploration ‚Üí structured pipeline**\n",
    "2. Understand **data ‚Üí features ‚Üí model ‚Üí artifact ‚Üí inference** flow\n",
    "3. Learn clean **modularization patterns** (YAML configs, folder structure, scripts)\n",
    "\n",
    "Visualization is incredibly useful ‚Äî but at this level, it tends to:\n",
    "\n",
    "* Distract from the *pipeline extraction process* (too many matplotlib/seaborn steps clutter the logic)\n",
    "* Encourage *manual exploration*, whereas we‚Äôre training the mindset of *reproducible processing*\n",
    "* Add dependencies (plotly, seaborn) before you‚Äôve nailed the reproducibility flow\n",
    "\n",
    "So: Skill-3 keeps it **minimal, deterministic, and linear** ‚Äî every cell builds toward automation.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° When Viz *Should* Enter\n",
    "\n",
    "Here‚Äôs the progression:\n",
    "\n",
    "| Skill Level      | Visualization Focus                          | Purpose                                      |\n",
    "| ---------------- | -------------------------------------------- | -------------------------------------------- |\n",
    "| **1‚Äì2**          | Bar/pie plots in notebook                    | Learn basic EDA                              |\n",
    "| **3 (this one)** | Light EDA only                               | Focus on reproducible modular pipeline       |\n",
    "| **4‚Äì5**          | Matplotlib/Seaborn dashboards                | Compare features, churn rates, tenure trends |\n",
    "| **6+**           | Plotly dashboards, SHAP, feature importances | Interpretability & explainability            |\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ That said‚Ä¶\n",
    "\n",
    "If you *do* want to integrate visualization at Skill 3 (say, for a learning bonus), add a single optional notebook called:\n",
    "\n",
    "### `05_visualize_results.ipynb`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"data/processed/telco_clean.csv\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=\"Contract\", y=\"Churn\", data=df)\n",
    "plt.title(\"Churn Rate by Contract Type\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df[\"tenure\"], hue=df[\"Churn\"], multiple=\"stack\", bins=30)\n",
    "plt.title(\"Tenure Distribution by Churn\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "It‚Äôs clean, safe, and doesn‚Äôt alter your codebase logic.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "I left out visualization intentionally because:\n",
    "\n",
    "* This level focuses on **workflow discipline** (reproducibility + modularization)\n",
    "* Visualization returns later as **interpretability** at Skill-4 and beyond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56579d2",
   "metadata": {},
   "source": [
    "># Would you like me to show where to naturally plug visualizations into this pipeline (e.g., post-EDA and post-evaluation)?\n",
    "\n",
    "Great‚Äîhere‚Äôs exactly where to plug visualizations into the Skill-3 pipeline, with drop-in cells and (optional) modularized viz so it stays reproducible.\n",
    "\n",
    "# Where to add viz\n",
    "\n",
    "## 1) In the notebooks\n",
    "\n",
    "### A) `01_load_eda.ipynb` ‚Äî quick sanity EDA (after cleaning/saving)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/processed/telco_clean.csv\")\n",
    "\n",
    "# Target balance\n",
    "df['Churn'].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Churn distribution (0=No, 1=Yes)\"); plt.xlabel(\"Churn\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Churn rate by Contract\n",
    "if 'Contract' in df.columns:\n",
    "    (df.groupby('Contract')['Churn'].mean()\n",
    "      .sort_values()\n",
    "      .plot(kind=\"bar\"))\n",
    "    plt.title(\"Mean churn rate by Contract\"); plt.ylabel(\"Churn rate\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Tenure vs churn (binned)\n",
    "if 'tenure' in df.columns:\n",
    "    bins = [0, 6, 12, 24, 36, 48, 60, 72]\n",
    "    cut = pd.cut(df['tenure'], bins=bins, right=False)\n",
    "    (df.groupby(cut)['Churn'].mean()\n",
    "      .plot(marker=\"o\"))\n",
    "    plt.title(\"Churn rate by Tenure bin\"); plt.xlabel(\"Tenure bin (months)\"); plt.ylabel(\"Churn rate\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "```\n",
    "\n",
    "### B) `03_evaluate.ipynb` ‚Äî evaluation plots (after computing preds)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix\n",
    "\n",
    "# ROC (already present, keep it)\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"ROC Curve\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Precision‚ÄìRecall\n",
    "PrecisionRecallDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"Precision‚ÄìRecall Curve\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Confusion matrix heatmap (simple matplotlib)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest')\n",
    "ax.set_title(\"Confusion Matrix\"); plt.colorbar(im, ax=ax)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "for (i,j),v in np.ndenumerate(cm):\n",
    "    ax.text(j, i, str(v), ha='center', va='center')\n",
    "plt.tight_layout(); plt.show()\n",
    "```\n",
    "\n",
    "### C) `04_inference_batch.ipynb` ‚Äî scoring distribution (after scoring)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "scored['churn_proba'].plot(kind='hist', bins=30)\n",
    "plt.title(\"Predicted churn probabilities\"); plt.xlabel(\"P(churn)\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) In the modular codebase (optional but clean)\n",
    "\n",
    "Add a tiny viz module so you can generate static PNGs on demand (keeps notebooks tidy and enables CI later).\n",
    "\n",
    "### `src/churn/visualize.py`\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix\n",
    "\n",
    "def ensure_dir(p): Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def churn_distribution(df: pd.DataFrame, out_path: str):\n",
    "    ensure_dir(out_path)\n",
    "    ax = df['Churn'].value_counts().sort_index().plot(kind=\"bar\")\n",
    "    ax.set_title(\"Churn distribution (0=No, 1=Yes)\"); ax.set_xlabel(\"Churn\"); ax.set_ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "\n",
    "def churn_by_contract(df: pd.DataFrame, out_path: str):\n",
    "    if 'Contract' not in df.columns: return\n",
    "    ensure_dir(out_path)\n",
    "    ax = (df.groupby('Contract')['Churn'].mean().sort_values()).plot(kind=\"bar\")\n",
    "    ax.set_title(\"Mean churn rate by Contract\"); ax.set_ylabel(\"Churn rate\")\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "\n",
    "def churn_by_tenure_bin(df: pd.DataFrame, out_path: str):\n",
    "    if 'tenure' not in df.columns: return\n",
    "    ensure_dir(out_path)\n",
    "    bins = [0, 6, 12, 24, 36, 48, 60, 72]\n",
    "    cut = pd.cut(df['tenure'], bins=bins, right=False)\n",
    "    ax = (df.groupby(cut)['Churn'].mean()).plot(marker=\"o\")\n",
    "    ax.set_title(\"Churn rate by Tenure bin\"); ax.set_xlabel(\"Tenure bin (months)\"); ax.set_ylabel(\"Churn rate\")\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "\n",
    "def eval_curves(clf, X_test, y_test, out_dir: str):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ROC\n",
    "    RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "    plt.title(\"ROC Curve\"); plt.tight_layout(); plt.savefig(f\"{out_dir}/roc.png\"); plt.close()\n",
    "\n",
    "    # PR\n",
    "    PrecisionRecallDisplay.from_estimator(clf, X_test, y_test)\n",
    "    plt.title(\"Precision‚ÄìRecall Curve\"); plt.tight_layout(); plt.savefig(f\"{out_dir}/pr.png\"); plt.close()\n",
    "\n",
    "def cm_plot(y_true, y_pred, out_path: str):\n",
    "    ensure_dir(out_path)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.set_title(\"Confusion Matrix\"); plt.colorbar(im, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    for (i,j),v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha='center', va='center')\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "```\n",
    "\n",
    "### `scripts/visualize.py`\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from churn.io import load_obj, read_csv\n",
    "from churn.visualize import (\n",
    "    churn_distribution, churn_by_contract, churn_by_tenure_bin, eval_curves, cm_plot\n",
    ")\n",
    "\n",
    "def main():\n",
    "    cfg = yaml.safe_load(open(\"configs/config.yaml\"))\n",
    "    df = read_csv(cfg[\"paths\"][\"processed\"])\n",
    "\n",
    "    # EDA figs\n",
    "    churn_distribution(df, \"reports/figures/churn_dist.png\")\n",
    "    churn_by_contract(df, \"reports/figures/churn_by_contract.png\")\n",
    "    churn_by_tenure_bin(df, \"reports/figures/churn_by_tenure.png\")\n",
    "\n",
    "    # Eval figs\n",
    "    clf = load_obj(cfg[\"paths\"][\"model\"])\n",
    "    X = df.drop(columns=['Churn']); y = df['Churn']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=cfg[\"training\"][\"test_size\"],\n",
    "        random_state=cfg[\"training\"][\"random_state\"], stratify=y\n",
    "    )\n",
    "    eval_curves(clf, X_test, y_test, \"reports/figures\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm_plot(y_test, y_pred, \"reports/figures/confusion_matrix.png\")\n",
    "\n",
    "    print(\"Saved figures to reports/figures\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Add to `requirements.txt` (already have `matplotlib`):\n",
    "\n",
    "```\n",
    "matplotlib\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```bash\n",
    "python scripts/visualize.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What plugs in where (quick map)\n",
    "\n",
    "* **After cleaning** (`01_load_eda`): target balance, churn by contract, tenure bins.\n",
    "* **After training/eval** (`03_evaluate`): ROC, PR, confusion matrix.\n",
    "* **After scoring** (`04_inference_batch`): histogram of predicted churn probabilities.\n",
    "* **Modular route**: `scripts/visualize.py` can generate all plots headlessly for reports/CI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071f798",
   "metadata": {},
   "source": [
    "># Want me to also drop in a minimal `reports/README.md` that explains each figure and how to interpret it (Skill-3 friendly)?\n",
    "Perfect ‚Äî here‚Äôs a **Skill-3-friendly `reports/README.md`** you can drop right into your project so your visualizations are documented and interpretable.\n",
    "It‚Äôs lightweight but professional‚Äîteaching you *how to read* what you‚Äôve built.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ `reports/README.md`\n",
    "\n",
    "````markdown\n",
    "# üìä Telco Churn Reports\n",
    "\n",
    "This folder stores all automatically generated visual outputs from your Telco Churn pipeline.  \n",
    "All figures are saved under `reports/figures/` by the script:\n",
    "\n",
    "```bash\n",
    "python scripts/visualize.py\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 1. churn_dist.png\n",
    "\n",
    "**Purpose:**\n",
    "Shows the distribution of churned vs retained customers in the dataset.\n",
    "\n",
    "* **Balanced dataset** ‚Üí model won‚Äôt need heavy class weighting.\n",
    "* **Imbalanced dataset (e.g., 25% churners)** ‚Üí consider using stratified sampling or rebalancing later.\n",
    "\n",
    "**Interpretation:**\n",
    "If the ‚Äú1‚Äù (churned) bar is much shorter than ‚Äú0‚Äù, your model will need to compensate for imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ 2. churn_by_contract.png\n",
    "\n",
    "**Purpose:**\n",
    "Visualizes how churn rate varies by contract type (Month-to-month, One year, Two year).\n",
    "**Interpretation:**\n",
    "\n",
    "* Typically, **Month-to-month customers churn more often** (less commitment).\n",
    "* **Longer contracts** usually mean lower churn risk.\n",
    "\n",
    "**Usage:**\n",
    "This helps identify key retention levers (e.g., offer longer contracts or loyalty incentives).\n",
    "\n",
    "---\n",
    "\n",
    "## üïí 3. churn_by_tenure.png\n",
    "\n",
    "**Purpose:**\n",
    "Shows average churn rate by tenure bins (0‚Äì6 months, 6‚Äì12, etc.).\n",
    "**Interpretation:**\n",
    "\n",
    "* **Early churn spike** = customers leaving soon after joining ‚Üí onboarding issues.\n",
    "* **Flat low churn** at high tenure = loyal base.\n",
    "\n",
    "**Usage:**\n",
    "A retention team can target low-tenure customers with special offers or better onboarding experiences.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ 4. confusion_matrix.png\n",
    "\n",
    "**Purpose:**\n",
    "Displays actual vs predicted churn outcomes.\n",
    "\n",
    "| Term           | Meaning                              |\n",
    "| -------------- | ------------------------------------ |\n",
    "| True Positive  | Predicted churn and actually churned |\n",
    "| True Negative  | Predicted stay and stayed            |\n",
    "| False Positive | Predicted churn but stayed           |\n",
    "| False Negative | Predicted stay but churned           |\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* **False Negatives** (missed churners) are typically the most costly.\n",
    "* You can tune your threshold (e.g., 0.4 instead of 0.5) to reduce them.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 5. roc.png\n",
    "\n",
    "**Purpose:**\n",
    "Receiver Operating Characteristic curve ‚Äî measures model‚Äôs ability to separate churners from non-churners.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* The **closer to the top-left corner**, the better.\n",
    "* **AUC (area under curve)** > 0.75 = solid baseline.\n",
    "\n",
    "**Usage:**\n",
    "A quick way to compare models objectively (LogisticRegression vs RandomForest later).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è 6. pr.png\n",
    "\n",
    "**Purpose:**\n",
    "Precision-Recall curve ‚Äî useful for imbalanced data.\n",
    "Shows the trade-off between catching more churners (recall) and avoiding false alarms (precision).\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* A **steep curve** indicates a good model.\n",
    "* When churners are rare, PR curves tell you more than ROC.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 7. churn_proba_hist.png (optional)\n",
    "\n",
    "**Purpose:**\n",
    "Histogram of predicted churn probabilities for new customers.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* **Sharp separation** between 0‚Äì0.3 and 0.7‚Äì1.0 = confident model.\n",
    "* **Overlapping middle** = uncertain cases (worth deeper analysis).\n",
    "\n",
    "**Usage:**\n",
    "Helps decide what probability threshold to use for marketing or retention alerts.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Workflow summary\n",
    "\n",
    "```text\n",
    "data/raw/ ‚Üí data/processed/ ‚Üí models/ ‚Üí reports/figures/\n",
    "```\n",
    "\n",
    "Each report ties back to a stage:\n",
    "\n",
    "| Notebook/Script      | Report                                         |\n",
    "| -------------------- | ---------------------------------------------- |\n",
    "| 01_load_eda          | churn_dist, churn_by_contract, churn_by_tenure |\n",
    "| 03_evaluate          | confusion_matrix, roc, pr                      |\n",
    "| 04_inference_batch   | churn_proba_hist                               |\n",
    "| scripts/visualize.py | regenerates all                                |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Next steps\n",
    "\n",
    "* Add `feature_importance.png` (Skill Level 4) to explain *why* customers churn.\n",
    "* Integrate into a dashboard (Plotly/Dash or Streamlit) at Skill 5.\n",
    "* Automate `python scripts/visualize.py` after each model retrain (Skill 6+ CI/CD).\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** You, the Telco Data Analyst\n",
    "**Level:** Skill 3/10 (Intro to modular pipelines + visual interpretation)\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527edb0",
   "metadata": {},
   "source": [
    "># Would you like me to generate the **`feature_importance.png` step** next (using coefficients from your logistic regression pipeline and saving it into `reports/figures/`)? That‚Äôs the perfect bridge to Skill-4.\n",
    "\n",
    "Awesome‚Äîhere‚Äôs the clean Skill-3 add-on to generate **feature_importance.png** (from your Logistic Regression pipeline), plus a CSV you can inspect.\n",
    "\n",
    "---\n",
    "\n",
    "# Option A ‚Äî One cell in a notebook (quickest)\n",
    "\n",
    "Drop this at the end of `03_evaluate.ipynb`:\n",
    "\n",
    "```python\n",
    "import joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "clf = joblib.load(\"models/churn_pipeline.joblib\")\n",
    "\n",
    "# 1) Get feature names from the ColumnTransformer\n",
    "prep = clf.named_steps[\"prep\"]\n",
    "feature_names = prep.get_feature_names_out()  # e.g., \"num__tenure\", \"cat__Contract_Month-to-month\"\n",
    "\n",
    "# 2) Get coefficients from LogisticRegression\n",
    "lr = clf.named_steps[\"model\"]\n",
    "coefs = lr.coef_.ravel()  # shape: (n_features,)\n",
    "\n",
    "# 3) Assemble DataFrame\n",
    "imp = (pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": coefs,\n",
    "    \"abs_coef\": np.abs(coefs),\n",
    "    \"sign\": np.where(coefs >= 0, \"+\", \"-\")\n",
    "})\n",
    ".sort_values(\"abs_coef\", ascending=False)\n",
    ".reset_index(drop=True))\n",
    "\n",
    "# 4) Save CSV and plot top-20\n",
    "imp.to_csv(\"reports/feature_importance.csv\", index=False)\n",
    "\n",
    "top_n = 20\n",
    "plot_df = imp.head(top_n).iloc[::-1]  # reverse for nicer horizontal bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(plot_df[\"feature\"], plot_df[\"abs_coef\"])\n",
    "plt.title(\"Top Feature Importances (|LogReg Coefficients|)\")\n",
    "plt.xlabel(\"|coefficient| (magnitude)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/feature_importance.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "imp.head(10)\n",
    "```\n",
    "\n",
    "**Interpretation tip (Skill-3):**\n",
    "\n",
    "* Positive coefficient ‚Üí increases churn risk (holding others constant).\n",
    "* Negative coefficient ‚Üí decreases churn risk.\n",
    "* Magnitude ‚Üí strength of impact.\n",
    "\n",
    "---\n",
    "\n",
    "# Option B ‚Äî Modular (keeps notebooks tidy)\n",
    "\n",
    "## 1) `src/churn/importance.py`\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_feature_importance_from_logreg_pipeline(clf) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns: feature, coef, abs_coef, sign\n",
    "    Works for a sklearn Pipeline with steps: prep (ColumnTransformer) + model (LogisticRegression)\n",
    "    \"\"\"\n",
    "    if \"prep\" not in clf.named_steps or \"model\" not in clf.named_steps:\n",
    "        raise ValueError(\"Pipeline must have 'prep' and 'model' steps.\")\n",
    "    prep = clf.named_steps[\"prep\"]\n",
    "    model = clf.named_steps[\"model\"]\n",
    "    if not hasattr(prep, \"get_feature_names_out\"):\n",
    "        raise ValueError(\"Preprocessor must implement get_feature_names_out().\")\n",
    "    if not hasattr(model, \"coef_\"):\n",
    "        raise ValueError(\"Model must expose coef_ (LogisticRegression expected).\")\n",
    "\n",
    "    feature_names = prep.get_feature_names_out()\n",
    "    coefs = model.coef_.ravel()\n",
    "    if len(feature_names) != len(coefs):\n",
    "        raise ValueError(f\"Shape mismatch: {len(feature_names)} features vs {len(coefs)} coefs.\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coef\": coefs,\n",
    "    })\n",
    "    df[\"abs_coef\"] = df[\"coef\"].abs()\n",
    "    df[\"sign\"] = np.where(df[\"coef\"] >= 0, \"+\", \"-\")\n",
    "    df = df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "```\n",
    "\n",
    "## 2) Extend `src/churn/visualize.py` (optional helper)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_top_feature_importance(df_imp, out_path: str, top_n: int = 20):\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    plot_df = df_imp.head(top_n).iloc[::-1]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(plot_df[\"feature\"], plot_df[\"abs_coef\"])\n",
    "    plt.title(\"Top Feature Importances (|LogReg Coefficients|)\")\n",
    "    plt.xlabel(\"|coefficient| (magnitude)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "```\n",
    "\n",
    "## 3) `scripts/feature_importance.py`\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from churn.io import load_obj\n",
    "from churn.importance import extract_feature_importance_from_logreg_pipeline\n",
    "from churn.visualize import plot_top_feature_importance\n",
    "\n",
    "def main():\n",
    "    cfg = yaml.safe_load(open(\"configs/config.yaml\"))\n",
    "    clf = load_obj(cfg[\"paths\"][\"model\"])\n",
    "\n",
    "    imp = extract_feature_importance_from_logreg_pipeline(clf)\n",
    "\n",
    "    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"reports/figures\").mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = \"reports/feature_importance.csv\"\n",
    "    imp.to_csv(out_csv, index=False)\n",
    "\n",
    "    plot_top_feature_importance(imp, \"reports/figures/feature_importance.png\", top_n=20)\n",
    "    print(f\"Saved {out_csv} and reports/figures/feature_importance.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "## 4) Run it\n",
    "\n",
    "```bash\n",
    "python scripts/feature_importance.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Add a short section to `reports/README.md`\n",
    "\n",
    "Append this:\n",
    "\n",
    "```markdown\n",
    "---\n",
    "\n",
    "## üß† 8. feature_importance.png / feature_importance.csv\n",
    "**What it is:**  \n",
    "Top features by absolute Logistic Regression coefficient (magnitude).  \n",
    "- **Positive sign** ‚Üí increases churn likelihood.  \n",
    "- **Negative sign** ‚Üí decreases churn likelihood.\n",
    "\n",
    "**How to use it:**  \n",
    "- Pair with `churn_by_contract.png`/`churn_by_tenure.png` to validate patterns you see.  \n",
    "- Use high-magnitude positives to design **retention actions** (e.g., month-to-month + paperless billing + electronic checks might indicate higher churn risk, depending on your data).\n",
    "\n",
    "**Caveat:**  \n",
    "Coefficients reflect associations after one-hot encoding & scaling; correlation ‚â† causation.\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd75ff",
   "metadata": {},
   "source": [
    "># If you want, I can also generate a **threshold-tuning helper** (Skill-3+): a tiny script that scans thresholds (0.1‚Üí0.9) and prints precision/recall/FN rate so you can pick a business-friendly operating point.\n",
    "\n",
    "Perfect. Here‚Äôs your **Skill-3+ ‚Äúthreshold tuning helper‚Äù** ‚Äî a small but powerful addition that teaches you how to *choose the best decision cutoff* (not always 0.5!) for business goals like minimizing missed churners.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è  Threshold Tuning Helper\n",
    "\n",
    "## üìÅ New file: `scripts/tune_threshold.py`\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from churn.io import load_obj, read_csv\n",
    "\n",
    "def main():\n",
    "    cfg = yaml.safe_load(open(\"configs/config.yaml\"))\n",
    "\n",
    "    # Load model and data\n",
    "    clf = load_obj(cfg[\"paths\"][\"model\"])\n",
    "    df = read_csv(cfg[\"paths\"][\"processed\"])\n",
    "\n",
    "    y = df[\"Churn\"]\n",
    "    X = df.drop(columns=[\"Churn\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=cfg[\"training\"][\"test_size\"],\n",
    "        random_state=cfg[\"training\"][\"random_state\"],\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Sweep thresholds 0.1 ‚Üí 0.9\n",
    "    results = []\n",
    "    for t in np.arange(0.1, 0.91, 0.05):\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        fn_rate = np.mean((y_test == 1) & (y_pred == 0))  # proportion of missed churners\n",
    "        results.append({\n",
    "            \"threshold\": round(t, 2),\n",
    "            \"precision\": round(prec, 3),\n",
    "            \"recall\": round(rec, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"roc_auc\": round(auc, 3),\n",
    "            \"false_negative_rate\": round(fn_rate, 3)\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"reports/threshold_tuning.csv\", index=False)\n",
    "    print(df_results)\n",
    "    print(\"\\nSaved results ‚Üí reports/threshold_tuning.csv\")\n",
    "\n",
    "    # Recommend threshold by F1 (balanced metric)\n",
    "    best_idx = df_results[\"f1\"].idxmax()\n",
    "    best_row = df_results.loc[best_idx]\n",
    "    print(\"\\n‚úÖ Recommended threshold by F1-score:\", best_row[\"threshold\"])\n",
    "    print(best_row.to_dict())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Run it\n",
    "\n",
    "```bash\n",
    "python scripts/tune_threshold.py\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "    threshold  precision  recall    f1  roc_auc  false_negative_rate\n",
    "0         0.1      0.56    0.95  0.71    0.83                 0.05\n",
    "1         0.15     0.60    0.91  0.72    0.83                 0.09\n",
    "...\n",
    "‚úÖ Recommended threshold by F1-score: 0.35\n",
    "{'threshold': 0.35, 'precision': 0.74, 'recall': 0.81, 'f1': 0.77, ...}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Optional Plot (if you want visual feedback)\n",
    "\n",
    "Add this at the end of the script (after `print(best_row.to_dict())`):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_results[\"threshold\"], df_results[\"precision\"], label=\"Precision\")\n",
    "plt.plot(df_results[\"threshold\"], df_results[\"recall\"], label=\"Recall\")\n",
    "plt.plot(df_results[\"threshold\"], df_results[\"f1\"], label=\"F1-score\")\n",
    "plt.title(\"Precision / Recall / F1 vs Threshold\")\n",
    "plt.xlabel(\"Decision Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/threshold_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Update `reports/README.md`\n",
    "\n",
    "Add:\n",
    "\n",
    "```markdown\n",
    "---\n",
    "\n",
    "## üéöÔ∏è 9. threshold_tuning.csv / threshold_curve.png\n",
    "**Purpose:**  \n",
    "Shows how precision, recall, and F1 change as you vary the churn probability cutoff.\n",
    "\n",
    "**Why it matters:**  \n",
    "- A **lower threshold** (e.g., 0.3) catches more churners but risks more false alarms.  \n",
    "- A **higher threshold** (e.g., 0.7) avoids false positives but misses real churners.  \n",
    "\n",
    "**Interpretation:**  \n",
    "Pick the threshold that fits your business:\n",
    "- **Retention teams** ‚Üí prefer *high recall* (catch every churner).  \n",
    "- **Finance teams** ‚Üí prefer *high precision* (don‚Äôt waste retention budget).  \n",
    "- **Balanced** ‚Üí choose the F1-optimal threshold shown in the output.\n",
    "\n",
    "**Usage:**  \n",
    "Update `configs/config.yaml` under `inference.threshold` to your chosen value.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Integration recap\n",
    "\n",
    "You now have:\n",
    "\n",
    "| Script                          | Purpose              | Output                       |\n",
    "| ------------------------------- | -------------------- | ---------------------------- |\n",
    "| `scripts/train.py`              | Train model          | models/churn_pipeline.joblib |\n",
    "| `scripts/predict.py`            | Score new data       | new_customers_scored.csv     |\n",
    "| `scripts/visualize.py`          | Generate figures     | reports/figures/*.png        |\n",
    "| `scripts/feature_importance.py` | Explain model        | feature_importance.csv/.png  |\n",
    "| **`scripts/tune_threshold.py`** | Tune decision cutoff | threshold_tuning.csv/.png    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e3f97",
   "metadata": {},
   "source": [
    "># **Automatically update your YAML config** with the best threshold after running this script (Skill-4 feature)?\n",
    "\n",
    "Awesome‚Äîlet‚Äôs make the tuner update your YAML automatically. Two options:\n",
    "\n",
    "# Option A ‚Äî Add a `--apply` flag to the existing tuner\n",
    "\n",
    "Drop-in replacement for `scripts/tune_threshold.py`:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Try ruamel.yaml for comment-preserving edits; fall back to PyYAML\n",
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True\n",
    "    _USE_RUAMEL = True\n",
    "except Exception:\n",
    "    import yaml as pyyaml\n",
    "    _USE_RUAMEL = False\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from churn.io import load_obj, read_csv\n",
    "\n",
    "def load_config(path: str):\n",
    "    if _USE_RUAMEL:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = yaml.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        import yaml as pyyaml\n",
    "        with open(path, \"r\") as f:\n",
    "            return pyyaml.safe_load(f)\n",
    "\n",
    "def save_config(cfg, path: str):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    if _USE_RUAMEL:\n",
    "        with open(path, \"w\") as f:\n",
    "            yaml.dump(cfg, f)\n",
    "    else:\n",
    "        import yaml as pyyaml\n",
    "        with open(path, \"w\") as f:\n",
    "            pyyaml.safe_dump(cfg, f, sort_keys=False)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Tune threshold and (optionally) apply best to config.\")\n",
    "    parser.add_argument(\"--config\", default=\"configs/config.yaml\")\n",
    "    parser.add_argument(\"--apply\", action=\"store_true\", help=\"Write best F1 threshold back to config.\")\n",
    "    parser.add_argument(\"--metric\", choices=[\"f1\",\"recall\",\"precision\"], default=\"f1\",\n",
    "                        help=\"Metric to optimize when selecting threshold.\")\n",
    "    parser.add_argument(\"--start\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--stop\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--step\", type=float, default=0.05)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg = load_config(args.config)\n",
    "    model_path = cfg[\"paths\"][\"model\"]\n",
    "    processed_path = cfg[\"paths\"][\"processed\"]\n",
    "\n",
    "    clf = load_obj(model_path)\n",
    "    df = read_csv(processed_path)\n",
    "    y = df[\"Churn\"]; X = df.drop(columns=[\"Churn\"])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=cfg[\"training\"][\"test_size\"],\n",
    "        random_state=cfg[\"training\"][\"random_state\"],\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    thresholds = np.arange(args.start, args.stop + 1e-9, args.step)\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": round(float(t), 2),\n",
    "            \"precision\": round(precision_score(y_test, y_pred, zero_division=0), 4),\n",
    "            \"recall\": round(recall_score(y_test, y_pred), 4),\n",
    "            \"f1\": round(f1_score(y_test, y_pred), 4),\n",
    "            \"roc_auc\": round(roc_auc_score(y_test, y_proba), 4),\n",
    "            \"false_negative_rate\": round(np.mean((y_test == 1) & (y_pred == 0)), 4),\n",
    "        })\n",
    "\n",
    "    res = pd.DataFrame(rows)\n",
    "    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "    res.to_csv(\"reports/threshold_tuning.csv\", index=False)\n",
    "    print(res)\n",
    "\n",
    "    best_idx = res[args.metric].idxmax()\n",
    "    best_row = res.loc[best_idx]\n",
    "    best_t = float(best_row[\"threshold\"])\n",
    "    print(f\"\\n‚úÖ Best threshold by {args.metric.upper()}: {best_t}\")\n",
    "    print(best_row.to_dict())\n",
    "\n",
    "    # Optional: plot\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        Path(\"reports/figures\").mkdir(parents=True, exist_ok=True)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(res[\"threshold\"], res[\"precision\"], label=\"Precision\")\n",
    "        plt.plot(res[\"threshold\"], res[\"recall\"], label=\"Recall\")\n",
    "        plt.plot(res[\"threshold\"], res[\"f1\"], label=\"F1\")\n",
    "        plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\"); plt.title(\"Precision/Recall/F1 vs Threshold\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(\"reports/figures/threshold_curve.png\", dpi=150)\n",
    "        # plt.show()  # optional\n",
    "        print(\"Saved plot ‚Üí reports/figures/threshold_curve.png\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Apply to YAML if requested\n",
    "    if args.apply:\n",
    "        old_t = cfg.get(\"inference\", {}).get(\"threshold\", None)\n",
    "        cfg.setdefault(\"inference\", {})\n",
    "        cfg[\"inference\"][\"threshold\"] = round(best_t, 2)\n",
    "        backup = f\"{args.config}.bak\"\n",
    "        Path(backup).write_text(Path(args.config).read_text())\n",
    "        save_config(cfg, args.config)\n",
    "        print(f\"\\nüìù Updated {args.config} (backup at {backup})\")\n",
    "        print(f\"Old threshold: {old_t}  ‚Üí  New threshold: {cfg['inference']['threshold']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Run it:\n",
    "\n",
    "```bash\n",
    "# just tune, don‚Äôt write\n",
    "python scripts/tune_threshold.py\n",
    "\n",
    "# optimize for F1 and write back to YAML\n",
    "python scripts/tune_threshold.py --apply\n",
    "\n",
    "# optimize for recall instead (catch more churners) and write\n",
    "python scripts/tune_threshold.py --metric recall --apply\n",
    "```\n",
    "\n",
    "> Notes\n",
    "> ‚Ä¢ If `ruamel.yaml` is installed, comments/formatting are preserved better. Otherwise PyYAML rewrites the file (functionally fine).\n",
    "> ‚Ä¢ A `.bak` backup is created before writing.\n",
    "\n",
    "Add to `requirements.txt` (optional but recommended to preserve YAML formatting):\n",
    "\n",
    "```\n",
    "ruamel.yaml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Option B ‚Äî Separate ‚Äúapply-best‚Äù script\n",
    "\n",
    "If you prefer to keep tuning and applying separate:\n",
    "\n",
    "`scripts/apply_best_threshold.py`\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "    yaml = YAML(); yaml.preserve_quotes = True\n",
    "    _USE_RUAMEL = True\n",
    "except Exception:\n",
    "    import yaml as pyyaml\n",
    "    _USE_RUAMEL = False\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CFG = \"configs/config.yaml\"\n",
    "CSV = \"reports/threshold_tuning.csv\"\n",
    "\n",
    "def load_config(path):\n",
    "    if _USE_RUAMEL:\n",
    "        with open(path) as f: return yaml.load(f)\n",
    "    else:\n",
    "        import yaml as pyyaml\n",
    "        with open(path) as f: return pyyaml.safe_load(f)\n",
    "\n",
    "def save_config(cfg, path):\n",
    "    if _USE_RUAMEL:\n",
    "        with open(path, \"w\") as f: yaml.dump(cfg, f)\n",
    "    else:\n",
    "        import yaml as pyyaml\n",
    "        with open(path, \"w\") as f: pyyaml.safe_dump(cfg, f, sort_keys=False)\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(CSV)\n",
    "    best = df.loc[df['f1'].idxmax()]  # or choose another column\n",
    "    best_t = float(best['threshold'])\n",
    "    cfg = load_config(CFG)\n",
    "    old = cfg.get(\"inference\", {}).get(\"threshold\")\n",
    "    cfg.setdefault(\"inference\", {})\n",
    "    cfg[\"inference\"][\"threshold\"] = round(best_t, 2)\n",
    "\n",
    "    backup = f\"{CFG}.bak\"\n",
    "    Path(backup).write_text(Path(CFG).read_text())\n",
    "    save_config(cfg, CFG)\n",
    "    print(f\"Updated {CFG} threshold: {old} ‚Üí {cfg['inference']['threshold']} (backup at {backup})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```bash\n",
    "python scripts/tune_threshold.py      # produces CSV\n",
    "python scripts/apply_best_threshold.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick sanity check flow\n",
    "\n",
    "```bash\n",
    "# 1) Train (if not already)\n",
    "python scripts/train.py\n",
    "\n",
    "# 2) Tune thresholds and apply the best by F1\n",
    "python scripts/tune_threshold.py --apply\n",
    "\n",
    "# 3) Score with the new threshold\n",
    "python scripts/predict.py data/scoring/new_customers.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53330a7",
   "metadata": {},
   "source": [
    "># Wire `predict.py` to accept a `--threshold` override at runtime (so you can A/B different cutoffs without touching YAML)?\n",
    "\n",
    "Awesome‚Äîhere‚Äôs a drop-in upgrade for `scripts/predict.py` that lets you **override the YAML threshold at runtime**.\n",
    "\n",
    "### `scripts/predict.py` (replace the whole file)\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from churn.io import load_obj\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Batch score Telco churn.\")\n",
    "    parser.add_argument(\"input_csv\", help=\"Path to CSV of records to score\")\n",
    "    parser.add_argument(\"--output\", help=\"Optional output path (CSV). If omitted, appends _scored.csv\")\n",
    "    parser.add_argument(\"--threshold\", type=float, help=\"Override decision threshold (0‚Äì1). If omitted, uses config.\")\n",
    "    parser.add_argument(\"--config\", default=\"configs/config.yaml\", help=\"Path to YAML config\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load config and model\n",
    "    with open(args.config) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    model_path = cfg[\"paths\"][\"model\"]\n",
    "    clf = load_obj(model_path)\n",
    "\n",
    "    # Load data\n",
    "    df_new = pd.read_csv(args.input_csv)\n",
    "\n",
    "    # If user accidentally included a label column, ignore it for scoring\n",
    "    for col in (\"Churn\", \"churn\", \"label\", \"target\"):\n",
    "        if col in df_new.columns:\n",
    "            df_new = df_new.drop(columns=[col])\n",
    "\n",
    "    # Predict\n",
    "    proba = clf.predict_proba(df_new)[:, 1]\n",
    "\n",
    "    # Decide threshold: CLI override > YAML > default 0.5\n",
    "    cfg_thresh = cfg.get(\"inference\", {}).get(\"threshold\", 0.5)\n",
    "    threshold = args.threshold if args.threshold is not None else cfg_thresh\n",
    "    if not (0.0 <= float(threshold) <= 1.0):\n",
    "        print(f\"ERROR: threshold must be in [0,1], got {threshold}\", file=sys.stderr)\n",
    "        sys.exit(2)\n",
    "\n",
    "    pred = (proba >= float(threshold)).astype(int)\n",
    "\n",
    "    # Assemble output\n",
    "    scored = df_new.copy()\n",
    "    scored[\"churn_proba\"] = proba\n",
    "    scored[\"churn_pred\"] = pred\n",
    "\n",
    "    # Output path\n",
    "    out_path = args.output or (str(Path(args.input_csv).with_suffix(\"\")) + \"_scored.csv\")\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    scored.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Applied threshold: {threshold}\")\n",
    "    print(f\"Scored ‚Üí {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "### How to use it\n",
    "\n",
    "```bash\n",
    "# Use threshold from configs/config.yaml (inference.threshold)\n",
    "python scripts/predict.py data/scoring/new_customers.csv\n",
    "\n",
    "# Override at runtime to 0.35 (no YAML change)\n",
    "python scripts/predict.py data/scoring/new_customers.csv --threshold 0.35\n",
    "\n",
    "# Write to a specific output file\n",
    "python scripts/predict.py data/scoring/new_customers.csv --threshold 0.4 --output data/scoring/new_scored_040.csv\n",
    "```\n",
    "\n",
    "**Precedence:** `--threshold` (CLI) ‚Üí YAML `inference.threshold` ‚Üí default `0.5`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f864a7",
   "metadata": {},
   "source": [
    "Excellent question ‚Äî and it‚Äôs a **very good sign** you‚Äôre thinking about how to make your dataset more structured and reproducible.\n",
    "\n",
    "The short answer: **not all columns need to go into a dictionary** ‚Äî only the *metadata* about them (types, groups, or roles) does.\n",
    "\n",
    "Let me explain clearly üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Why you *don‚Äôt* just store every column as key:value\n",
    "\n",
    "If you did something like:\n",
    "\n",
    "```python\n",
    "columns = {\n",
    "    \"customerID\": \"string\",\n",
    "    \"gender\": \"category\",\n",
    "    \"SeniorCitizen\": \"int\",\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "you‚Äôd basically be replicating your schema ‚Äî which is okay, but not very flexible and hard to maintain.\n",
    "\n",
    "Instead, you want to use a **column-role dictionary** or **schema metadata map** that tells your pipeline *how to treat* each column ‚Äî not just its name.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Recommended pattern (Skill 3‚Äì4 friendly)\n",
    "\n",
    "Create a `configs/columns.yaml` that **groups columns by role and type**:\n",
    "\n",
    "```yaml\n",
    "id_cols:\n",
    "  - customerID\n",
    "\n",
    "target_col: Churn\n",
    "\n",
    "numeric_cols:\n",
    "  - tenure\n",
    "  - MonthlyCharges\n",
    "  - TotalCharges\n",
    "  - SeniorCitizen\n",
    "\n",
    "categorical_cols:\n",
    "  - gender\n",
    "  - Partner\n",
    "  - Dependents\n",
    "  - PhoneService\n",
    "  - MultipleLines\n",
    "  - InternetService\n",
    "  - OnlineSecurity\n",
    "  - OnlineBackup\n",
    "  - DeviceProtection\n",
    "  - TechSupport\n",
    "  - StreamingTV\n",
    "  - StreamingMovies\n",
    "  - Contract\n",
    "  - PaperlessBilling\n",
    "  - PaymentMethod\n",
    "```\n",
    "\n",
    "That‚Äôs it.\n",
    "You‚Äôre telling your preprocessing step:\n",
    "\n",
    "> ‚ÄúHere‚Äôs how to handle each group of columns.‚Äù\n",
    "\n",
    "Then your code reads that and uses it dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Example: how you‚Äôd use it in code\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load column groups\n",
    "cfg_cols = yaml.safe_load(open(\"configs/columns.yaml\"))\n",
    "\n",
    "num_cols = cfg_cols[\"numeric_cols\"]\n",
    "cat_cols = cfg_cols[\"categorical_cols\"]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols)\n",
    "])\n",
    "```\n",
    "\n",
    "This makes your pipeline **declarative** ‚Äî no hardcoded column names inside notebooks or scripts.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why it matters later\n",
    "\n",
    "| Benefit             | Why it helps                                                            |\n",
    "| ------------------- | ----------------------------------------------------------------------- |\n",
    "| **Reproducibility** | You can change columns in YAML instead of code.                         |\n",
    "| **Versioning**      | You can track schema drift over time (new columns, renamed features).   |\n",
    "| **Portability**     | Works the same on local, CI/CD, and cloud.                              |\n",
    "| **Validation**      | You can add checks to ensure new data has these columns before scoring. |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Optional (Skill-4+)\n",
    "\n",
    "If you want to go one step further later, create a **schema dictionary** inside a YAML or JSON for validation:\n",
    "\n",
    "```yaml\n",
    "schema:\n",
    "  customerID: {type: string, nullable: false}\n",
    "  gender: {type: category, allowed: [Male, Female]}\n",
    "  tenure: {type: int, min: 0, max: 100}\n",
    "  TotalCharges: {type: float, nullable: true}\n",
    "  Churn: {type: category, allowed: [Yes, No]}\n",
    "```\n",
    "\n",
    "Then, before training or inference, you can validate incoming data programmatically (using `pandera`, `great_expectations`, or your own checks).\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "| Approach                                                  | When to use                      |\n",
    "| --------------------------------------------------------- | -------------------------------- |\n",
    "| Simple list of column groups (categorical/numeric/target) | ‚úÖ Now (Skill 3)                  |\n",
    "| Full schema with rules (types, ranges, allowed values)    | üß† Later (Skill 4‚Äì5)             |\n",
    "| Full data dictionary for documentation                    | üìñ Optional, for human reference |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99601db4",
   "metadata": {},
   "source": [
    "># Would you like me to show you a **`data_dictionary.yaml`** template that sits next to your config, documenting what each column *means* and *how it‚Äôs used in modeling*? It‚Äôs an ideal next step before you reach Skill Level 4.\n",
    "\n",
    "Perfect ‚Äî this is *exactly* what a professional pipeline does next.\n",
    "At **Skill Level 3+**, your model runs.\n",
    "At **Skill Level 4**, you start documenting *why* each feature exists and how to interpret it.\n",
    "\n",
    "Here‚Äôs your starter **`configs/data_dictionary.yaml`** ‚Äî lightweight, human-readable, and designed to grow with your project.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò `configs/data_dictionary.yaml`\n",
    "\n",
    "```yaml\n",
    "# Telco Customer Churn Data Dictionary\n",
    "# Purpose: Describe every column‚Äôs meaning, role, and modeling behavior.\n",
    "\n",
    "dataset:\n",
    "  name: IBM Telco Customer Churn\n",
    "  source: https://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/\n",
    "  description: >\n",
    "    Customer demographic, service, and account data from a fictional telco provider.\n",
    "    Target variable indicates whether the customer left (churned).\n",
    "\n",
    "columns:\n",
    "\n",
    "  customerID:\n",
    "    description: Unique customer identifier\n",
    "    dtype: string\n",
    "    role: id\n",
    "    used_in_model: false\n",
    "\n",
    "  gender:\n",
    "    description: Gender of the customer (Male/Female)\n",
    "    dtype: category\n",
    "    role: demographic\n",
    "    used_in_model: true\n",
    "    notes: Can reveal subtle behavioral differences, usually weak predictor.\n",
    "\n",
    "  SeniorCitizen:\n",
    "    description: Indicates if the customer is a senior (1) or not (0)\n",
    "    dtype: int\n",
    "    role: demographic\n",
    "    used_in_model: true\n",
    "\n",
    "  Partner:\n",
    "    description: Whether the customer has a partner (Yes/No)\n",
    "    dtype: category\n",
    "    role: demographic\n",
    "    used_in_model: true\n",
    "\n",
    "  Dependents:\n",
    "    description: Whether the customer has dependents (Yes/No)\n",
    "    dtype: category\n",
    "    role: demographic\n",
    "    used_in_model: true\n",
    "\n",
    "  tenure:\n",
    "    description: Number of months the customer has stayed with the company\n",
    "    dtype: int\n",
    "    role: account\n",
    "    used_in_model: true\n",
    "    notes: Strongly predictive ‚Äî longer tenure ‚Üí lower churn.\n",
    "\n",
    "  PhoneService:\n",
    "    description: Whether the customer has a phone service\n",
    "    dtype: category\n",
    "    role: product\n",
    "    used_in_model: true\n",
    "\n",
    "  MultipleLines:\n",
    "    description: Whether the customer has multiple lines\n",
    "    dtype: category\n",
    "    role: product\n",
    "    used_in_model: true\n",
    "\n",
    "  InternetService:\n",
    "    description: Type of Internet service (DSL/Fiber optic/None)\n",
    "    dtype: category\n",
    "    role: product\n",
    "    used_in_model: true\n",
    "\n",
    "  OnlineSecurity:\n",
    "    description: If customer has online security add-on\n",
    "    dtype: category\n",
    "    role: addon\n",
    "    used_in_model: true\n",
    "\n",
    "  OnlineBackup:\n",
    "    description: If customer has online backup add-on\n",
    "    dtype: category\n",
    "    role: addon\n",
    "    used_in_model: true\n",
    "\n",
    "  DeviceProtection:\n",
    "    description: If customer has device protection plan\n",
    "    dtype: category\n",
    "    role: addon\n",
    "    used_in_model: true\n",
    "\n",
    "  TechSupport:\n",
    "    description: If customer has technical support add-on\n",
    "    dtype: category\n",
    "    role: addon\n",
    "    used_in_model: true\n",
    "\n",
    "  StreamingTV:\n",
    "    description: If customer streams TV\n",
    "    dtype: category\n",
    "    role: addon\n",
    "    used_in_model: true\n",
    "\n",
    "  StreamingMovies:\n",
    "    description: If customer streams movies\n",
    "    dtype: category\n",
    "    role: addon\n",
    "    used_in_model: true\n",
    "\n",
    "  Contract:\n",
    "    description: Type of contract (Month-to-month/One year/Two year)\n",
    "    dtype: category\n",
    "    role: account\n",
    "    used_in_model: true\n",
    "    notes: One of the top predictors of churn.\n",
    "\n",
    "  PaperlessBilling:\n",
    "    description: Whether customer uses paperless billing\n",
    "    dtype: category\n",
    "    role: billing\n",
    "    used_in_model: true\n",
    "\n",
    "  PaymentMethod:\n",
    "    description: Payment method used (Electronic check, Mailed check, etc.)\n",
    "    dtype: category\n",
    "    role: billing\n",
    "    used_in_model: true\n",
    "\n",
    "  MonthlyCharges:\n",
    "    description: Monthly amount charged to customer\n",
    "    dtype: float\n",
    "    role: billing\n",
    "    used_in_model: true\n",
    "\n",
    "  TotalCharges:\n",
    "    description: Total amount charged to date\n",
    "    dtype: float\n",
    "    role: billing\n",
    "    used_in_model: true\n",
    "\n",
    "  Churn:\n",
    "    description: Target variable (Yes = customer left)\n",
    "    dtype: category\n",
    "    role: target\n",
    "    used_in_model: true\n",
    "    notes: Binary encoded 1 = Yes, 0 = No\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß How to use it\n",
    "\n",
    "### Read it in your notebooks or scripts\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "data_dict = yaml.safe_load(open(\"configs/data_dictionary.yaml\"))\n",
    "\n",
    "for col, meta in data_dict[\"columns\"].items():\n",
    "    if meta.get(\"used_in_model\"):\n",
    "        print(f\"{col:<20} ‚Üí {meta['role']} ({meta['dtype']})\")\n",
    "```\n",
    "\n",
    "### Cross-check your processed DataFrame\n",
    "\n",
    "```python\n",
    "missing = [c for c in data_dict[\"columns\"].keys() if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"‚ö†Ô∏è Missing columns:\", missing)\n",
    "```\n",
    "\n",
    "### Export for documentation\n",
    "\n",
    "You can even render it to Markdown:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df_doc = pd.DataFrame.from_dict(data_dict[\"columns\"], orient=\"index\")\n",
    "df_doc.to_markdown(\"reports/data_dictionary.md\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why this matters\n",
    "\n",
    "| Benefit               | Explanation                                                                |\n",
    "| --------------------- | -------------------------------------------------------------------------- |\n",
    "| **Transparency**      | Anyone (teammate, reviewer, interviewer) can see what each variable means. |\n",
    "| **Governance**        | Easy to check if you‚Äôre using sensitive or personal data.                  |\n",
    "| **Automation**        | Pipelines can validate dtype, inclusion, or handle roles programmatically. |\n",
    "| **Feature selection** | You can mark features as ‚Äúused_in_model: false‚Äù to exclude them cleanly.   |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ec486",
   "metadata": {},
   "source": [
    "># How to **auto-generate this dictionary** from your dataset (so you don‚Äôt have to type every column manually), then edit and save it as YAML? That‚Äôs a great bridge to Skill Level 4.\n",
    "\n",
    "Excellent ‚Äî this next step takes you from **manual documentation ‚Üí automated metadata generation**, a key **Skill Level 4** move in any professional data engineering or ML workflow.\n",
    "\n",
    "We‚Äôll build a script that **auto-scans your dataset**, creates a structured data dictionary (with guesses for dtype, role, and inclusion), and saves it as `configs/data_dictionary.yaml`.\n",
    "You can then review and edit it manually once for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è Step-by-Step: Auto-Generate Data Dictionary\n",
    "\n",
    "### üìÅ File: `scripts/generate_data_dictionary.py`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def guess_role(col: str) -> str:\n",
    "    \"\"\"Basic heuristic for role classification.\"\"\"\n",
    "    name = col.lower()\n",
    "    if \"id\" in name:\n",
    "        return \"id\"\n",
    "    if \"charge\" in name or \"payment\" in name or \"bill\" in name:\n",
    "        return \"billing\"\n",
    "    if \"contract\" in name or \"tenure\" in name:\n",
    "        return \"account\"\n",
    "    if name in (\"gender\", \"partner\", \"dependents\", \"seniorcitizen\"):\n",
    "        return \"demographic\"\n",
    "    if \"stream\" in name or \"online\" in name or \"device\" in name or \"tech\" in name:\n",
    "        return \"addon\"\n",
    "    if \"churn\" in name:\n",
    "        return \"target\"\n",
    "    return \"other\"\n",
    "\n",
    "def dtype_role(df: pd.DataFrame):\n",
    "    \"\"\"Infer dtype categories: numeric vs categorical.\"\"\"\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    return {c: (\"float\" if pd.api.types.is_float_dtype(df[c]) else \"int\") for c in num_cols}\n",
    "\n",
    "def main():\n",
    "    data_path = \"data/processed/telco_clean.csv\"\n",
    "    out_path = \"configs/data_dictionary.yaml\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    Path(\"configs\").mkdir(exist_ok=True)\n",
    "\n",
    "    # Base metadata\n",
    "    dict_yaml = {\n",
    "        \"dataset\": {\n",
    "            \"name\": \"Telco Customer Churn (Auto-Generated)\",\n",
    "            \"source\": str(data_path),\n",
    "            \"description\": \"Auto-generated metadata; please review descriptions and roles manually.\",\n",
    "        },\n",
    "        \"columns\": {},\n",
    "    }\n",
    "\n",
    "    num_map = dtype_role(df)\n",
    "    for col in df.columns:\n",
    "        inferred = {}\n",
    "        if col in num_map:\n",
    "            inferred[\"dtype\"] = num_map[col]\n",
    "        else:\n",
    "            inferred[\"dtype\"] = \"category\"\n",
    "\n",
    "        inferred[\"role\"] = guess_role(col)\n",
    "        inferred[\"used_in_model\"] = False if inferred[\"role\"] in (\"id\", \"other\") else True\n",
    "        inferred[\"description\"] = f\"Auto-generated description for {col}\"\n",
    "        dict_yaml[\"columns\"][col] = inferred\n",
    "\n",
    "    # Write YAML\n",
    "    with open(out_path, \"w\") as f:\n",
    "        yaml.safe_dump(dict_yaml, f, sort_keys=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved data dictionary to {out_path}\")\n",
    "    print(\"Columns detected:\", len(df.columns))\n",
    "    print(\"Roles summary:\")\n",
    "    roles = pd.Series([v[\"role\"] for v in dict_yaml[\"columns\"].values()]).value_counts()\n",
    "    print(roles)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Run it\n",
    "\n",
    "```bash\n",
    "python scripts/generate_data_dictionary.py\n",
    "```\n",
    "\n",
    "**Output example:**\n",
    "\n",
    "```\n",
    "‚úÖ Saved data dictionary to configs/data_dictionary.yaml\n",
    "Columns detected: 21\n",
    "Roles summary:\n",
    "billing         5\n",
    "demographic     4\n",
    "addon           6\n",
    "account         2\n",
    "target          1\n",
    "id              1\n",
    "other           2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ What you get\n",
    "\n",
    "```yaml\n",
    "dataset:\n",
    "  name: Telco Customer Churn (Auto-Generated)\n",
    "  source: data/processed/telco_clean.csv\n",
    "  description: Auto-generated metadata; please review descriptions and roles manually.\n",
    "\n",
    "columns:\n",
    "  customerID:\n",
    "    dtype: category\n",
    "    role: id\n",
    "    used_in_model: false\n",
    "    description: Auto-generated description for customerID\n",
    "  gender:\n",
    "    dtype: category\n",
    "    role: demographic\n",
    "    used_in_model: true\n",
    "    description: Auto-generated description for gender\n",
    "  ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ü™Ñ Next (manual curation)\n",
    "\n",
    "1. Open `configs/data_dictionary.yaml`.\n",
    "2. Replace the ‚ÄúAuto-generated description for ‚Ä¶‚Äù with clear, human-readable text (you can copy from the version I gave you earlier).\n",
    "3. Adjust `role` or `used_in_model` where needed.\n",
    "\n",
    "This hybrid approach saves time and keeps your schema **self-documenting**.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Optional extensions (Skill 4.5)\n",
    "\n",
    "| Enhancement                     | Purpose                                                                          |\n",
    "| ------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| **Validate data vs dictionary** | Ensure every required column is present and has the right dtype before training. |\n",
    "| **Generate Markdown table**     | Auto-export to `reports/data_dictionary.md` for project docs.                    |\n",
    "| **Integrate with `pandera`**    | Enforce schema in code for clean data contracts.                                 |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show how to **validate your dataset against this generated dictionary** (e.g., check missing columns, wrong dtypes, unexpected values) before training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe067ddd",
   "metadata": {},
   "source": [
    "># WHY would I do this?\n",
    "\n",
    "That‚Äôs a really sharp and important question ‚Äî because what you‚Äôre asking gets to the *why* behind doing all this extra metadata work.\n",
    "Let‚Äôs unpack it clearly:\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Real Purpose of a Data Dictionary (and Auto-Generation)\n",
    "\n",
    "You‚Äôre already at the point (Skill Level 3‚Äì4) where your **Telco churn pipeline runs reliably**.\n",
    "The next step isn‚Äôt about making it ‚Äúprettier‚Äù‚Äîit‚Äôs about making it **robust**, **explainable**, and **reusable**.\n",
    "\n",
    "### üß± In short:\n",
    "\n",
    "You do this to turn your notebook experiment into a **production-grade, living dataset** that anyone (or any script) can understand, validate, and extend‚Äîwithout breaking your model.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 1. For **data validation**\n",
    "\n",
    "Your model *assumes* certain columns exist, have specific data types, and use known categories.\n",
    "If the input data ever changes (new column names, nulls, or types), your model can silently break or give bad predictions.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* ‚ÄúTotalCharges‚Äù suddenly becomes ‚ÄúTotal_Charges‚Äù\n",
    "* ‚ÄúSeniorCitizen‚Äù starts coming in as `\"Yes\"/\"No\"` instead of `0/1`\n",
    "\n",
    "‚úÖ With a data dictionary, you can write a 10-line validation step that checks:\n",
    "\n",
    "```python\n",
    "for col, meta in data_dict[\"columns\"].items():\n",
    "    assert col in df.columns, f\"Missing column: {col}\"\n",
    "    assert str(df[col].dtype).startswith(meta[\"dtype\"]), f\"Wrong type for {col}\"\n",
    "```\n",
    "\n",
    "‚Üí You catch bad data *before* it corrupts your model.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 2. For **interpretability & explainability**\n",
    "\n",
    "When you later show feature importances or SHAP values, people will ask:\n",
    "\n",
    "> ‚ÄúWhat exactly does `OnlineSecurity_No` mean?‚Äù\n",
    "\n",
    "If you have your `data_dictionary.yaml`, you can instantly explain it.\n",
    "It also helps you (and future teammates) remember *why* certain variables are in the model at all.\n",
    "\n",
    "---\n",
    "\n",
    "## üë©‚Äçüíª 3. For **automation & pipelines**\n",
    "\n",
    "In bigger projects, multiple scripts need the same info:\n",
    "\n",
    "* Which columns are numeric?\n",
    "* Which are categorical?\n",
    "* What‚Äôs the target variable?\n",
    "\n",
    "Instead of repeating those lists everywhere, the dictionary acts as a **single source of truth**.\n",
    "If the schema changes, you update one YAML and every part of the pipeline (train, score, viz) stays consistent.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 4. For **schema drift detection**\n",
    "\n",
    "Over time, the dataset might evolve (new columns, renamed ones).\n",
    "With a saved dictionary, you can automatically compare the new schema vs old and alert:\n",
    "\n",
    "```bash\n",
    "‚ö†Ô∏è  Detected new column: FiberSpeed\n",
    "‚ö†Ô∏è  Column dropped: DeviceProtection\n",
    "```\n",
    "\n",
    "That‚Äôs data engineering 101 in production environments.\n",
    "\n",
    "---\n",
    "\n",
    "## üîê 5. For **compliance and governance**\n",
    "\n",
    "Even in small projects, it‚Äôs good practice to label columns that might be sensitive (e.g., demographics).\n",
    "Later, if you ever deploy your model publicly or collaborate, you‚Äôll already have clear documentation of what data you use and why.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò In human terms:\n",
    "\n",
    "You‚Äôre future-proofing your work.\n",
    "Today it‚Äôs *your* dataset and you remember everything.\n",
    "Three months (or one teammate) from now, you won‚Äôt.\n",
    "The dictionary is your *memory* ‚Äî for you, your model, and anyone else who touches it.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "You create and maintain a data dictionary to:\n",
    "\n",
    "| Reason             | Benefit                                                  |\n",
    "| ------------------ | -------------------------------------------------------- |\n",
    "| ‚úÖ Data validation  | Catch missing/malformed columns before they cause errors |\n",
    "| üß† Explainability  | Understand and explain what features mean                |\n",
    "| üß© Reusability     | Keep column roles/types consistent across scripts        |\n",
    "| üîç Drift detection | Alert when schema changes unexpectedly                   |\n",
    "| üìã Governance      | Document what data you use and why                       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3a4af",
   "metadata": {},
   "source": [
    "\n",
    "Would you like me to show a **10-line validation script** that checks your current Telco dataset against your dictionary (so you can see that benefit in action)?\n",
    "\n",
    "Perfect ‚Äî here‚Äôs your compact, no-nonsense **10-line data-validation script** for your Telco churn project, followed by a clear explanation of what *bad data* looks like in this context.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ `scripts/validate_dataset.py`\n",
    "\n",
    "```python\n",
    "import yaml, pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Load the reference data dictionary and dataset\n",
    "data_dict = yaml.safe_load(open(\"configs/data_dictionary.yaml\"))\n",
    "df = pd.read_csv(\"data/processed/telco_clean.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ Loop through expected columns\n",
    "for col, meta in data_dict[\"columns\"].items():\n",
    "    if col not in df.columns:\n",
    "        print(f\"‚ùå Missing column: {col}\")\n",
    "        continue\n",
    "    # 3Ô∏è‚É£ Check dtype consistency\n",
    "    expected = meta[\"dtype\"]\n",
    "    actual = str(df[col].dtype)\n",
    "    if expected.startswith(\"int\") and not \"int\" in actual:\n",
    "        print(f\"‚ö†Ô∏è  {col}: expected int, found {actual}\")\n",
    "    elif expected.startswith(\"float\") and not \"float\" in actual:\n",
    "        print(f\"‚ö†Ô∏è  {col}: expected float, found {actual}\")\n",
    "    elif expected == \"category\" and df[col].nunique() > 50:\n",
    "        print(f\"‚ö†Ô∏è  {col}: too many unique values ({df[col].nunique()}) for categorical\")\n",
    "\n",
    "# 4Ô∏è‚É£ Optional: Check for unexpected extra columns\n",
    "extra = set(df.columns) - set(data_dict[\"columns\"].keys())\n",
    "if extra:\n",
    "    print(f\"‚ö†Ô∏è  Unexpected extra columns: {extra}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What this does\n",
    "\n",
    "* **Line 1‚Äì3:** Loads your saved data dictionary and the cleaned Telco dataset.\n",
    "* **Line 5‚Äì13:**\n",
    "\n",
    "  * Flags **missing columns** (`‚ùå Missing column: TotalCharges`)\n",
    "  * Flags **type mismatches** (e.g., expected `float`, got `object`)\n",
    "  * Warns if a ‚Äúcategorical‚Äù feature has too many unique values (usually indicates it was mis-typed as numeric or ID-like).\n",
    "* **Line 15‚Äì17:** Warns about **unexpected new columns** not defined in the dictionary (potential schema drift).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Examples of ‚Äúbad data‚Äù in your Telco dataset\n",
    "\n",
    "Here‚Äôs what would make this validator complain ‚Äî and *why* it matters:\n",
    "\n",
    "| Type of issue              | Example                                               | Why it‚Äôs bad                                                      |\n",
    "| -------------------------- | ----------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| **Missing column**         | `TotalCharges` missing entirely                       | Model can‚Äôt compute key billing features ‚Üí predictions unreliable |\n",
    "| **Type mismatch**          | `SeniorCitizen` read as ‚ÄúYes/No‚Äù instead of 0/1       | One-hot encoder sees new text categories, breaking preprocessing  |\n",
    "| **Unexpected new column**  | `FiberSpeed` appears unexpectedly                     | Indicates schema drift ‚Üí model trained on different features      |\n",
    "| **Categorical explosion**  | `PaymentMethod` suddenly has 120 unique values        | Likely dirty data (typos, inconsistent naming)                    |\n",
    "| **Numeric stored as text** | `MonthlyCharges` looks like ‚Äú$70.85‚Äù                  | Fails scaling step in preprocessing pipeline                      |\n",
    "| **Target label drift**     | `Churn` values ‚ÄúY‚Äù/‚ÄúN‚Äù instead of ‚ÄúYes‚Äù/‚ÄúNo‚Äù          | Target encoding fails or mislabels data                           |\n",
    "| **NaN inflation**          | `TotalCharges` has 10% missing values (previously 0%) | Indicates upstream extraction or cleaning issue                   |\n",
    "\n",
    "---\n",
    "\n",
    "### üö¶What to do when validation fails\n",
    "\n",
    "* **Missing columns:** Check your data extraction step or upstream schema.\n",
    "* **Wrong dtype:** Fix with `pd.to_numeric()` or category encoding before retraining.\n",
    "* **Unexpected extras:** Decide if you should update the dictionary (new feature) or drop the column.\n",
    "* **Category blow-up:** Standardize category values (strip whitespace, unify case).\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "This 10-line check is your *early-warning system*:\n",
    "\n",
    "> It doesn‚Äôt stop bad data from existing ‚Äî it stops bad data from reaching your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8336f",
   "metadata": {},
   "source": [
    "\n",
    "Would you like me to expand this validator into a short reusable **`validate_dataframe(df, data_dict)` function** (so you can call it before every train or predict step automatically)?\n",
    "\n",
    "Perfect ‚Äî this is exactly how you start moving toward **automated data quality gates**, a key Skill-4 milestone.\n",
    "Here‚Äôs a clean, reusable function you can drop into your project to validate any dataset against your `data_dictionary.yaml`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ File: `src/churn/validate.py`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "def validate_dataframe(df: pd.DataFrame, dict_path: str = \"configs/data_dictionary.yaml\") -> bool:\n",
    "    \"\"\"\n",
    "    Validate a DataFrame against a YAML data dictionary.\n",
    "    Returns True if all checks pass, False otherwise.\n",
    "    \"\"\"\n",
    "    data_dict = yaml.safe_load(open(dict_path))\n",
    "    ok = True\n",
    "\n",
    "    # 1Ô∏è‚É£ Check each expected column\n",
    "    for col, meta in data_dict[\"columns\"].items():\n",
    "        if col not in df.columns:\n",
    "            print(f\"‚ùå Missing column: {col}\")\n",
    "            ok = False\n",
    "            continue\n",
    "\n",
    "        expected = meta.get(\"dtype\", \"\")\n",
    "        actual = str(df[col].dtype)\n",
    "\n",
    "        # Basic dtype consistency check\n",
    "        if expected.startswith(\"int\") and not \"int\" in actual:\n",
    "            print(f\"‚ö†Ô∏è  {col}: expected int, found {actual}\")\n",
    "            ok = False\n",
    "        elif expected.startswith(\"float\") and not \"float\" in actual:\n",
    "            print(f\"‚ö†Ô∏è  {col}: expected float, found {actual}\")\n",
    "            ok = False\n",
    "        elif expected == \"category\" and df[col].nunique() > 50:\n",
    "            print(f\"‚ö†Ô∏è  {col}: too many unique values ({df[col].nunique()}) for categorical\")\n",
    "            ok = False\n",
    "\n",
    "    # 2Ô∏è‚É£ Check for unexpected new columns\n",
    "    extra = set(df.columns) - set(data_dict[\"columns\"].keys())\n",
    "    if extra:\n",
    "        print(f\"‚ö†Ô∏è  Unexpected extra columns: {extra}\")\n",
    "        ok = False\n",
    "\n",
    "    # 3Ô∏è‚É£ Optionally check for NaN spikes\n",
    "    nan_ratio = df.isna().mean()\n",
    "    high_nan = nan_ratio[nan_ratio > 0.05]  # >5% missing\n",
    "    if not high_nan.empty:\n",
    "        print(\"‚ö†Ô∏è  Columns with >5% missing values:\")\n",
    "        print(high_nan)\n",
    "        ok = False\n",
    "\n",
    "    return ok\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example use\n",
    "\n",
    "In any notebook or script (like `train.py` or `predict.py`):\n",
    "\n",
    "```python\n",
    "from churn.validate import validate_dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/processed/telco_clean.csv\")\n",
    "\n",
    "if validate_dataframe(df):\n",
    "    print(\"‚úÖ Data passed validation, safe to proceed.\")\n",
    "else:\n",
    "    print(\"‚ùå Data failed validation ‚Äî check messages above.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why this is valuable\n",
    "\n",
    "* ‚úÖ **Reusable:** You can call this before training, scoring, or visualization.\n",
    "* üîÑ **Consistent:** Uses the same `data_dictionary.yaml` your entire pipeline references.\n",
    "* üõë **Fail-fast:** Alerts you before you fit a model on corrupted or shifted data.\n",
    "* üß© **Extensible:** You can easily add extra checks (e.g., allowed category values, numeric ranges).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf0c12",
   "metadata": {},
   "source": [
    "\n",
    "## üß® What counts as ‚Äúbad data‚Äù here\n",
    "\n",
    "Running `validate_dataframe()` will flag the same issues as before:\n",
    "\n",
    "* **Missing or renamed columns**\n",
    "* **Numeric fields stored as strings**\n",
    "* **Exploded category values** (e.g., 100 unique `PaymentMethod`s)\n",
    "* **Excessive missing values**\n",
    "* **Unexpected extra columns**\n",
    "\n",
    "That means your model won‚Äôt silently train or predict on wrong or mismatched data again.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6a306",
   "metadata": {},
   "source": [
    "># show how to integrate this validator directly inside your **`train.py`** and **`predict.py`** scripts so every run auto-checks the data first?\n",
    "\n",
    "Absolutely‚Äîhere are clean, drop-in upgrades that wire validation into both training and prediction. I‚Äôve also refined the validator to support **mode-aware checks** (train vs predict), so scoring data isn‚Äôt forced to include the target or ID columns.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Replace `src/churn/validate.py` (mode-aware)\n",
    "\n",
    "```python\n",
    "# src/churn/validate.py\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "def _load_dict(dict_path: str):\n",
    "    return yaml.safe_load(open(dict_path))\n",
    "\n",
    "def _expected_columns(data_dict, mode: str):\n",
    "    cols_meta = data_dict[\"columns\"]\n",
    "    if mode == \"train\":\n",
    "        # Expect everything listed in the dictionary\n",
    "        return list(cols_meta.keys())\n",
    "    elif mode == \"predict\":\n",
    "        # Only columns actually used for modeling (no target, no IDs)\n",
    "        used = []\n",
    "        for c, meta in cols_meta.items():\n",
    "            if not meta.get(\"used_in_model\", False):\n",
    "                continue\n",
    "            if meta.get(\"role\") in (\"target\", \"id\"):\n",
    "                continue\n",
    "            used.append(c)\n",
    "        return used\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'predict'\")\n",
    "\n",
    "def validate_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    dict_path: str = \"configs/data_dictionary.yaml\",\n",
    "    mode: str = \"train\",\n",
    "    categorical_cardinality_cap: int = 50,\n",
    "    nan_warn_ratio: float = 0.05,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Validate a DataFrame against a YAML data dictionary.\n",
    "\n",
    "    mode='train'  ‚Üí require all columns in dictionary.\n",
    "    mode='predict'‚Üí require only features with used_in_model=True and role not in {target,id}.\n",
    "\n",
    "    Returns True if all checks pass, False otherwise.\n",
    "    \"\"\"\n",
    "    data_dict = _load_dict(dict_path)\n",
    "    cols_meta = data_dict[\"columns\"]\n",
    "    ok = True\n",
    "\n",
    "    expected_cols = set(_expected_columns(data_dict, mode))\n",
    "    df_cols = set(df.columns)\n",
    "\n",
    "    # Missing & unexpected columns\n",
    "    missing = expected_cols - df_cols\n",
    "    if missing:\n",
    "        print(f\"‚ùå Missing required columns ({mode}): {sorted(missing)}\")\n",
    "        ok = False\n",
    "\n",
    "    unexpected = df_cols - set(cols_meta.keys())\n",
    "    if unexpected and mode == \"train\":\n",
    "        # Unexpected at train time is more dangerous\n",
    "        print(f\"‚ö†Ô∏è  Unexpected extra columns present: {sorted(unexpected)}\")\n",
    "        ok = False\n",
    "\n",
    "    # Dtype & simple categorical checks only for columns we both expect and have\n",
    "    check_cols = expected_cols & df_cols\n",
    "    for col in sorted(check_cols):\n",
    "        meta = cols_meta.get(col, {})\n",
    "        expected_dtype = meta.get(\"dtype\", \"\")\n",
    "        actual_dtype = str(df[col].dtype)\n",
    "\n",
    "        # Basic dtype consistency\n",
    "        if expected_dtype.startswith(\"int\") and \"int\" not in actual_dtype:\n",
    "            print(f\"‚ö†Ô∏è  {col}: expected int, found {actual_dtype}\")\n",
    "            ok = False\n",
    "        elif expected_dtype.startswith(\"float\") and \"float\" not in actual_dtype:\n",
    "            print(f\"‚ö†Ô∏è  {col}: expected float, found {actual_dtype}\")\n",
    "            ok = False\n",
    "        elif expected_dtype == \"category\":\n",
    "            nunique = df[col].nunique(dropna=True)\n",
    "            if nunique > categorical_cardinality_cap:\n",
    "                print(f\"‚ö†Ô∏è  {col}: too many unique values for categorical ({nunique} > {categorical_cardinality_cap})\")\n",
    "                ok = False\n",
    "            # Optional: enforce allowed categories if provided\n",
    "            allowed = meta.get(\"allowed\")\n",
    "            if allowed is not None:\n",
    "                bad = set(df[col].dropna().unique()) - set(allowed)\n",
    "                if bad:\n",
    "                    print(f\"‚ö†Ô∏è  {col}: unexpected categories {sorted(list(bad))}\")\n",
    "                    ok = False\n",
    "\n",
    "    # Missingness check (warn for > nan_warn_ratio)\n",
    "    nan_ratio = df[list(check_cols)].isna().mean()\n",
    "    high_nan = nan_ratio[nan_ratio > nan_warn_ratio]\n",
    "    if not high_nan.empty:\n",
    "        print(\"‚ö†Ô∏è  Columns with high missingness:\")\n",
    "        for c, r in high_nan.sort_values(ascending=False).items():\n",
    "            print(f\"   - {c}: {r:.1%} missing\")\n",
    "        ok = False\n",
    "\n",
    "    return ok\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Update `scripts/train.py` to validate before training\n",
    "\n",
    "Replace your current `scripts/train.py` with this (only differences: import + early validation + graceful exit):\n",
    "\n",
    "```python\n",
    "# scripts/train.py\n",
    "import sys\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from churn.io import read_csv, write_csv, save_obj\n",
    "from churn.data import basic_clean, split_xy\n",
    "from churn.features import build_preprocess\n",
    "from churn.model import build_pipeline\n",
    "from churn.validate import validate_dataframe  # NEW\n",
    "\n",
    "def main():\n",
    "    cfg = yaml.safe_load(open(\"configs/config.yaml\"))\n",
    "    df_raw = read_csv(cfg[\"paths\"][\"raw\"])\n",
    "    df = basic_clean(df_raw)\n",
    "\n",
    "    # Validate cleaned dataframe against dictionary (TRAIN mode)\n",
    "    if not validate_dataframe(df, dict_path=\"configs/data_dictionary.yaml\", mode=\"train\"):\n",
    "        print(\"‚ùå Data validation failed. Aborting training.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    write_csv(df, cfg[\"paths\"][\"processed\"])\n",
    "\n",
    "    X, y = split_xy(df, target=\"Churn\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=cfg[\"training\"][\"test_size\"],\n",
    "        random_state=cfg[\"training\"][\"random_state\"],\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    preprocess, num_cols, cat_cols = build_preprocess(X_train)\n",
    "    clf = build_pipeline(preprocess, max_iter=cfg[\"training\"][\"model\"][\"max_iter\"])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    save_obj(clf, cfg[\"paths\"][\"model\"])\n",
    "    save_obj({\"num_cols\": num_cols, \"cat_cols\": cat_cols}, cfg[\"paths\"][\"columns\"])\n",
    "    print(\"‚úÖ Model trained and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Update `scripts/predict.py` to validate before scoring (predict mode)\n",
    "\n",
    "Replace your current `scripts/predict.py` (the override-threshold version) with this small enhancement:\n",
    "\n",
    "```python\n",
    "# scripts/predict.py\n",
    "import argparse\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from churn.io import load_obj\n",
    "from churn.validate import validate_dataframe  # NEW\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Batch score Telco churn.\")\n",
    "    parser.add_argument(\"input_csv\", help=\"Path to CSV of records to score\")\n",
    "    parser.add_argument(\"--output\", help=\"Optional output path (CSV). If omitted, appends _scored.csv\")\n",
    "    parser.add_argument(\"--threshold\", type=float, help=\"Override decision threshold (0‚Äì1). If omitted, uses config.\")\n",
    "    parser.add_argument(\"--config\", default=\"configs/config.yaml\", help=\"Path to YAML config\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load config and model\n",
    "    with open(args.config) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    model_path = cfg[\"paths\"][\"model\"]\n",
    "    clf = load_obj(model_path)\n",
    "\n",
    "    # Load data\n",
    "    df_new = pd.read_csv(args.input_csv)\n",
    "\n",
    "    # If user accidentally included a label column, ignore it for scoring\n",
    "    for col in (\"Churn\", \"churn\", \"label\", \"target\"):\n",
    "        if col in df_new.columns:\n",
    "            df_new = df_new.drop(columns=[col])\n",
    "\n",
    "    # Validate features (PREDICT mode ‚Üí only requires model-used predictors)\n",
    "    if not validate_dataframe(df_new, dict_path=\"configs/data_dictionary.yaml\", mode=\"predict\"):\n",
    "        print(\"‚ùå Data validation failed. Aborting scoring.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Predict\n",
    "    proba = clf.predict_proba(df_new)[:, 1]\n",
    "\n",
    "    # Decide threshold: CLI override > YAML > default 0.5\n",
    "    cfg_thresh = cfg.get(\"inference\", {}).get(\"threshold\", 0.5)\n",
    "    threshold = args.threshold if args.threshold is not None else cfg_thresh\n",
    "    if not (0.0 <= float(threshold) <= 1.0):\n",
    "        print(f\"ERROR: threshold must be in [0,1], got {threshold}\", file=sys.stderr)\n",
    "        sys.exit(2)\n",
    "\n",
    "    pred = (proba >= float(threshold)).astype(int)\n",
    "\n",
    "    # Assemble output\n",
    "    scored = df_new.copy()\n",
    "    scored[\"churn_proba\"] = proba\n",
    "    scored[\"churn_pred\"] = pred\n",
    "\n",
    "    # Output path\n",
    "    out_path = args.output or (str(Path(args.input_csv).with_suffix(\"\")) + \"_scored.csv\")\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    scored.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Applied threshold: {threshold}\")\n",
    "    print(f\"‚úÖ Scored ‚Üí {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What this gives you (quick wins)\n",
    "\n",
    "* **Fail-fast safety**: Training or scoring halts with clear messages if schema/type issues show up.\n",
    "* **Mode awareness**: Scoring datasets don‚Äôt need the target or ID fields; training still enforces the full schema.\n",
    "* **Stronger contracts**: Your YAML dictionary is now the single source of truth for your pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4254ba",
   "metadata": {},
   "source": [
    "># If you want, I can also add a **`--strict` flag** to treat warnings (like high missingness) as hard errors, or wire this into a tiny CI step (pre-commit or GitHub Actions) so bad data never merges into `main`.\n",
    "\n",
    "Awesome‚Äîlet‚Äôs wire in a **strict mode** and a tiny **CI / pre-commit safety net** so bad data can‚Äôt sneak into your pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Upgrade the validator (adds `strict` + tunable behavior)\n",
    "\n",
    "### `src/churn/validate.py` (drop-in replacement)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "def _load_dict(dict_path: str):\n",
    "    return yaml.safe_load(open(dict_path))\n",
    "\n",
    "def _expected_columns(data_dict, mode: str):\n",
    "    cols_meta = data_dict[\"columns\"]\n",
    "    if mode == \"train\":\n",
    "        return list(cols_meta.keys())\n",
    "    elif mode == \"predict\":\n",
    "        used = []\n",
    "        for c, meta in cols_meta.items():\n",
    "            if not meta.get(\"used_in_model\", False):    # only model features\n",
    "                continue\n",
    "            if meta.get(\"role\") in (\"target\", \"id\"):    # no target/ids at predict time\n",
    "                continue\n",
    "            used.append(c)\n",
    "        return used\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'predict'\")\n",
    "\n",
    "def validate_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    dict_path: str = \"configs/data_dictionary.yaml\",\n",
    "    mode: str = \"train\",\n",
    "    categorical_cardinality_cap: int = 50,\n",
    "    nan_warn_ratio: float = 0.05,\n",
    "    strict: bool = False,   # ‚Üê NEW: if True, ANY warning becomes a failure\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Validate df against data_dictionary.yaml.\n",
    "    Returns True if checks pass per current 'strict' level, else False.\n",
    "    \"\"\"\n",
    "    data_dict = _load_dict(dict_path)\n",
    "    cols_meta = data_dict[\"columns\"]\n",
    "    ok = True\n",
    "\n",
    "    expected = set(_expected_columns(data_dict, mode))\n",
    "    have = set(df.columns)\n",
    "\n",
    "    # Missing columns (always hard error)\n",
    "    missing = expected - have\n",
    "    if missing:\n",
    "        print(f\"‚ùå Missing required columns ({mode}): {sorted(missing)}\")\n",
    "        ok = False\n",
    "\n",
    "    # Unexpected columns\n",
    "    unexpected = have - set(cols_meta.keys())\n",
    "    if unexpected:\n",
    "        msg = f\"‚ö†Ô∏è  Unexpected columns present: {sorted(unexpected)}\"\n",
    "        print(msg)\n",
    "        if strict or mode == \"train\":\n",
    "            ok = False   # train is less tolerant; predict can be lenient unless strict\n",
    "\n",
    "    # Dtype & categorical cardinality\n",
    "    check_cols = expected & have\n",
    "    for col in sorted(check_cols):\n",
    "        meta = cols_meta.get(col, {})\n",
    "        expected_dtype = meta.get(\"dtype\", \"\")\n",
    "        actual_dtype = str(df[col].dtype)\n",
    "        local_fail = False\n",
    "\n",
    "        if expected_dtype.startswith(\"int\") and \"int\" not in actual_dtype:\n",
    "            print(f\"‚ö†Ô∏è  {col}: expected int, found {actual_dtype}\"); local_fail = True\n",
    "        elif expected_dtype.startswith(\"float\") and \"float\" not in actual_dtype:\n",
    "            print(f\"‚ö†Ô∏è  {col}: expected float, found {actual_dtype}\"); local_fail = True\n",
    "        elif expected_dtype == \"category\":\n",
    "            nunique = df[col].nunique(dropna=True)\n",
    "            if nunique > categorical_cardinality_cap:\n",
    "                print(f\"‚ö†Ô∏è  {col}: too many unique values for categorical ({nunique} > {categorical_cardinality_cap})\")\n",
    "                local_fail = True\n",
    "            allowed = meta.get(\"allowed\")\n",
    "            if allowed is not None:\n",
    "                bad = set(df[col].dropna().unique()) - set(allowed)\n",
    "                if bad:\n",
    "                    print(f\"‚ö†Ô∏è  {col}: unexpected categories {sorted(list(bad))}\")\n",
    "                    local_fail = True\n",
    "\n",
    "        if local_fail and (strict or mode == \"train\"):\n",
    "            ok = False\n",
    "\n",
    "    # Missingness\n",
    "    if check_cols:\n",
    "        nan_ratio = df[list(check_cols)].isna().mean()\n",
    "        high_nan = nan_ratio[nan_ratio > nan_warn_ratio]\n",
    "        if not high_nan.empty:\n",
    "            print(\"‚ö†Ô∏è  Columns with high missingness:\")\n",
    "            for c, r in high_nan.sort_values(ascending=False).items():\n",
    "                print(f\"   - {c}: {r:.1%} missing\")\n",
    "            if strict:\n",
    "                ok = False\n",
    "\n",
    "    return ok\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Add CLI validation script (handy for pre-commit/CI)\n",
    "\n",
    "### `scripts/validate_cli.py`\n",
    "\n",
    "```python\n",
    "import argparse, sys, yaml, pandas as pd\n",
    "from churn.validate import validate_dataframe\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Validate a CSV against data_dictionary.yaml\")\n",
    "    ap.add_argument(\"--mode\", choices=[\"train\",\"predict\"], default=\"train\")\n",
    "    ap.add_argument(\"--file\", help=\"CSV to validate (defaults: processed for train; required for predict)\")\n",
    "    ap.add_argument(\"--dict\", default=\"configs/data_dictionary.yaml\")\n",
    "    ap.add_argument(\"--strict\", action=\"store_true\", help=\"Treat warnings as errors\")\n",
    "    ap.add_argument(\"--nan-cap\", type=float, default=0.05)\n",
    "    ap.add_argument(\"--cat-cap\", type=int, default=50)\n",
    "    ap.add_argument(\"--config\", default=\"configs/config.yaml\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.file:\n",
    "        csv_path = args.file\n",
    "    else:\n",
    "        if args.mode == \"train\":\n",
    "            cfg = yaml.safe_load(open(args.config))\n",
    "            csv_path = cfg[\"paths\"][\"processed\"]\n",
    "        else:\n",
    "            print(\"For --mode predict you must pass --file <csv>\", file=sys.stderr)\n",
    "            sys.exit(2)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    ok = validate_dataframe(\n",
    "        df, dict_path=args.dict, mode=args.mode,\n",
    "        categorical_cardinality_cap=args.cat_cap,\n",
    "        nan_warn_ratio=args.nan_cap, strict=args.strict\n",
    "    )\n",
    "    if not ok:\n",
    "        print(\"‚ùå Validation failed.\")\n",
    "        sys.exit(1)\n",
    "    print(\"‚úÖ Validation passed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Train / Predict with `--strict` flags\n",
    "\n",
    "### `scripts/train.py` (only the validation line changes)\n",
    "\n",
    "```python\n",
    "# ...\n",
    "from churn.validate import validate_dataframe\n",
    "\n",
    "# Validate cleaned dataframe (strict at train-time is recommended)\n",
    "if not validate_dataframe(df, dict_path=\"configs/data_dictionary.yaml\", mode=\"train\", strict=True):\n",
    "    print(\"‚ùå Data validation failed. Aborting training.\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "# ...\n",
    "```\n",
    "\n",
    "### `scripts/predict.py` (allow optional strict via CLI)\n",
    "\n",
    "```python\n",
    "# add to argparse:\n",
    "parser.add_argument(\"--strict\", action=\"store_true\", help=\"Treat warnings as errors during validation\")\n",
    "\n",
    "# pass through:\n",
    "if not validate_dataframe(df_new, dict_path=\"configs/data_dictionary.yaml\", mode=\"predict\", strict=args.strict):\n",
    "    print(\"‚ùå Data validation failed. Aborting scoring.\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```bash\n",
    "# strict train (recommended)\n",
    "python scripts/train.py\n",
    "\n",
    "# lenient predict\n",
    "python scripts/predict.py data/scoring/new_customers.csv\n",
    "\n",
    "# strict predict (e.g., production scoring)\n",
    "python scripts/predict.py data/scoring/new_customers.csv --strict\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Pre-commit hook (runs validation before every commit)\n",
    "\n",
    "### `.pre-commit-config.yaml`\n",
    "\n",
    "```yaml\n",
    "repos:\n",
    "  - repo: local\n",
    "    hooks:\n",
    "      - id: telco-validate-train\n",
    "        name: Telco Validate (train strict)\n",
    "        entry: bash -c 'test -f configs/config.yaml && python scripts/validate_cli.py --mode train --strict || true'\n",
    "        language: system\n",
    "        pass_filenames: false\n",
    "      - id: telco-validate-sample-predict\n",
    "        name: Telco Validate (predict lenient if sample exists)\n",
    "        entry: bash -c 'test -f data/scoring/new_customers.csv && python scripts/validate_cli.py --mode predict --file data/scoring/new_customers.csv || true'\n",
    "        language: system\n",
    "        pass_filenames: false\n",
    "```\n",
    "\n",
    "Install:\n",
    "\n",
    "```bash\n",
    "pip install pre-commit\n",
    "pre-commit install\n",
    "```\n",
    "\n",
    "Now every `git commit` will gate on validation.\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Lightweight GitHub Actions CI (validates on push/PR)\n",
    "\n",
    "### `.github/workflows/ci.yml`\n",
    "\n",
    "```yaml\n",
    "name: telco-ci\n",
    "on:\n",
    "  push:\n",
    "  pull_request:\n",
    "jobs:\n",
    "  validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/setup-python@v5\n",
    "        with: { python-version: '3.11' }\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt || true\n",
    "      - name: Validate train (strict)\n",
    "        run: |\n",
    "          if [ -f configs/config.yaml ]; then\n",
    "            python scripts/validate_cli.py --mode train --strict || exit 1\n",
    "          else\n",
    "            echo \"configs/config.yaml not found; skipping train validation.\"\n",
    "          fi\n",
    "      - name: Validate sample predict (lenient)\n",
    "        run: |\n",
    "          if [ -f data/scoring/new_customers.csv ]; then\n",
    "            python scripts/validate_cli.py --mode predict --file data/scoring/new_customers.csv || exit 1\n",
    "          else\n",
    "            echo \"No sample scoring file; skipping predict validation.\"\n",
    "          fi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What you get\n",
    "\n",
    "* **Strict mode** makes warnings fatal when you want hard gates (CI, training).\n",
    "* **Lenient mode** lets you explore data without blocking (ad-hoc scoring).\n",
    "* **Pre-commit + CI**: bad schemas/types are caught *before* code lands in main, and before training happens.\n",
    "\n",
    "If you want, I can also add a **‚Äúfix-suggestions‚Äù printout** (e.g., ‚Äútry `pd.to_numeric(..., errors='coerce')` on `TotalCharges`‚Äù) when dtype checks fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385030ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils/paths.py\n",
    "from pathlib import Path\n",
    "\n",
    "# Define project paths\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710734c",
   "metadata": {},
   "source": [
    "Phase 1: Extract second function:\n",
    "1.2 Initial Quality Assessment\n",
    "Missing value detection\n",
    "Duplicate row checking\n",
    "Data type inconsistencies (TotalCharges as object)\n",
    "Basic value range validation\n",
    "Initial observations documentation\n",
    "Create data_quality.py when utils.py hits 50+ lines\n",
    "Phase 2: Data Cleaning & Extracting Functions (Week 2)\n",
    "Cleaning with Modular Code (Week 2, Days 1-3)\n",
    "As soon as you write code twice, extract it:\n",
    "2.1 Create Cleaning Module\n",
    "# src/cleaning.py (NEW FILE)\n",
    "def fix_total_charges(df):\n",
    "    \"\"\"Convert TotalCharges to numeric, handle missing.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    # ... rest of cleaning logic\n",
    "    return df\n",
    "\n",
    "2.1 Handling Data Issues\n",
    "Convert TotalCharges to numeric\n",
    "Handle missing/empty values\n",
    "Fix inconsistent categories\n",
    "Remove or impute problematic records\n",
    "Document cleaning decisions\n",
    "2.2 Data Validation\n",
    "Verify cleaning effectiveness\n",
    "Cross-check related fields (tenure vs TotalCharges)\n",
    "Ensure business logic consistency\n",
    "Create cleaned dataset checkpoint\n",
    "Write cleaning functions\n",
    "2.2 Validation Functions\n",
    "Add validation to same module\n",
    "Import and use in notebooks\n",
    "Document cleaning decisions in notebook markdown\n",
    "\n",
    "\n",
    "Create Visualization Module\n",
    "# src/visualization.py (NEW FILE)\n",
    "def plot_churn_distribution(df):\n",
    "    \"\"\"Create churn distribution plots.\"\"\"\n",
    "    # ... plotting code\n",
    "    \n",
    "def plot_numerical_distributions(df, columns):\n",
    "    \"\"\"Create distribution plots for numerical features.\"\"\"\n",
    "    # ... plotting code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd6782",
   "metadata": {},
   "source": [
    "Phase 4: Feature Engineering (Week 4)\n",
    "Create Basic Package Structure\n",
    "Once you have 5-10 functions:\n",
    "project/\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ analysis.ipynb\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data_utils.py      # 3-5 functions\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cleaning.py         # 3-5 functions\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "4.1 Business-Driven Features\n",
    "Customer tenure groups (new/established/loyal)\n",
    "Service bundle indicators\n",
    "Payment behavior flags\n",
    "Contract risk indicators\n",
    "Value segmentation (high/medium/low)\n",
    "4.2 Statistical Features\n",
    "Ratio features (charges per tenure month)\n",
    "Binary indicators from continuous\n",
    "Interaction terms\n",
    "Aggregated service counts\n",
    "Normalized monetary values\n",
    "Create Feature Module\n",
    "# src/features.py (NEW FILE)\n",
    "def create_tenure_groups(df):\n",
    "    \"\"\"Create tenure-based customer segments.\"\"\"\n",
    "    df['TenureGroup'] = pd.cut(df['tenure'], \n",
    "                                bins=[0, 12, 24, 48, 72],\n",
    "                                labels=['New', 'Regular', 'Loyal', 'VIP'])\n",
    "    return df\n",
    "\n",
    "def create_value_segments(df):\n",
    "    \"\"\"Create customer value segments.\"\"\"\n",
    "    # ... implementation\n",
    "    return df\n",
    "Refactor Existing Code\n",
    "Move feature logic from notebooks to module\n",
    "Update notebooks to import from features.py\n",
    "Create package structure when you have 5+ modules:\n",
    "src/\n",
    "‚îî‚îÄ‚îÄ telco_analysis/  # Convert to package\n",
    "    ‚îú‚îÄ‚îÄ __init__.py\n",
    "    ‚îú‚îÄ‚îÄ data.py      # Renamed from utils.py\n",
    "    ‚îú‚îÄ‚îÄ cleaning.py\n",
    "    ‚îú‚îÄ‚îÄ features.py\n",
    "    ‚îî‚îÄ‚îÄ visualization.py\n",
    "Phase 5: Statistical Analysis (Week 5)\n",
    "5.1 Hypothesis Testing\n",
    "Chi-square for categorical associations\n",
    "T-tests for numerical differences\n",
    "Correlation significance testing\n",
    "Multiple testing corrections\n",
    "Effect size calculations\n",
    "5.2 Statistical Validation\n",
    "Assumption checking for tests\n",
    "Non-parametric alternatives when needed\n",
    "Confidence interval construction\n",
    "Statistical vs practical significance\n",
    "Business interpretation of results\n",
    "Statistics Module\n",
    "# src/telco_analysis/statistics.py (NEW FILE)\n",
    "def perform_churn_tests(df):\n",
    "    \"\"\"Run statistical tests for churn analysis.\"\"\"\n",
    "    results = {}\n",
    "    # Chi-square for categoricals\n",
    "    # T-tests for numericals\n",
    "    return results\n",
    "\n",
    "5.2 Progressive Organization\n",
    "Import all modules in __init__.py\n",
    "Make functions easily accessible\n",
    "Start using package imports:\n",
    "from telco_analysis import load_telco_data, create_tenure_groups\n",
    "Phase 6: Data Preparation for Modeling (Week 6)\n",
    "Expand Package for Modeling\n",
    "Add modules as needed:\n",
    "src/\n",
    "‚îú‚îÄ‚îÄ __init__.py            # Make it a package\n",
    "‚îú‚îÄ‚îÄ data_utils.py\n",
    "‚îú‚îÄ‚îÄ preprocessing.py        # Expanded from cleaning\n",
    "‚îú‚îÄ‚îÄ feature_engineering.py # New\n",
    "‚îî‚îÄ‚îÄ visualization.py       # Extracted plotting code\n",
    "6.1 Feature Preparation\n",
    "Encoding categorical variables\n",
    "Scaling numerical features\n",
    "Feature selection (statistical)\n",
    "Train/test/validation splits\n",
    "Stratification strategy\n",
    "6.2 Pipeline Development\n",
    "Preprocessing pipeline creation\n",
    "Transformation sequence definition\n",
    "Data leakage prevention\n",
    "Reproducibility setup\n",
    "Pipeline testing\n",
    "\n",
    "\n",
    "6.3 Preprocessing Module\n",
    "# src/telco_analysis/preprocessing.py\n",
    "def prepare_for_modeling(df):\n",
    "    \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "    # Encoding, scaling, splitting\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "6.4 Reorganize Package Structure\n",
    "src/telco_analysis/\n",
    "‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ loader.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cleaning.py\n",
    "‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ engineering.py\n",
    "‚îú‚îÄ‚îÄ analysis/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ statistics.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ visualization.py\n",
    "‚îî‚îÄ‚îÄ preprocessing.py\n",
    "Phase 7: Baseline Modeling (Week 7)\n",
    "7.1 Simple Models First\n",
    "Logistic regression baseline\n",
    "Decision tree (single)\n",
    "Naive Bayes classifier\n",
    "Model training basics\n",
    "Prediction generation\n",
    "7.2 Model Evaluation\n",
    "Confusion matrices\n",
    "Accuracy, precision, recall\n",
    "ROC curves and AUC\n",
    "Classification reports\n",
    "Business metric translation\n",
    "7.1 Create Models Module\n",
    "# src/telco_analysis/models/baseline.py\n",
    "def train_logistic_baseline(X_train, y_train):\n",
    "    \"\"\"Train baseline logistic regression.\"\"\"\n",
    "    # Implementation\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    # Implementation\n",
    "    return metrics\n",
    "\n",
    "7.2 Use Modules in Notebooks\n",
    "Import preprocessing and modeling functions\n",
    "Keep experimentation in notebooks\n",
    "Extract stable code to modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcfe97",
   "metadata": {},
   "source": [
    "\n",
    "Phase 8-9: Full Package Structure\n",
    "Complete organization:\n",
    "src/telco_analysis/\n",
    "‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ loader.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ validator.py\n",
    "‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ engineering.py\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ baseline.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py\n",
    "‚îî‚îÄ‚îÄ utils/\n",
    "    ‚îî‚îÄ‚îÄ config.py\n",
    "Phase 8: Advanced Models & Evaluation (Week 8)\n",
    "8.1 Advanced Algorithms\n",
    "Random Forest\n",
    "Gradient Boosting (XGBoost)\n",
    "Support Vector Machines\n",
    "Model comparison framework\n",
    "Ensemble strategies\n",
    "8.2 Hyperparameter Tuning\n",
    "Grid search basics\n",
    "Cross-validation implementation\n",
    "Parameter importance understanding\n",
    "Overfitting detection\n",
    "Optimal model selection\n",
    "Expand Models Module\n",
    "# src/telco_analysis/models/advanced.py\n",
    "def train_random_forest(X_train, y_train, **params):\n",
    "    \"\"\"Train random forest with given parameters.\"\"\"\n",
    "    # Implementation\n",
    "    \n",
    "# src/telco_analysis/models/evaluation.py  \n",
    "def plot_roc_curves(models, X_test, y_test):\n",
    "    \"\"\"Compare ROC curves for multiple models.\"\"\"\n",
    "    # Implementation\n",
    "Phase 9: Model Interpretation (Week 9)\n",
    "9.1 Feature Importance\n",
    "Coefficient analysis (logistic)\n",
    "Tree-based importances\n",
    "Permutation importance\n",
    "SHAP values introduction\n",
    "Business insight extraction\n",
    "9.2 Model Validation\n",
    "Performance on holdout set\n",
    "Temporal validation if possible\n",
    "Bias detection\n",
    "Calibration assessment\n",
    "Limitation documentation\n",
    "Model Optimization & Selection (Week 8)\n",
    "9.1 Hyperparameter Module\n",
    "# src/telco_analysis/models/tuning.py\n",
    "def tune_hyperparameters(model, param_grid, X, y):\n",
    "    \"\"\"Grid search with cross-validation.\"\"\"\n",
    "    # Implementation\n",
    "    return best_model, best_params\n",
    "\n",
    "9.2 Pipeline Integration\n",
    "Create end-to-end pipeline using all modules\n",
    "Consolidate best practices into reusable functions\n",
    "Phase 10: Productionization (Week 10)\n",
    "10.1 Code Modularization\n",
    "Function extraction from notebooks\n",
    "Module organization\n",
    "Package structure creation\n",
    "Documentation standards\n",
    "Testing implementation\n",
    "10.2 Model Deployment Prep\n",
    "Model serialization\n",
    "Prediction pipeline\n",
    "API wrapper basics\n",
    "Batch scoring setup\n",
    "Monitoring considerations\n",
    "Deployment Preparation\n",
    "Model serialization\n",
    "Config management\n",
    "CLI interface:\n",
    "# src/telco_analysis/cli.py\n",
    "def main():\n",
    "    \"\"\"Command-line interface for predictions.\"\"\"\n",
    "    # Implementation\n",
    "10.1 Add Production Features\n",
    "# src/telco_analysis/pipeline.py\n",
    "class ChurnPredictionPipeline:\n",
    "    \"\"\"End-to-end prediction pipeline.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Complete training pipeline\n",
    "        \n",
    "    def predict(self, df):\n",
    "        # Complete prediction pipeline\n",
    "\n",
    "10.2 Add Testing\n",
    "# tests/test_features.py\n",
    "def test_tenure_groups():\n",
    "    \"\"\"Test tenure group creation.\"\"\"\n",
    "    # Test implementation\n",
    "Phase 11: Business Integration (Week 11)\n",
    "Business Deliverables\n",
    "Executive dashboard notebook\n",
    "Technical report generation\n",
    "ROI calculations using modules\n",
    "11.1 Stakeholder Communication\n",
    "Executive summary creation\n",
    "Technical report writing\n",
    "Visualization dashboard\n",
    "ROI calculations\n",
    "Recommendation formulation\n",
    "11.2 Implementation Planning\n",
    "Deployment strategy\n",
    "A/B testing framework\n",
    "Success metrics definition\n",
    "Feedback loop design\n",
    "Maintenance planning\n",
    "\n",
    "Phase 12: Portfolio & Career (Week 12)\n",
    "12.1 Project Documentation\n",
    "GitHub repository polish\n",
    "README completion\n",
    "Code commenting\n",
    "Jupyter notebook narratives\n",
    "Results presentation\n",
    "Add comprehensive docstrings\n",
    "Create API documentation\n",
    "Write user guide in README\n",
    "12.2 Portfolio Polish\n",
    "Clean up repository\n",
    "Add comprehensive README\n",
    "Create presentation notebooks\n",
    "12.2 Career Artifacts\n",
    "LinkedIn project post\n",
    "Portfolio website entry\n",
    "Interview talking points\n",
    "Technical blog post\n",
    "Lessons learned reflection\n",
    "\n",
    "\n",
    "The Right Approach: Progressive Modularization\n",
    "Week 1-2: Notebook-Heavy, Light Functions\n",
    "Work primarily in notebooks\n",
    "Create utils.py for repeated code\n",
    "Import from utils into notebooks\n",
    "Week 3-4: Function-First Development\n",
    "Write new code as functions first\n",
    "Test in notebook cells\n",
    "Move stable functions to modules\n",
    "Week 5-6: Package Emergence\n",
    "Create proper package structure\n",
    "Add __init__.py files\n",
    "Start thinking about interfaces\n",
    "Week 7+: Package Refinement\n",
    "Organize into logical modules\n",
    "Add documentation\n",
    "Consider testing\n",
    "\n",
    "Practical Function Extraction Triggers\n",
    "Progressive Modularization Principles\n",
    "Why This Progressive Approach Works Better\n",
    "Natural Evolution: Code organization emerges from actual needs\n",
    "Less Refactoring: You don't have to reorganize everything at once\n",
    "Continuous Learning: Practice modularization throughout\n",
    "Earlier Benefits: Reuse code sooner, reduce duplication immediately\n",
    "Better Habits: Train yourself to recognize extraction opportunities\n",
    "Create a function when:\n",
    "You write similar code twice\n",
    "A cell exceeds 10-15 lines\n",
    "You need to reuse logic\n",
    "Code has a clear single purpose\n",
    "You copy-paste code (even once)\n",
    "You need the same logic in another notebook/You need to reuse logic\n",
    "You're about to write a complex operation\n",
    "Create a new module when:\n",
    "utils.py exceeds 100 lines\n",
    "You have 3-5+ related functions\n",
    "A module exceeds 100-150 lines\n",
    "Functions have a clear theme (data, features, models)\n",
    "Create package structure when:\n",
    "You have 3+ modules\n",
    "You need sub-modules\n",
    "Modules have interdependencies\n",
    "You're sharing code with others\n",
    "The key is: modularize incrementally as you go, not all at once at the end. This makes the learning smoother and the code better.\n",
    "Daily Development Flow\n",
    "Start in notebook - experiment freely\n",
    "Identify patterns - notice repetition\n",
    "Extract to function - create in appropriate module\n",
    "Import and use - replace notebook code with import\n",
    "Test and refine - ensure function works correctly\n",
    "Document - add docstring and comments\n",
    "Module Evolution Path\n",
    "Day 1-3:   utils.py (5-10 functions)\n",
    "Day 4-7:   utils.py + cleaning.py\n",
    "Week 2:    Add features.py, visualization.py\n",
    "Week 3:    Convert to package structure\n",
    "Week 4+:   Add sub-modules as needed\n",
    "\n",
    "Quality Checkpoints\n",
    "Before committing code:\n",
    "‚úÖ Functions have single responsibility\n",
    "‚úÖ Docstrings explain purpose\n",
    "‚úÖ Notebooks import from modules\n",
    "‚úÖ No code duplication\n",
    "‚úÖ Clear module organization\n",
    "This progressive approach ensures you build good habits from day one while maintaining flexibility for exploration and learning.\n",
    "\n",
    "\n",
    "Cadence Guidelines\n",
    "Daily Rhythm (2-3 hours/day)\n",
    "Morning (30 min): Review previous work, set daily goal\n",
    "Main Work (90 min): Implement new concepts/code\n",
    "Practice (30 min): Debug, refine, document\n",
    "Reflection (30 min): Note challenges, solutions, learnings\n",
    "Weekly Checkpoints\n",
    "Monday: Plan week's objectivesc\n",
    "Wednesday: Mid-week progress check\n",
    "Friday: Weekly review and documentation\n",
    "Weekend: Catch-up or exploration time\n",
    "Learning Principles\n",
    "Complete each phase before moving forward\n",
    "Document decisions and reasoning\n",
    "Save checkpoints after major milestones\n",
    "Practice explaining your work aloud\n",
    "Connect to business value at every step\n",
    "\n",
    "Quality Gates\n",
    "Before moving to next phase, ensure:\n",
    "‚úÖ Code runs without errors\n",
    "‚úÖ Results are reproducible\n",
    "‚úÖ Business insights documented\n",
    "‚úÖ Can explain work to non-technical audience\n",
    "‚úÖ Version controlled and backed up\n",
    "\n",
    "This cadence balances thorough learning with practical progress, ensuring you build strong foundations while maintaining momentum toward a production-ready solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35824c3",
   "metadata": {},
   "source": [
    "I'll create a comprehensive data analysis project for the IBM Telco churn dataset. Let me start by building the complete notebook-based project, then explain modularization.\n",
    "Great! I've created a complete notebook-based data analysis project for the IBM Telco churn dataset. The project is split into 4 parts:\n",
    "Data Loading & EDA - Initial exploration and understanding\n",
    "Preprocessing - Data cleaning and feature engineering\n",
    "Visualization - Insights and pattern discovery\n",
    "Model Building - Training and evaluation of multiple models\n",
    "Now, let me explain the modularization process:\n",
    "\n",
    "What is Modularization?\n",
    "Modularization is the process of organizing code into separate, reusable modules (Python files) with clear responsibilities. Instead of having everything in notebooks, you create a structured project with functions, classes, and modules that can be tested, maintained, and reused.\n",
    "\n",
    "Why Modularize?\n",
    "Current Problems with Notebook-Only Approach:\n",
    "No code reusability - Copy-paste between notebooks leads to duplication\n",
    "Hard to test - Can't easily write unit tests for notebook cells\n",
    "Version control issues - Notebooks don't play well with Git\n",
    "No reproducibility guarantees - Execution order matters; state can be messy\n",
    "Difficult collaboration - Merge conflicts are nightmares\n",
    "Can't easily deploy - Hard to put notebooks into production\n",
    "Maintainability - Hard to find and fix bugs across multiple notebooks\n",
    "Benefits of Modularization:\n",
    "DRY Principle (Don't Repeat Yourself) - Write once, use everywhere\n",
    "Testability - Unit tests ensure code correctness\n",
    "Maintainability - Easier to update and debug\n",
    "Collaboration - Multiple people can work on different modules\n",
    "Deployment - Easy to package and deploy as APIs or applications\n",
    "Documentation - Clear structure makes code self-documenting\n",
    "Performance - Can optimize specific modules independently\n",
    "\n",
    "How to Modularize the Project\n",
    "Let me create a complete modularized version of the project:\n",
    "Now let me create the actual modularized code files:\n",
    "Perfect! I've created a complete data analysis project for the IBM Telco churn dataset. Here's what I've delivered:\n",
    "Complete Notebook-Based Project (Parts 1-4)\n",
    "Data Loading & EDA - Initial exploration\n",
    "Preprocessing - Cleaning and encoding\n",
    "Visualization - Insights and patterns\n",
    "Model Building - Training and evaluation\n",
    "Complete Modularized Project Structure\n",
    "I've created a professional, production-ready structure with:\n",
    "Core Modules:\n",
    "data/loader.py - Data loading with validation\n",
    "data/preprocessor.py - Cleaning and encoding\n",
    "features/engineer.py - Feature creation\n",
    "models/train.py - Model training\n",
    "models/evaluate.py - Model evaluation\n",
    "visualization/plots.py - Publication-quality plots\n",
    "Supporting Files:\n",
    "main.py - Complete pipeline orchestration\n",
    "config/config.yaml - Centralized configuration\n",
    "requirements.txt - Dependencies\n",
    "tests/test_preprocessor.py - Unit test example\n",
    "README.md - Full documentation\n",
    "Comprehensive Guide:\n",
    "Detailed modularization explanation covering what, why, and how\n",
    "Before/after comparisons\n",
    "Best practices and design patterns\n",
    "Step-by-step transition strategy\n",
    "Key Benefits of the Modularized Approach:\n",
    "‚úÖ Reusable - Write once, use everywhere\n",
    "‚úÖ Testable - Unit tests ensure correctness\n",
    "‚úÖ Maintainable - Easy to update and debug\n",
    "‚úÖ Collaborative - Multiple developers can work together\n",
    "‚úÖ Production-ready - Can be deployed as API or application\n",
    "‚úÖ Professional - Industry-standard structure\n",
    "You can now run the entire pipeline with a single command:\n",
    "python main.py --data_path data/raw/telco.csv --save_models\n",
    "The modularized version transforms messy notebook code into clean, professional, maintainable software! üöÄ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5b2aa",
   "metadata": {},
   "source": [
    "2.1 Review Previous Work\n",
    "Objective: Identify repetitive code patterns from Levels 0-2 notebooks for extraction.\n",
    "2.2 Consolidation Strategy\n",
    "Before (Level 0-2 Approach):\n",
    "# Repeated in every notebook\n",
    "df = pd.read_csv('data/telco.csv')\n",
    "print(f\"Loaded {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "After (Level 3 Function):\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load and provide basic info about dataset.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "Where does it fit?\n",
    "\n",
    "\n",
    "## ü§î **It Depends on the Type of Statistical Analysis**\n",
    "\n",
    "\n",
    "### **‚úÖ Belongs in `01_data_exploration.ipynb`:**\n",
    "- **Descriptive statistics** (mean, median, std, percentiles)\n",
    "- **Distribution analysis** (skewness, kurtosis, normality tests)\n",
    "- **Basic hypothesis tests** (t-tests, chi-square for independence)\n",
    "- **Correlation analysis** between features\n",
    "- **Outlier detection** using statistical methods (IQR, z-scores)\n",
    "\n",
    "\n",
    "### **üéØ Belongs in `02_data_cleaning.ipynb`:**\n",
    "- **Statistical outlier removal** (based on z-scores, IQR)\n",
    "- **Statistical imputation** methods (mean, median, mode)\n",
    "- **Data quality statistics** (missing value patterns, duplicate analysis)\n",
    "\n",
    "\n",
    "### **üìä Belongs in `03_visualization_insights.ipynb`:**\n",
    "- **Statistical relationships** with the target variable\n",
    "- **Feature importance** using statistical tests\n",
    "- **Segmentation analysis** (churn rates by customer segments)\n",
    "- **Statistical significance** of observed patterns\n",
    "\n",
    "\n",
    "### **ü§ñ Belongs in `04_baseline_model.ipynb`:**\n",
    "- **Model performance statistics** (confidence intervals, significance tests)\n",
    "- **Feature selection** using statistical methods (ANOVA, chi-square)\n",
    "- **Statistical validation** of model assumptions\n",
    "\n",
    "\n",
    "## üìã **Recommended Approach for Level 3**\n",
    "\n",
    "\n",
    "**Keep it simple and logical:**\n",
    "\n",
    "\n",
    "```python\n",
    "# 01_data_exploration.ipynb - Focus on understanding your data\n",
    "- df.describe() with interpretation\n",
    "- Distribution analysis of key variables\n",
    "- Basic correlation matrix\n",
    "- Univariate statistical summaries\n",
    "\n",
    "\n",
    "# 03_visualization_insights.ipynb - Focus on business insights\n",
    "- Statistical relationships with churn\n",
    "- Significance tests for key findings\n",
    "- Comparative statistics across segments\n",
    "```\n",
    "\n",
    "\n",
    "## üí° **Example Split:**\n",
    "\n",
    "\n",
    "**In `01_data_exploration.ipynb`:**\n",
    "```python\n",
    "# Basic statistical profiling\n",
    "print(\"Numerical Variables Statistics:\")\n",
    "print(df[['tenure', 'MonthlyCharges', 'TotalCharges']].describe())\n",
    "\n",
    "\n",
    "# Distribution analysis\n",
    "from scipy import stats\n",
    "print(f\"Tenure skewness: {stats.skew(df['tenure'])}\")\n",
    "print(f\"MonthlyCharges normality test: {stats.normaltest(df['MonthlyCharges'])}\")\n",
    "```\n",
    "\n",
    "\n",
    "**In `03_visualization_insights.ipynb`:**\n",
    "```python\n",
    "# Business-focused statistical analysis\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "# Test if contract type is significantly related to churn\n",
    "contingency_table = pd.crosstab(df['Contract'], df['Churn'])\n",
    "chi2, p_value = chi2_contingency(contingency_table)[:2]\n",
    "print(f\"Contract vs Churn - Chi-square test p-value: {p_value}\")\n",
    "```\n",
    "\n",
    "\n",
    "## üéØ **For Level 3 Specifically:**\n",
    "\n",
    "\n",
    "Since you're building your **baseline model**, I'd recommend:\n",
    "\n",
    "\n",
    "1. **Keep descriptive statistics in `01_data_exploration`** - it's foundational data understanding\n",
    "2. **Move inferential statistics to `03_visualization_insights`** - it supports your business insights\n",
    "3. **Save advanced statistical modeling for Level 4+** when you focus on evaluation and thresholding\n",
    "\n",
    "\n",
    "This keeps each notebook focused on its primary purpose and maintains the logical flow of your analysis progression. \n",
    "Phase 3: Creating a Baseline Model using Notebooks then convert to Package Development\n",
    "## üìì Notebook Structure\n",
    "## Building the Complete Package {#complete-package}\n",
    "\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "\n",
    "```\n",
    "Level_3/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ telco_analysis/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         # Makes this a package\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ data_utils.py       # Data functions\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ preprocessing.py    # Cleaning functions\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ modeling.py         # Model functions\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 03_baseline_model.ipynb\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "   ‚îî‚îÄ‚îÄ raw/\n",
    "       ‚îî‚îÄ‚îÄ telco_data.csv\n",
    "```\n",
    "Phase 3.2 CreateBaseline Model using Notebooks.\n",
    "Comprehensive Evaluation Framework\n",
    "- Confusion matrices with business context\n",
    "- ROC/PR curves for threshold optimization\n",
    "- Feature importance analysis for interpretability\n",
    "- Calibration plots for probability reliability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "\n",
    "| Metric | Score | Business Meaning |\n",
    "|--------|-------|------------------|\n",
    "| **ROC-AUC** | 0.842 | Excellent customer ranking ability |\n",
    "| **Precision** | 0.653 | 65% of flagged customers actually churn |\n",
    "| **Recall** | 0.547 | Catches 55% of all churners |\n",
    "| **F1-Score** | 0.595 | Balanced precision-recall trade-off |\n",
    "\n",
    "\n",
    "### Key Insights Discovered\n",
    "1. **Contract type** is the strongest predictor (month-to-month = 2.3x churn risk)\n",
    "2. **Payment method** significantly impacts retention (electronic check = 45% churn rate)\n",
    "3. **Customer tenure** inversely correlates with churn risk (-0.65 coefficient)\n",
    "\n",
    "\n",
    "\n",
    "Q:\n",
    "Should my 02_statistical_analysis.ipynb notebook be written more tightly?\n",
    "A:\n",
    "Excellent question! Your notebook is definitely on the right track for Level 3, but yes, it could be more tightly structured. You're at the perfect stage to start thinking about professional notebook organization while maintaining the learning value.\n",
    "üéØ Current Strengths vs. Areas for Tightening\n",
    "What's Working Well:\n",
    "Logical flow from data loading ‚Üí statistical testing ‚Üí business insights\n",
    "Good use of modular imports\n",
    "Clear statistical methodology\n",
    "Business-focused conclusions\n",
    "Areas to Tighten:\n",
    "Level 3 Notebook Structure Guidelines\n",
    "üéØ Professional Notebook Organization\n",
    "Current Structure Issues to Address:\n",
    "Debugging Code Mixed with Analysis\n",
    "# This should be in a separate development/debugging notebook\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"stats\", stats_file)\n",
    "stats_module = importlib.util.module_from_spec(spec)\n",
    "# List all functions in the module\n",
    "functions = [item for item in dir(stats_module) if not item.startswith('_')]\n",
    "\n",
    "Configuration Loading Scattered\n",
    "# Should be consolidated into one clean section\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "Data Exploration vs. Analysis Mixed\n",
    "# This inspection belongs in exploratory phase, not statistical analysis\n",
    "tenure_0_customers = df_clean[df_clean['tenure'] == 0]\n",
    "from tabulate import tabulate\n",
    "print(tabulate(tenure_0_customers, headers='keys', tablefmt='psql'))\n",
    "\n",
    "\n",
    "üìã Recommended Notebook Structure\n",
    "Cell 1: Executive Summary & Objectives\n",
    "# Statistical Analysis: Telco Customer Churn\n",
    "## Objective\n",
    "Identify statistically significant drivers of customer churn and quantify business impact.\n",
    "\n",
    "## Key Questions\n",
    "1. Which customer characteristics are significantly associated with churn?\n",
    "2. What is the magnitude of these effects?\n",
    "3. Which segments represent the highest business risk?\n",
    "\n",
    "## Methodology\n",
    "- Statistical testing with effect size reporting\n",
    "- Risk segmentation analysis\n",
    "- Business impact quantification\n",
    "\n",
    "Cell 2: Environment Setup (Clean)\n",
    "# Core imports and configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Add project path\n",
    "HERE = Path().resolve()\n",
    "sys.path.insert(0, str(HERE.parent / \"src\"))\n",
    "\n",
    "# Data science stack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Project modules\n",
    "from utils.loader import DataLoader\n",
    "from utils.preprocessor import clean_telco_data\n",
    "from utils.stats import (\n",
    "    test_numerical_vs_churn,\n",
    "    test_categorical_vs_churn,\n",
    "    identify_risk_segments\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "\n",
    "Cell 3: Data Loading & Validation\n",
    "# Load and prepare data using modular functions\n",
    "loader = DataLoader(config)\n",
    "df_raw, load_report = loader.load_data(config['data']['raw_path'])\n",
    "df_clean = clean_telco_data(df_raw)\n",
    "\n",
    "# Data quality summary\n",
    "print(f\"Dataset: {df_clean.shape[0]:,} customers, {df_clean.shape[1]} features\")\n",
    "print(f\"Churn rate: {(df_clean['Churn'] == 'Yes').mean()*100:.1f}%\")\n",
    "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Save processed data\n",
    "processed_path = Path(config['data']['processed_path'])\n",
    "processed_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "print(f\"‚úÖ Clean data saved to {processed_path}\")\n",
    "\n",
    "Cell 4: Statistical Testing Framework\n",
    "# Define features to test\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "categorical_features = ['Contract', 'PaymentMethod', 'InternetService']\n",
    "\n",
    "# Initialize results storage\n",
    "statistical_results = {\n",
    "    'numerical': {},\n",
    "    'categorical': {}\n",
    "}\n",
    "\n",
    "print(\"üî¨ Running Statistical Tests\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "Cell 5: Numerical Feature Analysis\n",
    "# Test numerical features\n",
    "for feature in numerical_features:\n",
    "    result = test_numerical_vs_churn(df_clean, feature, 'Churn')\n",
    "    statistical_results['numerical'][feature] = result\n",
    "    \n",
    "    print(f\"\\n{feature.upper()}:\")\n",
    "    print(f\"  Test: {result['test_used']}\")\n",
    "    print(f\"  P-value: {result['p_value']:.4e}\")\n",
    "    print(f\"  Effect size: {result['cohens_d']:.3f} ({result['effect_size']})\")\n",
    "    print(f\"  Significant: {'‚úÖ' if result['significant'] else '‚ùå'}\")\n",
    "\n",
    "Cell 6: Categorical Feature Analysis\n",
    "# Test categorical features\n",
    "for feature in categorical_features:\n",
    "    result = test_categorical_vs_churn(df_clean, feature, 'Churn')\n",
    "    statistical_results['categorical'][feature] = result\n",
    "    \n",
    "    print(f\"\\n{feature.upper()}:\")\n",
    "    print(f\"  Chi-square: {result['chi2_statistic']:.2f}\")\n",
    "    print(f\"  P-value: {result['p_value']:.4e}\")\n",
    "    print(f\"  Cram√©r's V: {result['cramers_v']:.3f}\")\n",
    "    print(f\"  Highest risk: {result['highest_risk_category']}\")\n",
    "\n",
    "Cell 7: Key Findings Visualization\n",
    "# Create focused visualizations for significant findings\n",
    "significant_features = []\n",
    "\n",
    "# Identify significant results\n",
    "for category, results in statistical_results.items():\n",
    "    for feature, result in results.items():\n",
    "        if result['significant']:\n",
    "            significant_features.append((feature, result))\n",
    "\n",
    "print(f\"üìä Visualizing {len(significant_features)} significant findings\")\n",
    "\n",
    "# Create subplot grid\n",
    "n_features = len(significant_features)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (feature, result) in enumerate(significant_features[:4]):\n",
    "    # Your visualization code here\n",
    "    pass\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "Cell 8: Risk Segmentation\n",
    "# Business-focused risk analysis\n",
    "risk_segments = identify_risk_segments(df_clean)\n",
    "\n",
    "print(\"üéØ HIGH-RISK CUSTOMER SEGMENTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sort by risk level and revenue impact\n",
    "high_risk_segments = {k: v for k, v in risk_segments.items() \n",
    "                     if v['risk_level'] == 'HIGH'}\n",
    "\n",
    "for segment_name, data in high_risk_segments.items():\n",
    "    print(f\"\\n{segment_name.upper()}:\")\n",
    "    print(f\"  Size: {data['size']:,} customers ({data['percentage_of_base']:.1f}%)\")\n",
    "    print(f\"  Churn Rate: {data['churn_rate']:.1f}%\")\n",
    "    print(f\"  Revenue at Risk: ${data.get('monthly_revenue_at_risk', 0):,.0f}/month\")\n",
    "\n",
    "Cell 9: Executive Summary & Recommendations\n",
    "# Business intelligence summary\n",
    "print(\"üìã EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate total impact\n",
    "total_revenue_at_risk = sum(\n",
    "    segment.get('monthly_revenue_at_risk', 0) \n",
    "    for segment in risk_segments.values()\n",
    ")\n",
    "\n",
    "print(f\"\\nüí∞ BUSINESS IMPACT:\")\n",
    "print(f\"   Total Monthly Revenue at Risk: ${total_revenue_at_risk:,.0f}\")\n",
    "print(f\"   Annualized Impact: ${total_revenue_at_risk * 12:,.0f}\")\n",
    "\n",
    "print(f\"\\nüéØ TOP 3 RECOMMENDATIONS:\")\n",
    "\n",
    "# Generate recommendations from significant findings\n",
    "recommendations = []\n",
    "for feature, result in significant_features:\n",
    "    if feature == 'Contract' and result['significant']:\n",
    "        recommendations.append({\n",
    "            'priority': 1,\n",
    "            'action': 'Contract Incentive Program',\n",
    "            'rationale': f\"Month-to-month customers have {result['churn_rates_by_category']['Month-to-month']*100:.1f}% churn rate\",\n",
    "            'expected_impact': '20% reduction in contract-related churn'\n",
    "        })\n",
    "\n",
    "# Display top recommendations\n",
    "for i, rec in enumerate(recommendations[:3], 1):\n",
    "    print(f\"\\n   {i}. {rec['action']}\")\n",
    "    print(f\"      Rationale: {rec['rationale']}\")\n",
    "    print(f\"      Expected Impact: {rec['expected_impact']}\")\n",
    "\n",
    "Cell 10: Technical Appendix (Optional)\n",
    "# Detailed statistical results for technical stakeholders\n",
    "print(\"üìä DETAILED STATISTICAL RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Export detailed results\n",
    "results_export = {\n",
    "    'summary': {\n",
    "        'total_features_tested': len(numerical_features) + len(categorical_features),\n",
    "        'significant_findings': len(significant_features),\n",
    "        'alpha_level': 0.05\n",
    "    },\n",
    "    'detailed_results': statistical_results\n",
    "}\n",
    "\n",
    "# Save results for reporting\n",
    "import json\n",
    "with open('../results/statistical_analysis_results.json', 'w') as f:\n",
    "    json.dump(results_export, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ Results exported for technical documentation\")\n",
    "\n",
    "\n",
    "üéØ Key Improvements for Level 3\n",
    "1. Separation of Concerns\n",
    "# ‚ùå Mixed exploration and analysis\n",
    "tenure_0_customers = df_clean[df_clean['tenure'] == 0]\n",
    "print(tabulate(tenure_0_customers, headers='keys', tablefmt='psql'))\n",
    "\n",
    "# ‚úÖ Focused analysis only\n",
    "result = test_numerical_vs_churn(df_clean, 'tenure', 'Churn')\n",
    "print(f\"Tenure analysis: p={result['p_value']:.4e}, d={result['cohens_d']:.3f}\")\n",
    "\n",
    "2. Professional Output Formatting\n",
    "# ‚ùå Basic print statements\n",
    "print(\"Tenure Analysis Results:\")\n",
    "print(f\"  Test used: {tenure_results['test_used']}\")\n",
    "\n",
    "# ‚úÖ Structured, scannable output\n",
    "print(\"üî¨ STATISTICAL TEST RESULTS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Feature: {feature}\")\n",
    "print(f\"Test: {result['test_used']}\")\n",
    "print(f\"Significance: {'‚úÖ Significant' if result['significant'] else '‚ùå Not significant'}\")\n",
    "\n",
    "3. Result-Oriented Structure\n",
    "# ‚ùå Process-focused\n",
    "# \"First let's load the data, then clean it, then test it...\"\n",
    "\n",
    "# ‚úÖ Results-focused\n",
    "# \"Key Finding: Contract type significantly predicts churn (p<0.001)\"\n",
    "\n",
    "4. Notebook Metadata\n",
    "<!-- Add at the top -->\n",
    "---\n",
    "title: \"Statistical Analysis: Customer Churn Drivers\"\n",
    "author: \"Your Name\"\n",
    "date: \"2024-XX-XX\"\n",
    "purpose: \"Identify statistically significant predictors of customer churn\"\n",
    "audience: \"Business stakeholders and data science team\"\n",
    "runtime: \"~5 minutes\"\n",
    "---\n",
    "\n",
    "üöÄ Transition Strategy\n",
    "Create 02_statistical_analysis_CLEAN.ipynb with the tighter structure\n",
    "Keep your current notebook as 02_statistical_analysis_EXPLORATORY.ipynb for reference\n",
    "Use the exploratory version for debugging and investigation\n",
    "Use the clean version for presentations and stakeholder communication\n",
    "This approach lets you maintain both learning documentation and professional output - perfect for Level 3!\n",
    "üéØ Direct Answer: YES, Definitely Tighten It\n",
    "Your notebook should be significantly more focused for Level 3. Here's why:\n",
    "Current Issues:\n",
    "Debugging code mixed with analysis (the module inspection code)\n",
    "Exploratory data inspection mixed with statistical testing\n",
    "Configuration scattered across multiple cells\n",
    "No clear narrative flow for stakeholders\n",
    "Level 3 Standard:\n",
    "Results-focused rather than process-focused\n",
    "Professional presentation quality\n",
    "Clear executive summary\n",
    "Systematic methodology throughout\n",
    "üîÑ Quick Tightening Strategy\n",
    "Move to Separate Notebooks:\n",
    "02_statistical_analysis_CLEAN.ipynb ‚Üê This is your deliverable\n",
    "02_exploratory_debugging.ipynb ‚Üê Keep your exploration here\n",
    "Clean Version Should:\n",
    "Start with key findings (executive summary)\n",
    "Show systematic testing of all relevant features\n",
    "Focus on business implications of every statistical result\n",
    "End with clear recommendations and next steps\n",
    "Development Process:\n",
    "Exploratory Notebook ‚Üí Debug/Investigate ‚Üí Extract Insights ‚Üí Clean Presentation\n",
    "     (Keep private)                                        (Show stakeholders)\n",
    "üí° Why This Matters for Level 3\n",
    "Level 2: \"Here's what I found while exploring...\" \n",
    "Level 3: \"Here are the statistically significant business insights and recommended actions...\"\n",
    "Your analysis quality is already Level 3+, but your presentation needs to catch up. The statistical rigor is excellent - now package it professionally.\n",
    "Bottom Line: Create a tight, business-focused version for stakeholders while keeping your exploratory version for learning and development. This dual-notebook approach is exactly what professional data scientists do!\n",
    "\n",
    "### **`03_baseline_model.ipynb`**\n",
    "```python\n",
    "# Cell 1: Setup\n",
    "\"\"\"\n",
    "Level 3: Baseline Model with Modular Code\n",
    "=========================================\n",
    "Moving from monolithic notebooks to reusable functions!\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../src')  # Add src to path\n",
    "\n",
    "\n",
    "# Import our custom package\n",
    "from telco_analysis import (\n",
    "   load_data, clean_data, create_features,\n",
    "   split_data, train_baseline, evaluate_model, save_pipeline\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Cell 2: Load and Prepare Data\n",
    "\"\"\"\n",
    "Using our extracted functions instead of inline code!\n",
    "Before (Level 2): 50+ lines of data loading code\n",
    "Now (Level 3): 3 function calls\n",
    "\"\"\"\n",
    "# Load\n",
    "df = load_data()\n",
    "\n",
    "\n",
    "# Clean\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "\n",
    "# Engineer features\n",
    "df_features = create_features(df_clean)\n",
    "\n",
    "\n",
    "# Cell 3: Prepare for Modeling\n",
    "\"\"\"\n",
    "Convert data to modeling format\n",
    "\"\"\"\n",
    "from telco_analysis.preprocessing import prepare_for_modeling\n",
    "\n",
    "\n",
    "df_model, numeric_cols, categorical_cols = prepare_for_modeling(df_features)\n",
    "\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols[:5]}...\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols[:5]}...\")\n",
    "\n",
    "\n",
    "# Cell 4: Train-Test Split\n",
    "\"\"\"\n",
    "Split with stratification to maintain class balance\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "   df_model,\n",
    "   target='Churn',\n",
    "   test_size=0.2,\n",
    "   random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Cell 5: Train Baseline Model\n",
    "\"\"\"\n",
    "Train our first baseline - Logistic Regression\n",
    "Why Logistic Regression?\n",
    "- Simple and interpretable\n",
    "- Fast to train\n",
    "- Good baseline for binary classification\n",
    "- Provides probability estimates\n",
    "\"\"\"\n",
    "pipeline = train_baseline(X_train, y_train, numeric_cols, categorical_cols)\n",
    "\n",
    "\n",
    "# Cell 6: Evaluate Performance\n",
    "\"\"\"\n",
    "Get baseline metrics to beat in future iterations\n",
    "\"\"\"\n",
    "metrics = evaluate_model(pipeline, X_test, y_test)\n",
    "\n",
    "\n",
    "# Detailed classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred,\n",
    "                         target_names=['No Churn', 'Churn']))\n",
    "\n",
    "\n",
    "# Cell 7: Save Model Artifacts\n",
    "\"\"\"\n",
    "Persist our trained model for future use\n",
    "\"\"\"\n",
    "save_pipeline(pipeline, metrics)\n",
    "\n",
    "\n",
    "# Cell 8: Feature Importance Analysis\n",
    "\"\"\"\n",
    "Understand what drives predictions\n",
    "\"\"\"\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (\n",
    "   numeric_cols +\n",
    "   list(pipeline.named_steps['preprocessor']\n",
    "        .named_transformers_['cat']\n",
    "        .get_feature_names_out(categorical_cols))\n",
    ")\n",
    "\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "   'feature': feature_names,\n",
    "   'coefficient': coefficients,\n",
    "   'abs_coefficient': abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Top 15 Feature Importances (Logistic Regression)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Cell 9: Test Model Loading\n",
    "\"\"\"\n",
    "Verify we can reload and use the saved model\n",
    "\"\"\"\n",
    "from telco_analysis.modeling import load_pipeline\n",
    "\n",
    "\n",
    "# Load saved model\n",
    "loaded_pipeline = load_pipeline()\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = loaded_pipeline.predict(X_test[:5])\n",
    "print(\"Sample predictions:\", test_predictions)\n",
    "print(\"Actual values:\", y_test[:5].values)\n",
    "```\n",
    "3. Technical Implementation\n",
    "3.1 Key Code Transformations\n",
    "Before: Repetitive Data Loading (Levels 0-2)\n",
    "# Repeated in every notebook\n",
    "df = pd.read_csv('data/telco_customer_churn.csv')\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Memory: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "# ... 15 more lines of validation\n",
    "\n",
    "After: Modular Function (Level 3)\n",
    "from telco_analysis import load_data\n",
    "df = load_data()  # All validation included\n",
    "\n",
    "\n",
    "3.2 Pipeline Development\n",
    "Implemented complete sklearn pipeline combining preprocessing and modeling:\n",
    "Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "Benefits achieved:\n",
    "Single object for fit/predict operations\n",
    "Prevents data leakage\n",
    "Easily deployable\n",
    "Reproducible preprocessing\n",
    "3.3 Feature Engineering\n",
    "Created 6 business-driven features:\n",
    "AvgChargesPerMonth: Revenue efficiency metric\n",
    "IsNewCustomer: Risk segmentation (tenure ‚â§ 12 months)\n",
    "IsElectronicCheck: Payment risk indicator\n",
    "IsMonthToMonth: Contract risk indicator\n",
    "HasInternet: Service adoption indicator\n",
    "HasPhone: Service adoption indicator\n",
    "Rule of thumb\n",
    "Extract a function the second time you paste it.\n",
    "If a notebook cell has I/O, state, or parameters, move it into src/ and call it from the notebook.\n",
    "Start modularizing incrementally once you have a working baseline (L3), and ‚Äúharden‚Äù it at L9.\n",
    "Common Repetitive Patterns Identified:\n",
    "Data Loading: pd.read_csv() + shape reporting\n",
    "Data Cleaning: TotalCharges dtype conversion + missing value handling\n",
    "Validation: Schema checks + data quality assessments\n",
    "Feature Engineering: Tenure grouping, service counting, value segmentation\n",
    "Visualization: Distribution plots, correlation analysis, churn analysis\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ed641",
   "metadata": {},
   "source": [
    "\n",
    "üî¨ Phase 1: Advanced EDA with Statistical Foundation\n",
    "1.1 Data Loading with Statistical Validation\n",
    "Building on Level 2: Use your optimized loading functions but add statistical validation layers.\n",
    "New Level 3 Concepts to Integrate:\n",
    "Statistical Data Profiling:\n",
    "python\n",
    "# Your Level 2 function + statistical enhancements\n",
    "def advanced_data_profiling(df):\n",
    "    \"\"\"Enhanced profiling with statistical measures\"\"\"\n",
    "    # Your existing profiling logic\n",
    "    # + Statistical distribution testing\n",
    "    # + Outlier detection using IQR and z-score methods\n",
    "    # + Normality testing for continuous variables\n",
    "    # + Independence testing preparation\n",
    "Why Statistical Profiling Now:\n",
    "Foundation for modeling: Understanding distributions informs algorithm choice\n",
    "Assumption validation: Many models assume normal distributions or independence\n",
    "Quality assurance: Statistical outlier detection is more robust than visual inspection\n",
    "1.2 Hypothesis-Driven Exploration\n",
    "Core Level 3 Advancement: Transition from exploratory to confirmatory analysis\n",
    "Hypothesis Framework to Implement:\n",
    "Business Hypothesis Formation: Based on Level 2 insights\n",
    "Statistical Hypothesis Translation: Convert business questions to testable hypotheses\n",
    "Test Selection & Execution: Choose appropriate statistical tests\n",
    "Results Interpretation: Business-context statistical conclusions\n",
    "Example Progression from Level 2:\n",
    "Level 2 Insight: \"Month-to-month contracts show 42.7% churn rate\"\n",
    "Level 3 Question: \"Is this difference statistically significant vs longer contracts?\"\n",
    "Statistical Test: Chi-square test of independence\n",
    "Business Validation: Confidence intervals for business planning\n",
    "1.3 Advanced Visualization with Statistical Elements\n",
    "Enhanced Visualization Strategy:\n",
    "Statistical Plots to Add:\n",
    "Confidence intervals on your existing distribution plots\n",
    "Correlation matrices with significance testing\n",
    "Box plots with statistical annotations (quartiles, outliers)\n",
    "Residual plots for relationship validation\n",
    "Why These Visualizations:\n",
    "Confidence intervals: Quantify uncertainty for business decisions\n",
    "Significance testing: Distinguish real patterns from random variation\n",
    "Residual analysis: Validate assumptions for future modeling\n",
    "üß™ Phase 2: Statistical Testing Integration\n",
    "2.1 Test Selection Framework\n",
    "Level 3 Skill Development: Learn to choose appropriate tests based on data characteristics.\n",
    "Test Categories to Master:\n",
    "Categorical vs Categorical:\n",
    "Chi-square test of independence\n",
    "Fisher's exact test (small samples)\n",
    "Application: Contract type vs Churn status\n",
    "Continuous vs Categorical:\n",
    "Independent t-test (2 groups)\n",
    "ANOVA (multiple groups)\n",
    "Mann-Whitney U (non-parametric)\n",
    "Application: Monthly charges across churn groups\n",
    "Continuous vs Continuous:\n",
    "Pearson correlation (parametric)\n",
    "Spearman correlation (non-parametric)\n",
    "Application: Tenure vs Total charges relationship\n",
    "2.2 Function Development for Statistical Testing\n",
    "Building on Level 2 Function Skills:\n",
    "Create a statistical testing module that mirrors your Level 2 systematic approach:\n",
    "python\n",
    "def statistical_relationship_analyzer(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Automatically select and execute appropriate statistical test\n",
    "    based on variable types, with business interpretation\n",
    "    \"\"\"\n",
    "    # Variable type detection\n",
    "    # Test selection logic\n",
    "    # Execution with assumption checking\n",
    "    # Business-context interpretation\n",
    "    # Visualization integration\n",
    "Why This Approach:\n",
    "Systematic consistency: Maintains your Level 2 modular philosophy\n",
    "Reusability: Apply across all variable combinations\n",
    "Decision documentation: Clear rationale for test choices\n",
    "Business integration: Statistical significance ‚Üí business significance\n",
    "2.3 Multiple Testing Correction\n",
    "Advanced Statistical Concept: When testing multiple hypotheses, adjust for false discovery rate.\n",
    "Implementation Strategy:\n",
    "Bonferroni correction for conservative approach\n",
    "False Discovery Rate (FDR) for exploratory analysis\n",
    "Document correction choice rationale\n",
    "Business Context: Prevents over-interpretation of chance findings in comprehensive EDA.\n",
    "\n",
    "ü§ñ Phase 3: Basic Modeling Introduction\n",
    "3.1 Model Selection Framework\n",
    "Level 3 Modeling Philosophy: Start simple, build systematically.\n",
    "Algorithm Progression for Churn Prediction:\n",
    "Baseline Models (Start Here):\n",
    "Logistic Regression: Interpretable, handles mixed data types\n",
    "Decision Tree: Visual decision rules, handles non-linearity\n",
    "Naive Bayes: Handles categorical features well\n",
    "Why These Models First:\n",
    "Interpretability: Business stakeholders can understand results\n",
    "Diagnostic capability: Easy to identify issues and patterns\n",
    "Foundation building: Core concepts for advanced models later\n",
    "3.2 Feature Engineering for Modeling\n",
    "Advanced Feature Engineering Building on Level 2:\n",
    "Your Level 2 Features (Customer Value, Lifecycle Stage, etc.) PLUS:\n",
    "Model-Specific Enhancements:\n",
    "Encoding strategies: One-hot vs ordinal vs target encoding\n",
    "Scaling approaches: StandardScaler vs MinMaxScaler vs RobustScaler\n",
    "Interaction features: Contract type √ó Payment method combinations\n",
    "Polynomial features: Non-linear relationship capture\n",
    "Decision Framework to Develop:\n",
    "python\n",
    "def feature_engineering_pipeline(df, target_column, encoding_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Systematic feature preparation with business logic validation\n",
    "    \"\"\"\n",
    "    # Your Level 2 engineered features\n",
    "    # + Model-appropriate transformations\n",
    "    # + Validation against business logic\n",
    "    # + Documentation of choices\n",
    "3.3 Model Evaluation Framework\n",
    "Level 3 Evaluation Strategy: Beyond accuracy to business metrics.\n",
    "Evaluation Metrics Hierarchy:\n",
    "Business Metrics (Primary):\n",
    "Revenue impact per prediction type\n",
    "Customer lifetime value considerations\n",
    "Implementation cost analysis\n",
    "Statistical Metrics (Secondary):\n",
    "Precision/Recall for churn class\n",
    "ROC-AUC for ranking capability\n",
    "Confusion matrix interpretation\n",
    "Model Diagnostic Metrics:\n",
    "Feature importance analysis\n",
    "Prediction confidence assessment\n",
    "Bias detection across customer segments\n",
    "\n",
    "üìä Phase 4: Pipeline Development Introduction\n",
    "4.1 Basic Pipeline Architecture\n",
    "Transition Toward Level 4-5 Skills: Introduce systematic workflow management.\n",
    "Pipeline Components to Implement:\n",
    "python\n",
    "# Basic pipeline structure (still in notebook)\n",
    "def modeling_pipeline(raw_data, target_column):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline: raw data ‚Üí predictions\n",
    "    \"\"\"\n",
    "    # 1. Data loading & validation (your Level 2 functions)\n",
    "    # 2. Statistical profiling & testing (Level 3 additions)\n",
    "    # 3. Feature engineering (enhanced from Level 2)\n",
    "    # 4. Model training & evaluation (new Level 3)\n",
    "    # 5. Results interpretation & documentation\n",
    "Why Pipeline Thinking Now:\n",
    "Reproducibility: Same analysis, consistent results\n",
    "Scalability preparation: Foundation for Level 4-5 automation\n",
    "Quality assurance: Systematic validation at each step\n",
    "Documentation: Clear audit trail of decisions\n",
    "4.2 Cross-Validation Implementation\n",
    "Statistical Rigor Enhancement: Validate model stability across data subsets.\n",
    "Cross-Validation Strategy:\n",
    "Stratified K-fold for balanced evaluation\n",
    "Time-aware splitting if temporal patterns exist\n",
    "Statistical significance testing of model performance differences\n",
    "Business Integration: Confidence intervals on expected performance in production.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
