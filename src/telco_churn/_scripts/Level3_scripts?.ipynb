{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd92e61",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "\n",
    "**single reusable `eda_explorer.py` module** that you can import into any notebook?\n",
    "combine all these **EDA relationship explorers** (categorical, numeric, interactions, churn, ANOVA, Tukey) into a\n",
    "\n",
    "</summary> \n",
    "\n",
    "Perfect üî• ‚Äî this will finish your **EDA relationship suite** with a focused comparison of **numeric features vs churn**.\n",
    "\n",
    "Awesome ‚Äî here‚Äôs a single, reusable module you can drop into `src/eda_explorer.py`.\n",
    "It wraps everything we built: numeric summary (with skew/kurtosis), categorical summary (with entropy), categorical‚Üîchurn explorer (with Cram√©r‚Äôs V & chi-square), categorical√ócategorical interactions, continuous√ócategorical ANOVA + Tukey HSD, and numeric vs churn (with Welch t-test & Cohen‚Äôs d) ‚Äî plus tidy plotting helpers.\n",
    "\n",
    "> Save as: **`src/eda_explorer.py`**\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "EDA Explorer ‚Äî Telco-ready utilities for fast, professional EDA.\n",
    "\n",
    "Functions cover:\n",
    "- Numeric descriptive summary (with skew & kurtosis)\n",
    "- Categorical descriptive summary (counts, proportions, entropy)\n",
    "- Categorical vs Churn explorer (Cram√©r‚Äôs V, chi¬≤, churn lift)\n",
    "- Categorical √ó Categorical interactions vs Churn\n",
    "- Continuous √ó Categorical ANOVA and Tukey HSD\n",
    "- Numeric vs Churn (Welch t-test, Cohen's d)\n",
    "- Matplotlib/Seaborn plotting helpers\n",
    "\n",
    "Author: you\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chi2_contingency, ttest_ind, f_oneway\n",
    "try:\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    _HAS_SM = True\n",
    "except Exception:\n",
    "    _HAS_SM = False\n",
    "\n",
    "# ---------------------------\n",
    "# Core Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def _numeric_cols(df: pd.DataFrame, exclude: List[str] = None) -> List[str]:\n",
    "    exclude = set((exclude or []))\n",
    "    return [c for c in df.select_dtypes(include=\"number\").columns if c not in exclude]\n",
    "\n",
    "def _cat_cols(df: pd.DataFrame, exclude: List[str] = None) -> List[str]:\n",
    "    exclude = set((exclude or []))\n",
    "    return [c for c in df.select_dtypes(include=[\"object\", \"category\"]).columns if c not in exclude]\n",
    "\n",
    "def _cramers_v(ct: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Return (Cram√©r's V, p-value) for a contingency table.\"\"\"\n",
    "    chi2, p, dof, exp = chi2_contingency(ct)\n",
    "    n = ct.values.sum()\n",
    "    k = min(ct.shape) - 1\n",
    "    if n == 0 or k <= 0:\n",
    "        return np.nan, p\n",
    "    v = float(np.sqrt((chi2 / n) / k))\n",
    "    return v, float(p)\n",
    "\n",
    "def _cohens_d(x: pd.Series, y: pd.Series) -> float:\n",
    "    nx, ny = len(x), len(y)\n",
    "    if nx < 2 or ny < 2:\n",
    "        return np.nan\n",
    "    sx, sy = x.std(ddof=1), y.std(ddof=1)\n",
    "    pooled = np.sqrt(((nx - 1) * sx**2 + (ny - 1) * sy**2) / (nx + ny - 2))\n",
    "    if pooled == 0:\n",
    "        return 0.0\n",
    "    return float((x.mean() - y.mean()) / pooled)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Numeric descriptive summary\n",
    "# ---------------------------\n",
    "\n",
    "def numeric_summary(\n",
    "    df: pd.DataFrame,\n",
    "    exclude_cols: List[str] = None,\n",
    "    round_: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return describe().T with skew and kurtosis added.\n",
    "    \"\"\"\n",
    "    num_cols = _numeric_cols(df, exclude=exclude_cols)\n",
    "    if not num_cols:\n",
    "        return pd.DataFrame()\n",
    "    desc = df[num_cols].describe().T\n",
    "    desc[\"skew\"] = df[num_cols].skew(numeric_only=True)\n",
    "    desc[\"kurtosis\"] = df[num_cols].kurtosis(numeric_only=True)\n",
    "    return desc.round(round_)\n",
    "\n",
    "def plot_numeric_summary_style(desc: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Pretty style for numeric summary (optional).\n",
    "    \"\"\"\n",
    "    if desc.empty:\n",
    "        return desc\n",
    "    return (desc.style\n",
    "        .background_gradient(cmap=\"Blues\", subset=[\"mean\", \"50%\"])\n",
    "        .background_gradient(cmap=\"Purples\", subset=[\"std\"])\n",
    "        .format(precision=2)\n",
    "        .set_caption(\"üìä Descriptive Statistics for Numeric Features\")\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Categorical descriptive summary\n",
    "# ---------------------------\n",
    "\n",
    "def categorical_summary(\n",
    "    df: pd.DataFrame,\n",
    "    exclude_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return table with unique count, top category, top %, and Shannon entropy.\n",
    "    \"\"\"\n",
    "    cats = _cat_cols(df, exclude=exclude_cols)\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for col in cats:\n",
    "        vc = df[col].value_counts(dropna=False)\n",
    "        if vc.empty:\n",
    "            continue\n",
    "        top = vc.index[0]\n",
    "        top_pct = float(vc.iloc[0]) / n if n > 0 else np.nan\n",
    "        n_unique = int(df[col].nunique(dropna=True))\n",
    "        p = (vc / vc.sum()).astype(float)\n",
    "        entropy = float(-(p * np.log2(p)).sum())\n",
    "        rows.append({\n",
    "            \"feature\": col,\n",
    "            \"unique\": n_unique,\n",
    "            \"top_category\": str(top),\n",
    "            \"top_%\": round(top_pct * 100, 1),\n",
    "            \"entropy(bits)\": round(entropy, 2),\n",
    "        })\n",
    "    out = pd.DataFrame(rows).sort_values(\"entropy(bits)\", ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def plot_categorical_summary_style(cat_df: pd.DataFrame):\n",
    "    if cat_df.empty:\n",
    "        return cat_df\n",
    "    return (cat_df.style\n",
    "        .bar(subset=[\"top_%\"], color=\"#93c5fd\", vmin=0, vmax=100)\n",
    "        .background_gradient(subset=[\"entropy(bits)\"], cmap=\"Purples\")\n",
    "        .set_caption(\"üìã Categorical Descriptive Statistics\")\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Categorical vs Churn\n",
    "# ---------------------------\n",
    "\n",
    "def categorical_vs_churn(\n",
    "    df: pd.DataFrame,\n",
    "    churn_str: str = \"Churn\",      # \"Yes\"/\"No\"\n",
    "    churn_bin: str = \"Churn_flag\", # 0/1\n",
    "    topk_cats: int = 12,\n",
    "    exclude: List[str] = None\n",
    ") -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each categorical feature, compute churn rate per category,\n",
    "    churn lift vs global, and Cram√©r's V + chi2 p-value.\n",
    "    Returns:\n",
    "      per_feature_tables: dict[feature] -> table\n",
    "      summary_rank: ranking of features by max_abs_delta, cramers_v\n",
    "    \"\"\"\n",
    "    cats = _cat_cols(df, exclude=exclude or [churn_str.lower(), \"customerid\"])\n",
    "    per_feature = {}\n",
    "    rows = []\n",
    "    global_rate = float(df[churn_bin].mean())\n",
    "\n",
    "    for col in cats:\n",
    "        vc = df[col].value_counts(dropna=False)\n",
    "        cats_eval = vc.index[:topk_cats]\n",
    "        sub = df[df[col].isin(cats_eval)].copy()\n",
    "\n",
    "        ct = pd.crosstab(sub[col], sub[churn_str])\n",
    "        v, p = _cramers_v(ct) if ct.shape[1] == 2 and ct.shape[0] >= 2 else (np.nan, np.nan)\n",
    "\n",
    "        grp = sub.groupby(col)[churn_bin].agg([\"mean\", \"count\"]).rename(columns={\"mean\": \"churn_rate\"})\n",
    "        grp[\"category\"] = grp.index.astype(str)\n",
    "        grp[\"feature\"] = col\n",
    "        grp[\"global_rate\"] = global_rate\n",
    "        grp[\"delta_vs_global\"] = grp[\"churn_rate\"] - global_rate\n",
    "        grp[\"abs_delta\"] = grp[\"delta_vs_global\"].abs()\n",
    "        grp[\"cramers_v\"] = v\n",
    "        grp[\"chi2_p\"] = p\n",
    "        per_feature[col] = grp.sort_values(\"abs_delta\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        rows.append({\n",
    "            \"feature\": col,\n",
    "            \"n_categories_total\": int(vc.shape[0]),\n",
    "            \"n_categories_evaluated\": int(len(cats_eval)),\n",
    "            \"global_churn_rate\": global_rate,\n",
    "            \"max_abs_delta\": float(grp[\"abs_delta\"].max()),\n",
    "            \"cramers_v\": v,\n",
    "            \"chi2_p\": p,\n",
    "        })\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values([\"max_abs_delta\", \"cramers_v\"], ascending=[False, False]).reset_index(drop=True)\n",
    "    return per_feature, summary\n",
    "\n",
    "def plot_churn_bar(tbl: pd.DataFrame, title_feature_name: Optional[str] = None, top_n: int = 12):\n",
    "    \"\"\"\n",
    "    Plot churn rates by category for a single feature table (from categorical_vs_churn).\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    t = tbl.nlargest(top_n, \"abs_delta\").copy()\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    ax = sns.barplot(data=t, x=\"category\", y=\"churn_rate\", color=\"#ef4444\", edgecolor=\"black\")\n",
    "    ax.axhline(t[\"global_rate\"].iloc[0], color=\"#3b82f6\", linestyle=\"--\", label=\"Global churn\")\n",
    "    ax.set_title(f\"{title_feature_name or t['feature'].iloc[0]} ‚Äî Churn Rate by Category\\n\"\n",
    "                 f\"(Cram√©r‚Äôs V={t['cramers_v'].iloc[0]:.3f}, p={t['chi2_p'].iloc[0]:.3g})\")\n",
    "    ax.set_xlabel(title_feature_name or t[\"feature\"].iloc[0]); ax.set_ylabel(\"Churn rate\")\n",
    "    ax.set_ylim(0, max(0.001, float(t[\"churn_rate\"].max()) * 1.15))\n",
    "    ax.legend()\n",
    "    ax.set_yticklabels([f\"{tick*100:.0f}%\" for tick in ax.get_yticks()])\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Categorical √ó Categorical interactions\n",
    "# ---------------------------\n",
    "\n",
    "def interactions_categorical(\n",
    "    df: pd.DataFrame,\n",
    "    churn_str: str = \"Churn\",\n",
    "    churn_bin: str = \"Churn_flag\",\n",
    "    topk_cats: int = 8,\n",
    "    max_pairs: int = 30,\n",
    "    exclude: List[str] = None\n",
    ") -> Tuple[Dict[Tuple[str, str], pd.DataFrame], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate feature pairs (cat √ó cat) vs churn; return per-pair tables and ranking.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    cats = _cat_cols(df, exclude=(exclude or []) + [churn_str.lower(), \"customerid\"])\n",
    "    pairs = list(itertools.combinations(cats, 2))[:max_pairs]\n",
    "\n",
    "    tables = {}\n",
    "    rows = []\n",
    "    global_rate = float(df[churn_bin].mean())\n",
    "\n",
    "    for colA, colB in pairs:\n",
    "        topA = df[colA].value_counts().index[:topk_cats]\n",
    "        topB = df[colB].value_counts().index[:topk_cats]\n",
    "        sub = df[df[colA].isin(topA) & df[colB].isin(topB)].copy()\n",
    "        if sub.empty: \n",
    "            continue\n",
    "        sub[\"combo\"] = sub[colA].astype(str) + \" √ó \" + sub[colB].astype(str)\n",
    "        ct = pd.crosstab(sub[\"combo\"], sub[churn_str])\n",
    "        v, p = _cramers_v(ct) if ct.shape[1] == 2 and ct.shape[0] >= 2 else (np.nan, np.nan)\n",
    "\n",
    "        grp = sub.groupby(\"combo\")[churn_bin].agg([\"mean\", \"count\"]).rename(columns={\"mean\": \"churn_rate\"})\n",
    "        grp[\"pair\"] = f\"{colA} √ó {colB}\"\n",
    "        grp[\"global_rate\"] = global_rate\n",
    "        grp[\"delta\"] = grp[\"churn_rate\"] - global_rate\n",
    "        grp[\"abs_delta\"] = grp[\"delta\"].abs()\n",
    "        grp[\"cramers_v\"] = v\n",
    "        grp[\"chi2_p\"] = p\n",
    "        tables[(colA, colB)] = grp\n",
    "\n",
    "        rows.append({\n",
    "            \"pair\": f\"{colA} √ó {colB}\",\n",
    "            \"n_combos\": int(len(grp)),\n",
    "            \"max_abs_delta\": float(grp[\"abs_delta\"].max()),\n",
    "            \"cramers_v\": v,\n",
    "            \"chi2_p\": p,\n",
    "        })\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values([\"max_abs_delta\", \"cramers_v\"], ascending=[False, False]).reset_index(drop=True)\n",
    "    return tables, summary\n",
    "\n",
    "def plot_interaction(tbl: pd.DataFrame, top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Bar plot for a single interaction table (from interactions_categorical).\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    t = tbl.nlargest(top_n, \"abs_delta\").copy()\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    sns.barplot(data=t, x=\"combo\", y=\"churn_rate\", color=\"#ef4444\", edgecolor=\"black\")\n",
    "    plt.axhline(t[\"global_rate\"].iloc[0], color=\"#3b82f6\", linestyle=\"--\", label=\"Global churn\")\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.ylabel(\"Churn rate\"); plt.xlabel(t[\"pair\"].iloc[0])\n",
    "    plt.title(f\"{t['pair'].iloc[0]}\\nCram√©r‚Äôs V={t['cramers_v'].iloc[0]:.3f}, p={t['chi2_p'].iloc[0]:.3g}\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Continuous √ó Categorical (ANOVA) + Tukey\n",
    "# ---------------------------\n",
    "\n",
    "def anova_continuous_vs_categorical(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: List[str],\n",
    "    cat_cols: List[str],\n",
    "    max_cats: int = 6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each numeric √ó categorical, run one-way ANOVA, compute mean_diff (max-min group mean).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for num_col in numeric_cols:\n",
    "        for cat_col in cat_cols:\n",
    "            if df[cat_col].nunique() < 2:\n",
    "                continue\n",
    "            vc = df[cat_col].value_counts()\n",
    "            cats = vc.index[:max_cats]\n",
    "            groups = [df.loc[df[cat_col] == c, num_col].dropna() for c in cats]\n",
    "            if len(groups) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                stat, p = f_oneway(*groups)\n",
    "            except Exception:\n",
    "                stat, p = np.nan, np.nan\n",
    "            group_means = df.groupby(cat_col)[num_col].mean()\n",
    "            spread = float(group_means.max() - group_means.min())\n",
    "            rows.append({\n",
    "                \"numeric\": num_col,\n",
    "                \"categorical\": cat_col,\n",
    "                \"mean_diff\": spread,\n",
    "                \"anova_F\": float(stat),\n",
    "                \"p_value\": float(p),\n",
    "                \"n_groups\": int(df[cat_col].nunique()),\n",
    "            })\n",
    "    return (pd.DataFrame(rows)\n",
    "            .dropna(subset=[\"p_value\"])\n",
    "            .sort_values([\"mean_diff\", \"anova_F\"], ascending=[False, False])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "def tukey_posthoc(\n",
    "    df: pd.DataFrame,\n",
    "    numeric: str,\n",
    "    categorical: str,\n",
    "    max_cats: int = 6\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Tukey HSD for a given numeric √ó categorical (requires statsmodels).\n",
    "    \"\"\"\n",
    "    if not _HAS_SM:\n",
    "        raise ImportError(\"statsmodels is required for Tukey HSD. Install: pip install statsmodels\")\n",
    "    vc = df[categorical].value_counts()\n",
    "    cats = vc.index[:max_cats]\n",
    "    sub = df[df[categorical].isin(cats)][[numeric, categorical]].dropna()\n",
    "    if sub[categorical].nunique() < 2:\n",
    "        return None\n",
    "    tukey = pairwise_tukeyhsd(endog=sub[numeric], groups=sub[categorical], alpha=0.05)\n",
    "    res_df = pd.DataFrame(tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n",
    "    res_df[\"numeric\"] = numeric\n",
    "    res_df[\"categorical\"] = categorical\n",
    "    # Normalize types\n",
    "    res_df[\"meandiff\"] = res_df[\"meandiff\"].astype(float)\n",
    "    res_df[\"p-adj\"] = res_df[\"p-adj\"].astype(float)\n",
    "    res_df[\"reject\"] = res_df[\"reject\"].astype(bool)\n",
    "    return res_df\n",
    "\n",
    "def plot_num_vs_cat(df: pd.DataFrame, num_col: str, cat_col: str, top_k: int = 6):\n",
    "    \"\"\"\n",
    "    Boxplot for numeric vs categorical with top-k categories.\n",
    "    \"\"\"\n",
    "    cats = df[cat_col].value_counts().index[:top_k]\n",
    "    sub = df[df[cat_col].isin(cats)]\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(7.5, 4.5))\n",
    "    sns.boxplot(data=sub, x=cat_col, y=num_col, palette=\"pastel\", order=cats)\n",
    "    plt.title(f\"{num_col} by {cat_col}\")\n",
    "    plt.xticks(rotation=20, ha=\"right\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Numeric vs Churn\n",
    "# ---------------------------\n",
    "\n",
    "def numeric_vs_churn(\n",
    "    df: pd.DataFrame,\n",
    "    churn_bin: str = \"Churn_flag\",\n",
    "    numeric_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare numeric features between churned vs non-churned:\n",
    "    Welch t-test + Cohen's d + mean differences.\n",
    "    \"\"\"\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = _numeric_cols(df, exclude=[churn_bin])\n",
    "\n",
    "    rows = []\n",
    "    for col in numeric_cols:\n",
    "        x0 = df.loc[df[churn_bin] == 0, col].dropna()\n",
    "        x1 = df.loc[df[churn_bin] == 1, col].dropna()\n",
    "        if len(x0) < 2 or len(x1) < 2:\n",
    "            continue\n",
    "        tstat, pval = ttest_ind(x0, x1, equal_var=False)\n",
    "        d = _cohens_d(x1, x0)\n",
    "        rows.append({\n",
    "            \"feature\": col,\n",
    "            \"mean_churn0\": float(x0.mean()),\n",
    "            \"mean_churn1\": float(x1.mean()),\n",
    "            \"diff\": float(x1.mean() - x0.mean()),\n",
    "            \"p_value\": float(pval),\n",
    "            \"cohen_d\": float(d),\n",
    "        })\n",
    "    return (pd.DataFrame(rows)\n",
    "            .sort_values(\"p_value\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "def plot_numeric_vs_churn(df: pd.DataFrame, col: str, churn_bin: str = \"Churn_flag\"):\n",
    "    \"\"\"\n",
    "    Violin/box visualization for a numeric feature vs churn flag.\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(7.5, 4.5))\n",
    "    sns.violinplot(data=df, x=churn_bin, y=col, palette=[\"#93c5fd\", \"#f87171\"], cut=0)\n",
    "    plt.title(f\"{col} Distribution by Churn\")\n",
    "    plt.xticks([0, 1], [\"No Churn\", \"Churn\"])\n",
    "    plt.xlabel(\"\"); plt.tight_layout(); plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick usage example (in your notebook)\n",
    "\n",
    "```python\n",
    "from src.eda_explorer import *\n",
    "\n",
    "# Numeric summary\n",
    "desc = numeric_summary(df, exclude_cols=[\"customerID\"])\n",
    "display(plot_numeric_summary_style(desc))\n",
    "\n",
    "# Categorical summary\n",
    "cat_df = categorical_summary(df, exclude_cols=[\"customerID\", \"Churn\"])\n",
    "display(plot_categorical_summary_style(cat_df))\n",
    "\n",
    "# Categorical vs Churn\n",
    "per_feat, cat_rank = categorical_vs_churn(df, churn_str=\"Churn\", churn_bin=\"Churn_flag\", topk_cats=12)\n",
    "display(cat_rank.style.format({'global_churn_rate':'{:.2%}','max_abs_delta':'{:.2%}','cramers_v':'{:.3f}','chi2_p':'{:.3g}'}))\n",
    "# Plot the top feature\n",
    "top_feature = cat_rank.loc[0, \"feature\"]\n",
    "plot_churn_bar(per_feat[top_feature], title_feature_name=top_feature)\n",
    "\n",
    "# Interactions (cat √ó cat)\n",
    "tables, pairs_rank = interactions_categorical(df, churn_str=\"Churn\", churn_bin=\"Churn_flag\", topk_cats=8, max_pairs=30)\n",
    "display(pairs_rank.style.format({'max_abs_delta':'{:.2%}','cramers_v':'{:.3f}','chi2_p':'{:.3g}'}))\n",
    "# Plot the top interaction\n",
    "first_pair_name = pairs_rank.loc[0, \"pair\"]\n",
    "colA, colB = first_pair_name.split(\" √ó \")\n",
    "plot_interaction(tables[(colA, colB)])\n",
    "\n",
    "# ANOVA: numeric √ó categorical\n",
    "anova_df = anova_continuous_vs_categorical(df, numeric_cols=['tenure','MonthlyCharges','TotalCharges'],\n",
    "                                           cat_cols=_cat_cols(df, exclude=['customerID','Churn']), max_cats=6)\n",
    "display(anova_df.style.format({'mean_diff':'{:.2f}','anova_F':'{:.1f}','p_value':'{:.3g}'}))\n",
    "\n",
    "# Tukey HSD (optional, requires statsmodels)\n",
    "# from src.eda_explorer import tukey_posthoc\n",
    "# tuk = tukey_posthoc(df, numeric='MonthlyCharges', categorical='InternetService', max_cats=6)\n",
    "# display(tuk.head())\n",
    "\n",
    "# Numeric vs Churn (Welch t-test + Cohen's d)\n",
    "num_vs = numeric_vs_churn(df, churn_bin=\"Churn_flag\", numeric_cols=['tenure','MonthlyCharges','TotalCharges'])\n",
    "display(num_vs.style.format({'mean_churn0':'{:.2f}','mean_churn1':'{:.2f}','diff':'{:+.2f}','p_value':'{:.3g}','cohen_d':'{:.2f}'}))\n",
    "for col in ['tenure','MonthlyCharges','TotalCharges']:\n",
    "    plot_numeric_vs_churn(df, col, churn_bin=\"Churn_flag\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Optional dependency for Tukey HSD: `pip install statsmodels`\n",
    "* All functions are **dataset-agnostic**; the Telco defaults (e.g., `Churn`, `Churn_flag`) are parameters.\n",
    "* Keep your earlier **guard cell** to ensure `Churn_flag` exists and dtypes are correct before calling these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_append_csv(df_like, path: Path):\n",
    "    \"\"\"Append dataframe-like rows to CSV atomically (temp file + replace).\"\"\"\n",
    "    import tempfile, os\n",
    "    df_obj = df_like if hasattr(df_like, \"to_csv\") else pd.DataFrame([df_like])\n",
    "    header = not path.exists()\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=path.parent, suffix=\".csv\") as tmp:\n",
    "        df_obj.to_csv(tmp.name, mode=\"w\", index=False, header=header)\n",
    "        tmp.flush()\n",
    "        os.fsync(tmp.fileno())\n",
    "        # Append by concatenating files iff path exists; else just rename\n",
    "    if path.exists():\n",
    "        # concatenate existing + tmp into a new file atomically\n",
    "        with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=path.parent, suffix=\".csv\") as merged:\n",
    "            with open(path, \"r\") as old, open(tmp.name, \"r\") as new:\n",
    "                if header:\n",
    "                    merged.write(old.read())\n",
    "                    merged.write(new.read())\n",
    "                else:\n",
    "                    merged.write(old.read())\n",
    "                    # skip header of new\n",
    "                    merged.write(\"\".join(new.readlines()[1:]))\n",
    "            os.replace(merged.name, path)\n",
    "        os.remove(tmp.name)\n",
    "    else:\n",
    "        os.replace(tmp.name, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.12.1-2 üéØ Target & Demographic Diagnostics (Churn + SeniorCitizen) \n",
    "# 2.12.0 block** that adds both **Churn Imbalance** and **SeniorCitizen Audit** and **appends audit rows to your unified report** in the same atomic/align style you‚Äôve been using.\n",
    "print(\"\\n2.12.1) üéØ Target & Demographic Diagnostics\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "assert \"df\" in globals(), \"df not defined\"\n",
    "assert \"SECTION2_REPORT_PATH\" in globals(), \"SECTION2_REPORT_PATH not defined\"\n",
    "REPORT_PATH = SECTION2_REPORT_PATH  # unified CSV you‚Äôve been appending to\n",
    "\n",
    "def _atomic_append(path: pd.Path | str, chunk: pd.DataFrame):\n",
    "    \"\"\"Append chunk to unified CSV with schema alignment (atomic replace).\"\"\"\n",
    "    path = pd.Path(path) if not isinstance(path, pd.Path) else path\n",
    "    tmp  = path.with_suffix(path.suffix + \".tmp\")\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if path.exists():\n",
    "            existing = pd.read_csv(path)\n",
    "            all_cols = pd.Index(existing.columns).union(chunk.columns)\n",
    "            out = pd.concat(\n",
    "                [existing.reindex(columns=all_cols), chunk.reindex(columns=all_cols)],\n",
    "                ignore_index=True\n",
    "            )\n",
    "        else:\n",
    "            out = chunk\n",
    "        # tidy a few numeric columns if present\n",
    "        for col in (\"percent\", \"imbalance_ratio\", \"pct_inconsistent\", \"top_freq\", \"pct_not_allowed\"):\n",
    "            if col in out.columns:\n",
    "                out[col] = pd.to_numeric(out[col], errors=\"coerce\").round(4)\n",
    "        out.to_csv(tmp, index=False)\n",
    "        os.replace(tmp, path)\n",
    "        print(f\"üßæ Appended diagnostics ‚Üí {path}\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            if tmp.exists(): tmp.unlink(missing_ok=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"‚ö†Ô∏è Could not append diagnostics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-phase categorical QA, application, and verification.\n",
    "\n",
    "\n",
    "\n",
    "Short answer: combining them in one cell **works**, but the **most effective, professional** approach is to keep them as **two steps** with **shared utilities**‚Äîi.e., factor common logic into a small module and call it from the notebook.\n",
    "\n",
    "Here‚Äôs how I‚Äôd do it, ranked:\n",
    "\n",
    "# 1) Best-practice (production-ready)\n",
    "\n",
    "* **Separate concerns**\n",
    "\n",
    "  * **2.7.1 Pre-apply** = detect/counterexample audit *before* you mutate.\n",
    "  * **2.7.2 Post-apply** = verify conformance *after* your cleaning.\n",
    "* **DRY utilities** (one source of truth): normalization, allowlist check, atomic writer, schema alignment, float rounding, timestamping.\n",
    "* **Config-driven**: keep `ALLOWED` in YAML (and load it), protect/target columns in config.\n",
    "* **Testable**: unit tests for normalization and allowlist logic; smoke test for the writer.\n",
    "* **Notebook = orchestration only**: call functions, display previews, and link artifacts.\n",
    "\n",
    "Minimal skeleton:\n",
    "\n",
    "```\n",
    "project/\n",
    "‚îú‚îÄ src/telco_quality/\n",
    "‚îÇ  ‚îú‚îÄ __init__.py\n",
    "‚îÇ  ‚îú‚îÄ categorical.py          # normalize_text, pre_validate, post_validate\n",
    "‚îÇ  ‚îî‚îÄ report_io.py            # append_atomic, shape_schema\n",
    "‚îú‚îÄ config/\n",
    "‚îÇ  ‚îî‚îÄ allowed_values.yaml\n",
    "‚îî‚îÄ notebooks/\n",
    "   ‚îî‚îÄ 01_EDA.ipynb\n",
    "```\n",
    "\n",
    "```python\n",
    "# src/telco_quality/categorical.py\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_text(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.replace(\"\\u00A0\", \" \", regex=False).str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    return s.str.strip()\n",
    "\n",
    "def pre_validate(df, allowed: dict | None, protect: set, target: set) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # returns (summary_df, issues_df)\n",
    "    ...\n",
    "\n",
    "def post_validate(df, allowed: dict | None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # returns (summary_df, issues_df)\n",
    "    ...\n",
    "```\n",
    "\n",
    "```python\n",
    "# src/telco_quality/report_io.py\n",
    "import os, pandas as pd\n",
    "def append_atomic(chunk: pd.DataFrame, path) -> None:\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if path.exists():\n",
    "        existing = pd.read_csv(path)\n",
    "        all_cols = pd.Index(existing.columns).union(chunk.columns)\n",
    "        out = pd.concat([existing.reindex(columns=all_cols),\n",
    "                         chunk.reindex(columns=all_cols)], ignore_index=True)\n",
    "    else:\n",
    "        out = chunk\n",
    "    out.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "```\n",
    "\n",
    "Notebook call (clean and readable):\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from telco_quality.categorical import pre_validate, post_validate\n",
    "from telco_quality.report_io import append_atomic\n",
    "\n",
    "SECTION2_REPORT_PATH = Path(\"Level_3/reports/section2_data_quality.csv\")\n",
    "\n",
    "pre_sum, pre_issues   = pre_validate(df, ALLOWED, {\"customerID\"}, {\"Churn\"})\n",
    "post_sum, post_issues = post_validate(df, ALLOWED)\n",
    "\n",
    "append_atomic(pre_sum,  SECTION2_REPORT_PATH)\n",
    "append_atomic(post_sum, SECTION2_REPORT_PATH)\n",
    "display(pre_sum.sort_values(...).head(25))\n",
    "display(post_sum.sort_values(...).head(20))\n",
    "```\n",
    "\n",
    "**Why this is ‚Äúbest‚Äù:**\n",
    "\n",
    "* Clean boundaries, fewer bugs, easy to extend.\n",
    "* Reuse *one* normalization function + *one* writer across many sections.\n",
    "* Trivially unit-testable and interview-ready.\n",
    "\n",
    "# 2) Good compromise (your current notebook, no `def`)\n",
    "\n",
    "If you **must** avoid functions, your combined cell is fine. To make it more ‚Äúpro‚Äù without defs:\n",
    "\n",
    "* **Single config block** at the top (paths, `protect_cols`, `target_cols`, `has_allow`, timestamp).\n",
    "* **One normalization snippet** copied verbatim in both loops (pre & post) to stay deterministic.\n",
    "* **One atomic writer** for both chunks (you did this).\n",
    "* **Stable column order**: shape to a unified schema *once* and reuse that list.\n",
    "* **Section tags**: use `\"2.7.1_pre_validate\"` and `\"2.7.2_post_validate\"` consistently.\n",
    "\n",
    "# 3) Should you combine the two steps?\n",
    "\n",
    "* **Purpose:** yes, they serve **different purposes** (pre-apply diagnostics vs post-apply verification). Keep them as **two labeled stages** even if they live in the same cell.\n",
    "* **Combining** is OK for convenience, but **don‚Äôt merge the logic paths** (you want a clear boundary between pre and post to catch regressions).\n",
    "\n",
    "# 4) Extra polish to look ‚Äúsenior‚Äù\n",
    "\n",
    "* **Idempotency:** pre-apply must never mutate `df`. Post-apply should only read.\n",
    "* **Deterministic artifacts:** include `section` + `run_ts` in every row; write separate `__pre_issues.csv` / `__post_issues.csv` for sampling.\n",
    "* **Logging:** lightweight prints like `2.7.1)‚Ä¶`, counts, and artifact paths.\n",
    "* **Schema stability:** always union columns on append (you already do).\n",
    "* **Future-proof:** if this grows, graduate to **Great Expectations** or **Pandera** for declarative checks, but keep your writer for the unified CSV.\n",
    "\n",
    "---\n",
    "\n",
    "## Final recommendation\n",
    "\n",
    "* For your portfolio and maintainability: **use Option 1** (small module with functions) and call it from the notebook.\n",
    "* If you want to stay purely notebook-inline: your combined cell is **good**, just keep **2.7.1** and **2.7.2** distinct inside it and reuse **one writer + one normalization** block.\n",
    "\n",
    "If you want, I can turn your current combined cell into the small `src/telco_quality/categorical.py` module (with exactly the same behavior and column schema) so you can import it immediately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Telco Customer Churn - Exploratory Data Analysis\n",
    "Simple yet comprehensive EDA for understanding customer churn patterns\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Telco Customer Churn dataset\"\"\"\n",
    "    data_path = Path(\"data/raw/Telco-Customer-Churn.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "def basic_info(df):\n",
    "    \"\"\"Display basic information about the dataset\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "def check_missing_values(df):\n",
    "    \"\"\"Check for missing values and data quality issues\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DATA QUALITY CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    \n",
    "    print(\"Missing Values:\")\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Check for empty strings or spaces\n",
    "    print(\"\\nChecking for empty strings...\")\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        empty_count = (df[col] == '').sum() + (df[col] == ' ').sum()\n",
    "        if empty_count > 0:\n",
    "            print(f\"{col}: {empty_count} empty values\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "def analyze_target_variable(df):\n",
    "    \"\"\"Analyze the target variable (Churn)\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    churn_counts = df['Churn'].value_counts()\n",
    "    churn_pct = df['Churn'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(\"Churn Distribution:\")\n",
    "    for val, count, pct in zip(churn_counts.index, churn_counts.values, churn_pct.values):\n",
    "        print(f\"{val}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    sns.countplot(data=df, x='Churn', ax=ax1)\n",
    "    ax1.set_title('Churn Distribution')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Churn Percentage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_numerical_features(df):\n",
    "    \"\"\"Analyze numerical features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"NUMERICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Convert TotalCharges to numeric (it might be stored as string)\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    \n",
    "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    \n",
    "    print(\"Statistical Summary:\")\n",
    "    print(df[numerical_cols].describe())\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        # Distribution\n",
    "        sns.histplot(data=df, x=col, hue='Churn', kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'{col} Distribution by Churn')\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(data=df, x='Churn', y=col, ax=axes[i+3])\n",
    "        axes[i+3].set_title(f'{col} by Churn Status')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_categorical_features(df):\n",
    "    \"\"\"Analyze categorical features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and col not in ['customerID', 'Churn']]\n",
    "    \n",
    "    # Key categorical features for visualization\n",
    "    key_features = ['Contract', 'PaymentMethod', 'InternetService', 'gender', 'SeniorCitizen']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(key_features):\n",
    "        if i < len(axes):\n",
    "            # Create crosstab\n",
    "            ct = pd.crosstab(df[col], df['Churn'], normalize='index') * 100\n",
    "            ct.plot(kind='bar', ax=axes[i], rot=45)\n",
    "            axes[i].set_title(f'Churn Rate by {col}')\n",
    "            axes[i].set_ylabel('Churn Rate (%)')\n",
    "            axes[i].legend(title='Churn')\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(key_features) < len(axes):\n",
    "        fig.delaxes(axes[-1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print churn rates for key categories\n",
    "    print(\"\\nChurn Rates by Category:\")\n",
    "    for col in key_features:\n",
    "        print(f\"\\n{col}:\")\n",
    "        churn_rate = df.groupby(col)['Churn'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "        for category, rate in churn_rate.items():\n",
    "            print(f\"  {category}: {rate:.1f}%\")\n",
    "\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"Analyze correlations between features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Convert categorical variables to numerical for correlation\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Binary encoding for Yes/No columns\n",
    "    binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "    for col in binary_cols:\n",
    "        df_encoded[col] = (df_encoded[col] == 'Yes').astype(int)\n",
    "    \n",
    "    # Encode SeniorCitizen (already 0/1)\n",
    "    # Encode other categorical variables\n",
    "    categorical_to_encode = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', \n",
    "                           'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n",
    "                           'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "    \n",
    "    for col in categorical_to_encode:\n",
    "        if col in df_encoded.columns:\n",
    "            df_encoded[col] = pd.Categorical(df_encoded[col]).codes\n",
    "    \n",
    "    # Convert TotalCharges to numeric\n",
    "    df_encoded['TotalCharges'] = pd.to_numeric(df_encoded['TotalCharges'], errors='coerce')\n",
    "    \n",
    "    # Select numerical columns for correlation\n",
    "    numerical_cols = df_encoded.select_dtypes(include=[np.number]).columns\n",
    "    numerical_cols = [col for col in numerical_cols if col != 'customerID']\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = df_encoded[numerical_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlations with Churn\n",
    "    churn_corr = corr_matrix['Churn'].abs().sort_values(ascending=False)\n",
    "    print(\"\\nFeatures most correlated with Churn:\")\n",
    "    for feature, corr in churn_corr.items():\n",
    "        if feature != 'Churn':\n",
    "            print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "def generate_insights(df):\n",
    "    \"\"\"Generate key insights from the analysis\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convert TotalCharges to numeric\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Churn rate\n",
    "    churn_rate = (df['Churn'] == 'Yes').mean() * 100\n",
    "    insights.append(f\"Overall churn rate: {churn_rate:.1f}%\")\n",
    "    \n",
    "    # Contract insights\n",
    "    contract_churn = df.groupby('Contract')['Churn'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "    insights.append(f\"Month-to-month contracts have highest churn: {contract_churn['Month-to-month']:.1f}%\")\n",
    "    \n",
    "    # Tenure insights\n",
    "    avg_tenure_churned = df[df['Churn'] == 'Yes']['tenure'].mean()\n",
    "    avg_tenure_retained = df[df['Churn'] == 'No']['tenure'].mean()\n",
    "    insights.append(f\"Average tenure - Churned: {avg_tenure_churned:.1f} months, Retained: {avg_tenure_retained:.1f} months\")\n",
    "    \n",
    "    # Payment method insights\n",
    "    payment_churn = df.groupby('PaymentMethod')['Churn'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "    highest_churn_payment = payment_churn.idxmax()\n",
    "    insights.append(f\"Highest churn payment method: {highest_churn_payment} ({payment_churn[highest_churn_payment]:.1f}%)\")\n",
    "    \n",
    "    # Internet service insights\n",
    "    internet_churn = df.groupby('InternetService')['Churn'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "    insights.append(f\"Fiber optic users have higher churn: {internet_churn['Fiber optic']:.1f}%\")\n",
    "    \n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i}. {insight}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete EDA\"\"\"\n",
    "    print(\"Starting Telco Customer Churn EDA...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    \n",
    "    # Run analysis\n",
    "    basic_info(df)\n",
    "    check_missing_values(df)\n",
    "    analyze_target_variable(df)\n",
    "    analyze_numerical_features(df)\n",
    "    analyze_categorical_features(df)\n",
    "    correlation_analysis(df)\n",
    "    generate_insights(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EDA COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
