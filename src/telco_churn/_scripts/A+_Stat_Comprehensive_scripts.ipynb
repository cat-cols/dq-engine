{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Statistical Foundation & Hypothesis Testing\n",
    "3.1 Statistical Testing Framework\n",
    "Why Statistical Testing in Level 3?\n",
    "Level 2: Descriptive statistics - \"Churn rate is 26.5%\"\n",
    "Level 3: Inferential statistics - \"Is this difference real or random chance?\"\n",
    "Level 4: Predictive modeling - \"Can we predict who will churn?\"\n",
    "3.2 Test Selection Decision Tree\n",
    "def choose_statistical_test(data_type1, data_type2):\n",
    "    \"\"\"\n",
    "    Systematic test selection based on data characteristics.\n",
    "    \"\"\"\n",
    "    if data_type1 == 'categorical' and data_type2 == 'categorical':\n",
    "        return 'chi_square'  # Test independence\n",
    "    elif data_type1 == 'numerical' and data_type2 == 'categorical':\n",
    "        # Check normality\n",
    "        if check_normality(data):\n",
    "            return 't_test'  # Parametric\n",
    "        else:\n",
    "            return 'mann_whitney_u'  # Non-parametric\n",
    "    elif data_type1 == 'numerical' and data_type2 == 'numerical':\n",
    "        return 'correlation'  # Pearson or Spearman\n",
    "\n",
    "3.3 Statistical Testing Module (stats.py)\n",
    "\"\"\"\n",
    "Statistical Analysis Module - Level 3 Enhancement\n",
    "LEARNING NOTES:\n",
    "- Statistical validation prevents false conclusions\n",
    "- Effect size matters more than p-values in business contexts\n",
    "- Different tests for different data types and assumptions\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def test_numerical_vs_churn(df, numerical_col, target_col='Churn'):\n",
    "    \"\"\"\n",
    "    Test if numerical feature differs between churn groups.\n",
    "    \n",
    "    DECISION TREE:\n",
    "    1. Test normality (Shapiro-Wilk)\n",
    "    2. If normal: use t-test (parametric)\n",
    "    3. If not normal: use Mann-Whitney U (non-parametric)\n",
    "    4. Always report effect size (Cohen's d)\n",
    "    \"\"\"\n",
    "    churned = df[df[target_col] == 'Yes'][numerical_col].dropna()\n",
    "    retained = df[df[target_col] == 'No'][numerical_col].dropna()\n",
    "    \n",
    "    # Test normality\n",
    "    _, p_norm_churned = stats.shapiro(churned.sample(min(100, len(churned))))\n",
    "    _, p_norm_retained = stats.shapiro(retained.sample(min(100, len(retained))))\n",
    "    \n",
    "    is_normal = (p_norm_churned > 0.05) and (p_norm_retained > 0.05)\n",
    "    \n",
    "    # Choose appropriate test\n",
    "    if is_normal:\n",
    "        statistic, p_value = ttest_ind(churned, retained)\n",
    "        test_used = 't-test'\n",
    "    else:\n",
    "        statistic, p_value = mannwhitneyu(churned, retained)\n",
    "        test_used = 'Mann-Whitney U'\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(churned)-1)*churned.std()**2 + \n",
    "                          (len(retained)-1)*retained.std()**2) / \n",
    "                         (len(churned) + len(retained) - 2))\n",
    "    cohens_d = (churned.mean() - retained.mean()) / pooled_std\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    \n",
    "    return {\n",
    "        'test_used': test_used,\n",
    "        'statistic': float(statistic),\n",
    "        'p_value': float(p_value),\n",
    "        'significant': p_value < 0.05,\n",
    "        'churned_mean': float(churned.mean()),\n",
    "        'retained_mean': float(retained.mean()),\n",
    "        'cohens_d': float(cohens_d),\n",
    "        'effect_size': effect_interpretation\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd875cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Approaches for Telco Customer Churn Analysis\n",
    "## A Comprehensive Dissertation\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Executive Summary](#1-executive-summary)\n",
    "2. [Introduction to Statistical Analysis in Churn Prediction](#2-introduction)\n",
    "3. [Descriptive Statistics](#3-descriptive-statistics)\n",
    "4. [Inferential Statistics](#4-inferential-statistics)\n",
    "5. [Hypothesis Testing](#5-hypothesis-testing)\n",
    "6. [Correlation and Association Analysis](#6-correlation-and-association-analysis)\n",
    "7. [Distribution Analysis](#7-distribution-analysis)\n",
    "8. [Time Series and Survival Analysis](#8-time-series-and-survival-analysis)\n",
    "9. [Multivariate Statistical Techniques](#9-multivariate-statistical-techniques)\n",
    "10. [Statistical Assumptions and Validation](#10-statistical-assumptions)\n",
    "11. [Practical Implementation Guide](#11-practical-implementation)\n",
    "12. [Case Studies and Applications](#12-case-studies)\n",
    "13. [Conclusion](#13-conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "This dissertation provides a comprehensive guide to statistical approaches essential for analyzing customer churn in telecommunications. We cover 15+ statistical methods, their theoretical foundations, practical applications, and implementation in Python.\n",
    "\n",
    "### Key Statistical Methods Covered:\n",
    "\n",
    "- **Descriptive Statistics**: Central tendency, dispersion, distribution shapes\n",
    "- **Hypothesis Testing**: t-tests, chi-square tests, ANOVA\n",
    "- **Correlation Analysis**: Pearson, Spearman, point-biserial\n",
    "- **Distribution Analysis**: Normality tests, Q-Q plots\n",
    "- **Survival Analysis**: Kaplan-Meier, Cox regression\n",
    "- **Multivariate Techniques**: PCA, factor analysis, cluster analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Statistical Analysis in Churn Prediction\n",
    "\n",
    "### 2.1 Why Statistics Matter in Churn Analysis\n",
    "\n",
    "Statistical analysis forms the foundation of data-driven churn prediction by:\n",
    "\n",
    "1. **Quantifying Relationships**: Measure strength between features and churn\n",
    "2. **Testing Hypotheses**: Validate business assumptions scientifically\n",
    "3. **Identifying Patterns**: Discover hidden trends in customer behavior\n",
    "4. **Ensuring Validity**: Verify model assumptions and results\n",
    "5. **Supporting Decisions**: Provide evidence-based recommendations\n",
    "\n",
    "### 2.2 The Statistical Analysis Pipeline\n",
    "\n",
    "```\n",
    "Data Collection ‚Üí Descriptive Statistics ‚Üí Exploratory Analysis ‚Üí\n",
    "Hypothesis Testing ‚Üí Model Building ‚Üí Validation ‚Üí Interpretation\n",
    "```\n",
    "\n",
    "### 2.3 Types of Variables in Churn Analysis\n",
    "\n",
    "| Variable Type | Examples | Statistical Methods |\n",
    "|---------------|----------|---------------------|\n",
    "| **Binary** | Churn (Yes/No), Gender | Chi-square, logistic regression |\n",
    "| **Nominal** | Contract type, Payment method | Chi-square, ANOVA |\n",
    "| **Ordinal** | Satisfaction ratings, Tenure groups | Mann-Whitney U, Kruskal-Wallis |\n",
    "| **Continuous** | Monthly charges, Tenure (months) | t-tests, correlation, regression |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the main features of your dataset.\n",
    "\n",
    "### 3.1 Measures of Central Tendency\n",
    "\n",
    "#### 3.1.1 Mean (Average)\n",
    "\n",
    "**Definition**: Sum of all values divided by count\n",
    "\n",
    "**Formula**: \n",
    "```\n",
    "Œº = (Œ£x) / n\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- Continuous variables (tenure, charges)\n",
    "- Normally distributed data\n",
    "- No extreme outliers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean\n",
    "mean_tenure = df['tenure'].mean()\n",
    "mean_monthly_charges = df['MonthlyCharges'].mean()\n",
    "\n",
    "# By churn status\n",
    "df.groupby('Churn')['tenure'].mean()\n",
    "\n",
    "# Interpretation\n",
    "print(f\"Average tenure: {mean_tenure:.2f} months\")\n",
    "print(f\"Churned customers avg tenure: {df[df['Churn']=='Yes']['tenure'].mean():.2f}\")\n",
    "print(f\"Retained customers avg tenure: {df[df['Churn']=='No']['tenure'].mean():.2f}\")\n",
    "```\n",
    "\n",
    "**Interpretation for Churn**:\n",
    "- If churned customers have lower mean tenure ‚Üí New customers at risk\n",
    "- If churned customers have higher mean charges ‚Üí Price sensitivity issue\n",
    "\n",
    "#### 3.1.2 Median\n",
    "\n",
    "**Definition**: Middle value when data is sorted\n",
    "\n",
    "**When to Use**:\n",
    "- Skewed distributions\n",
    "- Presence of outliers\n",
    "- Ordinal data\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate median\n",
    "median_tenure = df['tenure'].median()\n",
    "\n",
    "# Compare mean vs median to detect skewness\n",
    "print(f\"Mean tenure: {df['tenure'].mean():.2f}\")\n",
    "print(f\"Median tenure: {df['tenure'].median():.2f}\")\n",
    "\n",
    "# If mean > median: Right-skewed (long tail of high values)\n",
    "# If mean < median: Left-skewed (long tail of low values)\n",
    "```\n",
    "\n",
    "#### 3.1.3 Mode\n",
    "\n",
    "**Definition**: Most frequently occurring value\n",
    "\n",
    "**When to Use**:\n",
    "- Categorical variables\n",
    "- Identify most common category\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Most common contract type\n",
    "mode_contract = df['Contract'].mode()[0]\n",
    "print(f\"Most common contract: {mode_contract}\")\n",
    "\n",
    "# Mode by churn status\n",
    "df[df['Churn']=='Yes']['Contract'].mode()[0]\n",
    "df[df['Churn']=='No']['Contract'].mode()[0]\n",
    "```\n",
    "\n",
    "### 3.2 Measures of Dispersion\n",
    "\n",
    "#### 3.2.1 Standard Deviation\n",
    "\n",
    "**Definition**: Average distance from the mean\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "œÉ = sqrt(Œ£(x - Œº)¬≤ / n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate standard deviation\n",
    "std_charges = df['MonthlyCharges'].std()\n",
    "\n",
    "# Coefficient of Variation (CV) - standardized measure\n",
    "cv = (std_charges / df['MonthlyCharges'].mean()) * 100\n",
    "print(f\"CV: {cv:.2f}% - Shows relative variability\")\n",
    "\n",
    "# Compare variability between groups\n",
    "churned_std = df[df['Churn']=='Yes']['MonthlyCharges'].std()\n",
    "retained_std = df[df['Churn']=='No']['MonthlyCharges'].std()\n",
    "\n",
    "# Higher variability in churned group may indicate pricing issues\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Low std dev: Homogeneous customer base\n",
    "- High std dev: Diverse customer segments\n",
    "- Compare between churn groups to identify differences\n",
    "\n",
    "#### 3.2.2 Variance\n",
    "\n",
    "**Definition**: Square of standard deviation\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "variance = df['tenure'].var()\n",
    "\n",
    "# Variance explained in churn analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "for i, var in enumerate(explained_variance_ratio[:5]):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}%\")\n",
    "```\n",
    "\n",
    "#### 3.2.3 Range and Interquartile Range (IQR)\n",
    "\n",
    "**Range**: Maximum - Minimum\n",
    "\n",
    "**IQR**: Q3 - Q1 (middle 50% of data)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate range\n",
    "data_range = df['MonthlyCharges'].max() - df['MonthlyCharges'].min()\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df['MonthlyCharges'].quantile(0.25)\n",
    "Q3 = df['MonthlyCharges'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['MonthlyCharges'] < lower_bound) | \n",
    "              (df['MonthlyCharges'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 3.3 Measures of Shape\n",
    "\n",
    "#### 3.3.1 Skewness\n",
    "\n",
    "**Definition**: Measure of asymmetry in distribution\n",
    "\n",
    "**Interpretation**:\n",
    "- Skewness = 0: Perfectly symmetric\n",
    "- Skewness > 0: Right-skewed (tail on right)\n",
    "- Skewness < 0: Left-skewed (tail on left)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Calculate skewness\n",
    "tenure_skew = skew(df['tenure'])\n",
    "charges_skew = skew(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Tenure skewness: {tenure_skew:.3f}\")\n",
    "print(f\"Monthly charges skewness: {charges_skew:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(tenure_skew) < 0.5:\n",
    "    print(\"Tenure is approximately symmetric\")\n",
    "elif tenure_skew > 0:\n",
    "    print(\"Tenure is right-skewed (many new customers)\")\n",
    "else:\n",
    "    print(\"Tenure is left-skewed (many long-term customers)\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['tenure'], bins=30, edgecolor='black')\n",
    "axes[0].set_title(f'Tenure Distribution (Skewness: {tenure_skew:.2f})')\n",
    "axes[0].axvline(df['tenure'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].axvline(df['tenure'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df['MonthlyCharges'], bins=30, edgecolor='black')\n",
    "axes[1].set_title(f'Monthly Charges (Skewness: {charges_skew:.2f})')\n",
    "axes[1].axvline(df['MonthlyCharges'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].axvline(df['MonthlyCharges'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### 3.3.2 Kurtosis\n",
    "\n",
    "**Definition**: Measure of \"tailedness\" or extreme values\n",
    "\n",
    "**Interpretation**:\n",
    "- Kurtosis = 3: Normal distribution (mesokurtic)\n",
    "- Kurtosis > 3: Heavy tails, more outliers (leptokurtic)\n",
    "- Kurtosis < 3: Light tails, fewer outliers (platykurtic)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate excess kurtosis (subtract 3 for comparison to normal)\n",
    "tenure_kurt = kurtosis(df['tenure'], fisher=True)  # fisher=True gives excess kurtosis\n",
    "\n",
    "print(f\"Tenure excess kurtosis: {tenure_kurt:.3f}\")\n",
    "\n",
    "if tenure_kurt > 0:\n",
    "    print(\"‚Üí More extreme values than normal distribution\")\n",
    "    print(\"‚Üí May need robust statistical methods\")\n",
    "elif tenure_kurt < 0:\n",
    "    print(\"‚Üí Fewer extreme values than normal distribution\")\n",
    "    print(\"‚Üí More uniform distribution\")\n",
    "```\n",
    "\n",
    "### 3.4 Comprehensive Descriptive Statistics Summary\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_summary(df, column, churn_col='Churn'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive descriptive statistics for a column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE STATISTICS: {column}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"Overall Statistics:\")\n",
    "    print(f\"  Count: {df[column].count()}\")\n",
    "    print(f\"  Mean: {df[column].mean():.2f}\")\n",
    "    print(f\"  Median: {df[column].median():.2f}\")\n",
    "    print(f\"  Mode: {df[column].mode()[0] if len(df[column].mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Std Dev: {df[column].std():.2f}\")\n",
    "    print(f\"  Variance: {df[column].var():.2f}\")\n",
    "    print(f\"  Min: {df[column].min():.2f}\")\n",
    "    print(f\"  Max: {df[column].max():.2f}\")\n",
    "    print(f\"  Range: {df[column].max() - df[column].min():.2f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in [25, 50, 75, 90, 95, 99]:\n",
    "        print(f\"  {p}th: {df[column].quantile(p/100):.2f}\")\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"\\nDistribution Shape:\")\n",
    "    print(f\"  Skewness: {skew(df[column].dropna()):.3f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis(df[column].dropna(), fisher=True):.3f}\")\n",
    "    \n",
    "    # By churn status\n",
    "    print(f\"\\nBy Churn Status:\")\n",
    "    for churn_val in df[churn_col].unique():\n",
    "        subset = df[df[churn_col]==churn_val][column]\n",
    "        print(f\"  {churn_val}:\")\n",
    "        print(f\"    Mean: {subset.mean():.2f}\")\n",
    "        print(f\"    Median: {subset.median():.2f}\")\n",
    "        print(f\"    Std Dev: {subset.std():.2f}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_pct = (df[column].isnull().sum() / len(df)) * 100\n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"  Missing: {df[column].isnull().sum()} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Usage\n",
    "comprehensive_summary(df, 'tenure')\n",
    "comprehensive_summary(df, 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inferential Statistics\n",
    "\n",
    "Inferential statistics allow us to make predictions and inferences about a population based on sample data.\n",
    "\n",
    "### 4.1 Confidence Intervals\n",
    "\n",
    "**Definition**: Range of values that likely contains the true population parameter\n",
    "\n",
    "**Formula for Mean**:\n",
    "```\n",
    "CI = xÃÑ ¬± (t * (s / sqrt(n)))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- xÃÑ = sample mean\n",
    "- t = t-value from t-distribution\n",
    "- s = sample standard deviation\n",
    "- n = sample size\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for mean.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37042c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Approaches for Telco Customer Churn Analysis\n",
    "## A Comprehensive Dissertation\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Executive Summary](#1-executive-summary)\n",
    "2. [Introduction to Statistical Analysis in Churn Prediction](#2-introduction)\n",
    "3. [Descriptive Statistics](#3-descriptive-statistics)\n",
    "4. [Inferential Statistics](#4-inferential-statistics)\n",
    "5. [Hypothesis Testing](#5-hypothesis-testing)\n",
    "6. [Correlation and Association Analysis](#6-correlation-and-association-analysis)\n",
    "7. [Distribution Analysis](#7-distribution-analysis)\n",
    "8. [Time Series and Survival Analysis](#8-time-series-and-survival-analysis)\n",
    "9. [Multivariate Statistical Techniques](#9-multivariate-statistical-techniques)\n",
    "10. [Statistical Assumptions and Validation](#10-statistical-assumptions)\n",
    "11. [Practical Implementation Guide](#11-practical-implementation)\n",
    "12. [Case Studies and Applications](#12-case-studies)\n",
    "13. [Conclusion](#13-conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "This dissertation provides a comprehensive guide to statistical approaches essential for analyzing customer churn in telecommunications. We cover 15+ statistical methods, their theoretical foundations, practical applications, and implementation in Python.\n",
    "\n",
    "### Key Statistical Methods Covered:\n",
    "\n",
    "- **Descriptive Statistics**: Central tendency, dispersion, distribution shapes\n",
    "- **Hypothesis Testing**: t-tests, chi-square tests, ANOVA\n",
    "- **Correlation Analysis**: Pearson, Spearman, point-biserial\n",
    "- **Distribution Analysis**: Normality tests, Q-Q plots\n",
    "- **Survival Analysis**: Kaplan-Meier, Cox regression\n",
    "- **Multivariate Techniques**: PCA, factor analysis, cluster analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Statistical Analysis in Churn Prediction\n",
    "\n",
    "### 2.1 Why Statistics Matter in Churn Analysis\n",
    "\n",
    "Statistical analysis forms the foundation of data-driven churn prediction by:\n",
    "\n",
    "1. **Quantifying Relationships**: Measure strength between features and churn\n",
    "2. **Testing Hypotheses**: Validate business assumptions scientifically\n",
    "3. **Identifying Patterns**: Discover hidden trends in customer behavior\n",
    "4. **Ensuring Validity**: Verify model assumptions and results\n",
    "5. **Supporting Decisions**: Provide evidence-based recommendations\n",
    "\n",
    "### 2.2 The Statistical Analysis Pipeline\n",
    "\n",
    "```\n",
    "Data Collection ‚Üí Descriptive Statistics ‚Üí Exploratory Analysis ‚Üí\n",
    "Hypothesis Testing ‚Üí Model Building ‚Üí Validation ‚Üí Interpretation\n",
    "```\n",
    "\n",
    "### 2.3 Types of Variables in Churn Analysis\n",
    "\n",
    "| Variable Type | Examples | Statistical Methods |\n",
    "|---------------|----------|---------------------|\n",
    "| **Binary** | Churn (Yes/No), Gender | Chi-square, logistic regression |\n",
    "| **Nominal** | Contract type, Payment method | Chi-square, ANOVA |\n",
    "| **Ordinal** | Satisfaction ratings, Tenure groups | Mann-Whitney U, Kruskal-Wallis |\n",
    "| **Continuous** | Monthly charges, Tenure (months) | t-tests, correlation, regression |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the main features of your dataset.\n",
    "\n",
    "### 3.1 Measures of Central Tendency\n",
    "\n",
    "#### 3.1.1 Mean (Average)\n",
    "\n",
    "**Definition**: Sum of all values divided by count\n",
    "\n",
    "**Formula**: \n",
    "```\n",
    "Œº = (Œ£x) / n\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- Continuous variables (tenure, charges)\n",
    "- Normally distributed data\n",
    "- No extreme outliers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean\n",
    "mean_tenure = df['tenure'].mean()\n",
    "mean_monthly_charges = df['MonthlyCharges'].mean()\n",
    "\n",
    "# By churn status\n",
    "df.groupby('Churn')['tenure'].mean()\n",
    "\n",
    "# Interpretation\n",
    "print(f\"Average tenure: {mean_tenure:.2f} months\")\n",
    "print(f\"Churned customers avg tenure: {df[df['Churn']=='Yes']['tenure'].mean():.2f}\")\n",
    "print(f\"Retained customers avg tenure: {df[df['Churn']=='No']['tenure'].mean():.2f}\")\n",
    "```\n",
    "\n",
    "**Interpretation for Churn**:\n",
    "- If churned customers have lower mean tenure ‚Üí New customers at risk\n",
    "- If churned customers have higher mean charges ‚Üí Price sensitivity issue\n",
    "\n",
    "#### 3.1.2 Median\n",
    "\n",
    "**Definition**: Middle value when data is sorted\n",
    "\n",
    "**When to Use**:\n",
    "- Skewed distributions\n",
    "- Presence of outliers\n",
    "- Ordinal data\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate median\n",
    "median_tenure = df['tenure'].median()\n",
    "\n",
    "# Compare mean vs median to detect skewness\n",
    "print(f\"Mean tenure: {df['tenure'].mean():.2f}\")\n",
    "print(f\"Median tenure: {df['tenure'].median():.2f}\")\n",
    "\n",
    "# If mean > median: Right-skewed (long tail of high values)\n",
    "# If mean < median: Left-skewed (long tail of low values)\n",
    "```\n",
    "\n",
    "#### 3.1.3 Mode\n",
    "\n",
    "**Definition**: Most frequently occurring value\n",
    "\n",
    "**When to Use**:\n",
    "- Categorical variables\n",
    "- Identify most common category\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Most common contract type\n",
    "mode_contract = df['Contract'].mode()[0]\n",
    "print(f\"Most common contract: {mode_contract}\")\n",
    "\n",
    "# Mode by churn status\n",
    "df[df['Churn']=='Yes']['Contract'].mode()[0]\n",
    "df[df['Churn']=='No']['Contract'].mode()[0]\n",
    "```\n",
    "\n",
    "### 3.2 Measures of Dispersion\n",
    "\n",
    "#### 3.2.1 Standard Deviation\n",
    "\n",
    "**Definition**: Average distance from the mean\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "œÉ = sqrt(Œ£(x - Œº)¬≤ / n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate standard deviation\n",
    "std_charges = df['MonthlyCharges'].std()\n",
    "\n",
    "# Coefficient of Variation (CV) - standardized measure\n",
    "cv = (std_charges / df['MonthlyCharges'].mean()) * 100\n",
    "print(f\"CV: {cv:.2f}% - Shows relative variability\")\n",
    "\n",
    "# Compare variability between groups\n",
    "churned_std = df[df['Churn']=='Yes']['MonthlyCharges'].std()\n",
    "retained_std = df[df['Churn']=='No']['MonthlyCharges'].std()\n",
    "\n",
    "# Higher variability in churned group may indicate pricing issues\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Low std dev: Homogeneous customer base\n",
    "- High std dev: Diverse customer segments\n",
    "- Compare between churn groups to identify differences\n",
    "\n",
    "#### 3.2.2 Variance\n",
    "\n",
    "**Definition**: Square of standard deviation\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "variance = df['tenure'].var()\n",
    "\n",
    "# Variance explained in churn analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "for i, var in enumerate(explained_variance_ratio[:5]):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}%\")\n",
    "```\n",
    "\n",
    "#### 3.2.3 Range and Interquartile Range (IQR)\n",
    "\n",
    "**Range**: Maximum - Minimum\n",
    "\n",
    "**IQR**: Q3 - Q1 (middle 50% of data)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate range\n",
    "data_range = df['MonthlyCharges'].max() - df['MonthlyCharges'].min()\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df['MonthlyCharges'].quantile(0.25)\n",
    "Q3 = df['MonthlyCharges'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['MonthlyCharges'] < lower_bound) | \n",
    "              (df['MonthlyCharges'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 3.3 Measures of Shape\n",
    "\n",
    "#### 3.3.1 Skewness\n",
    "\n",
    "**Definition**: Measure of asymmetry in distribution\n",
    "\n",
    "**Interpretation**:\n",
    "- Skewness = 0: Perfectly symmetric\n",
    "- Skewness > 0: Right-skewed (tail on right)\n",
    "- Skewness < 0: Left-skewed (tail on left)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Calculate skewness\n",
    "tenure_skew = skew(df['tenure'])\n",
    "charges_skew = skew(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Tenure skewness: {tenure_skew:.3f}\")\n",
    "print(f\"Monthly charges skewness: {charges_skew:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(tenure_skew) < 0.5:\n",
    "    print(\"Tenure is approximately symmetric\")\n",
    "elif tenure_skew > 0:\n",
    "    print(\"Tenure is right-skewed (many new customers)\")\n",
    "else:\n",
    "    print(\"Tenure is left-skewed (many long-term customers)\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['tenure'], bins=30, edgecolor='black')\n",
    "axes[0].set_title(f'Tenure Distribution (Skewness: {tenure_skew:.2f})')\n",
    "axes[0].axvline(df['tenure'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].axvline(df['tenure'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df['MonthlyCharges'], bins=30, edgecolor='black')\n",
    "axes[1].set_title(f'Monthly Charges (Skewness: {charges_skew:.2f})')\n",
    "axes[1].axvline(df['MonthlyCharges'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].axvline(df['MonthlyCharges'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### 3.3.2 Kurtosis\n",
    "\n",
    "**Definition**: Measure of \"tailedness\" or extreme values\n",
    "\n",
    "**Interpretation**:\n",
    "- Kurtosis = 3: Normal distribution (mesokurtic)\n",
    "- Kurtosis > 3: Heavy tails, more outliers (leptokurtic)\n",
    "- Kurtosis < 3: Light tails, fewer outliers (platykurtic)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate excess kurtosis (subtract 3 for comparison to normal)\n",
    "tenure_kurt = kurtosis(df['tenure'], fisher=True)  # fisher=True gives excess kurtosis\n",
    "\n",
    "print(f\"Tenure excess kurtosis: {tenure_kurt:.3f}\")\n",
    "\n",
    "if tenure_kurt > 0:\n",
    "    print(\"‚Üí More extreme values than normal distribution\")\n",
    "    print(\"‚Üí May need robust statistical methods\")\n",
    "elif tenure_kurt < 0:\n",
    "    print(\"‚Üí Fewer extreme values than normal distribution\")\n",
    "    print(\"‚Üí More uniform distribution\")\n",
    "```\n",
    "\n",
    "### 3.4 Comprehensive Descriptive Statistics Summary\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_summary(df, column, churn_col='Churn'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive descriptive statistics for a column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE STATISTICS: {column}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"Overall Statistics:\")\n",
    "    print(f\"  Count: {df[column].count()}\")\n",
    "    print(f\"  Mean: {df[column].mean():.2f}\")\n",
    "    print(f\"  Median: {df[column].median():.2f}\")\n",
    "    print(f\"  Mode: {df[column].mode()[0] if len(df[column].mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Std Dev: {df[column].std():.2f}\")\n",
    "    print(f\"  Variance: {df[column].var():.2f}\")\n",
    "    print(f\"  Min: {df[column].min():.2f}\")\n",
    "    print(f\"  Max: {df[column].max():.2f}\")\n",
    "    print(f\"  Range: {df[column].max() - df[column].min():.2f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in [25, 50, 75, 90, 95, 99]:\n",
    "        print(f\"  {p}th: {df[column].quantile(p/100):.2f}\")\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"\\nDistribution Shape:\")\n",
    "    print(f\"  Skewness: {skew(df[column].dropna()):.3f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis(df[column].dropna(), fisher=True):.3f}\")\n",
    "    \n",
    "    # By churn status\n",
    "    print(f\"\\nBy Churn Status:\")\n",
    "    for churn_val in df[churn_col].unique():\n",
    "        subset = df[df[churn_col]==churn_val][column]\n",
    "        print(f\"  {churn_val}:\")\n",
    "        print(f\"    Mean: {subset.mean():.2f}\")\n",
    "        print(f\"    Median: {subset.median():.2f}\")\n",
    "        print(f\"    Std Dev: {subset.std():.2f}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_pct = (df[column].isnull().sum() / len(df)) * 100\n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"  Missing: {df[column].isnull().sum()} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Usage\n",
    "comprehensive_summary(df, 'tenure')\n",
    "comprehensive_summary(df, 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inferential Statistics\n",
    "\n",
    "Inferential statistics allow us to make predictions and inferences about a population based on sample data.\n",
    "\n",
    "### 4.1 Confidence Intervals\n",
    "\n",
    "**Definition**: Range of values that likely contains the true population parameter\n",
    "\n",
    "**Formula for Mean**:\n",
    "```\n",
    "CI = xÃÑ ¬± (t * (s / sqrt(n)))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- xÃÑ = sample mean\n",
    "- t = t-value from t-distribution\n",
    "- s = sample standard deviation\n",
    "- n = sample size\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for mean.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  # Standard error of mean\n",
    "    margin_error = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    ci_lower = mean - margin_error\n",
    "    ci_upper = mean + margin_error\n",
    "    \n",
    "    return mean, ci_lower, ci_upper\n",
    "\n",
    "# Example: Confidence interval for average tenure\n",
    "churned_tenure = df[df['Churn']=='Yes']['tenure']\n",
    "retained_tenure = df[df['Churn']=='No']['tenure']\n",
    "\n",
    "mean_c, lower_c, upper_c = calculate_confidence_interval(churned_tenure)\n",
    "mean_r, lower_r, upper_r = calculate_confidence_interval(retained_tenure)\n",
    "\n",
    "print(\"Average Tenure with 95% Confidence Intervals:\")\n",
    "print(f\"Churned: {mean_c:.2f} months [{lower_c:.2f}, {upper_c:.2f}]\")\n",
    "print(f\"Retained: {mean_r:.2f} months [{lower_r:.2f}, {upper_r:.2f}]\")\n",
    "\n",
    "# Interpretation\n",
    "if upper_c < lower_r:\n",
    "    print(\"‚Üí Churned customers have significantly lower tenure (no overlap)\")\n",
    "elif lower_c > upper_r:\n",
    "    print(\"‚Üí Churned customers have significantly higher tenure\")\n",
    "else:\n",
    "    print(\"‚Üí Confidence intervals overlap - difference may not be significant\")\n",
    "```\n",
    "\n",
    "**Business Application**:\n",
    "- Estimate true average monthly revenue from customers\n",
    "- Predict churn rate with confidence bounds\n",
    "- Compare segments with statistical rigor\n",
    "\n",
    "### 4.2 Standard Error\n",
    "\n",
    "**Definition**: Standard deviation of the sampling distribution\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "SE = œÉ / sqrt(n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Calculate standard error for monthly charges\n",
    "se_charges = sem(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Standard Error of Monthly Charges: ${se_charges:.2f}\")\n",
    "print(f\"This means our sample mean is accurate within ¬±${se_charges:.2f}\")\n",
    "\n",
    "# Compare standard errors\n",
    "se_churned = sem(df[df['Churn']=='Yes']['MonthlyCharges'])\n",
    "se_retained = sem(df[df['Churn']=='No']['MonthlyCharges'])\n",
    "\n",
    "print(f\"\\nSE for churned customers: ${se_churned:.2f}\")\n",
    "print(f\"SE for retained customers: ${se_retained:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is crucial for making data-driven decisions about churn drivers.\n",
    "\n",
    "### 5.1 Framework for Hypothesis Testing\n",
    "\n",
    "**Standard Process**:\n",
    "\n",
    "1. **State Hypotheses**:\n",
    "   - H‚ÇÄ (Null): No difference/relationship exists\n",
    "   - H‚ÇÅ (Alternative): Difference/relationship exists\n",
    "\n",
    "2. **Choose Significance Level (Œ±)**:\n",
    "   - Common: Œ± = 0.05 (5% chance of Type I error)\n",
    "\n",
    "3. **Calculate Test Statistic**\n",
    "\n",
    "4. **Find p-value**\n",
    "\n",
    "5. **Make Decision**:\n",
    "   - If p-value < Œ±: Reject H‚ÇÄ (significant result)\n",
    "   - If p-value ‚â• Œ±: Fail to reject H‚ÇÄ\n",
    "\n",
    "### 5.2 Independent Samples t-Test\n",
    "\n",
    "**Purpose**: Compare means of two independent groups\n",
    "\n",
    "**Assumptions**:\n",
    "- Both groups are normally distributed\n",
    "- Equal variances (or use Welch's t-test)\n",
    "- Independent observations\n",
    "\n",
    "**When to Use in Churn Analysis**:\n",
    "- Compare tenure between churned vs retained\n",
    "- Compare charges between customer segments\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind, levene, shapiro\n",
    "\n",
    "def perform_t_test(group1, group2, group1_name, group2_name, \n",
    "                   variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform comprehensive independent t-test with assumption checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"INDEPENDENT T-TEST: {variable_name}\")\n",
    "    print(f\"Comparing {group1_name} vs {group2_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 1. Check normality assumption\n",
    "    print(\"1. Normality Tests (Shapiro-Wilk):\")\n",
    "    _, p_norm1 = shapiro(group1.sample(min(5000, len(group1))))  # Sample for large datasets\n",
    "    _, p_norm2 = shapiro(group2.sample(min(5000, len(group2))))\n",
    "    \n",
    "    print(f\"   {group1_name}: p-value = {p_norm1:.4f}\")\n",
    "    print(f\"   {group2_name}: p-value = {p_norm2:.4f}\")\n",
    "    \n",
    "    if p_norm1 > 0.05 and p_norm2 > 0.05:\n",
    "        print(\"   ‚úì Both groups appear normally distributed\")\n",
    "        normality_met = True\n",
    "    else:\n",
    "        print(\"   ‚ö† At least one group deviates from normality\")\n",
    "        print(\"   ‚Üí Consider using Mann-Whitney U test instead\")\n",
    "        normality_met = False\n",
    "    \n",
    "    # 2. Check equal variance assumption\n",
    "    print(\"\\n2. Equal Variance Test (Levene's Test):\")\n",
    "    _, p_var = levene(group1, group2)\n",
    "    print(f\"   p-value = {p_var:.4f}\")\n",
    "    \n",
    "    if p_var > 0.05:\n",
    "        print(\"   ‚úì Variances are equal\")\n",
    "        equal_var = True\n",
    "    else:\n",
    "        print(\"   ‚ö† Variances are unequal\")\n",
    "        print(\"   ‚Üí Using Welch's t-test (doesn't assume equal variance)\")\n",
    "        equal_var = False\n",
    "    \n",
    "    # 3. Perform t-test\n",
    "    print(\"\\n3. T-Test Results:\")\n",
    "    t_stat, p_value = ttest_ind(group1, group2, equal_var=equal_var)\n",
    "    \n",
    "    print(f\"   t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Significance level: {alpha}\")\n",
    "    \n",
    "    # 4. Calculate effect size (Cohen's d)\n",
    "    mean1, mean2 = group1.mean(), group2.mean()\n",
    "    std1, std2 = group1.std(), group2.std()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1-1)*std1**2 + (n2-1)*std2**2) / (n1+n2-2))\n",
    "    cohens_d = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    print(f\"\\n4. Effect Size (Cohen's d): {cohens_d:.4f}\")\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"   Effect size is {effect}\")\n",
    "    \n",
    "    # 5. Interpretation\n",
    "    print(\"\\n5. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Reject null hypothesis\")\n",
    "        print(f\"   ‚Üí {group1_name} and {group2_name} have different {variable_name}\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí Fail to reject null hypothesis\")\n",
    "        print(f\"   ‚Üí Insufficient evidence of difference\")\n",
    "    \n",
    "    # 6. Descriptive statistics\n",
    "    print(\"\\n6. Descriptive Statistics:\")\n",
    "    print(f\"   {group1_name}: Mean = {mean1:.2f}, SD = {std1:.2f}, n = {n1}\")\n",
    "    print(f\"   {group2_name}: Mean = {mean2:.2f}, SD = {std2:.2f}, n = {n2}\")\n",
    "    print(f\"   Mean Difference: {abs(mean1 - mean2):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare tenure between churned and retained customers\n",
    "churned = df[df['Churn']=='Yes']['tenure'].dropna()\n",
    "retained = df[df['Churn']=='No']['tenure'].dropna()\n",
    "\n",
    "results = perform_t_test(churned, retained, \n",
    "                         'Churned Customers', 'Retained Customers',\n",
    "                         'Tenure (months)')\n",
    "```\n",
    "\n",
    "**Business Interpretation**:\n",
    "\n",
    "```python\n",
    "# If significant difference found:\n",
    "if results['significant']:\n",
    "    print(\"\\nüìä BUSINESS INSIGHT:\")\n",
    "    print(\"Churned and retained customers have significantly different tenure.\")\n",
    "    print(\"‚Üí Action: Focus retention efforts on specific tenure segments\")\n",
    "    print(\"‚Üí Investigate: What happens at critical tenure milestones?\")\n",
    "```\n",
    "\n",
    "### 5.3 Paired Samples t-Test\n",
    "\n",
    "**Purpose**: Compare means of same group at two time points\n",
    "\n",
    "**When to Use**:\n",
    "- Before/after retention campaign\n",
    "- Monthly charges across time periods\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example: Compare customer satisfaction before and after intervention\n",
    "# (hypothetical data)\n",
    "satisfaction_before = df['satisfaction_before']\n",
    "satisfaction_after = df['satisfaction_after']\n",
    "\n",
    "t_stat, p_value = ttest_rel(satisfaction_before, satisfaction_after)\n",
    "\n",
    "print(f\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"‚Üí Intervention had significant effect on satisfaction\")\n",
    "```\n",
    "\n",
    "### 5.4 Chi-Square Test for Independence\n",
    "\n",
    "**Purpose**: Test relationship between two categorical variables\n",
    "\n",
    "**When to Use in Churn Analysis**:\n",
    "- Relationship between Contract type and Churn\n",
    "- Relationship between Payment method and Churn\n",
    "- Any categorical variable vs Churn\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform chi-square test of independence.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CHI-SQUARE TEST: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(df[var1], df[var2])\n",
    "    \n",
    "    print(\"1. Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "    print()\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(\"2. Test Results:\")\n",
    "    print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Degrees of freedom: {dof}\")\n",
    "    \n",
    "    # Check expected frequencies assumption\n",
    "    print(\"\\n3. Assumption Check:\")\n",
    "    print(\"   Expected frequencies (should all be ‚â• 5):\")\n",
    "    print(pd.DataFrame(expected, \n",
    "                       index=contingency_table.index,\n",
    "                       columns=contingency_table.columns).round(2))\n",
    "    \n",
    "    min_expected = expected.min()\n",
    "    if min_expected >= 5:\n",
    "        print(f\"   ‚úì All expected frequencies ‚â• 5 (min: {min_expected:.2f})\")\n",
    "        print(\"   ‚úì Chi-square test is valid\")\n",
    "    else:\n",
    "        print(f\"   ‚ö† Some expected frequencies < 5 (min: {min_expected:.2f})\")\n",
    "        print(\"   ‚ö† Consider Fisher's exact test or combine categories\")\n",
    "    \n",
    "    # Calculate effect size (Cram√©r's V)\n",
    "    n = contingency_table.sum().sum()\n",
    "    min_dim = min(contingency_table.shape[0]-1, contingency_table.shape[1]-1)\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "    \n",
    "    print(f\"\\n4. Effect Size (Cram√©r's V): {cramers_v:.4f}\")\n",
    "    if cramers_v < 0.1:\n",
    "        effect = \"negligible\"\n",
    "    elif cramers_v < 0.3:\n",
    "        effect = \"small\"\n",
    "    elif cramers_v < 0.5:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"   Effect size is {effect}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n5. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT ASSOCIATION (p < {alpha})\")\n",
    "        print(f\"   ‚Üí {var1} and {var2} are related\")\n",
    "        print(f\"   ‚Üí Variables are NOT independent\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT ASSOCIATION (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí Insufficient evidence of relationship\")\n",
    "    \n",
    "    # Calculate percentages for interpretation\n",
    "    print(\"\\n6. Percentage Breakdown:\")\n",
    "    pct_table = pd.crosstab(df[var1], df[var2], normalize='index') * 100\n",
    "    print(pct_table.round(2))\n",
    "    \n",
    "    return {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'cramers_v': cramers_v,\n",
    "        'significant': p_value < alpha,\n",
    "        'contingency_table': contingency_table\n",
    "    }\n",
    "\n",
    "# Example: Test relationship between Contract and Churn\n",
    "result = chi_square_test(df, 'Contract', 'Churn')\n",
    "\n",
    "# Business interpretation\n",
    "if result['significant']:\n",
    "    print(\"\\nüìä BUSINESS INSIGHT:\")\n",
    "    print(\"Contract type is significantly related to churn.\")\n",
    "    print(\"‚Üí Action: Analyze churn rates by contract type\")\n",
    "    print(\"‚Üí Strategy: Incentivize longer contracts\")\n",
    "```\n",
    "\n",
    "### 5.5 ANOVA (Analysis of Variance)\n",
    "\n",
    "**Purpose**: Compare means across 3+ groups\n",
    "\n",
    "**When to Use**:\n",
    "- Compare charges across multiple contract types\n",
    "- Compare tenure across service tiers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "def perform_anova(df, group_var, numeric_var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform one-way ANOVA with post-hoc analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ONE-WAY ANOVA: {numeric_var} across {group_var}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Get groups\n",
    "    groups = df[group_var].unique()\n",
    "    group_data = [df[df[group_var]==g][numeric_var].dropna() for g in groups]\n",
    "    \n",
    "    # 1. Descriptive statistics\n",
    "    print(\"1. Descriptive Statistics by Group:\")\n",
    "    for g, data in zip(groups, group_data):\n",
    "        print(f\"   {g}: Mean={data.mean():.2f}, SD={data.std():.2f}, n={len(data)}\")\n",
    "    \n",
    "    # 2. Perform ANOVA\n",
    "    print(\"\\n2. ANOVA Results:\")\n",
    "    f_stat, p_value = f_oneway(*group_data)\n",
    "    \n",
    "    print(f\"   F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # 3. Effect size (eta-squared)\n",
    "    # Calculate between-group and total sum of squares\n",
    "    grand_mean = df[numeric_var].mean()\n",
    "    ss_between = sum([len(data) * (data.mean() - grand_mean)**2 \n",
    "                      for data in group_data])\n",
    "    ss_total = sum([(x - grand_mean)**2 for data in group_data for x in data])\n",
    "    eta_squared = ss_between / ss_total\n",
    "    \n",
    "    print(f\"\\n3. Effect Size (Œ∑¬≤): {eta_squared:.4f}\")\n",
    "    print(f\"   {eta_squared*100:.2f}% of variance explained by {group_var}\")\n",
    "    \n",
    "    # 4. Interpretation\n",
    "    print(\"\\n4. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí At least one group differs significantly\")\n",
    "        print(f\"   ‚Üí Recommend post-hoc tests (Tukey HSD)\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí All groups have similar means\")\n",
    "    \n",
    "    # 5. Post-hoc test (Tukey HSD) if significant\n",
    "    if p_value < alpha:\n",
    "        from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "        \n",
    "        print(\"\\n5. Post-Hoc Analysis (Tukey HSD):\")\n",
    "        tukey = pairwise_tukeyhsd(df[numeric_var], df[group_var], alpha=alpha)\n",
    "        print(tukey)\n",
    "    \n",
    "    return {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value,\n",
    "        'eta_squared': eta_squared,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare monthly charges across contract types\n",
    "result = perform_anova(df, 'Contract', 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "### 5.6 Mann-Whitney U Test (Non-Parametric Alternative)\n",
    "\n",
    "**Purpose**: Compare distributions of two groups without normality assumption\n",
    "\n",
    "**When to Use**:\n",
    "- Data is not normally distributed\n",
    "- Ordinal data\n",
    "- Small sample sizes\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def mann_whitney_test(group1, group2, group1_name, group2_name, \n",
    "                      variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Mann-Whitney U test (non-parametric alternative to t-test).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MANN-WHITNEY U TEST: {variable_name}\")\n",
    "    print(f\"Comparing {group1_name} vs {group2_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Perform test\n",
    "    u_stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    \n",
    "    print(\"1. Test Results:\")\n",
    "    print(f\"   U-statistic: {u_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Calculate effect size (rank-biserial correlation)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    r = 1 - (2*u_stat) / (n1 * n2)  # rank-biserial correlation\n",
    "    \n",
    "    print(f\"\\n2. Effect Size (rank-biserial r): {r:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n3. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Distributions differ significantly\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "    \n",
    "    # Medians for interpretation\n",
    "    print(\"\\n4. Median Comparison:\")\n",
    "    print(f\"   {group1_name}: Median = {group1.median():.2f}\")\n",
    "    print(f\"   {group2_name}: Median = {group2.median():.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'u_statistic': u_stat,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': r,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: When data is not normally distributed\n",
    "churned_charges = df[df['Churn']=='Yes']['MonthlyCharges'].dropna()\n",
    "retained_charges = df[df['Churn']=='No']['MonthlyCharges'].dropna()\n",
    "\n",
    "result = mann_whitney_test(churned_charges, retained_charges,\n",
    "                           'Churned', 'Retained', 'Monthly Charges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Correlation and Association Analysis\n",
    "\n",
    "Understanding relationships between variables is crucial for feature selection and model building.\n",
    "\n",
    "### 6.1 Pearson Correlation\n",
    "\n",
    "**Purpose**: Measure linear relationship between two continuous variables\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "r = Œ£((x - xÃÑ)(y - »≥)) / sqrt(Œ£(x - xÃÑ)¬≤ √ó Œ£(y - »≥)¬≤)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- r = 1: Perfect positive correlation\n",
    "- r = 0: No linear correlation\n",
    "- r = -1: Perfect negative correlation\n",
    "- |r| < 0.3: Weak\n",
    "- 0.3 ‚â§ |r| < 0.7: Moderate\n",
    "- |r| ‚â• 0.7: Strong\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson_correlation_analysis(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive Pearson correlation analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PEARSON CORRELATION: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    data = df[[var1, var2]].dropna()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r, p_value = pearsonr(data[var1], data[var2])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Pearson r: {r:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Interpret strength\n",
    "    if abs(r) < 0.3:\n",
    "        strength = \"weak\"\n",
    "    elif abs(r) < 0.7:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    \n",
    "    direction = \"positive\" if r > 0 else \"negative\"\n",
    "    \n",
    "    print(f\"   Strength: {strength} {direction} correlation\")\n",
    "    \n",
    "    # Calculate coefficient of determination\n",
    "    r_squared = r ** 2\n",
    "    print(f\"\\n2. Coefficient of Determination (r¬≤): {r_squared:.4f}\")\n",
    "    print(f\"   {r_squared*100:.2f}% of variance in {var2} explained by {var1}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"\\n3. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT CORRELATION (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Relationship is statistically significant\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT CORRELATION (p ‚â• {alpha})\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data[var1], data[var2], alpha=0.5)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(data[var1], data[var2], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(data[var1], p(data[var1]), \"r--\", linewidth=2, label='Regression line')\n",
    "    \n",
    "    plt.xlabel(var1, fontsize=12)\n",
    "    plt.ylabel(var2, fontsize=12)\n",
    "    plt.title(f'{var1} vs {var2}\\n(r = {r:.3f}, p = {p_value:.4f})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {'r': r, 'p_value': p_value, 'r_squared': r_squared}\n",
    "\n",
    "# Example: Correlation between tenure and total charges\n",
    "result = pearson_correlation_analysis(df, 'tenure', 'TotalCharges')\n",
    "```\n",
    "\n",
    "### 6.2 Spearman Correlation\n",
    "\n",
    "**Purpose**: Measure monotonic relationship (not necessarily linear)\n",
    "\n",
    "**When to Use**:\n",
    "- Ordinal variables\n",
    "- Non-linear relationships\n",
    "- Non-normal distributions\n",
    "- Outliers present\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def spearman_correlation_analysis(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Spearman rank correlation analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SPEARMAN CORRELATION: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    data = df[[var1, var2]].dropna()\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    rho, p_value = spearmanr(data[var1], data[var2])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Spearman œÅ (rho): {rho:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Compare with Pearson\n",
    "    r_pearson, _ = pearsonr(data[var1], data[var2])\n",
    "    print(f\"\\n2. Comparison:\")\n",
    "    print(f\"   Pearson r:  {r_pearson:.4f}\")\n",
    "    print(f\"   Spearman œÅ: {rho:.4f}\")\n",
    "    print(f\"   Difference: {abs(r_pearson - rho):.4f}\")\n",
    "    \n",
    "    if abs(r_pearson - rho) > 0.1:\n",
    "        print(\"   ‚ö† Large difference suggests non-linear relationship\")\n",
    "    else:\n",
    "        print(\"   ‚úì Similar values suggest linear relationship\")\n",
    "    \n",
    "    return {'rho': rho, 'p_value': p_value}\n",
    "\n",
    "# Example\n",
    "result = spearman_correlation_analysis(df, 'tenure', 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "### 6.3 Point-Biserial Correlation\n",
    "\n",
    "**Purpose**: Correlation between continuous and binary variable\n",
    "\n",
    "**When to Use**:\n",
    "- Relationship between numeric variable and Churn (binary)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "def point_biserial_analysis(df, continuous_var, binary_var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Point-biserial correlation for continuous vs binary variable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"POINT-BISERIAL CORRELATION\")\n",
    "    print(f\"{continuous_var} vs {binary_var}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Ensure binary variable is 0/1\n",
    "    data = df[[continuous_var, binary_var]].dropna()\n",
    "    if data[binary_var].dtype == 'object':\n",
    "        binary_map = {data[binary_var].unique()[0]: 0,\n",
    "                     data[binary_var].unique()[1]: 1}\n",
    "        data[binary_var] = data[binary_var].map(binary_map)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r_pb, p_value = pointbiserialr(data[binary_var], data[continuous_var])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Point-biserial r: {r_pb:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n2. Interpretation:\")\n",
    "    if r_pb > 0:\n",
    "        print(f\"   Positive correlation: Higher {continuous_var} ‚Üí More likely {binary_var}=1\")\n",
    "    else:\n",
    "        print(f\"   Negative correlation: Higher {continuous_var} ‚Üí More likely {binary_var}=0\")\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        print(f\"\\n3. Conclusion: SIGNIFICANT relationship (p < {alpha})\")\n",
    "    else:\n",
    "        print(f\"\\n3. Conclusion: NO significant relationship (p ‚â• {alpha})\")\n",
    "    \n",
    "    return {'r_pb': r_pb, 'p_value': p_value}\n",
    "\n",
    "# Example: Tenure vs Churn\n",
    "result = point_biserial_analysis(df, 'tenure', 'Churn')\n",
    "```\n",
    "\n",
    "### 6.4 Correlation Matrix and Heatmap\n",
    "\n",
    "**Purpose**: Visualize all pairwise correlations\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_correlation_analysis(df, method='pearson'):\n",
    "    \"\"\"\n",
    "    Create comprehensive correlation matrix with visualization.\n",
    "    \"\"\"\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    if method == 'pearson':\n",
    "        corr_matrix = numeric_df.corr()\n",
    "    elif method == 'spearman':\n",
    "        corr_matrix = numeric_df.corr(method='spearman')\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, fmt='.2f', square=True, linewidths=1,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    \n",
    "    plt.title(f'{method.capitalize()} Correlation Matrix', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strongest correlations with target (if Churn exists)\n",
    "    if 'Churn' in corr_matrix.columns:\n",
    "        print(\"\\nStrongest Correlations with Churn:\")\n",
    "        churn_corr = corr_matrix['Churn'].abs().sort_values(ascending=False)\n",
    "        print(churn_corr[1:11])  # Top 10, excluding Churn itself\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Usage\n",
    "corr_matrix = comprehensive_correlation_analysis(df, method='pearson')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Distribution Analysis\n",
    "\n",
    "Understanding data distributions is critical for choosing appropriate statistical tests and models.\n",
    "\n",
    "### 7.1 Normality Tests\n",
    "\n",
    "#### 7.1.1 Shapiro-Wilk Test\n",
    "\n",
    "**Purpose**: Test if data comes from normal distribution\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def test_normality(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive normality testing.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NORMALITY TEST: {variable_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    stat, p_value = shapiro(data.sample(min(5000, len(data))))  # Sample for large datasets\n",
    "    \n",
    "    print(\"1. Shapiro-Wilk Test:\")\n",
    "    print(f\"   Statistic: {stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"   ‚úì Data appears normally distributed (p > {alpha})\")\n",
    "        normal = True\n",
    "    else:\n",
    "        print(f\"   ‚úó Data deviates from normal distribution (p ‚â§ {alpha})\")\n",
    "        normal = False\n",
    "    \n",
    "    # Visual checks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Visual checks\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Histogram with normal curve overlay\n",
    "    axes[0].hist(data, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "    mu, sigma = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal distribution')\n",
    "    axes[0].set_title('Histogram with Normal Curve', fontweight='bold')\n",
    "    axes[0].set_xlabel(variable_name)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(data, dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[2].boxplot(data, vert=True)\n",
    "    axes[2].set_title('Box Plot', fontweight='bold')\n",
    "    axes[2].set_ylabel(variable_name)\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n2. Visual Interpretation:\")\n",
    "    print(\"   - Histogram: Should resemble bell curve\")\n",
    "    print(\"   - Q-Q Plot: Points should fall on diagonal line\")\n",
    "    print(\"   - Box Plot: Should be roughly symmetric\")\n",
    "    \n",
    "    return {'statistic': stat, 'p_value': p_value, 'normal': normal}\n",
    "\n",
    "# Example\n",
    "result = test_normality(df['tenure'], 'Tenure (months)')\n",
    "result = test_normality(df['MonthlyCharges'], 'Monthly Charges ($)')\n",
    "```\n",
    "\n",
    "#### 7.1.2 Kolmogorov-Smirnov Test\n",
    "\n",
    "**Purpose**: Alternative normality test, better for larger samples\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import kstest\n",
    "\n",
    "def ks_normality_test(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Kolmogorov-Smirnov test for normality.\n",
    "    \"\"\"\n",
    "    # Standardize data\n",
    "    data_std = (data - data.mean()) / data.std()\n",
    "    \n",
    "    # Perform KS test\n",
    "    stat, p_value = kstest(data_std, 'norm')\n",
    "    \n",
    "    print(f\"\\nKolmogorov-Smirnov Test for {variable_name}:\")\n",
    "    print(f\"  Statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"  ‚úì Data appears normally distributed\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Data deviates from normality\")\n",
    "    \n",
    "    return stat, p_value\n",
    "\n",
    "ks_normality_test(df['tenure'], 'Tenure')\n",
    "```\n",
    "\n",
    "### 7.2 Skewness and Kurtosis Tests\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import skewtest, kurtosistest\n",
    "\n",
    "def distribution_shape_tests(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test skewness and kurtosis significance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DISTRIBUTION SHAPE TESTS: {variable_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Calculate skewness\n",
    "    skew_val = skew(data)\n",
    "    skew_stat, skew_p = skewtest(data)\n",
    "    \n",
    "    print(\"1. Skewness Test:\")\n",
    "    print(f\"   Skewness: {skew_val:.4f}\")\n",
    "    print(f\"   Test statistic: {skew_stat:.4f}\")\n",
    "    print(f\"   p-value: {skew_p:.4f}\")\n",
    "    \n",
    "    if skew_p < alpha:\n",
    "        if skew_val > 0:\n",
    "            print(\"   ‚úì Significantly right-skewed\")\n",
    "        else:\n",
    "            print(\"   ‚úì Significantly left-skewed\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Skewness not significantly different from 0\")\n",
    "    \n",
    "    # Calculate kurtosis\n",
    "    kurt_val = kurtosis(data, fisher=True)\n",
    "    kurt_stat, kurt_p = kurtosistest(data)\n",
    "    \n",
    "    print(\"\\n2. Kurtosis Test:\")\n",
    "    print(f\"   Excess kurtosis: {kurt_val:.4f}\")\n",
    "    print(f\"   Test statistic: {kurt_stat:.4f}\")\n",
    "    print(f\"   p-value: {kurt_p:.4f}\")\n",
    "    \n",
    "    if kurt_p < alpha:\n",
    "        if kurt_val > 0:\n",
    "            print(\"   ‚úì Significantly leptokurtic (heavy tails)\")\n",
    "        else:\n",
    "            print(\"   ‚úì Significantly platykurtic (light tails)\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Kurtosis not significantly different from normal\")\n",
    "\n",
    "distribution_shape_tests(df['tenure'], 'Tenure')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Time Series and Survival Analysis\n",
    "\n",
    "### 8.1 Survival Analysis (Kaplan-Meier)\n",
    "\n",
    "**Purpose**: Analyze time until event (churn) occurs\n",
    "\n",
    "**When to Use**:\n",
    "- Understand customer lifetime\n",
    "- Identify critical time periods for churn\n",
    "- Compare survival across customer segments\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "def survival_analysis(df, duration_col, event_col, group_col=None):\n",
    "    \"\"\"\n",
    "    Comprehensive survival analysis for churn.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SURVIVAL ANALYSIS (KAPLAN-MEIER)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize Kaplan-Meier fitter\n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    if group_col is None:\n",
    "        # Overall survival curve\n",
    "        kmf.fit(df[duration_col], df[event_col], label='All Customers')\n",
    "        \n",
    "        print(\"Overall Survival Statistics:\")\n",
    "        print(f\"  Median survival time: {kmf.median_survival_time_:.2f} months\")\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        kmf.plot_survival_function()\n",
    "        plt.title('Customer Survival Curve', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Tenure (months)')\n",
    "        plt.ylabel('Survival Probability')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Survival by groups\n",
    "        print(f\"Survival Analysis by {group_col}:\\n\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        groups = df[group_col].unique()\n",
    "        group_data = []\n",
    "        \n",
    "        for group in groups:\n",
    "            mask = df[group_col] == group\n",
    "            group_subset = df[mask]\n",
    "            \n",
    "            kmf.fit(group_subset[duration_col], \n",
    "                   group_subset[event_col],\n",
    "                   label=str(group))\n",
    "            \n",
    "            kmf.plot_survival_function()\n",
    "            \n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Median survival: {kmf.median_survival_time_:.2f} months\")\n",
    "            print(f\"    1-year survival: {kmf.survival_function_at_times(12).values[0]:.2%}\")\n",
    "            print(f\"    2-year survival: {kmf.survival_function_at_times(24).values[0]:.2%}\\n\")\n",
    "            \n",
    "            group_data.append((group_subset[duration_col], group_subset[event_col]))\n",
    "        \n",
    "        plt.title(f'Survival Curves by {group_col}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Tenure (months)')\n",
    "        plt.ylabel('Survival Probability')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Log-rank test if 2 groups\n",
    "        if len(groups) == 2:\n",
    "            result = logrank_test(group_data[0][0], group_data[1][0],\n",
    "                                 group_data[0][1], group_data[1][1])\n",
    "            \n",
    "            print(f\"Log-Rank Test:\")\n",
    "            print(f\"  Test statistic: {result.test_statistic:.4f}\")\n",
    "            print(f\"  p-value: {result.p_value:.4f}\")\n",
    "            \n",
    "            if result.p_value < 0.05:\n",
    "                print(f\"  ‚úì Survival curves are significantly different\")\n",
    "            else:\n",
    "                print(f\"  ‚úó No significant difference between groups\")\n",
    "\n",
    "# Prepare data (tenure as duration, Churn as event)\n",
    "df_survival = df.copy()\n",
    "df_survival['Churn_binary'] = (df_survival['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Overall survival\n",
    "survival_analysis(df_survival, 'tenure', 'Churn_binary')\n",
    "\n",
    "# Survival by contract type\n",
    "survival_analysis(df_survival, 'tenure', 'Churn_binary', 'Contract')\n",
    "```\n",
    "\n",
    "**Business Interpretation**:\n",
    "\n",
    "```python\n",
    "print(\"\\nüìä BUSINESS INSIGHTS FROM SURVIVAL ANALYSIS:\")\n",
    "print(\"1. Median survival time tells us typical customer lifetime\")\n",
    "print(\"2. Steep drops indicate critical churn periods\")\n",
    "print(\"3. Compare curves across segments to prioritize interventions\")\n",
    "print(\"4. 1-year survival rate = retention rate at 12 months\")\n",
    "```\n",
    "\n",
    "### 8.2 Cox Proportional Hazards Model\n",
    "\n",
    "**Purpose**: Identify factors affecting time to churn\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "def cox_regression_analysis(df, duration_col, event_col, covariates):\n",
    "    \"\"\"\n",
    "    Cox proportional hazards model for churn.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COX PROPORTIONAL HAZARDS MODEL\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    analysis_df = df[[duration_col, event_col] + covariates].dropna()\n",
    "    \n",
    "    # Fit model\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(analysis_df, duration_col=duration_col, event_col=event_col)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Model Summary:\")\n",
    "    print(cph.summary)\n",
    "    \n",
    "    print(\"\\n\\nInterpretation of Hazard Ratios:\")\n",
    "    print(\"(exp(coef) = Hazard Ratio)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for var in covariates:\n",
    "        coef = cph.params_[var]\n",
    "        hr = np.exp(coef)\n",
    "        p_val = cph.summary.loc[var, 'p']\n",
    "        \n",
    "        print(f\"\\n{var}:\")\n",
    "        print(f\"  Hazard Ratio: {hr:.4f}\")\n",
    "        \n",
    "        if hr > 1:\n",
    "            print(f\"  ‚Üí Increases churn risk by {(hr-1)*100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  ‚Üí Decreases churn risk by {(1-hr)*100:.1f}%\")\n",
    "        \n",
    "        if p_val < 0.05:\n",
    "            print(f\"  ‚úì Statistically significant (p={p_val:.4f})\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Not significant (p={p_val:.4f})\")\n",
    "    \n",
    "    # Plot\n",
    "    cph.plot()\n",
    "    plt.title('Hazard Ratios with 95% CI', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cph\n",
    "\n",
    "# Example with numeric predictors\n",
    "covariates = ['MonthlyCharges', 'SeniorCitizen', 'Partner', 'Dependents']\n",
    "cph_model = cox_regression_analysis(df_survival, 'tenure', 'Churn_binary', covariates)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Multivariate Statistical Techniques\n",
    "\n",
    "### 9.1 Principal Component Analysis (PCA)\n",
    "\n",
    "**Purpose**: Reduce dimensionality while preserving variance\n",
    "\n",
    "**When to Use**:\n",
    "- Many correlated features\n",
    "- Visualization of high-dimensional data\n",
    "- Feature extraction\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def comprehensive_pca_analysis(df, n_components=None):\n",
    "    \"\"\"\n",
    "    Complete PCA analysis with interpretation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numeric_cols].dropna()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Fit PCA\n",
    "    if n_components is None:\n",
    "        n_components = min(len(numeric_cols), len(X))\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # 1. Variance explained\n",
    "    print(\"1. Variance Explained:\")\n",
    "    cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    for i, (var, cumvar) in enumerate(zip(pca.explained_variance_ratio_, cumsum_var)):\n",
    "        print(f\"   PC{i+1}: {var*100:.2f}% (Cumulative: {cumvar*100:.2f}%)\")\n",
    "        if cumvar >= 0.95:\n",
    "            print(f\"   ‚Üí 95% variance explained with {i+1} components\")\n",
    "            break\n",
    "    \n",
    "    # 2. Scree plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Variance explained\n",
    "    axes[0].bar(range(1, len(pca.explained_variance_ratio_)+1), \n",
    "                pca.explained_variance_ratio_, alpha=0.7)\n",
    "    axes[0].plot(range(1, len(cumsum_var)+1), cumsum_var, 'r-o', linewidth=2)\n",
    "    axes[0].set_xlabel('Principal Component', fontweight='bold')\n",
    "    axes[0].set_ylabel('Variance Explained', fontweight='bold')\n",
    "    axes[0].set_title('Scree Plot', fontweight='bold')\n",
    "    axes[0].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Biplot (first 2 components)\n",
    "    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "    axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontweight='bold')\n",
    "    axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontweight='bold')\n",
    "    axes[1].set_title('PCA Biplot (First 2 Components)', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Component loadings\n",
    "    print(\"\\n2. Top Feature Loadings for First 3 Components:\")\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "        index=numeric_cols\n",
    "    )\n",
    "    \n",
    "    for i in range(min(3, pca.n_components_)):\n",
    "        print(f\"\\n   PC{i+1} - Top Features:\")\n",
    "        top_features = loadings[f'PC{i+1}'].abs().sort_values(ascending=False).head(5)\n",
    "        for feature, loading in top_features.items():\n",
    "            actual_loading = loadings.loc[feature, f'PC{i+1}']\n",
    "            print(f\"     {feature}: {actual_loading:.3f}\")\n",
    "    \n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "# Example\n",
    "pca_model, X_transformed, loadings = comprehensive_pca_analysis(df)\n",
    "```\n",
    "\n",
    "### 9.2 Factor Analysis\n",
    "\n",
    "**Purpose**: Identify latent factors underlying observed variables\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "def factor_analysis_comprehensive(df, n_factors=3):\n",
    "    \"\"\"\n",
    "    Comprehensive factor analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FACTOR ANALYSIS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numeric_cols].dropna()\n",
    "    \n",
    "    # 1. Test suitability for factor analysis\n",
    "    print(\"1. Suitability Tests:\")\n",
    "    \n",
    "    # KMO Test\n",
    "    kmo_all, kmo_model = calculate_kmo(X)\n",
    "    print(f\"   Kaiser-Meyer-Olkin (KMO) Test: {kmo_model:.3f}\")\n",
    "    if kmo_model >= 0.6:\n",
    "        print(\"   ‚úì Data suitable for factor analysis (KMO > 0.6)\")\n",
    "    else:\n",
    "        print(\"   ‚ö† Data may not be suitable (KMO < 0.6)\")\n",
    "    \n",
    "    # Bartlett's Test\n",
    "    chi_square, p_value = calculate_bartlett_sphericity(X)\n",
    "    print(f\"\\n   Bartlett's Test of Sphericity:\")\n",
    "    print(f\"     Chi-square: {chi_square:.2f}\")\n",
    "    print(f\"     p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"   ‚úì Variables are correlated (suitable for FA)\")\n",
    "    else:\n",
    "        print(\"   ‚ö† Variables may not be sufficiently correlated\")\n",
    "    \n",
    "    # 2. Fit factor analysis\n",
    "    print(f\"\\n2. Fitting {n_factors}-Factor Model:\")\n",
    "    \n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
    "    fa.fit(X)\n",
    "    \n",
    "    # Get factor loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        fa.loadings_,\n",
    "        index=numeric_cols,\n",
    "        columns=[f'Factor{i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n   Factor Loadings:\")\n",
    "    print(loadings.round(3))\n",
    "    \n",
    "    # 3. Variance explained\n",
    "    variance = fa.get_factor_variance()\n",
    "    \n",
    "    print(\"\\n3. Variance Explained:\")\n",
    "    print(f\"   Proportional variance: {variance[1]}\")\n",
    "    print(f\"   Cumulative variance: {variance[2]}\")\n",
    "    \n",
    "    # 4. Interpret factors\n",
    "    print(\"\\n4. Factor Interpretation:\")\n",
    "    for i in range(n_factors):\n",
    "        print(f\"\\n   Factor {i+1} - Top Loaded Variables:\")\n",
    "        top_vars = loadings[f'Factor{i+1}'].abs().sort_values(ascending=False).head(5)\n",
    "        for var, loading in top_vars.items():\n",
    "            actual = loadings.loc[var, f'Factor{i+1}']\n",
    "            print(f\"     {var}: {actual:.3f}\")\n",
    "    \n",
    "    return fa, loadings\n",
    "\n",
    "# Example\n",
    "fa_model, factor_loadings = factor_analysis_comprehensive(df, n_factors=3)\n",
    "```\n",
    "\n",
    "### 9.3 Cluster Analysis\n",
    "\n",
    "**Purpose**: Group similar customers together\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "def cluster_analysis(df, n_clusters_range=range(2, 11)):\n",
    "    \"\"\"\n",
    "    K-means clustering with optimal cluster selection.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CLUSTER ANALYSIS (K-MEANS)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numeric_cols].dropna()\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 1. Determine optimal number of clusters\n",
    "    print(\"1. Finding Optimal Number of Clusters:\")\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in n_clusters_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        \n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Plot elbow curve and silhouette scores\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Elbow method\n",
    "    axes[0].plot(n_clusters_range, inertias, 'bo-', linewidth=2)\n",
    "    axes[0].set_xlabel('Number of Clusters', fontweight='bold')\n",
    "    axes[0].set_ylabel('Inertia', fontweight='bold')\n",
    "    axes[0].set_title('Elbow Method', fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Silhouette score\n",
    "    axes[1].plot(n_clusters_range, silhouette_scores, 'ro-', linewidth=2)\n",
    "    axes[1].set_xlabel('Number of Clusters', fontweight='bold')\n",
    "    axes[1].set_ylabel('Silhouette Score', fontweight='bold')\n",
    "    axes[1].set_title('Silhouette Analysis', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select optimal k (highest silhouette score)\n",
    "    optimal_k = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"\\n   Optimal number of clusters: {optimal_k}\")\n",
    "    print(f\"   Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "    # 2. Fit final model\n",
    "    print(f\"\\n2. Fitting {optimal_k}-Cluster Model:\")\n",
    "    \n",
    "    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans_final.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add clusters to dataframe\n",
    "    df_clustered = X.copy()\n",
    "    df_clustered['Cluster'] = clusters\n",
    "    \n",
    "    # 3. Analyze clusters\n",
    "    print(f\"\\n3. Cluster Profiles:\")\n",
    "    \n",
    "    for i in range(optimal_k):\n",
    "        cluster_data = df_clustered[df_clustered['Cluster'] == i]\n",
    "        print(f\"\\n   Cluster {i} (n={len(cluster_data)}):\")\n",
    "        print(f\"     Mean values:\")\n",
    "        for col in numeric_cols[:5]:  # Show top 5 features\n",
    "            print(f\"       {col}: {cluster_data[col].mean():.2f}\")\n",
    "    \n",
    "    # 4. Visualize clusters (2D PCA)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontweight='bold')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontweight='bold')\n",
    "    plt.title('Customer Segments (K-Means Clustering)', fontweight='bold', fontsize=14)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans_final, clusters, df_clustered\n",
    "\n",
    "# Example\n",
    "kmeans_model, cluster_labels, df_with_clusters = cluster_analysis(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Statistical Assumptions and Validation\n",
    "\n",
    "### 10.1 Checking Linear Regression Assumptions\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import jarque_bera\n",
    "\n",
    "def check_regression_assumptions(X, y, feature_names):\n",
    "    \"\"\"\n",
    "    Comprehensive check of linear regression assumptions.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LINEAR REGRESSION ASSUMPTIONS CHECK\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # 1. Linearity\n",
    "    print(\"1. LINEARITY (Residuals vs Fitted Values):\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Fitted Values', fontweight='bold')\n",
    "    plt.ylabel('Residuals', fontweight='bold')\n",
    "    plt.title('Residual Plot', fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    print(\"   ‚Üí Pattern should be random scatter around zero\")\n",
    "    print(\"   ‚Üí Funnel shape indicates heteroscedasticity\")\n",
    "    print(\"   ‚Üí Curve indicates non-linearity\")\n",
    "    \n",
    "    # 2. Normality of residuals\n",
    "    print(\"\\n2. NORMALITY OF RESIDUALS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title('Histogram of Residuals', fontweight='bold')\n",
    "    axes[0].set_xlabel('Residuals')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Jarque-Bera test\n",
    "    jb_stat, jb_p = jarque_bera(residuals)\n",
    "    print(f\"   Jarque-Bera Test:\")\n",
    "    print(f\"     Statistic: {jb_stat:.4f}\")\n",
    "    print(f\"     p-value: {jb_p:.4f}\")\n",
    "    \n",
    "    if jb_p > 0.05:\n",
    "        print(\"   ‚úì Residuals appear normally distributed\")\n",
    "    else:\n",
    "        print(\"   ‚ö† Residuals deviate from normality\")\n",
    "    \n",
    "    # 3. Homoscedasticity (constant variance)\n",
    "    print(\"\\n3. HOMOSCEDASTICITY (Constant Variance):\")\n",
    "    \n",
    "    # Breusch-Pagan test would go here (requires statsmodels)\n",
    "    print(\"   Visual check: See residual plot above\")\n",
    "    print(\"   ‚Üí Variance should be constant across fitted values\")\n",
    "    \n",
    "    # 4. Independence (Durbin-Watson)\n",
    "    print(\"\\n4. INDEPENDENCE OF RESIDUALS:\")\n",
    "    \n",
    "    # Calculate Durbin-Watson statistic\n",
    "    dw = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)\n",
    "    print(f\"   Durbin-Watson statistic: {dw:.4f}\")\n",
    "    print(\"   ‚Üí Values near 2 indicate no autocorrelation\")\n",
    "    print(\"   ‚Üí Values < 2: positive autocorrelation\")\n",
    "    print(\"   ‚Üí Values > 2: negative autocorrelation\")\n",
    "    \n",
    "    # 5. Multicollinearity (VIF)\n",
    "    print(\"\\n5. MULTICOLLINEARITY CHECK:\")\n",
    "    \n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = feature_names\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "    \n",
    "    print(vif_data.round(2))\n",
    "    print(\"\\n   Interpretation:\")\n",
    "    print(\"   ‚Üí VIF = 1: No correlation\")\n",
    "    print(\"   ‚Üí VIF < 5: Moderate correlation (acceptable)\")\n",
    "    print(\"   ‚Üí VIF > 5: High correlation (problematic)\")\n",
    "    print(\"   ‚Üí VIF > 10: Severe multicollinearity\")\n",
    "    \n",
    "    high_vif = vif_data[vif_data['VIF'] > 5]\n",
    "    if len(high_vif) > 0:\n",
    "        print(f\"\\n   ‚ö† Features with high VIF:\")\n",
    "        print(high_vif)\n",
    "    else:\n",
    "        print(\"\\n   ‚úì No severe multicollinearity detected\")\n",
    "    \n",
    "    return model, residuals, vif_data\n",
    "\n",
    "# Example (prepare numeric data first)\n",
    "X_numeric = df[['tenure', 'MonthlyCharges', 'TotalCharges']].dropna()\n",
    "y_numeric = df.loc[X_numeric.index, 'Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "model, residuals, vif_df = check_regression_assumptions(\n",
    "    X_numeric.values, \n",
    "    y_numeric.values,\n",
    "    X_numeric.columns.tolist()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Practical Implementation Guide\n",
    "\n",
    "### 11.1 Complete Statistical Analysis Workflow\n",
    "\n",
    "```python\n",
    "def complete_statistical_analysis_pipeline(df, target_col='Churn'):\n",
    "    \"\"\"\n",
    "    Execute complete statistical analysis for churn dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE STATISTICAL ANALYSIS PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # PHASE 1: DESCRIPTIVE STATISTICS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 1: DESCRIPTIVE STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        comprehensive_summary(df, col, target_col)\n",
    "    \n",
    "    # PHASE 2: DISTRIBUTION ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        test_normality(df[col], col)\n",
    "    \n",
    "    # PHASE 3: HYPOTHESIS TESTING\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3: HYPOTHESIS TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # T-tests for numeric variables\n",
    "    for col in numeric_cols:\n",
    "        churned = df[df# Statistical Approaches for Telco Customer Churn Analysis\n",
    "## A Comprehensive Dissertation\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Executive Summary](#1-executive-summary)\n",
    "2. [Introduction to Statistical Analysis in Churn Prediction](#2-introduction)\n",
    "3. [Descriptive Statistics](#3-descriptive-statistics)\n",
    "4. [Inferential Statistics](#4-inferential-statistics)\n",
    "5. [Hypothesis Testing](#5-hypothesis-testing)\n",
    "6. [Correlation and Association Analysis](#6-correlation-and-association-analysis)\n",
    "7. [Distribution Analysis](#7-distribution-analysis)\n",
    "8. [Time Series and Survival Analysis](#8-time-series-and-survival-analysis)\n",
    "9. [Multivariate Statistical Techniques](#9-multivariate-statistical-techniques)\n",
    "10. [Statistical Assumptions and Validation](#10-statistical-assumptions)\n",
    "11. [Practical Implementation Guide](#11-practical-implementation)\n",
    "12. [Case Studies and Applications](#12-case-studies)\n",
    "13. [Conclusion](#13-conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "This dissertation provides a comprehensive guide to statistical approaches essential for analyzing customer churn in telecommunications. We cover 15+ statistical methods, their theoretical foundations, practical applications, and implementation in Python.\n",
    "\n",
    "### Key Statistical Methods Covered:\n",
    "\n",
    "- **Descriptive Statistics**: Central tendency, dispersion, distribution shapes\n",
    "- **Hypothesis Testing**: t-tests, chi-square tests, ANOVA\n",
    "- **Correlation Analysis**: Pearson, Spearman, point-biserial\n",
    "- **Distribution Analysis**: Normality tests, Q-Q plots\n",
    "- **Survival Analysis**: Kaplan-Meier, Cox regression\n",
    "- **Multivariate Techniques**: PCA, factor analysis, cluster analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Statistical Analysis in Churn Prediction\n",
    "\n",
    "### 2.1 Why Statistics Matter in Churn Analysis\n",
    "\n",
    "Statistical analysis forms the foundation of data-driven churn prediction by:\n",
    "\n",
    "1. **Quantifying Relationships**: Measure strength between features and churn\n",
    "2. **Testing Hypotheses**: Validate business assumptions scientifically\n",
    "3. **Identifying Patterns**: Discover hidden trends in customer behavior\n",
    "4. **Ensuring Validity**: Verify model assumptions and results\n",
    "5. **Supporting Decisions**: Provide evidence-based recommendations\n",
    "\n",
    "### 2.2 The Statistical Analysis Pipeline\n",
    "\n",
    "```\n",
    "Data Collection ‚Üí Descriptive Statistics ‚Üí Exploratory Analysis ‚Üí\n",
    "Hypothesis Testing ‚Üí Model Building ‚Üí Validation ‚Üí Interpretation\n",
    "```\n",
    "\n",
    "### 2.3 Types of Variables in Churn Analysis\n",
    "\n",
    "| Variable Type | Examples | Statistical Methods |\n",
    "|---------------|----------|---------------------|\n",
    "| **Binary** | Churn (Yes/No), Gender | Chi-square, logistic regression |\n",
    "| **Nominal** | Contract type, Payment method | Chi-square, ANOVA |\n",
    "| **Ordinal** | Satisfaction ratings, Tenure groups | Mann-Whitney U, Kruskal-Wallis |\n",
    "| **Continuous** | Monthly charges, Tenure (months) | t-tests, correlation, regression |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the main features of your dataset.\n",
    "\n",
    "### 3.1 Measures of Central Tendency\n",
    "\n",
    "#### 3.1.1 Mean (Average)\n",
    "\n",
    "**Definition**: Sum of all values divided by count\n",
    "\n",
    "**Formula**: \n",
    "```\n",
    "Œº = (Œ£x) / n\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- Continuous variables (tenure, charges)\n",
    "- Normally distributed data\n",
    "- No extreme outliers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean\n",
    "mean_tenure = df['tenure'].mean()\n",
    "mean_monthly_charges = df['MonthlyCharges'].mean()\n",
    "\n",
    "# By churn status\n",
    "df.groupby('Churn')['tenure'].mean()\n",
    "\n",
    "# Interpretation\n",
    "print(f\"Average tenure: {mean_tenure:.2f} months\")\n",
    "print(f\"Churned customers avg tenure: {df[df['Churn']=='Yes']['tenure'].mean():.2f}\")\n",
    "print(f\"Retained customers avg tenure: {df[df['Churn']=='No']['tenure'].mean():.2f}\")\n",
    "```\n",
    "\n",
    "**Interpretation for Churn**:\n",
    "- If churned customers have lower mean tenure ‚Üí New customers at risk\n",
    "- If churned customers have higher mean charges ‚Üí Price sensitivity issue\n",
    "\n",
    "#### 3.1.2 Median\n",
    "\n",
    "**Definition**: Middle value when data is sorted\n",
    "\n",
    "**When to Use**:\n",
    "- Skewed distributions\n",
    "- Presence of outliers\n",
    "- Ordinal data\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate median\n",
    "median_tenure = df['tenure'].median()\n",
    "\n",
    "# Compare mean vs median to detect skewness\n",
    "print(f\"Mean tenure: {df['tenure'].mean():.2f}\")\n",
    "print(f\"Median tenure: {df['tenure'].median():.2f}\")\n",
    "\n",
    "# If mean > median: Right-skewed (long tail of high values)\n",
    "# If mean < median: Left-skewed (long tail of low values)\n",
    "```\n",
    "\n",
    "#### 3.1.3 Mode\n",
    "\n",
    "**Definition**: Most frequently occurring value\n",
    "\n",
    "**When to Use**:\n",
    "- Categorical variables\n",
    "- Identify most common category\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Most common contract type\n",
    "mode_contract = df['Contract'].mode()[0]\n",
    "print(f\"Most common contract: {mode_contract}\")\n",
    "\n",
    "# Mode by churn status\n",
    "df[df['Churn']=='Yes']['Contract'].mode()[0]\n",
    "df[df['Churn']=='No']['Contract'].mode()[0]\n",
    "```\n",
    "\n",
    "### 3.2 Measures of Dispersion\n",
    "\n",
    "#### 3.2.1 Standard Deviation\n",
    "\n",
    "**Definition**: Average distance from the mean\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "œÉ = sqrt(Œ£(x - Œº)¬≤ / n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate standard deviation\n",
    "std_charges = df['MonthlyCharges'].std()\n",
    "\n",
    "# Coefficient of Variation (CV) - standardized measure\n",
    "cv = (std_charges / df['MonthlyCharges'].mean()) * 100\n",
    "print(f\"CV: {cv:.2f}% - Shows relative variability\")\n",
    "\n",
    "# Compare variability between groups\n",
    "churned_std = df[df['Churn']=='Yes']['MonthlyCharges'].std()\n",
    "retained_std = df[df['Churn']=='No']['MonthlyCharges'].std()\n",
    "\n",
    "# Higher variability in churned group may indicate pricing issues\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Low std dev: Homogeneous customer base\n",
    "- High std dev: Diverse customer segments\n",
    "- Compare between churn groups to identify differences\n",
    "\n",
    "#### 3.2.2 Variance\n",
    "\n",
    "**Definition**: Square of standard deviation\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "variance = df['tenure'].var()\n",
    "\n",
    "# Variance explained in churn analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "for i, var in enumerate(explained_variance_ratio[:5]):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}%\")\n",
    "```\n",
    "\n",
    "#### 3.2.3 Range and Interquartile Range (IQR)\n",
    "\n",
    "**Range**: Maximum - Minimum\n",
    "\n",
    "**IQR**: Q3 - Q1 (middle 50% of data)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate range\n",
    "data_range = df['MonthlyCharges'].max() - df['MonthlyCharges'].min()\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df['MonthlyCharges'].quantile(0.25)\n",
    "Q3 = df['MonthlyCharges'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['MonthlyCharges'] < lower_bound) | \n",
    "              (df['MonthlyCharges'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 3.3 Measures of Shape\n",
    "\n",
    "#### 3.3.1 Skewness\n",
    "\n",
    "**Definition**: Measure of asymmetry in distribution\n",
    "\n",
    "**Interpretation**:\n",
    "- Skewness = 0: Perfectly symmetric\n",
    "- Skewness > 0: Right-skewed (tail on right)\n",
    "- Skewness < 0: Left-skewed (tail on left)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Calculate skewness\n",
    "tenure_skew = skew(df['tenure'])\n",
    "charges_skew = skew(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Tenure skewness: {tenure_skew:.3f}\")\n",
    "print(f\"Monthly charges skewness: {charges_skew:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(tenure_skew) < 0.5:\n",
    "    print(\"Tenure is approximately symmetric\")\n",
    "elif tenure_skew > 0:\n",
    "    print(\"Tenure is right-skewed (many new customers)\")\n",
    "else:\n",
    "    print(\"Tenure is left-skewed (many long-term customers)\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['tenure'], bins=30, edgecolor='black')\n",
    "axes[0].set_title(f'Tenure Distribution (Skewness: {tenure_skew:.2f})')\n",
    "axes[0].axvline(df['tenure'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].axvline(df['tenure'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df['MonthlyCharges'], bins=30, edgecolor='black')\n",
    "axes[1].set_title(f'Monthly Charges (Skewness: {charges_skew:.2f})')\n",
    "axes[1].axvline(df['MonthlyCharges'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].axvline(df['MonthlyCharges'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### 3.3.2 Kurtosis\n",
    "\n",
    "**Definition**: Measure of \"tailedness\" or extreme values\n",
    "\n",
    "**Interpretation**:\n",
    "- Kurtosis = 3: Normal distribution (mesokurtic)\n",
    "- Kurtosis > 3: Heavy tails, more outliers (leptokurtic)\n",
    "- Kurtosis < 3: Light tails, fewer outliers (platykurtic)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate excess kurtosis (subtract 3 for comparison to normal)\n",
    "tenure_kurt = kurtosis(df['tenure'], fisher=True)  # fisher=True gives excess kurtosis\n",
    "\n",
    "print(f\"Tenure excess kurtosis: {tenure_kurt:.3f}\")\n",
    "\n",
    "if tenure_kurt > 0:\n",
    "    print(\"‚Üí More extreme values than normal distribution\")\n",
    "    print(\"‚Üí May need robust statistical methods\")\n",
    "elif tenure_kurt < 0:\n",
    "    print(\"‚Üí Fewer extreme values than normal distribution\")\n",
    "    print(\"‚Üí More uniform distribution\")\n",
    "```\n",
    "\n",
    "### 3.4 Comprehensive Descriptive Statistics Summary\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_summary(df, column, churn_col='Churn'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive descriptive statistics for a column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE STATISTICS: {column}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"Overall Statistics:\")\n",
    "    print(f\"  Count: {df[column].count()}\")\n",
    "    print(f\"  Mean: {df[column].mean():.2f}\")\n",
    "    print(f\"  Median: {df[column].median():.2f}\")\n",
    "    print(f\"  Mode: {df[column].mode()[0] if len(df[column].mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Std Dev: {df[column].std():.2f}\")\n",
    "    print(f\"  Variance: {df[column].var():.2f}\")\n",
    "    print(f\"  Min: {df[column].min():.2f}\")\n",
    "    print(f\"  Max: {df[column].max():.2f}\")\n",
    "    print(f\"  Range: {df[column].max() - df[column].min():.2f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in [25, 50, 75, 90, 95, 99]:\n",
    "        print(f\"  {p}th: {df[column].quantile(p/100):.2f}\")\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"\\nDistribution Shape:\")\n",
    "    print(f\"  Skewness: {skew(df[column].dropna()):.3f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis(df[column].dropna(), fisher=True):.3f}\")\n",
    "    \n",
    "    # By churn status\n",
    "    print(f\"\\nBy Churn Status:\")\n",
    "    for churn_val in df[churn_col].unique():\n",
    "        subset = df[df[churn_col]==churn_val][column]\n",
    "        print(f\"  {churn_val}:\")\n",
    "        print(f\"    Mean: {subset.mean():.2f}\")\n",
    "        print(f\"    Median: {subset.median():.2f}\")\n",
    "        print(f\"    Std Dev: {subset.std():.2f}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_pct = (df[column].isnull().sum() / len(df)) * 100\n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"  Missing: {df[column].isnull().sum()} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Usage\n",
    "comprehensive_summary(df, 'tenure')\n",
    "comprehensive_summary(df, 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inferential Statistics\n",
    "\n",
    "Inferential statistics allow us to make predictions and inferences about a population based on sample data.\n",
    "\n",
    "### 4.1 Confidence Intervals\n",
    "\n",
    "**Definition**: Range of values that likely contains the true population parameter\n",
    "\n",
    "**Formula for Mean**:\n",
    "```\n",
    "CI = xÃÑ ¬± (t * (s / sqrt(n)))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- xÃÑ = sample mean\n",
    "- t = t-value from t-distribution\n",
    "- s = sample standard deviation\n",
    "- n = sample size\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for mean.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  # Standard error of mean\n",
    "    margin_error = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    ci_lower = mean - margin_error\n",
    "    ci_upper = mean + margin_error\n",
    "    \n",
    "    return mean, ci_lower, ci_upper\n",
    "\n",
    "# Example: Confidence interval for average tenure\n",
    "churned_tenure = df[df['Churn']=='Yes']['tenure']\n",
    "retained_tenure = df[df['Churn']=='No']['tenure']\n",
    "\n",
    "mean_c, lower_c, upper_c = calculate_confidence_interval(churned_tenure)\n",
    "mean_r, lower_r, upper_r = calculate_confidence_interval(retained_tenure)\n",
    "\n",
    "print(\"Average Tenure with 95% Confidence Intervals:\")\n",
    "print(f\"Churned: {mean_c:.2f} months [{lower_c:.2f}, {upper_c:.2f}]\")\n",
    "print(f\"Retained: {mean_r:.2f} months [{lower_r:.2f}, {upper_r:.2f}]\")\n",
    "\n",
    "# Interpretation\n",
    "if upper_c < lower_r:\n",
    "    print(\"‚Üí Churned customers have significantly lower tenure (no overlap)\")\n",
    "elif lower_c > upper_r:\n",
    "    print(\"‚Üí Churned customers have significantly higher tenure\")\n",
    "else:\n",
    "    print(\"‚Üí Confidence intervals overlap - difference may not be significant\")\n",
    "```\n",
    "\n",
    "**Business Application**:\n",
    "- Estimate true average monthly revenue from customers\n",
    "- Predict churn rate with confidence bounds\n",
    "- Compare segments with statistical rigor\n",
    "\n",
    "### 4.2 Standard Error\n",
    "\n",
    "**Definition**: Standard deviation of the sampling distribution\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "SE = œÉ / sqrt(n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Calculate standard error for monthly charges\n",
    "se_charges = sem(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Standard Error of Monthly Charges: ${se_charges:.2f}\")\n",
    "print(f\"This means our sample mean is accurate within ¬±${se_charges:.2f}\")\n",
    "\n",
    "# Compare standard errors\n",
    "se_churned = sem(df[df['Churn']=='Yes']['MonthlyCharges'])\n",
    "se_retained = sem(df[df['Churn']=='No']['MonthlyCharges'])\n",
    "\n",
    "print(f\"\\nSE for churned customers: ${se_churned:.2f}\")\n",
    "print(f\"SE for retained customers: ${se_retained:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is crucial for making data-driven decisions about churn drivers.\n",
    "\n",
    "### 5.1 Framework for Hypothesis Testing\n",
    "\n",
    "**Standard Process**:\n",
    "\n",
    "1. **State Hypotheses**:\n",
    "   - H‚ÇÄ (Null): No difference/relationship exists\n",
    "   - H‚ÇÅ (Alternative): Difference/relationship exists\n",
    "\n",
    "2. **Choose Significance Level (Œ±)**:\n",
    "   - Common: Œ± = 0.05 (5% chance of Type I error)\n",
    "\n",
    "3. **Calculate Test Statistic**\n",
    "\n",
    "4. **Find p-value**\n",
    "\n",
    "5. **Make Decision**:\n",
    "   - If p-value < Œ±: Reject H‚ÇÄ (significant result)\n",
    "   - If p-value ‚â• Œ±: Fail to reject H‚ÇÄ\n",
    "\n",
    "### 5.2 Independent Samples t-Test\n",
    "\n",
    "**Purpose**: Compare means of two independent groups\n",
    "\n",
    "**Assumptions**:\n",
    "- Both groups are normally distributed\n",
    "- Equal variances (or use Welch's t-test)\n",
    "- Independent observations\n",
    "\n",
    "**When to Use in Churn Analysis**:\n",
    "- Compare tenure between churned vs retained\n",
    "- Compare charges between customer segments\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind, levene, shapiro\n",
    "\n",
    "def perform_t_test(group1, group2, group1_name, group2_name, \n",
    "                   variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform comprehensive independent t-test with assumption checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"INDEPENDENT T-TEST: {variable_name}\")\n",
    "    print(f\"Comparing {group1_name} vs {group2_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 1. Check normality assumption\n",
    "    print(\"1. Normality Tests (Shapiro-Wilk):\")\n",
    "    _, p_norm1 = shapiro(group1.sample(min(5000, len(group1))))  # Sample for large datasets\n",
    "    _, p_norm2 = shapiro(group2.sample(min(5000, len(group2))))\n",
    "    \n",
    "    print(f\"   {group1_name}: p-value = {p_norm1:.4f}\")\n",
    "    print(f\"   {group2_name}: p-value = {p_norm2:.4f}\")\n",
    "    \n",
    "    if p_norm1 > 0.05 and p_norm2 > 0.05:\n",
    "        print(\"   ‚úì Both groups appear normally distributed\")\n",
    "        normality_met = True\n",
    "    else:\n",
    "        print(\"   ‚ö† At least one group deviates from normality\")\n",
    "        print(\"   ‚Üí Consider using Mann-Whitney U test instead\")\n",
    "        normality_met = False\n",
    "    \n",
    "    # 2. Check equal variance assumption\n",
    "    print(\"\\n2. Equal Variance Test (Levene's Test):\")\n",
    "    _, p_var = levene(group1, group2)\n",
    "    print(f\"   p-value = {p_var:.4f}\")\n",
    "    \n",
    "    if p_var > 0.05:\n",
    "        print(\"   ‚úì Variances are equal\")\n",
    "        equal_var = True\n",
    "    else:\n",
    "        print(\"   ‚ö† Variances are unequal\")\n",
    "        print(\"   ‚Üí Using Welch's t-test (doesn't assume equal variance)\")\n",
    "        equal_var = False\n",
    "    \n",
    "    # 3. Perform t-test\n",
    "    print(\"\\n3. T-Test Results:\")\n",
    "    t_stat, p_value = ttest_ind(group1, group2, equal_var=equal_var)\n",
    "    \n",
    "    print(f\"   t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Significance level: {alpha}\")\n",
    "    \n",
    "    # 4. Calculate effect size (Cohen's d)\n",
    "    mean1, mean2 = group1.mean(), group2.mean()\n",
    "    std1, std2 = group1.std(), group2.std()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1-1)*std1**2 + (n2-1)*std2**2) / (n1+n2-2))\n",
    "    cohens_d = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    print(f\"\\n4. Effect Size (Cohen's d): {cohens_d:.4f}\")\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"   Effect size is {effect}\")\n",
    "    \n",
    "    # 5. Interpretation\n",
    "    print(\"\\n5. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Reject null hypothesis\")\n",
    "        print(f\"   ‚Üí {group1_name} and {group2_name} have different {variable_name}\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí Fail to reject null hypothesis\")\n",
    "        print(f\"   ‚Üí Insufficient evidence of difference\")\n",
    "    \n",
    "    # 6. Descriptive statistics\n",
    "    print(\"\\n6. Descriptive Statistics:\")\n",
    "    print(f\"   {group1_name}: Mean = {mean1:.2f}, SD = {std1:.2f}, n = {n1}\")\n",
    "    print(f\"   {group2_name}: Mean = {mean2:.2f}, SD = {std2:.2f}, n = {n2}\")\n",
    "    print(f\"   Mean Difference: {abs(mean1 - mean2):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare tenure between churned and retained customers\n",
    "churned = df[df['Churn']=='Yes']['tenure'].dropna()\n",
    "retained = df[df['Churn']=='No']['tenure'].dropna()\n",
    "\n",
    "results = perform_t_test(churned, retained, \n",
    "                         'Churned Customers', 'Retained Customers',\n",
    "                         'Tenure (months)')\n",
    "```\n",
    "\n",
    "**Business Interpretation**:\n",
    "\n",
    "```python\n",
    "# If significant difference found:\n",
    "if results['significant']:\n",
    "    print(\"\\nüìä BUSINESS INSIGHT:\")\n",
    "    print(\"Churned and retained customers have significantly different tenure.\")\n",
    "    print(\"‚Üí Action: Focus retention efforts on specific tenure segments\")\n",
    "    print(\"‚Üí Investigate: What happens at critical tenure milestones?\")\n",
    "```\n",
    "\n",
    "### 5.3 Paired Samples t-Test\n",
    "\n",
    "**Purpose**: Compare means of same group at two time points\n",
    "\n",
    "**When to Use**:\n",
    "- Before/after retention campaign\n",
    "- Monthly charges across time periods\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example: Compare customer satisfaction before and after intervention\n",
    "# (hypothetical data)\n",
    "satisfaction_before = df['satisfaction_before']\n",
    "satisfaction_after = df['satisfaction_after']\n",
    "\n",
    "t_stat, p_value = ttest_rel(satisfaction_before, satisfaction_after)\n",
    "\n",
    "print(f\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"‚Üí Intervention had significant effect on satisfaction\")\n",
    "```\n",
    "\n",
    "### 5.4 Chi-Square Test for Independence\n",
    "\n",
    "**Purpose**: Test relationship between two categorical variables\n",
    "\n",
    "**When to Use in Churn Analysis**:\n",
    "- Relationship between Contract type and Churn\n",
    "- Relationship between Payment method and Churn\n",
    "- Any categorical variable vs Churn\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform chi-square test of independence.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CHI-SQUARE TEST: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(df[var1], df[var2])\n",
    "    \n",
    "    print(\"1. Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "    print()\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(\"2. Test Results:\")\n",
    "    print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Degrees of freedom: {dof}\")\n",
    "    \n",
    "    # Check expected frequencies assumption\n",
    "    print(\"\\n3. Assumption Check:\")\n",
    "    print(\"   Expected frequencies (should all be ‚â• 5):\")\n",
    "    print(pd.DataFrame(expected, \n",
    "                       index=contingency_table.index,\n",
    "                       columns=contingency_table.columns).round(2))\n",
    "    \n",
    "    min_expected = expected.min()\n",
    "    if min_expected >= 5:\n",
    "        print(f\"   ‚úì All expected frequencies ‚â• 5 (min: {min_expected:.2f})\")\n",
    "        print(\"   ‚úì Chi-square test is valid\")\n",
    "    else:\n",
    "        print(f\"   ‚ö† Some expected frequencies < 5 (min: {min_expected:.2f})\")\n",
    "        print(\"   ‚ö† Consider Fisher's exact test or combine categories\")\n",
    "    \n",
    "    # Calculate effect size (Cram√©r's V)\n",
    "    n = contingency_table.sum().sum()\n",
    "    min_dim = min(contingency_table.shape[0]-1, contingency_table.shape[1]-1)\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "    \n",
    "    print(f\"\\n4. Effect Size (Cram√©r's V): {cramers_v:.4f}\")\n",
    "    if cramers_v < 0.1:\n",
    "        effect = \"negligible\"\n",
    "    elif cramers_v < 0.3:\n",
    "        effect = \"small\"\n",
    "    elif cramers_v < 0.5:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"   Effect size is {effect}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n5. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT ASSOCIATION (p < {alpha})\")\n",
    "        print(f\"   ‚Üí {var1} and {var2} are related\")\n",
    "        print(f\"   ‚Üí Variables are NOT independent\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT ASSOCIATION (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí Insufficient evidence of relationship\")\n",
    "    \n",
    "    # Calculate percentages for interpretation\n",
    "    print(\"\\n6. Percentage Breakdown:\")\n",
    "    pct_table = pd.crosstab(df[var1], df[var2], normalize='index') * 100\n",
    "    print(pct_table.round(2))\n",
    "    \n",
    "    return {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'cramers_v': cramers_v,\n",
    "        'significant': p_value < alpha,\n",
    "        'contingency_table': contingency_table\n",
    "    }\n",
    "\n",
    "# Example: Test relationship between Contract and Churn\n",
    "result = chi_square_test(df, 'Contract', 'Churn')\n",
    "\n",
    "# Business interpretation\n",
    "if result['significant']:\n",
    "    print(\"\\nüìä BUSINESS INSIGHT:\")\n",
    "    print(\"Contract type is significantly related to churn.\")\n",
    "    print(\"‚Üí Action: Analyze churn rates by contract type\")\n",
    "    print(\"‚Üí Strategy: Incentivize longer contracts\")\n",
    "```\n",
    "\n",
    "### 5.5 ANOVA (Analysis of Variance)\n",
    "\n",
    "**Purpose**: Compare means across 3+ groups\n",
    "\n",
    "**When to Use**:\n",
    "- Compare charges across multiple contract types\n",
    "- Compare tenure across service tiers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "def perform_anova(df, group_var, numeric_var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform one-way ANOVA with post-hoc analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ONE-WAY ANOVA: {numeric_var} across {group_var}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Get groups\n",
    "    groups = df[group_var].unique()\n",
    "    group_data = [df[df[group_var]==g][numeric_var].dropna() for g in groups]\n",
    "    \n",
    "    # 1. Descriptive statistics\n",
    "    print(\"1. Descriptive Statistics by Group:\")\n",
    "    for g, data in zip(groups, group_data):\n",
    "        print(f\"   {g}: Mean={data.mean():.2f}, SD={data.std():.2f}, n={len(data)}\")\n",
    "    \n",
    "    # 2. Perform ANOVA\n",
    "    print(\"\\n2. ANOVA Results:\")\n",
    "    f_stat, p_value = f_oneway(*group_data)\n",
    "    \n",
    "    print(f\"   F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # 3. Effect size (eta-squared)\n",
    "    # Calculate between-group and total sum of squares\n",
    "    grand_mean = df[numeric_var].mean()\n",
    "    ss_between = sum([len(data) * (data.mean() - grand_mean)**2 \n",
    "                      for data in group_data])\n",
    "    ss_total = sum([(x - grand_mean)**2 for data in group_data for x in data])\n",
    "    eta_squared = ss_between / ss_total\n",
    "    \n",
    "    print(f\"\\n3. Effect Size (Œ∑¬≤): {eta_squared:.4f}\")\n",
    "    print(f\"   {eta_squared*100:.2f}% of variance explained by {group_var}\")\n",
    "    \n",
    "    # 4. Interpretation\n",
    "    print(\"\\n4. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí At least one group differs significantly\")\n",
    "        print(f\"   ‚Üí Recommend post-hoc tests (Tukey HSD)\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí All groups have similar means\")\n",
    "    \n",
    "    # 5. Post-hoc test (Tukey HSD) if significant\n",
    "    if p_value < alpha:\n",
    "        from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "        \n",
    "        print(\"\\n5. Post-Hoc Analysis (Tukey HSD):\")\n",
    "        tukey = pairwise_tukeyhsd(df[numeric_var], df[group_var], alpha=alpha)\n",
    "        print(tukey)\n",
    "    \n",
    "    return {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value,\n",
    "        'eta_squared': eta_squared,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare monthly charges across contract types\n",
    "result = perform_anova(df, 'Contract', 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "### 5.6 Mann-Whitney U Test (Non-Parametric Alternative)\n",
    "\n",
    "**Purpose**: Compare distributions of two groups without normality assumption\n",
    "\n",
    "**When to Use**:\n",
    "- Data is not normally distributed\n",
    "- Ordinal data\n",
    "- Small sample sizes\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def mann_whitney_test(group1, group2, group1_name, group2_name, \n",
    "                      variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Mann-Whitney U test (non-parametric alternative to t-test).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MANN-WHITNEY U TEST: {variable_name}\")\n",
    "    print(f\"Comparing {group1_name} vs {group2_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Perform test\n",
    "    u_stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    \n",
    "    print(\"1. Test Results:\")\n",
    "    print(f\"   U-statistic: {u_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Calculate effect size (rank-biserial correlation)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    r = 1 - (2*u_stat) / (n1 * n2)  # rank-biserial correlation\n",
    "    \n",
    "    print(f\"\\n2. Effect Size (rank-biserial r): {r:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n3. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Distributions differ significantly\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "    \n",
    "    # Medians for interpretation\n",
    "    print(\"\\n4. Median Comparison:\")\n",
    "    print(f\"   {group1_name}: Median = {group1.median():.2f}\")\n",
    "    print(f\"   {group2_name}: Median = {group2.median():.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'u_statistic': u_stat,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': r,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: When data is not normally distributed\n",
    "churned_charges = df[df['Churn']=='Yes']['MonthlyCharges'].dropna()\n",
    "retained_charges = df[df['Churn']=='No']['MonthlyCharges'].dropna()\n",
    "\n",
    "result = mann_whitney_test(churned_charges, retained_charges,\n",
    "                           'Churned', 'Retained', 'Monthly Charges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Correlation and Association Analysis\n",
    "\n",
    "Understanding relationships between variables is crucial for feature selection and model building.\n",
    "\n",
    "### 6.1 Pearson Correlation\n",
    "\n",
    "**Purpose**: Measure linear relationship between two continuous variables\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "r = Œ£((x - xÃÑ)(y - »≥)) / sqrt(Œ£(x - xÃÑ)¬≤ √ó Œ£(y - »≥)¬≤)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- r = 1: Perfect positive correlation\n",
    "- r = 0: No linear correlation\n",
    "- r = -1: Perfect negative correlation\n",
    "- |r| < 0.3: Weak\n",
    "- 0.3 ‚â§ |r| < 0.7: Moderate\n",
    "- |r| ‚â• 0.7: Strong\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson_correlation_analysis(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive Pearson correlation analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PEARSON CORRELATION: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    data = df[[var1, var2]].dropna()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r, p_value = pearsonr(data[var1], data[var2])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Pearson r: {r:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Interpret strength\n",
    "    if abs(r) < 0.3:\n",
    "        strength = \"weak\"\n",
    "    elif abs(r) < 0.7:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    \n",
    "    direction = \"positive\" if r > 0 else \"negative\"\n",
    "    \n",
    "    print(f\"   Strength: {strength} {direction} correlation\")\n",
    "    \n",
    "    # Calculate coefficient of determination\n",
    "    r_squared = r ** 2\n",
    "    print(f\"\\n2. Coefficient of Determination (r¬≤): {r_squared:.4f}\")\n",
    "    print(f\"   {r_squared*100:.2f}% of variance in {var2} explained by {var1}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"\\n3. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT CORRELATION (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Relationship is statistically significant\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT CORRELATION (p ‚â• {alpha})\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data[var1], data[var2], alpha=0.5)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(data[var1], data[var2], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(data[var1], p(data[var1]), \"r--\", linewidth=2, label='Regression line')\n",
    "    \n",
    "    plt.xlabel(var1, fontsize=12)\n",
    "    plt.ylabel(var2, fontsize=12)\n",
    "    plt.title(f'{var1} vs {var2}\\n(r = {r:.3f}, p = {p_value:.4f})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {'r': r, 'p_value': p_value, 'r_squared': r_squared}\n",
    "\n",
    "# Example: Correlation between tenure and total charges\n",
    "result = pearson_correlation_analysis(df, 'tenure', 'TotalCharges')\n",
    "```\n",
    "\n",
    "### 6.2 Spearman Correlation\n",
    "\n",
    "**Purpose**: Measure monotonic relationship (not necessarily linear)\n",
    "\n",
    "**When to Use**:\n",
    "- Ordinal variables\n",
    "- Non-linear relationships\n",
    "- Non-normal distributions\n",
    "- Outliers present\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def spearman_correlation_analysis(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Spearman rank correlation analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SPEARMAN CORRELATION: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    data = df[[var1, var2]].dropna()\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    rho, p_value = spearmanr(data[var1], data[var2])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Spearman œÅ (rho): {rho:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Compare with Pearson\n",
    "    r_pearson, _ = pearsonr(data[var1], data[var2])\n",
    "    print(f\"\\n2. Comparison:\")\n",
    "    print(f\"   Pearson r:  {r_pearson:.4f}\")\n",
    "    print(f\"   Spearman œÅ: {rho:.4f}\")\n",
    "    print(f\"   Difference: {abs(r_pearson - rho):.4f}\")\n",
    "    \n",
    "    if abs(r_pearson - rho) > 0.1:\n",
    "        print(\"   ‚ö† Large difference suggests non-linear relationship\")\n",
    "    else:\n",
    "        print(\"   ‚úì Similar values suggest linear relationship\")\n",
    "    \n",
    "    return {'rho': rho, 'p_value': p_value}\n",
    "\n",
    "# Example\n",
    "result = spearman_correlation_analysis(df, 'tenure', 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "### 6.3 Point-Biserial Correlation\n",
    "\n",
    "**Purpose**: Correlation between continuous and binary variable\n",
    "\n",
    "**When to Use**:\n",
    "- Relationship between numeric variable and Churn (binary)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "def point_biserial_analysis(df, continuous_var, binary_var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Point-biserial correlation for continuous vs binary variable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"POINT-BISERIAL CORRELATION\")\n",
    "    print(f\"{continuous_var} vs {binary_var}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Ensure binary variable is 0/1\n",
    "    data = df[[continuous_var, binary_var]].dropna()\n",
    "    if data[binary_var].dtype == 'object':\n",
    "        binary_map = {data[binary_var].unique()[0]: 0,\n",
    "                     data[binary_var].unique()[1]: 1}\n",
    "        data[binary_var] = data[binary_var].map(binary_map)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r_pb, p_value = pointbiserialr(data[binary_var], data[continuous_var])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Point-biserial r: {r_pb:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n2. Interpretation:\")\n",
    "    if r_pb > 0:\n",
    "        print(f\"   Positive correlation: Higher {continuous_var} ‚Üí More likely {binary_var}=1\")\n",
    "    else:\n",
    "        print(f\"   Negative correlation: Higher {continuous_var} ‚Üí More likely {binary_var}=0\")\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        print(f\"\\n3. Conclusion: SIGNIFICANT relationship (p < {alpha})\")\n",
    "    else:\n",
    "        print(f\"\\n3. Conclusion: NO significant relationship (p ‚â• {alpha})\")\n",
    "    \n",
    "    return {'r_pb': r_pb, 'p_value': p_value}\n",
    "\n",
    "# Example: Tenure vs Churn\n",
    "result = point_biserial_analysis(df, 'tenure', 'Churn')\n",
    "```\n",
    "\n",
    "### 6.4 Correlation Matrix and Heatmap\n",
    "\n",
    "**Purpose**: Visualize all pairwise correlations\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_correlation_analysis(df, method='pearson'):\n",
    "    \"\"\"\n",
    "    Create comprehensive correlation matrix with visualization.\n",
    "    \"\"\"\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    if method == 'pearson':\n",
    "        corr_matrix = numeric_df.corr()\n",
    "    elif method == 'spearman':\n",
    "        corr_matrix = numeric_df.corr(method='spearman')\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, fmt='.2f', square=True, linewidths=1,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    \n",
    "    plt.title(f'{method.capitalize()} Correlation Matrix', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strongest correlations with target (if Churn exists)\n",
    "    if 'Churn' in corr_matrix.columns:\n",
    "        print(\"\\nStrongest Correlations with Churn:\")\n",
    "        churn_corr = corr_matrix['Churn'].abs().sort_values(ascending=False)\n",
    "        print(churn_corr[1:11])  # Top 10, excluding Churn itself\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Usage\n",
    "corr_matrix = comprehensive_correlation_analysis(df, method='pearson')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Distribution Analysis\n",
    "\n",
    "Understanding data distributions is critical for choosing appropriate statistical tests and models.\n",
    "\n",
    "### 7.1 Normality Tests\n",
    "\n",
    "#### 7.1.1 Shapiro-Wilk Test\n",
    "\n",
    "**Purpose**: Test if data comes from normal distribution\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def test_normality(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive normality testing.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NORMALITY TEST: {variable_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    stat, p_value = shapiro(data.sample(min(5000, len(data))))  # Sample for large datasets\n",
    "    \n",
    "    print(\"1. Shapiro-Wilk Test:\")\n",
    "    print(f\"   Statistic: {stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"   ‚úì Data appears normally distributed (p > {alpha})\")\n",
    "        normal = True\n",
    "    else:\n",
    "        print(f\"   ‚úó Data deviates from normal distribution (p ‚â§ {alpha})\")\n",
    "        normal = False\n",
    "    \n",
    "    # Visual checks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual checks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram with normal curve overlay\n",
    "axes[0].hist(data, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "mu, sigma = data.mean(), data.std()\n",
    "x = np.linspace(data.min(), data.max(), 100)\n",
    "axes[0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal distribution')\n",
    "axes[0].set_title('Histogram with Normal Curve', fontweight='bold')\n",
    "axes[0].set_xlabel(variable_name)\n",
    "axes[0].legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(data, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[2].boxplot(data, vert=True)\n",
    "axes[2].set_title('Box Plot', fontweight='bold')\n",
    "axes[2].set_ylabel(variable_name)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n2. Visual Interpretation:\")\n",
    "print(\"   - Histogram: Should resemble bell curve\")\n",
    "print(\"   - Q-Q Plot: Points should fall on diagonal line\")\n",
    "print(\"   - Box Plot: Should be roughly symmetric\")\n",
    "\n",
    "return {'statistic': stat, 'p_value': p_value, 'normal': normal}\n",
    "\n",
    "# Example\n",
    "result = test_normality(df['tenure'], 'Tenure (months)')\n",
    "result = test_normality(df['MonthlyCharges'], 'Monthly Charges ($)')\n",
    "```\n",
    "\n",
    "#### 7.1.2 Kolmogorov-Smirnov Test\n",
    "\n",
    "**Purpose**: Alternative normality test, better for larger samples\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import kstest\n",
    "\n",
    "def ks_normality_test(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Kolmogorov-Smirnov test for normality.\n",
    "    \"\"\"\n",
    "    # Standardize data\n",
    "    data_std = (data - data.mean()) / data.std()\n",
    "    \n",
    "    # Perform KS test\n",
    "    stat, p_value = kstest(data_std, 'norm')\n",
    "    \n",
    "    print(f\"\\nKolmogorov-Smirnov Test for {variable_name}:\")\n",
    "    print(f\"  Statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"  ‚úì Data appears normally distributed\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Data deviates from normality\")\n",
    "    \n",
    "    return stat, p_value\n",
    "\n",
    "ks_normality_test(df['tenure'], 'Tenure')\n",
    "```\n",
    "\n",
    "### 7.2 Skewness and Kurtosis Tests\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import skewtest, kurtosistest\n",
    "\n",
    "def distribution_shape_tests(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test skewness and kurtosis significance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DISTRIBUTION SHAPE TESTS: {variable_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Calculate skewness\n",
    "    skew_val = skew(data)\n",
    "    skew_stat, skew_p = skewtest(data)\n",
    "    \n",
    "    print(\"1. Skewness Test:\")\n",
    "    print(f\"   Skewness: {skew_val:.4f}\")\n",
    "    print(f\"   Test statistic: {skew_stat:.4f}\")\n",
    "    print(f\"   p-value: {skew_p:.4f}\")\n",
    "    \n",
    "    if skew_p < alpha:\n",
    "        if skew_val > 0:\n",
    "            print(\"   ‚úì Significantly right-skewed\")\n",
    "        else:\n",
    "            print(\"   ‚úì Significantly left-skewed\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Skewness not significantly different from 0\")\n",
    "    \n",
    "    # Calculate kurtosis\n",
    "    kurt_val = kurtosis(data, fisher=True)\n",
    "    kurt_stat, kurt_p = kurtosistest(data)\n",
    "    \n",
    "    print(\"\\n2. Kurtosis Test:\")\n",
    "    print(f\"   Excess kurtosis: {kurt_val:.4f}\")\n",
    "    print(f\"   Test statistic: {kurt_stat:.4f}\")\n",
    "    print(f\"   p-value: {kurt_p:.4f}\")\n",
    "    \n",
    "    if kurt_p < alpha:\n",
    "        if kurt_val > 0:\n",
    "            print(\"   ‚úì Significantly leptokurtic (heavy tails)\")\n",
    "        else:\n",
    "            print(\"   ‚úì Significantly platykurtic (light tails)\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Kurtosis not significantly different from normal\")\n",
    "\n",
    "distribution_shape_tests(df['tenure'], 'Tenure')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Time Series and Survival Analysis\n",
    "\n",
    "### 8.1 Survival Analysis (Kaplan-Meier)\n",
    "\n",
    "**Purpose**: Analyze time until event (churn) occurs\n",
    "\n",
    "**When to Use**:\n",
    "- Understand customer lifetime\n",
    "- Identify critical time periods for churn\n",
    "- Compare survival across customer segments\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "def survival_analysis(df, duration_col, event_col, group_col=None):\n",
    "    \"\"\"\n",
    "    Comprehensive survival analysis for churn.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SURVIVAL ANALYSIS (KAPLAN-MEIER)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize Kaplan-Meier fitter\n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    if group_col is None:\n",
    "        # Overall survival curve\n",
    "        kmf.fit(df[duration_col], df[event_col], label='All Customers')\n",
    "        \n",
    "        print(\"Overall Survival Statistics:\")\n",
    "        print(f\"  Median survival time: {kmf.median_survival_time_:.2f} months\")\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        kmf.plot_survival_function()\n",
    "        plt.title('Customer Survival Curve', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Tenure (months)')\n",
    "        plt.ylabel('Survival Probability')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Survival by groups\n",
    "        print(f\"Survival Analysis by {group_col}:\\n\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        groups = df[group_col].unique()\n",
    "        group_data = []\n",
    "        \n",
    "        for group in groups:\n",
    "            mask = df[group_col] == group\n",
    "            group_subset = df[mask]\n",
    "            \n",
    "            kmf.fit(group_subset[duration_col], \n",
    "                   group_subset[event_col],\n",
    "                   label=str(group))\n",
    "            \n",
    "            kmf.plot_survival_function()\n",
    "            \n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Median survival: {kmf.median_survival_time_:.2f} months\")\n",
    "            print(f\"    1-year survival: {kmf.survival_function_at_times(12).values[0]:.2%}\")\n",
    "            print(f\"    2-year survival: {kmf.survival_function_at_times(24).values[0]:.2%}\\n\")\n",
    "            \n",
    "            group_data.append((group_subset[duration_col], group_subset[event_col]))\n",
    "        \n",
    "        plt.title(f'Survival Curves by {group_col}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Tenure (months)')\n",
    "        plt.ylabel('Survival Probability')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Log-rank test if 2 groups\n",
    "        if len(groups) == 2:\n",
    "            result = logrank_test(group_data[0][0], group_data[1][0],\n",
    "                                 group_data[0][1], group_data[1][1])\n",
    "            \n",
    "            print(f\"Log-Rank Test:\")\n",
    "            print(f\"  Test statistic: {result.test_statistic:.4f}\")\n",
    "            print(f\"  p-value: {result.p_value:.4f}\")\n",
    "            \n",
    "            if result.p_value < 0.05:\n",
    "                print(f\"  ‚úì Survival curves are significantly different\")\n",
    "            else:\n",
    "                print(f\"  ‚úó No significant difference between groups\")\n",
    "\n",
    "# Prepare data (tenure as duration, Churn as event)\n",
    "df_survival = df.copy()\n",
    "df_survival['Churn_binary'] = (df_survival['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Overall survival\n",
    "survival_analysis(df_survival, 'tenure', 'Churn_binary')\n",
    "\n",
    "# Survival by contract type\n",
    "survival_analysis(df_survival, 'tenure', 'Churn_binary', 'Contract')\n",
    "```\n",
    "\n",
    "**Business Interpretation**:\n",
    "\n",
    "```python\n",
    "print(\"\\nüìä BUSINESS INSIGHTS FROM SURVIVAL ANALYSIS:\")\n",
    "print(\"1. Median survival time tells us typical customer lifetime\")\n",
    "print(\"2. Steep drops indicate critical churn periods\")\n",
    "print(\"3. Compare curves across segments to prioritize interventions\")\n",
    "print(\"4. 1-year survival rate = retention rate at 12 months\")\n",
    "```\n",
    "\n",
    "### 8.2 Cox Proportional Hazards Model\n",
    "\n",
    "**Purpose**: Identify factors affecting time to churn\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "def cox_regression_analysis(df, duration_col, event_col, covariates):\n",
    "    \"\"\"\n",
    "    Cox proportional hazards model for churn.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COX PROPORTIONAL HAZARDS MODEL\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    analysis_df = df[[duration_col, event_col] + covariates].dropna()\n",
    "    \n",
    "    # Fit model\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(analysis_df, duration_col=duration_col, event_col=event_col)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Model Summary:\")\n",
    "    print(cph.summary)\n",
    "    \n",
    "    print(\"\\n\\nInterpretation of Hazard Ratios:\")\n",
    "    print(\"(exp(coef) = Hazard Ratio)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for var in covariates:\n",
    "        coef = cph.params_[var]\n",
    "        hr = np.exp(coef)\n",
    "        p_val = cph.summary.loc[var, 'p']\n",
    "        \n",
    "        print(f\"\\n{var}:\")\n",
    "        print(f\"  Hazard Ratio: {hr:.4f}\")\n",
    "        \n",
    "        if hr > 1:\n",
    "            print(f\"  ‚Üí Increases churn risk by {(hr-1)*100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  ‚Üí Decreases churn risk by {(1-hr)*100:.1f}%\")\n",
    "        \n",
    "        if p_val < 0.05:\n",
    "            print(f\"  ‚úì Statistically significant (p={p_val:.4f})\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Not significant (p={p_val:.4f})\")\n",
    "    \n",
    "    # Plot\n",
    "    cph.plot()\n",
    "    plt.title('Hazard Ratios with 95% CI', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cph\n",
    "\n",
    "# Example with numeric predictors\n",
    "covariates = ['MonthlyCharges', 'SeniorCitizen', 'Partner', 'Dependents']\n",
    "cph_model = cox_regression_analysis(df_survival, 'tenure', 'Churn_binary', covariates)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Multivariate Statistical Techniques\n",
    "\n",
    "### 9.1 Principal Component Analysis (PCA)\n",
    "\n",
    "**Purpose**: Reduce dimensionality while preserving variance\n",
    "\n",
    "**When to Use**:\n",
    "- Many correlated features\n",
    "- Visualization of high-dimensional data\n",
    "- Feature extraction\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def comprehensive_pca_analysis(df, n_components=None):\n",
    "    \"\"\"\n",
    "    Complete PCA analysis with interpretation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numeric_cols].dropna()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Fit PCA\n",
    "    if n_components is None:\n",
    "        n_components = min(len(numeric_cols), len(X))\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # 1. Variance explained\n",
    "    print(\"1. Variance Explained:\")\n",
    "    cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    for i, (var, cumvar) in enumerate(zip(pca.explained_variance_ratio_, cumsum_var)):\n",
    "        print(f\"   PC{i+1}: {var*100:.2f}% (Cumulative: {cumvar*100:.2f}%)\")\n",
    "        if cumvar >= 0.95:\n",
    "            print(f\"   ‚Üí 95% variance explained with {i+1} components\")\n",
    "            break\n",
    "    \n",
    "    # 2. Scree plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Variance explained\n",
    "    axes[0].bar(range(1, len(pca.explained_variance_ratio_)+1), \n",
    "                pca.explained_variance_ratio_, alpha=0.7)\n",
    "    axes[0].plot(range(1, len(cumsum_var)+1), cumsum_var, 'r-o', linewidth=2)\n",
    "    axes[0].set_xlabel('Principal Component', fontweight='bold')\n",
    "    axes[0].set_ylabel('Variance Explained', fontweight='bold')\n",
    "    axes[0].set_title('Scree Plot', fontweight='bold')\n",
    "    axes[0].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Biplot (first 2 components)\n",
    "    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "    axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontweight='bold')\n",
    "    axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontweight='bold')\n",
    "    axes[1].set_title('PCA Biplot (First 2 Components)', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Component loadings\n",
    "    print(\"\\n2. Top Feature Loadings for First 3 Components:\")\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "        index=numeric_cols\n",
    "    )\n",
    "    \n",
    "    for i in range(min(3, pca.n_components_)):\n",
    "        print(f\"\\n   PC{i+1} - Top Features:\")\n",
    "        top_features = loadings[f'PC{i+1}'].abs().sort_values(ascending=False).head(5)\n",
    "        for feature, loading in top_features.items():\n",
    "            actual_loading = loadings.loc[feature, f'PC{i+1}']\n",
    "            print(f\"     {feature}: {actual_loading:.3f}\")\n",
    "    \n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "# Example\n",
    "pca_model, X_transformed, loadings = comprehensive_pca_analysis(df)\n",
    "```\n",
    "\n",
    "### 9.2 Factor Analysis\n",
    "\n",
    "**Purpose**: Identify latent factors underlying observed variables\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "def factor_analysis_comprehensive(df, n_factors=3):\n",
    "    \"\"\"\n",
    "    Comprehensive factor analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FACTOR ANALYSIS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numeric_cols].dropna()\n",
    "    \n",
    "    # 1. Test suitability for factor analysis\n",
    "    print(\"1. Suitability Tests:\")\n",
    "    \n",
    "    # KMO Test\n",
    "    kmo_all, kmo_model = calculate_kmo(X)\n",
    "    print(f\"   Kaiser-Meyer-Olkin (KMO) Test: {kmo_model:.3f}\")\n",
    "    if kmo_model >= 0.6:\n",
    "        print(\"   ‚úì Data suitable for factor analysis (KMO > 0.6)\")\n",
    "    else:\n",
    "        print(\"   ‚ö† Data may not be suitable (KMO < 0.6)\")\n",
    "    \n",
    "    # Bartlett's Test\n",
    "    chi_square, p_value = calculate_bartlett_sphericity(X)\n",
    "    print(f\"\\n   Bartlett's Test of Sphericity:\")\n",
    "    print(f\"     Chi-square: {chi_square:.2f}\")\n",
    "    print(f\"     p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"   ‚úì Variables are correlated (suitable for FA)\")\n",
    "    else:\n",
    "        print(\"   ‚ö† Variables may not be sufficiently correlated\")\n",
    "    \n",
    "    # 2. Fit factor analysis\n",
    "    print(f\"\\n2. Fitting {n_factors}-Factor Model:\")\n",
    "    \n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
    "    fa.fit(X)\n",
    "    \n",
    "    # Get factor loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        fa.loadings_,\n",
    "        index=numeric_cols,\n",
    "        columns=[f'Factor{i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n   Factor Loadings:\")\n",
    "    print(loadings.round(3))\n",
    "    \n",
    "    # 3. Variance explained\n",
    "    variance = fa.get_factor_variance()\n",
    "    \n",
    "    print(\"\\n3. Variance Explained:\")\n",
    "    print(f\"   Proportional variance: {variance[1]}\")\n",
    "    print(f\"   Cumulative variance: {variance[2]}\")\n",
    "    \n",
    "    # 4. Interpret factors\n",
    "    print(\"\\n4. Factor Interpretation:\")\n",
    "    for i in range(n_factors):\n",
    "        print(f\"\\n   Factor {i+1} - Top Loaded Variables:\")\n",
    "        top_vars = loadings[f'Factor{i+1}'].abs().sort_values(ascending=False).head(5)\n",
    "        for var, loading in top_vars.items():\n",
    "            actual = loadings.loc[var, f'Factor{i+1}']\n",
    "            print(f\"     {var}: {actual:.3f}\")\n",
    "    \n",
    "    return fa, loadings\n",
    "\n",
    "# Example\n",
    "fa_model, factor_loadings = factor_analysis_comprehensive(df, n_factors=3)\n",
    "```\n",
    "\n",
    "### 9.3 Cluster Analysis\n",
    "\n",
    "**Purpose**: Group similar customers together\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "def cluster_analysis(df, n_clusters_range=range(2, 11)):\n",
    "    \"\"\"\n",
    "    K-means clustering with optimal cluster selection.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CLUSTER ANALYSIS (K-MEANS)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numeric_cols].dropna()\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 1. Determine optimal number of clusters\n",
    "    print(\"1. Finding Optimal Number of Clusters:\")\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in n_clusters_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        \n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Plot elbow curve and silhouette scores\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Elbow method\n",
    "    axes[0].plot(n_clusters_range, inertias, 'bo-', linewidth=2)\n",
    "    axes[0].set_xlabel('Number of Clusters', fontweight='bold')\n",
    "    axes[0].set_ylabel('Inertia', fontweight='bold')\n",
    "    axes[0].set_title('Elbow Method', fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Silhouette score\n",
    "    axes[1].plot(n_clusters_range, silhouette_scores, 'ro-', linewidth=2)\n",
    "    axes[1].set_xlabel('Number of Clusters', fontweight='bold')\n",
    "    axes[1].set_ylabel('Silhouette Score', fontweight='bold')\n",
    "    axes[1].set_title('Silhouette Analysis', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select optimal k (highest silhouette score)\n",
    "    optimal_k = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"\\n   Optimal number of clusters: {optimal_k}\")\n",
    "    print(f\"   Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "    # 2. Fit final model\n",
    "    print(f\"\\n2. Fitting {optimal_k}-Cluster Model:\")\n",
    "    \n",
    "    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans_final.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add clusters to dataframe\n",
    "    df_clustered = X.copy()\n",
    "    df_clustered['Cluster'] = clusters\n",
    "    \n",
    "    # 3. Analyze clusters\n",
    "    print(f\"\\n3. Cluster Profiles:\")\n",
    "    \n",
    "    for i in range(optimal_k):\n",
    "        cluster_data = df_clustered[df_clustered['Cluster'] == i]\n",
    "        print(f\"\\n   Cluster {i} (n={len(cluster_data)}):\")\n",
    "        print(f\"     Mean values:\")\n",
    "        for col in numeric_cols[:5]:  # Show top 5 features\n",
    "            print(f\"       {col}: {cluster_data[col].mean():.2f}\")\n",
    "    \n",
    "    # 4. Visualize clusters (2D PCA)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontweight='bold')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontweight='bold')\n",
    "    plt.title('Customer Segments (K-Means Clustering)', fontweight='bold', fontsize=14)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans_final, clusters, df_clustered\n",
    "\n",
    "# Example\n",
    "kmeans_model, cluster_labels, df_with_clusters = cluster_analysis(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Statistical Assumptions and Validation\n",
    "\n",
    "### 10.1 Checking Linear Regression Assumptions\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import jarque_bera\n",
    "\n",
    "def check_regression_assumptions(X, y, feature_names):\n",
    "    \"\"\"\n",
    "    Comprehensive check of linear regression assumptions.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LINEAR REGRESSION ASSUMPTIONS CHECK\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # 1. Linearity\n",
    "    print(\"1. LINEARITY (Residuals vs Fitted Values):\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Fitted Values', fontweight='bold')\n",
    "    plt.ylabel('Residuals', fontweight='bold')\n",
    "    plt.title('Residual Plot', fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    print(\"   ‚Üí Pattern should be random scatter around zero\")\n",
    "    print(\"   ‚Üí Funnel shape indicates heteroscedasticity\")\n",
    "    print(\"   ‚Üí Curve indicates non-linearity\")\n",
    "    \n",
    "    # 2. Normality of residuals\n",
    "    print(\"\\n2. NORMALITY OF RESIDUALS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title('Histogram of Residuals', fontweight='bold')\n",
    "    axes[0].set_xlabel('Residuals')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Jarque-Bera test\n",
    "    jb_stat, jb_p = jarque_bera(residuals)\n",
    "    print(f\"   Jarque-Bera Test:\")\n",
    "    print(f\"     Statistic: {jb_stat:.4f}\")\n",
    "    print(f\"     p-value: {jb_p:.4f}\")\n",
    "    \n",
    "    if jb_p > 0.05:\n",
    "        print(\"   ‚úì Residuals appear normally distributed\")\n",
    "    else:\n",
    "        print(\"   ‚ö† Residuals deviate from normality\")\n",
    "    \n",
    "    # 3. Homoscedasticity (constant variance)\n",
    "    print(\"\\n3. HOMOSCEDASTICITY (Constant Variance):\")\n",
    "    \n",
    "    # Breusch-Pagan test would go here (requires statsmodels)\n",
    "    print(\"   Visual check: See residual plot above\")\n",
    "    print(\"   ‚Üí Variance should be constant across fitted values\")\n",
    "    \n",
    "    # 4. Independence (Durbin-Watson)\n",
    "    print(\"\\n4. INDEPENDENCE OF RESIDUALS:\")\n",
    "    \n",
    "    # Calculate Durbin-Watson statistic\n",
    "    dw = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)\n",
    "    print(f\"   Durbin-Watson statistic: {dw:.4f}\")\n",
    "    print(\"   ‚Üí Values near 2 indicate no autocorrelation\")\n",
    "    print(\"   ‚Üí Values < 2: positive autocorrelation\")\n",
    "    print(\"   ‚Üí Values > 2: negative autocorrelation\")\n",
    "    \n",
    "    # 5. Multicollinearity (VIF)\n",
    "    print(\"\\n5. MULTICOLLINEARITY CHECK:\")\n",
    "    \n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = feature_names\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "    \n",
    "    print(vif_data.round(2))\n",
    "    print(\"\\n   Interpretation:\")\n",
    "    print(\"   ‚Üí VIF = 1: No correlation\")\n",
    "    print(\"   ‚Üí VIF < 5: Moderate correlation (acceptable)\")\n",
    "    print(\"   ‚Üí VIF > 5: High correlation (problematic)\")\n",
    "    print(\"   ‚Üí VIF > 10: Severe multicollinearity\")\n",
    "    \n",
    "    high_vif = vif_data[vif_data['VIF'] > 5]\n",
    "    if len(high_vif) > 0:\n",
    "        print(f\"\\n   ‚ö† Features with high VIF:\")\n",
    "        print(high_vif)\n",
    "    else:\n",
    "        print(\"\\n   ‚úì No severe multicollinearity detected\")\n",
    "    \n",
    "    return model, residuals, vif_data\n",
    "\n",
    "# Example (prepare numeric data first)\n",
    "X_numeric = df[['tenure', 'MonthlyCharges', 'TotalCharges']].dropna()\n",
    "y_numeric = df.loc[X_numeric.index, 'Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "model, residuals, vif_df = check_regression_assumptions(\n",
    "    X_numeric.values, \n",
    "    y_numeric.values,\n",
    "    X_numeric.columns.tolist()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Practical Implementation Guide\n",
    "\n",
    "### 11.1 Complete Statistical Analysis Workflow\n",
    "\n",
    "```python\n",
    "def complete_statistical_analysis_pipeline(df, target_col='Churn'):\n",
    "    \"\"\"\n",
    "    Execute complete statistical analysis for churn dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE STATISTICAL ANALYSIS PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # PHASE 1: DESCRIPTIVE STATISTICS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 1: DESCRIPTIVE STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        comprehensive_summary(df, col, target_col)\n",
    "    \n",
    "    # PHASE 2: DISTRIBUTION ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        test_normality(df[col], col)\n",
    "    \n",
    "    # PHASE 3: HYPOTHESIS TESTING\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3: HYPOTHESIS TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # T-tests for numeric variables\n",
    "    for col in numeric_cols:\n",
    "        churned = df[df[target_col]=='Yes'][col].dropna()\n",
    "        retained = df[df[target_col]=='No'][col].dropna()\n",
    "        \n",
    "        results[f'{col}_ttest'] = perform_t_test(\n",
    "            churned, retained, 'Churned', 'Retained', col\n",
    "        )\n",
    "    \n",
    "    # Chi-square tests for categorical variables\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols = [c for c in categorical_cols if c != target_col]\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        results[f'{col}_chisquare'] = chi_square_test(df, col, target_col)\n",
    "    \n",
    "    # PHASE 4: CORRELATION ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 4: CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results['correlation_matrix'] = comprehensive_correlation_analysis(df, method='pearson')\n",
    "    \n",
    "    # PHASE 5: MULTIVARIATE ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 5: MULTIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # PCA\n",
    "    pca_model, X_pca, loadings = comprehensive_pca_analysis(df)\n",
    "    results['pca'] = {'model': pca_model, 'transformed': X_pca, 'loadings': loadings}\n",
    "    \n",
    "    # Clustering\n",
    "    kmeans_model, clusters, df_clustered = cluster_analysis(df)\n",
    "    results['clusters'] = {'model': kmeans_model, 'labels': clusters, 'data': df_clustered}\n",
    "    \n",
    "    # PHASE 6: SURVIVAL ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 6: SURVIVAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'tenure' in df.columns:\n",
    "        df_survival = df.copy()\n",
    "        df_survival['Churn_binary'] = (df_survival[target_col] == 'Yes').astype(int)\n",
    "        survival_analysis(df_survival, 'tenure', 'Churn_binary')\n",
    "    \n",
    "    # FINAL SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE - KEY FINDINGS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. Significant Differences (t-tests):\")\n",
    "    for key, result in results.items():\n",
    "        if '_ttest' in key and result.get('significant'):\n",
    "            print(f\"   ‚úì {key.replace('_ttest', '')}: p={result['p_value']:.4f}\")\n",
    "    \n",
    "    print(\"\\n2. Significant Associations (chi-square):\")\n",
    "    for key, result in results.items():\n",
    "        if '_chisquare' in key and result.get('significant'):\n",
    "            print(f\"   ‚úì {key.replace('_chisquare', '')}: p={result['p_value']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute complete pipeline\n",
    "results = complete_statistical_analysis_pipeline(df, target_col='Churn')\n",
    "```\n",
    "\n",
    "### 11.2 Statistical Report Generator\n",
    "\n",
    "```python\n",
    "def generate_statistical_report(df, output_file='statistical_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistical report.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Redirect output to file\n",
    "    original_stdout = sys.stdout\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        sys.stdout = f\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"TELCO CUSTOMER CHURN - STATISTICAL ANALYSIS REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Dataset: {len(df)} customers, {len(df.columns)} features\")\n",
    "        \n",
    "        # Execute analyses\n",
    "        results = complete_statistical_analysis_pipeline(df)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"END OF REPORT\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Restore stdout\n",
    "    sys.stdout = original_stdout\n",
    "    \n",
    "    print(f\"\\n‚úì Statistical report saved to: {output_file}\")\n",
    "    return results\n",
    "\n",
    "# Generate report\n",
    "report_results = generate_statistical_report(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Case Studies and Applications\n",
    "\n",
    "### 12.1 Case Study: Identifying High-Risk Customers\n",
    "\n",
    "**Objective**: Use statistical methods to segment customers by churn risk\n",
    "\n",
    "**Approach**:\n",
    "\n",
    "```python\n",
    "def identify_high_risk_customers(df):\n",
    "    \"\"\"\n",
    "    Statistical approach to identify high-risk customers.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CASE STUDY: IDENTIFYING HIGH-RISK CUSTOMERS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Statistical profiling\n",
    "    print(\"\\n1. STATISTICAL PROFILING OF CHURNED CUSTOMERS:\")\n",
    "    \n",
    "    churned = df[df['Churn'] == 'Yes']\n",
    "    retained = df[df['Churn'] == 'No']\n",
    "    \n",
    "    # Identify significant differences\n",
    "    risk_factors = {}\n",
    "    \n",
    "    # Numeric variables\n",
    "    numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        result = perform_t_test(\n",
    "            churned[col].dropna(),\n",
    "            retained[col].dropna(),\n",
    "            'Churned', 'Retained', col\n",
    "        )\n",
    "        \n",
    "        if result['significant']:\n",
    "            risk_factors[col] = {\n",
    "                'type': 'numeric',\n",
    "                'churned_mean': churned[col].mean(),\n",
    "                'retained_mean': retained[col].mean(),\n",
    "                'effect_size': result['cohens_d']\n",
    "            }\n",
    "    \n",
    "    # Categorical variables\n",
    "    categorical_cols = ['Contract', 'PaymentMethod', 'InternetService']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        result = chi_square_test(df, col, 'Churn')\n",
    "        \n",
    "        if result['significant']:\n",
    "            # Calculate churn rates by category\n",
    "            churn_rates = df.groupby(col)['Churn'].apply(\n",
    "                lambda x: (x == 'Yes').sum() / len(x) * 100\n",
    "            )\n",
    "            \n",
    "            risk_factors[col] = {\n",
    "                'type': 'categorical',\n",
    "                'churn_rates': churn_rates.to_dict(),\n",
    "                'highest_risk': churn_rates.idxmax()\n",
    "            }\n",
    "    \n",
    "    # Step 2: Create risk score\n",
    "    print(\"\\n2. CREATING RISK SCORE:\")\n",
    "    \n",
    "    df_risk = df.copy()\n",
    "    df_risk['risk_score'] = 0\n",
    "    \n",
    "    # Add points based on statistical findings\n",
    "    for factor, info in risk_factors.items():\n",
    "        if info['type'] == 'numeric':\n",
    "            if info['churned_mean'] < info['retained_mean']:\n",
    "                # Lower values indicate higher risk (e.g., tenure)\n",
    "                threshold = info['retained_mean']\n",
    "                df_risk['risk_score'] += (df_risk[factor] < threshold).astype(int)\n",
    "            else:\n",
    "                # Higher values indicate higher risk (e.g., charges)\n",
    "                threshold = info['retained_mean']\n",
    "                df_risk['risk_score'] += (df_risk[factor] > threshold).astype(int)\n",
    "        \n",
    "        elif info['type'] == 'categorical':\n",
    "            # Highest risk category gets a point\n",
    "            highest_risk_cat = info['highest_risk']\n",
    "            df_risk['risk_score'] += (df_risk[factor] == highest_risk_cat).astype(int)\n",
    "    \n",
    "    # Normalize to 0-100 scale\n",
    "    max_score = df_risk['risk_score'].max()\n",
    "    df_risk['risk_score_normalized'] = (df_risk['risk_score'] / max_score * 100).round(0)\n",
    "    \n",
    "    # Step 3: Validate risk score\n",
    "    print(\"\\n3. VALIDATING RISK SCORE:\")\n",
    "    \n",
    "    # Correlation with actual churn\n",
    "    df_risk['Churn_binary'] = (df_risk['Churn'] == 'Yes').astype(int)\n",
    "    correlation = df_risk[['risk_score_normalized', 'Churn_binary']].corr().iloc[0, 1]\n",
    "    \n",
    "    print(f\"   Correlation with actual churn: {correlation:.4f}\")\n",
    "    \n",
    "    # ROC-AUC\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    \n",
    "    roc_auc = roc_auc_score(df_risk['Churn_binary'], df_risk['risk_score_normalized'])\n",
    "    print(f\"   ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(df_risk['Churn_binary'], df_risk['risk_score_normalized'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'Risk Score (AUC={roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title('ROC Curve - Risk Score Validation', fontweight='bold', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 4: Segment customers\n",
    "    print(\"\\n4. CUSTOMER SEGMENTATION BY RISK:\")\n",
    "    \n",
    "    # Create risk categories\n",
    "    df_risk['risk_category'] = pd.cut(\n",
    "        df_risk['risk_score_normalized'],\n",
    "        bins=[0, 33, 66, 100],\n",
    "        labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
    "    )\n",
    "    \n",
    "    # Analyze by segment\n",
    "    for category in ['Low Risk', 'Medium Risk', 'High Risk']:\n",
    "        segment = df_risk[df_risk['risk_category'] == category]\n",
    "        actual_churn_rate = (segment['Churn'] == 'Yes').sum() / len(segment) * 100\n",
    "        \n",
    "        print(f\"\\n   {category}:\")\n",
    "        print(f\"     Customers: {len(segment):,}\")\n",
    "        print(f\"     Actual churn rate: {actual_churn_rate:.2f}%\")\n",
    "        print(f\"     Avg risk score: {segment['risk_score_normalized'].mean():.1f}\")\n",
    "    \n",
    "    return df_risk, risk_factors\n",
    "\n",
    "# Execute case study\n",
    "df_with_risk, risk_factors = identify_high_risk_customers(df)\n",
    "```\n",
    "\n",
    "### 12.2 Case Study: A/B Testing Retention Campaign\n",
    "\n",
    "**Objective**: Test if retention campaign reduces churn\n",
    "\n",
    "**Approach**:\n",
    "\n",
    "```python\n",
    "def ab_test_retention_campaign(df_control, df_treatment, metric='Churn'):\n",
    "    \"\"\"\n",
    "    Statistical A/B test for retention campaign.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CASE STUDY: A/B TESTING RETENTION CAMPAIGN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Convert churn to binary\n",
    "    control_churn = (df_control[metric] == 'Yes').astype(int)\n",
    "    treatment_churn = (df_treatment[metric] == 'Yes').astype(int)\n",
    "    \n",
    "    # Step 1: Sample size and power analysis\n",
    "    print(\"\\n1. SAMPLE SIZE ANALYSIS:\")\n",
    "    print(f\"   Control group: {len(df_control):,} customers\")\n",
    "    print(f\"   Treatment group: {len(df_treatment):,} customers\")\n",
    "    \n",
    "    # Step 2: Check for balance\n",
    "    print(\"\\n2. BALANCE CHECK (Pre-treatment characteristics):\")\n",
    "    \n",
    "    numeric_vars = ['tenure', 'MonthlyCharges']\n",
    "    \n",
    "    for var in numeric_vars:\n",
    "        t_stat, p_value = ttest_ind(df_control[var], df_treatment[var])\n",
    "        print(f\"   {var}: p={p_value:.4f}\", end=\"\")\n",
    "        if p_value > 0.05:\n",
    "            print(\" ‚úì Balanced\")\n",
    "        else:\n",
    "            print(\" ‚ö† Imbalanced - adjust analysis\")\n",
    "    \n",
    "    # Step 3: Test for difference in churn rates\n",
    "    print(\"\\n3. HYPOTHESIS TEST:\")\n",
    "    print(\"   H‚ÇÄ: Treatment has no effect on churn\")\n",
    "    print(\"   H‚ÇÅ: Treatment reduces churn\")\n",
    "    \n",
    "    # Two-proportion z-test\n",
    "    from statsmodels.stats.proportion import proportions_ztest\n",
    "    \n",
    "    count = np.array([control_churn.sum(), treatment_churn.sum()])\n",
    "    nobs = np.array([len(control_churn), len(treatment_churn)])\n",
    "    \n",
    "    z_stat, p_value = proportions_ztest(count, nobs, alternative='larger')\n",
    "    \n",
    "    print(f\"\\n   Control churn rate: {control_churn.mean()*100:.2f}%\")\n",
    "    print(f\"   Treatment churn rate: {treatment_churn.mean()*100:.2f}%\")\n",
    "    print(f\"   Difference: {(control_churn.mean() - treatment_churn.mean())*100:.2f} percentage points\")\n",
    "    print(f\"\\n   Z-statistic: {z_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n   ‚úì SIGNIFICANT EFFECT: Treatment reduces churn\")\n",
    "    else:\n",
    "        print(\"\\n   ‚úó NO SIGNIFICANT EFFECT: Treatment does not significantly reduce churn\")\n",
    "    \n",
    "    # Step 4: Effect size and confidence interval\n",
    "    print(\"\\n4. EFFECT SIZE & CONFIDENCE INTERVAL:\")\n",
    "    \n",
    "    # Relative risk\n",
    "    rr = (treatment_churn.mean() / control_churn.mean())\n",
    "    print(f\"   Relative Risk: {rr:.4f}\")\n",
    "    print(f\"   ‚Üí Treatment group has {(1-rr)*100:.1f}% lower churn risk\")\n",
    "    \n",
    "    # Absolute risk reduction\n",
    "    arr = (control_churn.mean() - treatment_churn.mean()) * 100\n",
    "    print(f\"   Absolute Risk Reduction: {arr:.2f} percentage points\")\n",
    "    \n",
    "    # Number needed to treat\n",
    "    if arr > 0:\n",
    "        nnt = 100 / arr\n",
    "        print(f\"   Number Needed to Treat: {nnt:.1f}\")\n",
    "        print(f\"   ‚Üí Need to treat {nnt:.0f} customers to prevent 1 churn\")\n",
    "    \n",
    "    # Step 5: Business impact\n",
    "    print(\"\\n5. BUSINESS IMPACT:\")\n",
    "    \n",
    "    customers_saved = len(df_treatment) * (control_churn.mean() - treatment_churn.mean())\n",
    "    cost_per_customer = 50  # Assumed campaign cost\n",
    "    total_cost = len(df_treatment) * cost_per_customer\n",
    "    value_per_customer = 500  # Assumed customer lifetime value\n",
    "    total_value = customers_saved * value_per_customer\n",
    "    roi = (total_value - total_cost) / total_cost * 100\n",
    "    \n",
    "    print(f\"   Customers saved: {customers_saved:.0f}\")\n",
    "    print(f\"   Campaign cost: ${total_cost:,.0f}\")\n",
    "    print(f\"   Value generated: ${total_value:,.0f}\")\n",
    "    print(f\"   ROI: {roi:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "        'effect_size': arr,\n",
    "        'roi': roi\n",
    "    }\n",
    "\n",
    "# Example (create simulated treatment data)\n",
    "# In practice, this would be actual A/B test data\n",
    "np.random.seed(42)\n",
    "control_indices = df.sample(frac=0.5).index\n",
    "treatment_indices = df.drop(control_indices).index\n",
    "\n",
    "df_control = df.loc[control_indices]\n",
    "df_treatment = df.loc[treatment_indices].copy()\n",
    "\n",
    "# Simulate treatment effect (reduce churn by 5%)\n",
    "treatment_effect = np.random.random(len(df_treatment)) < 0.05\n",
    "df_treatment.loc[df_treatment['Churn']=='Yes', 'Churn'] = np.where(\n",
    "    treatment_effect[df_treatment['Churn']=='Yes'],\n",
    "    'No',\n",
    "    df_treatment.loc[df_treatment['Churn']=='Yes', 'Churn']\n",
    ")\n",
    "\n",
    "results = ab_test_retention_campaign(df_control, df_treatment)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Conclusion\n",
    "\n",
    "### 13.1 Summary of Statistical Methods\n",
    "\n",
    "This dissertation has covered comprehensive statistical approaches for telco churn analysis:\n",
    "\n",
    "**Descriptive Statistics**:\n",
    "- Central tendency (mean, median, mode)\n",
    "- Dispersion (std dev, variance, IQR)\n",
    "- Shape (skewness, kurtosis)\n",
    "\n",
    "**Inferential Statistics**:\n",
    "- Confidence intervals\n",
    "- Hypothesis testing (t-tests, chi-square, ANOVA)\n",
    "- Effect size calculations\n",
    "\n",
    "**Correlation Analysis**:\n",
    "- Pearson, Spearman, point-biserial correlations\n",
    "- Correlation matrices and heatmaps\n",
    "\n",
    "**Advanced Techniques**:\n",
    "- Survival analysis (Kaplan-Meier, Cox regression)\n",
    "- Multivariate methods (PCA, factor analysis, clustering)\n",
    "- A/B testing and experimental design\n",
    "\n",
    "### 13.2 Best Practices\n",
    "\n",
    "1. **Always Check Assumptions**: Don't blindly apply tests\n",
    "2. **Report Effect Sizes**: p-values alone aren't enough\n",
    "3. **Use Multiple Methods**: Triangulate findings\n",
    "4. **Visualize Results**: Graphics aid interpretation\n",
    "5. **Consider Business Context**: Statistical significance ‚â† practical significance\n",
    "6. **Document Thoroughly**: Reproducibility matters\n",
    "\n",
    "### 13.3 Common Pitfalls to Avoid\n",
    "\n",
    "```python\n",
    "print(\"\"\"\n",
    "COMMON STATISTICAL PITFALLS IN CHURN ANALYSIS:\n",
    "\n",
    "1. P-HACKING\n",
    "   ‚ùå Testing many hypotheses until finding p<0.05\n",
    "   ‚úì Pre-register hypotheses, adjust for multiple testing\n",
    "\n",
    "2. IGNORING ASSUMPTIONS\n",
    "   ‚ùå Using t-test on non-normal data\n",
    "   ‚úì Check assumptions, use appropriate alternatives\n",
    "\n",
    "3. CONFUSING CORRELATION WITH CAUSATION\n",
    "   ‚ùå \"High charges cause churn\"\n",
    "   ‚úì \"High charges are associated with churn\"\n",
    "\n",
    "4. CHERRY-PICKING DATA\n",
    "   ‚ùå Removing \"outliers\" to get desired results\n",
    "   ‚úì Document all data cleaning decisions\n",
    "\n",
    "5. SMALL SAMPLE SIZES\n",
    "   ‚ùå Drawing conclusions from n=30\n",
    "   ‚úì Conduct power analysis, collect sufficient data\n",
    "\n",
    "6. IGNORING EFFECT SIZE\n",
    "   ‚ùå \"p<0.001, must be important!\"\n",
    "   ‚úì Check practical significance (Cohen's d, etc.)\n",
    "\n",
    "7. MULTIPLE TESTING WITHOUT CORRECTION\n",
    "   ‚ùå Running 100 tests at Œ±=0.05\n",
    "   ‚úì Use Bonferroni or FDR correction\n",
    "\n",
    "8. TREATING NON-INDEPENDENT DATA AS INDEPENDENT\n",
    "   ‚ùå Repeated measures from same customers\n",
    "   ‚úì Use appropriate paired/repeated measures tests\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 13.4 Recommended Statistical Workflow\n",
    "\n",
    "```python\n",
    "def recommended_statistical_workflow():\n",
    "    \"\"\"\n",
    "    Step-by-step guide for statistical analysis.\n",
    "    \"\"\"\n",
    "    workflow = \"\"\"\n",
    "    RECOMMENDED STATISTICAL ANALYSIS WORKFLOW:\n",
    "    \n",
    "    STEP 1: EXPLORATION\n",
    "    ‚ñ° Load and inspect data\n",
    "    ‚ñ° Calculate descriptive statistics\n",
    "    ‚ñ° Create visualizations\n",
    "    ‚ñ° Identify patterns and anomalies\n",
    "    \n",
    "    STEP 2: ASSUMPTION CHECKING\n",
    "    ‚ñ° Test normality (Shapiro-Wilk, Q-Q plots)\n",
    "    ‚ñ° Check for outliers\n",
    "    ‚ñ° Assess homogeneity of variance\n",
    "    ‚ñ° Verify independence\n",
    "    \n",
    "    STEP 3: HYPOTHESIS FORMULATION\n",
    "    ‚ñ° Define null and alternative hypotheses\n",
    "    ‚ñ° Set significance level (Œ± = 0.05)\n",
    "    ‚ñ° Determine required sample size\n",
    "    \n",
    "    STEP 4: TEST SELECTION\n",
    "    ‚ñ° Choose appropriate statistical test\n",
    "    ‚ñ° Consider parametric vs non-parametric\n",
    "    ‚ñ° Account for multiple comparisons\n",
    "    \n",
    "    STEP 5: EXECUTION\n",
    "    ‚ñ° Perform statistical tests\n",
    "    ‚ñ° Calculate effect sizes\n",
    "    ‚ñ° Compute confidence intervals\n",
    "    \n",
    "    STEP 6: INTERPRETATION\n",
    "    ‚ñ° Assess statistical significance\n",
    "    ‚ñ° Evaluate practical significance\n",
    "    ‚ñ° Consider business context\n",
    "    \n",
    "    STEP 7: VALIDATION\n",
    "    ‚ñ° Check assumptions post-hoc\n",
    "    ‚ñ° Perform sensitivity analyses\n",
    "    ‚ñ° Validate on holdout set\n",
    "    \n",
    "    STEP 8: REPORTING\n",
    "    ‚ñ° Document all decisions\n",
    "    ‚ñ° Create visualizations\n",
    "    ‚ñ° Write clear interpretations\n",
    "    ‚ñ° Include limitations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(workflow)\n",
    "    return workflow\n",
    "\n",
    "workflow = recommended_statistical_workflow()\n",
    "```\n",
    "\n",
    "### 13.5 Final Recommendations\n",
    "\n",
    "**For Practitioners**:\n",
    "\n",
    "1. **Start Simple**: Begin with descriptive statistics and visualizations\n",
    "2. **Build Up**: Progress to hypothesis testing and multivariate methods\n",
    "3. **Validate Everything**: Check assumptions and validate results\n",
    "4. **Think Business**: Always connect statistics to business outcomes\n",
    "5. **Stay Updated**: Statistical methods evolve - keep learning\n",
    "\n",
    "**For Stakeholders**:\n",
    "\n",
    "1. Statistical significance ‚â† business importance\n",
    "2. Confidence intervals provide more information than p-values\n",
    "3. Effect sizes tell you \"how much\" not just \"if\"\n",
    "4. Correlation doesn't imply causation\n",
    "5. All models are wrong, but some are useful\n",
    "\n",
    "### 13.6 Resources for Further Learning\n",
    "\n",
    "```python\n",
    "resources = {\n",
    "    'Books': [\n",
    "        'Statistics in Plain English by Timothy Urdan',\n",
    "        'Practical Statistics for Data Scientists by Bruce & Bruce',\n",
    "        'The Elements of Statistical Learning by Hastie, Tibshirani, Friedman'\n",
    "    ],\n",
    "    'Online Courses': [\n",
    "        'Khan Academy - Statistics and Probability',\n",
    "        'Coursera - Statistical Inference',\n",
    "        'DataCamp - Statistical Thinking in Python'\n",
    "    ],\n",
    "    'Python Libraries': [\n",
    "        'scipy.stats - Statistical functions',\n",
    "        'statsmodels - Statistical models',\n",
    "        'pingouin - Statistical tests',\n",
    "        'lifelines - Survival analysis'\n",
    "    ],\n",
    "    'Documentation': [\n",
    "        'SciPy Stats Documentation',\n",
    "        'Statsmodels Documentation',\n",
    "        'Scikit-learn User Guide'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nüìö RECOMMENDED RESOURCES:\\n\")\n",
    "for category, items in resources.items():\n",
    "    print(f\"{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚Ä¢ {item}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix: Quick Reference Guide\n",
    "\n",
    "```python\n",
    "def statistical_methods_quick_reference():\n",
    "    \"\"\"\n",
    "    Quick reference for choosing statistical methods.\n",
    "    \"\"\"\n",
    "    guide = \"\"\"\n",
    "    STATISTICAL METHODS QUICK REFERENCE\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    COMPARING TWO GROUPS:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Continuous data:\n",
    "      ‚Ä¢ Normal distribution ‚Üí Independent t-test\n",
    "      ‚Ä¢ Non-normal ‚Üí Mann-Whitney U test\n",
    "      ‚Ä¢ Paired samples ‚Üí Paired t-test\n",
    "    \n",
    "    Categorical data:\n",
    "      ‚Ä¢ 2x2 table ‚Üí Chi-square or Fisher's exact\n",
    "      ‚Ä¢ Larger tables ‚Üí Chi-square test\n",
    "    \n",
    "    COMPARING 3+ GROUPS:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Continuous data:\n",
    "      ‚Ä¢ Normal ‚Üí One-way ANOVA\n",
    "      ‚Ä¢ Non-normal ‚Üí Kruskal-Wallis test\n",
    "    \n",
    "    RELATIONSHIPS:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Two continuous variables:\n",
    "      ‚Ä¢ Linear relationship ‚Üí Pearson correlation\n",
    "      ‚Ä¢ Monotonic ‚Üí Spearman correlation\n",
    "    \n",
    "    Continuous + Binary:\n",
    "      ‚Ä¢ Point-biserial correlation\n",
    "    \n",
    "    Two categorical:\n",
    "      ‚Ä¢ Chi-square test\n",
    "    \n",
    "    PREDICTION:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Binary outcome:\n",
    "      ‚Ä¢ Logistic regression\n",
    "      ‚Ä¢ Survival analysis (if time involved)\n",
    "    \n",
    "    Continuous outcome:\n",
    "      ‚Ä¢ Linear regression\n",
    "      ‚Ä¢ Multiple regression\n",
    "    \n",
    "    DIMENSION REDUCTION:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ‚Ä¢ PCA - uncorrelated components\n",
    "    ‚Ä¢ Factor analysis - latent factors\n",
    "    \n",
    "    GROUPING:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ‚Ä¢ K-means clustering\n",
    "    ‚Ä¢ Hierarchical clustering\n",
    "    \"\"\"\n",
    "    \n",
    "    print(guide)\n",
    "    return guide\n",
    "\n",
    "quick_ref = statistical_methods_quick_reference()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**END OF DISSERTATION**\n",
    "\n",
    "This comprehensive guide provides the statistical foundation necessary for rigorous telco churn analysis. Apply these methods thoughtfully, always considering both statistical and business significance.\n",
    "\n",
    "**Remember**: Statistics is a tool for understanding, not just for achieving p<0.05!# Statistical Approaches for Telco Customer Churn Analysis\n",
    "## A Comprehensive Dissertation\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Executive Summary](#1-executive-summary)\n",
    "2. [Introduction to Statistical Analysis in Churn Prediction](#2-introduction)\n",
    "3. [Descriptive Statistics](#3-descriptive-statistics)\n",
    "4. [Inferential Statistics](#4-inferential-statistics)\n",
    "5. [Hypothesis Testing](#5-hypothesis-testing)\n",
    "6. [Correlation and Association Analysis](#6-correlation-and-association-analysis)\n",
    "7. [Distribution Analysis](#7-distribution-analysis)\n",
    "8. [Time Series and Survival Analysis](#8-time-series-and-survival-analysis)\n",
    "9. [Multivariate Statistical Techniques](#9-multivariate-statistical-techniques)\n",
    "10. [Statistical Assumptions and Validation](#10-statistical-assumptions)\n",
    "11. [Practical Implementation Guide](#11-practical-implementation)\n",
    "12. [Case Studies and Applications](#12-case-studies)\n",
    "13. [Conclusion](#13-conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "This dissertation provides a comprehensive guide to statistical approaches essential for analyzing customer churn in telecommunications. We cover 15+ statistical methods, their theoretical foundations, practical applications, and implementation in Python.\n",
    "\n",
    "### Key Statistical Methods Covered:\n",
    "\n",
    "- **Descriptive Statistics**: Central tendency, dispersion, distribution shapes\n",
    "- **Hypothesis Testing**: t-tests, chi-square tests, ANOVA\n",
    "- **Correlation Analysis**: Pearson, Spearman, point-biserial\n",
    "- **Distribution Analysis**: Normality tests, Q-Q plots\n",
    "- **Survival Analysis**: Kaplan-Meier, Cox regression\n",
    "- **Multivariate Techniques**: PCA, factor analysis, cluster analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Statistical Analysis in Churn Prediction\n",
    "\n",
    "### 2.1 Why Statistics Matter in Churn Analysis\n",
    "\n",
    "Statistical analysis forms the foundation of data-driven churn prediction by:\n",
    "\n",
    "1. **Quantifying Relationships**: Measure strength between features and churn\n",
    "2. **Testing Hypotheses**: Validate business assumptions scientifically\n",
    "3. **Identifying Patterns**: Discover hidden trends in customer behavior\n",
    "4. **Ensuring Validity**: Verify model assumptions and results\n",
    "5. **Supporting Decisions**: Provide evidence-based recommendations\n",
    "\n",
    "### 2.2 The Statistical Analysis Pipeline\n",
    "\n",
    "```\n",
    "Data Collection ‚Üí Descriptive Statistics ‚Üí Exploratory Analysis ‚Üí\n",
    "Hypothesis Testing ‚Üí Model Building ‚Üí Validation ‚Üí Interpretation\n",
    "```\n",
    "\n",
    "### 2.3 Types of Variables in Churn Analysis\n",
    "\n",
    "| Variable Type | Examples | Statistical Methods |\n",
    "|---------------|----------|---------------------|\n",
    "| **Binary** | Churn (Yes/No), Gender | Chi-square, logistic regression |\n",
    "| **Nominal** | Contract type, Payment method | Chi-square, ANOVA |\n",
    "| **Ordinal** | Satisfaction ratings, Tenure groups | Mann-Whitney U, Kruskal-Wallis |\n",
    "| **Continuous** | Monthly charges, Tenure (months) | t-tests, correlation, regression |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the main features of your dataset.\n",
    "\n",
    "### 3.1 Measures of Central Tendency\n",
    "\n",
    "#### 3.1.1 Mean (Average)\n",
    "\n",
    "**Definition**: Sum of all values divided by count\n",
    "\n",
    "**Formula**: \n",
    "```\n",
    "Œº = (Œ£x) / n\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- Continuous variables (tenure, charges)\n",
    "- Normally distributed data\n",
    "- No extreme outliers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean\n",
    "mean_tenure = df['tenure'].mean()\n",
    "mean_monthly_charges = df['MonthlyCharges'].mean()\n",
    "\n",
    "# By churn status\n",
    "df.groupby('Churn')['tenure'].mean()\n",
    "\n",
    "# Interpretation\n",
    "print(f\"Average tenure: {mean_tenure:.2f} months\")\n",
    "print(f\"Churned customers avg tenure: {df[df['Churn']=='Yes']['tenure'].mean():.2f}\")\n",
    "print(f\"Retained customers avg tenure: {df[df['Churn']=='No']['tenure'].mean():.2f}\")\n",
    "```\n",
    "\n",
    "**Interpretation for Churn**:\n",
    "- If churned customers have lower mean tenure ‚Üí New customers at risk\n",
    "- If churned customers have higher mean charges ‚Üí Price sensitivity issue\n",
    "\n",
    "#### 3.1.2 Median\n",
    "\n",
    "**Definition**: Middle value when data is sorted\n",
    "\n",
    "**When to Use**:\n",
    "- Skewed distributions\n",
    "- Presence of outliers\n",
    "- Ordinal data\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate median\n",
    "median_tenure = df['tenure'].median()\n",
    "\n",
    "# Compare mean vs median to detect skewness\n",
    "print(f\"Mean tenure: {df['tenure'].mean():.2f}\")\n",
    "print(f\"Median tenure: {df['tenure'].median():.2f}\")\n",
    "\n",
    "# If mean > median: Right-skewed (long tail of high values)\n",
    "# If mean < median: Left-skewed (long tail of low values)\n",
    "```\n",
    "\n",
    "#### 3.1.3 Mode\n",
    "\n",
    "**Definition**: Most frequently occurring value\n",
    "\n",
    "**When to Use**:\n",
    "- Categorical variables\n",
    "- Identify most common category\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Most common contract type\n",
    "mode_contract = df['Contract'].mode()[0]\n",
    "print(f\"Most common contract: {mode_contract}\")\n",
    "\n",
    "# Mode by churn status\n",
    "df[df['Churn']=='Yes']['Contract'].mode()[0]\n",
    "df[df['Churn']=='No']['Contract'].mode()[0]\n",
    "```\n",
    "\n",
    "### 3.2 Measures of Dispersion\n",
    "\n",
    "#### 3.2.1 Standard Deviation\n",
    "\n",
    "**Definition**: Average distance from the mean\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "œÉ = sqrt(Œ£(x - Œº)¬≤ / n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate standard deviation\n",
    "std_charges = df['MonthlyCharges'].std()\n",
    "\n",
    "# Coefficient of Variation (CV) - standardized measure\n",
    "cv = (std_charges / df['MonthlyCharges'].mean()) * 100\n",
    "print(f\"CV: {cv:.2f}% - Shows relative variability\")\n",
    "\n",
    "# Compare variability between groups\n",
    "churned_std = df[df['Churn']=='Yes']['MonthlyCharges'].std()\n",
    "retained_std = df[df['Churn']=='No']['MonthlyCharges'].std()\n",
    "\n",
    "# Higher variability in churned group may indicate pricing issues\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Low std dev: Homogeneous customer base\n",
    "- High std dev: Diverse customer segments\n",
    "- Compare between churn groups to identify differences\n",
    "\n",
    "#### 3.2.2 Variance\n",
    "\n",
    "**Definition**: Square of standard deviation\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "variance = df['tenure'].var()\n",
    "\n",
    "# Variance explained in churn analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "for i, var in enumerate(explained_variance_ratio[:5]):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}%\")\n",
    "```\n",
    "\n",
    "#### 3.2.3 Range and Interquartile Range (IQR)\n",
    "\n",
    "**Range**: Maximum - Minimum\n",
    "\n",
    "**IQR**: Q3 - Q1 (middle 50% of data)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate range\n",
    "data_range = df['MonthlyCharges'].max() - df['MonthlyCharges'].min()\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df['MonthlyCharges'].quantile(0.25)\n",
    "Q3 = df['MonthlyCharges'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['MonthlyCharges'] < lower_bound) | \n",
    "              (df['MonthlyCharges'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 3.3 Measures of Shape\n",
    "\n",
    "#### 3.3.1 Skewness\n",
    "\n",
    "**Definition**: Measure of asymmetry in distribution\n",
    "\n",
    "**Interpretation**:\n",
    "- Skewness = 0: Perfectly symmetric\n",
    "- Skewness > 0: Right-skewed (tail on right)\n",
    "- Skewness < 0: Left-skewed (tail on left)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Calculate skewness\n",
    "tenure_skew = skew(df['tenure'])\n",
    "charges_skew = skew(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Tenure skewness: {tenure_skew:.3f}\")\n",
    "print(f\"Monthly charges skewness: {charges_skew:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(tenure_skew) < 0.5:\n",
    "    print(\"Tenure is approximately symmetric\")\n",
    "elif tenure_skew > 0:\n",
    "    print(\"Tenure is right-skewed (many new customers)\")\n",
    "else:\n",
    "    print(\"Tenure is left-skewed (many long-term customers)\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['tenure'], bins=30, edgecolor='black')\n",
    "axes[0].set_title(f'Tenure Distribution (Skewness: {tenure_skew:.2f})')\n",
    "axes[0].axvline(df['tenure'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].axvline(df['tenure'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df['MonthlyCharges'], bins=30, edgecolor='black')\n",
    "axes[1].set_title(f'Monthly Charges (Skewness: {charges_skew:.2f})')\n",
    "axes[1].axvline(df['MonthlyCharges'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].axvline(df['MonthlyCharges'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### 3.3.2 Kurtosis\n",
    "\n",
    "**Definition**: Measure of \"tailedness\" or extreme values\n",
    "\n",
    "**Interpretation**:\n",
    "- Kurtosis = 3: Normal distribution (mesokurtic)\n",
    "- Kurtosis > 3: Heavy tails, more outliers (leptokurtic)\n",
    "- Kurtosis < 3: Light tails, fewer outliers (platykurtic)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "# Calculate excess kurtosis (subtract 3 for comparison to normal)\n",
    "tenure_kurt = kurtosis(df['tenure'], fisher=True)  # fisher=True gives excess kurtosis\n",
    "\n",
    "print(f\"Tenure excess kurtosis: {tenure_kurt:.3f}\")\n",
    "\n",
    "if tenure_kurt > 0:\n",
    "    print(\"‚Üí More extreme values than normal distribution\")\n",
    "    print(\"‚Üí May need robust statistical methods\")\n",
    "elif tenure_kurt < 0:\n",
    "    print(\"‚Üí Fewer extreme values than normal distribution\")\n",
    "    print(\"‚Üí More uniform distribution\")\n",
    "```\n",
    "\n",
    "### 3.4 Comprehensive Descriptive Statistics Summary\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_summary(df, column, churn_col='Churn'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive descriptive statistics for a column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE STATISTICS: {column}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"Overall Statistics:\")\n",
    "    print(f\"  Count: {df[column].count()}\")\n",
    "    print(f\"  Mean: {df[column].mean():.2f}\")\n",
    "    print(f\"  Median: {df[column].median():.2f}\")\n",
    "    print(f\"  Mode: {df[column].mode()[0] if len(df[column].mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Std Dev: {df[column].std():.2f}\")\n",
    "    print(f\"  Variance: {df[column].var():.2f}\")\n",
    "    print(f\"  Min: {df[column].min():.2f}\")\n",
    "    print(f\"  Max: {df[column].max():.2f}\")\n",
    "    print(f\"  Range: {df[column].max() - df[column].min():.2f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in [25, 50, 75, 90, 95, 99]:\n",
    "        print(f\"  {p}th: {df[column].quantile(p/100):.2f}\")\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"\\nDistribution Shape:\")\n",
    "    print(f\"  Skewness: {skew(df[column].dropna()):.3f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis(df[column].dropna(), fisher=True):.3f}\")\n",
    "    \n",
    "    # By churn status\n",
    "    print(f\"\\nBy Churn Status:\")\n",
    "    for churn_val in df[churn_col].unique():\n",
    "        subset = df[df[churn_col]==churn_val][column]\n",
    "        print(f\"  {churn_val}:\")\n",
    "        print(f\"    Mean: {subset.mean():.2f}\")\n",
    "        print(f\"    Median: {subset.median():.2f}\")\n",
    "        print(f\"    Std Dev: {subset.std():.2f}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_pct = (df[column].isnull().sum() / len(df)) * 100\n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"  Missing: {df[column].isnull().sum()} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Usage\n",
    "comprehensive_summary(df, 'tenure')\n",
    "comprehensive_summary(df, 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inferential Statistics\n",
    "\n",
    "Inferential statistics allow us to make predictions and inferences about a population based on sample data.\n",
    "\n",
    "### 4.1 Confidence Intervals\n",
    "\n",
    "**Definition**: Range of values that likely contains the true population parameter\n",
    "\n",
    "**Formula for Mean**:\n",
    "```\n",
    "CI = xÃÑ ¬± (t * (s / sqrt(n)))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- xÃÑ = sample mean\n",
    "- t = t-value from t-distribution\n",
    "- s = sample standard deviation\n",
    "- n = sample size\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for mean.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  # Standard error of mean\n",
    "    margin_error = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    ci_lower = mean - margin_error\n",
    "    ci_upper = mean + margin_error\n",
    "    \n",
    "    return mean, ci_lower, ci_upper\n",
    "\n",
    "# Example: Confidence interval for average tenure\n",
    "churned_tenure = df[df['Churn']=='Yes']['tenure']\n",
    "retained_tenure = df[df['Churn']=='No']['tenure']\n",
    "\n",
    "mean_c, lower_c, upper_c = calculate_confidence_interval(churned_tenure)\n",
    "mean_r, lower_r, upper_r = calculate_confidence_interval(retained_tenure)\n",
    "\n",
    "print(\"Average Tenure with 95% Confidence Intervals:\")\n",
    "print(f\"Churned: {mean_c:.2f} months [{lower_c:.2f}, {upper_c:.2f}]\")\n",
    "print(f\"Retained: {mean_r:.2f} months [{lower_r:.2f}, {upper_r:.2f}]\")\n",
    "\n",
    "# Interpretation\n",
    "if upper_c < lower_r:\n",
    "    print(\"‚Üí Churned customers have significantly lower tenure (no overlap)\")\n",
    "elif lower_c > upper_r:\n",
    "    print(\"‚Üí Churned customers have significantly higher tenure\")\n",
    "else:\n",
    "    print(\"‚Üí Confidence intervals overlap - difference may not be significant\")\n",
    "```\n",
    "\n",
    "**Business Application**:\n",
    "- Estimate true average monthly revenue from customers\n",
    "- Predict churn rate with confidence bounds\n",
    "- Compare segments with statistical rigor\n",
    "\n",
    "### 4.2 Standard Error\n",
    "\n",
    "**Definition**: Standard deviation of the sampling distribution\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "SE = œÉ / sqrt(n)\n",
    "```\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Calculate standard error for monthly charges\n",
    "se_charges = sem(df['MonthlyCharges'])\n",
    "\n",
    "print(f\"Standard Error of Monthly Charges: ${se_charges:.2f}\")\n",
    "print(f\"This means our sample mean is accurate within ¬±${se_charges:.2f}\")\n",
    "\n",
    "# Compare standard errors\n",
    "se_churned = sem(df[df['Churn']=='Yes']['MonthlyCharges'])\n",
    "se_retained = sem(df[df['Churn']=='No']['MonthlyCharges'])\n",
    "\n",
    "print(f\"\\nSE for churned customers: ${se_churned:.2f}\")\n",
    "print(f\"SE for retained customers: ${se_retained:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is crucial for making data-driven decisions about churn drivers.\n",
    "\n",
    "### 5.1 Framework for Hypothesis Testing\n",
    "\n",
    "**Standard Process**:\n",
    "\n",
    "1. **State Hypotheses**:\n",
    "   - H‚ÇÄ (Null): No difference/relationship exists\n",
    "   - H‚ÇÅ (Alternative): Difference/relationship exists\n",
    "\n",
    "2. **Choose Significance Level (Œ±)**:\n",
    "   - Common: Œ± = 0.05 (5% chance of Type I error)\n",
    "\n",
    "3. **Calculate Test Statistic**\n",
    "\n",
    "4. **Find p-value**\n",
    "\n",
    "5. **Make Decision**:\n",
    "   - If p-value < Œ±: Reject H‚ÇÄ (significant result)\n",
    "   - If p-value ‚â• Œ±: Fail to reject H‚ÇÄ\n",
    "\n",
    "### 5.2 Independent Samples t-Test\n",
    "\n",
    "**Purpose**: Compare means of two independent groups\n",
    "\n",
    "**Assumptions**:\n",
    "- Both groups are normally distributed\n",
    "- Equal variances (or use Welch's t-test)\n",
    "- Independent observations\n",
    "\n",
    "**When to Use in Churn Analysis**:\n",
    "- Compare tenure between churned vs retained\n",
    "- Compare charges between customer segments\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind, levene, shapiro\n",
    "\n",
    "def perform_t_test(group1, group2, group1_name, group2_name, \n",
    "                   variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform comprehensive independent t-test with assumption checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"INDEPENDENT T-TEST: {variable_name}\")\n",
    "    print(f\"Comparing {group1_name} vs {group2_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 1. Check normality assumption\n",
    "    print(\"1. Normality Tests (Shapiro-Wilk):\")\n",
    "    _, p_norm1 = shapiro(group1.sample(min(5000, len(group1))))  # Sample for large datasets\n",
    "    _, p_norm2 = shapiro(group2.sample(min(5000, len(group2))))\n",
    "    \n",
    "    print(f\"   {group1_name}: p-value = {p_norm1:.4f}\")\n",
    "    print(f\"   {group2_name}: p-value = {p_norm2:.4f}\")\n",
    "    \n",
    "    if p_norm1 > 0.05 and p_norm2 > 0.05:\n",
    "        print(\"   ‚úì Both groups appear normally distributed\")\n",
    "        normality_met = True\n",
    "    else:\n",
    "        print(\"   ‚ö† At least one group deviates from normality\")\n",
    "        print(\"   ‚Üí Consider using Mann-Whitney U test instead\")\n",
    "        normality_met = False\n",
    "    \n",
    "    # 2. Check equal variance assumption\n",
    "    print(\"\\n2. Equal Variance Test (Levene's Test):\")\n",
    "    _, p_var = levene(group1, group2)\n",
    "    print(f\"   p-value = {p_var:.4f}\")\n",
    "    \n",
    "    if p_var > 0.05:\n",
    "        print(\"   ‚úì Variances are equal\")\n",
    "        equal_var = True\n",
    "    else:\n",
    "        print(\"   ‚ö† Variances are unequal\")\n",
    "        print(\"   ‚Üí Using Welch's t-test (doesn't assume equal variance)\")\n",
    "        equal_var = False\n",
    "    \n",
    "    # 3. Perform t-test\n",
    "    print(\"\\n3. T-Test Results:\")\n",
    "    t_stat, p_value = ttest_ind(group1, group2, equal_var=equal_var)\n",
    "    \n",
    "    print(f\"   t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Significance level: {alpha}\")\n",
    "    \n",
    "    # 4. Calculate effect size (Cohen's d)\n",
    "    mean1, mean2 = group1.mean(), group2.mean()\n",
    "    std1, std2 = group1.std(), group2.std()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1-1)*std1**2 + (n2-1)*std2**2) / (n1+n2-2))\n",
    "    cohens_d = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    print(f\"\\n4. Effect Size (Cohen's d): {cohens_d:.4f}\")\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"   Effect size is {effect}\")\n",
    "    \n",
    "    # 5. Interpretation\n",
    "    print(\"\\n5. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Reject null hypothesis\")\n",
    "        print(f\"   ‚Üí {group1_name} and {group2_name} have different {variable_name}\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí Fail to reject null hypothesis\")\n",
    "        print(f\"   ‚Üí Insufficient evidence of difference\")\n",
    "    \n",
    "    # 6. Descriptive statistics\n",
    "    print(\"\\n6. Descriptive Statistics:\")\n",
    "    print(f\"   {group1_name}: Mean = {mean1:.2f}, SD = {std1:.2f}, n = {n1}\")\n",
    "    print(f\"   {group2_name}: Mean = {mean2:.2f}, SD = {std2:.2f}, n = {n2}\")\n",
    "    print(f\"   Mean Difference: {abs(mean1 - mean2):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare tenure between churned and retained customers\n",
    "churned = df[df['Churn']=='Yes']['tenure'].dropna()\n",
    "retained = df[df['Churn']=='No']['tenure'].dropna()\n",
    "\n",
    "results = perform_t_test(churned, retained, \n",
    "                         'Churned Customers', 'Retained Customers',\n",
    "                         'Tenure (months)')\n",
    "```\n",
    "\n",
    "**Business Interpretation**:\n",
    "\n",
    "```python\n",
    "# If significant difference found:\n",
    "if results['significant']:\n",
    "    print(\"\\nüìä BUSINESS INSIGHT:\")\n",
    "    print(\"Churned and retained customers have significantly different tenure.\")\n",
    "    print(\"‚Üí Action: Focus retention efforts on specific tenure segments\")\n",
    "    print(\"‚Üí Investigate: What happens at critical tenure milestones?\")\n",
    "```\n",
    "\n",
    "### 5.3 Paired Samples t-Test\n",
    "\n",
    "**Purpose**: Compare means of same group at two time points\n",
    "\n",
    "**When to Use**:\n",
    "- Before/after retention campaign\n",
    "- Monthly charges across time periods\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example: Compare customer satisfaction before and after intervention\n",
    "# (hypothetical data)\n",
    "satisfaction_before = df['satisfaction_before']\n",
    "satisfaction_after = df['satisfaction_after']\n",
    "\n",
    "t_stat, p_value = ttest_rel(satisfaction_before, satisfaction_after)\n",
    "\n",
    "print(f\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"‚Üí Intervention had significant effect on satisfaction\")\n",
    "```\n",
    "\n",
    "### 5.4 Chi-Square Test for Independence\n",
    "\n",
    "**Purpose**: Test relationship between two categorical variables\n",
    "\n",
    "**When to Use in Churn Analysis**:\n",
    "- Relationship between Contract type and Churn\n",
    "- Relationship between Payment method and Churn\n",
    "- Any categorical variable vs Churn\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform chi-square test of independence.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CHI-SQUARE TEST: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(df[var1], df[var2])\n",
    "    \n",
    "    print(\"1. Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "    print()\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(\"2. Test Results:\")\n",
    "    print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Degrees of freedom: {dof}\")\n",
    "    \n",
    "    # Check expected frequencies assumption\n",
    "    print(\"\\n3. Assumption Check:\")\n",
    "    print(\"   Expected frequencies (should all be ‚â• 5):\")\n",
    "    print(pd.DataFrame(expected, \n",
    "                       index=contingency_table.index,\n",
    "                       columns=contingency_table.columns).round(2))\n",
    "    \n",
    "    min_expected = expected.min()\n",
    "    if min_expected >= 5:\n",
    "        print(f\"   ‚úì All expected frequencies ‚â• 5 (min: {min_expected:.2f})\")\n",
    "        print(\"   ‚úì Chi-square test is valid\")\n",
    "    else:\n",
    "        print(f\"   ‚ö† Some expected frequencies < 5 (min: {min_expected:.2f})\")\n",
    "        print(\"   ‚ö† Consider Fisher's exact test or combine categories\")\n",
    "    \n",
    "    # Calculate effect size (Cram√©r's V)\n",
    "    n = contingency_table.sum().sum()\n",
    "    min_dim = min(contingency_table.shape[0]-1, contingency_table.shape[1]-1)\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "    \n",
    "    print(f\"\\n4. Effect Size (Cram√©r's V): {cramers_v:.4f}\")\n",
    "    if cramers_v < 0.1:\n",
    "        effect = \"negligible\"\n",
    "    elif cramers_v < 0.3:\n",
    "        effect = \"small\"\n",
    "    elif cramers_v < 0.5:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"   Effect size is {effect}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n5. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT ASSOCIATION (p < {alpha})\")\n",
    "        print(f\"   ‚Üí {var1} and {var2} are related\")\n",
    "        print(f\"   ‚Üí Variables are NOT independent\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT ASSOCIATION (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí Insufficient evidence of relationship\")\n",
    "    \n",
    "    # Calculate percentages for interpretation\n",
    "    print(\"\\n6. Percentage Breakdown:\")\n",
    "    pct_table = pd.crosstab(df[var1], df[var2], normalize='index') * 100\n",
    "    print(pct_table.round(2))\n",
    "    \n",
    "    return {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'cramers_v': cramers_v,\n",
    "        'significant': p_value < alpha,\n",
    "        'contingency_table': contingency_table\n",
    "    }\n",
    "\n",
    "# Example: Test relationship between Contract and Churn\n",
    "result = chi_square_test(df, 'Contract', 'Churn')\n",
    "\n",
    "# Business interpretation\n",
    "if result['significant']:\n",
    "    print(\"\\nüìä BUSINESS INSIGHT:\")\n",
    "    print(\"Contract type is significantly related to churn.\")\n",
    "    print(\"‚Üí Action: Analyze churn rates by contract type\")\n",
    "    print(\"‚Üí Strategy: Incentivize longer contracts\")\n",
    "```\n",
    "\n",
    "### 5.5 ANOVA (Analysis of Variance)\n",
    "\n",
    "**Purpose**: Compare means across 3+ groups\n",
    "\n",
    "**When to Use**:\n",
    "- Compare charges across multiple contract types\n",
    "- Compare tenure across service tiers\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "def perform_anova(df, group_var, numeric_var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform one-way ANOVA with post-hoc analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ONE-WAY ANOVA: {numeric_var} across {group_var}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Get groups\n",
    "    groups = df[group_var].unique()\n",
    "    group_data = [df[df[group_var]==g][numeric_var].dropna() for g in groups]\n",
    "    \n",
    "    # 1. Descriptive statistics\n",
    "    print(\"1. Descriptive Statistics by Group:\")\n",
    "    for g, data in zip(groups, group_data):\n",
    "        print(f\"   {g}: Mean={data.mean():.2f}, SD={data.std():.2f}, n={len(data)}\")\n",
    "    \n",
    "    # 2. Perform ANOVA\n",
    "    print(\"\\n2. ANOVA Results:\")\n",
    "    f_stat, p_value = f_oneway(*group_data)\n",
    "    \n",
    "    print(f\"   F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # 3. Effect size (eta-squared)\n",
    "    # Calculate between-group and total sum of squares\n",
    "    grand_mean = df[numeric_var].mean()\n",
    "    ss_between = sum([len(data) * (data.mean() - grand_mean)**2 \n",
    "                      for data in group_data])\n",
    "    ss_total = sum([(x - grand_mean)**2 for data in group_data for x in data])\n",
    "    eta_squared = ss_between / ss_total\n",
    "    \n",
    "    print(f\"\\n3. Effect Size (Œ∑¬≤): {eta_squared:.4f}\")\n",
    "    print(f\"   {eta_squared*100:.2f}% of variance explained by {group_var}\")\n",
    "    \n",
    "    # 4. Interpretation\n",
    "    print(\"\\n4. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí At least one group differs significantly\")\n",
    "        print(f\"   ‚Üí Recommend post-hoc tests (Tukey HSD)\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "        print(f\"   ‚Üí All groups have similar means\")\n",
    "    \n",
    "    # 5. Post-hoc test (Tukey HSD) if significant\n",
    "    if p_value < alpha:\n",
    "        from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "        \n",
    "        print(\"\\n5. Post-Hoc Analysis (Tukey HSD):\")\n",
    "        tukey = pairwise_tukeyhsd(df[numeric_var], df[group_var], alpha=alpha)\n",
    "        print(tukey)\n",
    "    \n",
    "    return {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value,\n",
    "        'eta_squared': eta_squared,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare monthly charges across contract types\n",
    "result = perform_anova(df, 'Contract', 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "### 5.6 Mann-Whitney U Test (Non-Parametric Alternative)\n",
    "\n",
    "**Purpose**: Compare distributions of two groups without normality assumption\n",
    "\n",
    "**When to Use**:\n",
    "- Data is not normally distributed\n",
    "- Ordinal data\n",
    "- Small sample sizes\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def mann_whitney_test(group1, group2, group1_name, group2_name, \n",
    "                      variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Mann-Whitney U test (non-parametric alternative to t-test).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MANN-WHITNEY U TEST: {variable_name}\")\n",
    "    print(f\"Comparing {group1_name} vs {group2_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Perform test\n",
    "    u_stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    \n",
    "    print(\"1. Test Results:\")\n",
    "    print(f\"   U-statistic: {u_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Calculate effect size (rank-biserial correlation)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    r = 1 - (2*u_stat) / (n1 * n2)  # rank-biserial correlation\n",
    "    \n",
    "    print(f\"\\n2. Effect Size (rank-biserial r): {r:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n3. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT DIFFERENCE (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Distributions differ significantly\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT DIFFERENCE (p ‚â• {alpha})\")\n",
    "    \n",
    "    # Medians for interpretation\n",
    "    print(\"\\n4. Median Comparison:\")\n",
    "    print(f\"   {group1_name}: Median = {group1.median():.2f}\")\n",
    "    print(f\"   {group2_name}: Median = {group2.median():.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'u_statistic': u_stat,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': r,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: When data is not normally distributed\n",
    "churned_charges = df[df['Churn']=='Yes']['MonthlyCharges'].dropna()\n",
    "retained_charges = df[df['Churn']=='No']['MonthlyCharges'].dropna()\n",
    "\n",
    "result = mann_whitney_test(churned_charges, retained_charges,\n",
    "                           'Churned', 'Retained', 'Monthly Charges')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Correlation and Association Analysis\n",
    "\n",
    "Understanding relationships between variables is crucial for feature selection and model building.\n",
    "\n",
    "### 6.1 Pearson Correlation\n",
    "\n",
    "**Purpose**: Measure linear relationship between two continuous variables\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "r = Œ£((x - xÃÑ)(y - »≥)) / sqrt(Œ£(x - xÃÑ)¬≤ √ó Œ£(y - »≥)¬≤)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- r = 1: Perfect positive correlation\n",
    "- r = 0: No linear correlation\n",
    "- r = -1: Perfect negative correlation\n",
    "- |r| < 0.3: Weak\n",
    "- 0.3 ‚â§ |r| < 0.7: Moderate\n",
    "- |r| ‚â• 0.7: Strong\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson_correlation_analysis(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive Pearson correlation analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PEARSON CORRELATION: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    data = df[[var1, var2]].dropna()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r, p_value = pearsonr(data[var1], data[var2])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Pearson r: {r:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Interpret strength\n",
    "    if abs(r) < 0.3:\n",
    "        strength = \"weak\"\n",
    "    elif abs(r) < 0.7:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    \n",
    "    direction = \"positive\" if r > 0 else \"negative\"\n",
    "    \n",
    "    print(f\"   Strength: {strength} {direction} correlation\")\n",
    "    \n",
    "    # Calculate coefficient of determination\n",
    "    r_squared = r ** 2\n",
    "    print(f\"\\n2. Coefficient of Determination (r¬≤): {r_squared:.4f}\")\n",
    "    print(f\"   {r_squared*100:.2f}% of variance in {var2} explained by {var1}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"\\n3. Statistical Conclusion:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"   ‚úì SIGNIFICANT CORRELATION (p < {alpha})\")\n",
    "        print(f\"   ‚Üí Relationship is statistically significant\")\n",
    "    else:\n",
    "        print(f\"   ‚úó NO SIGNIFICANT CORRELATION (p ‚â• {alpha})\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data[var1], data[var2], alpha=0.5)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(data[var1], data[var2], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(data[var1], p(data[var1]), \"r--\", linewidth=2, label='Regression line')\n",
    "    \n",
    "    plt.xlabel(var1, fontsize=12)\n",
    "    plt.ylabel(var2, fontsize=12)\n",
    "    plt.title(f'{var1} vs {var2}\\n(r = {r:.3f}, p = {p_value:.4f})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {'r': r, 'p_value': p_value, 'r_squared': r_squared}\n",
    "\n",
    "# Example: Correlation between tenure and total charges\n",
    "result = pearson_correlation_analysis(df, 'tenure', 'TotalCharges')\n",
    "```\n",
    "\n",
    "### 6.2 Spearman Correlation\n",
    "\n",
    "**Purpose**: Measure monotonic relationship (not necessarily linear)\n",
    "\n",
    "**When to Use**:\n",
    "- Ordinal variables\n",
    "- Non-linear relationships\n",
    "- Non-normal distributions\n",
    "- Outliers present\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def spearman_correlation_analysis(df, var1, var2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Spearman rank correlation analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SPEARMAN CORRELATION: {var1} vs {var2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    data = df[[var1, var2]].dropna()\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    rho, p_value = spearmanr(data[var1], data[var2])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Spearman œÅ (rho): {rho:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Compare with Pearson\n",
    "    r_pearson, _ = pearsonr(data[var1], data[var2])\n",
    "    print(f\"\\n2. Comparison:\")\n",
    "    print(f\"   Pearson r:  {r_pearson:.4f}\")\n",
    "    print(f\"   Spearman œÅ: {rho:.4f}\")\n",
    "    print(f\"   Difference: {abs(r_pearson - rho):.4f}\")\n",
    "    \n",
    "    if abs(r_pearson - rho) > 0.1:\n",
    "        print(\"   ‚ö† Large difference suggests non-linear relationship\")\n",
    "    else:\n",
    "        print(\"   ‚úì Similar values suggest linear relationship\")\n",
    "    \n",
    "    return {'rho': rho, 'p_value': p_value}\n",
    "\n",
    "# Example\n",
    "result = spearman_correlation_analysis(df, 'tenure', 'MonthlyCharges')\n",
    "```\n",
    "\n",
    "### 6.3 Point-Biserial Correlation\n",
    "\n",
    "**Purpose**: Correlation between continuous and binary variable\n",
    "\n",
    "**When to Use**:\n",
    "- Relationship between numeric variable and Churn (binary)\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "def point_biserial_analysis(df, continuous_var, binary_var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Point-biserial correlation for continuous vs binary variable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"POINT-BISERIAL CORRELATION\")\n",
    "    print(f\"{continuous_var} vs {binary_var}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Ensure binary variable is 0/1\n",
    "    data = df[[continuous_var, binary_var]].dropna()\n",
    "    if data[binary_var].dtype == 'object':\n",
    "        binary_map = {data[binary_var].unique()[0]: 0,\n",
    "                     data[binary_var].unique()[1]: 1}\n",
    "        data[binary_var] = data[binary_var].map(binary_map)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r_pb, p_value = pointbiserialr(data[binary_var], data[continuous_var])\n",
    "    \n",
    "    print(\"1. Correlation Results:\")\n",
    "    print(f\"   Point-biserial r: {r_pb:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n2. Interpretation:\")\n",
    "    if r_pb > 0:\n",
    "        print(f\"   Positive correlation: Higher {continuous_var} ‚Üí More likely {binary_var}=1\")\n",
    "    else:\n",
    "        print(f\"   Negative correlation: Higher {continuous_var} ‚Üí More likely {binary_var}=0\")\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        print(f\"\\n3. Conclusion: SIGNIFICANT relationship (p < {alpha})\")\n",
    "    else:\n",
    "        print(f\"\\n3. Conclusion: NO significant relationship (p ‚â• {alpha})\")\n",
    "    \n",
    "    return {'r_pb': r_pb, 'p_value': p_value}\n",
    "\n",
    "# Example: Tenure vs Churn\n",
    "result = point_biserial_analysis(df, 'tenure', 'Churn')\n",
    "```\n",
    "\n",
    "### 6.4 Correlation Matrix and Heatmap\n",
    "\n",
    "**Purpose**: Visualize all pairwise correlations\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "def comprehensive_correlation_analysis(df, method='pearson'):\n",
    "    \"\"\"\n",
    "    Create comprehensive correlation matrix with visualization.\n",
    "    \"\"\"\n",
    "    # Select numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    if method == 'pearson':\n",
    "        corr_matrix = numeric_df.corr()\n",
    "    elif method == 'spearman':\n",
    "        corr_matrix = numeric_df.corr(method='spearman')\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, fmt='.2f', square=True, linewidths=1,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    \n",
    "    plt.title(f'{method.capitalize()} Correlation Matrix', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strongest correlations with target (if Churn exists)\n",
    "    if 'Churn' in corr_matrix.columns:\n",
    "        print(\"\\nStrongest Correlations with Churn:\")\n",
    "        churn_corr = corr_matrix['Churn'].abs().sort_values(ascending=False)\n",
    "        print(churn_corr[1:11])  # Top 10, excluding Churn itself\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Usage\n",
    "corr_matrix = comprehensive_correlation_analysis(df, method='pearson')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Distribution Analysis\n",
    "\n",
    "Understanding data distributions is critical for choosing appropriate statistical tests and models.\n",
    "\n",
    "### 7.1 Normality Tests\n",
    "\n",
    "#### 7.1.1 Shapiro-Wilk Test\n",
    "\n",
    "**Purpose**: Test if data comes from normal distribution\n",
    "\n",
    "**Python Implementation**:\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def test_normality(data, variable_name, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive normality testing.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NORMALITY TEST: {variable_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    stat, p_value = shapiro(data.sample(min(5000, len(data))))  # Sample for large datasets\n",
    "    \n",
    "    print(\"1. Shapiro-Wilk Test:\")\n",
    "    print(f\"   Statistic: {stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"   ‚úì Data appears normally distributed (p > {alpha})\")\n",
    "        normal = True\n",
    "    else:\n",
    "        print(f\"   ‚úó Data deviates from normal distribution (p ‚â§ {alpha})\")\n",
    "        normal = False\n",
    "    \n",
    "    # Visual checks\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
