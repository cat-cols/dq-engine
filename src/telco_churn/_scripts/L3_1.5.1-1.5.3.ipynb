{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cc9e0b",
   "metadata": {},
   "source": [
    "Here‚Äôs a self-contained Python script that does exactly what your 1.5.1‚Äì1.5.3 outline describes:\n",
    "\n",
    "* **1.5.1** Load raw dataset from `CONFIG[\"PATHS\"][\"RAW_DATA\"]`\n",
    "* **1.5.2** Compute file hash and compare against a **version registry**\n",
    "* **1.5.3** Log schema version info to `dataset_load_log.csv`\n",
    "\n",
    "You can drop this into something like `scripts/1_5_dataset_loader.py` and tweak paths as needed.\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "1.5.x Dataset Load & Version Logging\n",
    "\n",
    "1.5.1 Load Raw Dataset (CSV or Parquet)\n",
    "1.5.2 Hash / Snapshot Validation (version registry)\n",
    "1.5.3 Schema Version Logging (dataset_load_log.csv)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import yaml  # pip install pyyaml\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config loading\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "\n",
    "DEFAULT_CONFIG_PATH = PROJECT_ROOT / \"config\" / \"project_config.yaml\"\n",
    "\n",
    "# Example config structure (for reference):\n",
    "# PATHS:\n",
    "#   RAW_DATA: \"data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "#   ARTIFACTS: \"artifacts\"\n",
    "\n",
    "\n",
    "def load_config(path: Path = DEFAULT_CONFIG_PATH) -> Dict[str, Any]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found at {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def compute_file_hash(path: Path, algo: str = \"sha256\", chunk_size: int = 1 << 20) -> str:\n",
    "    \"\"\"Compute a stable hash of the file contents.\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def load_dataset(raw_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV or Parquet based on file extension.\"\"\"\n",
    "    if not raw_path.exists():\n",
    "        raise FileNotFoundError(f\"RAW_DATA file not found at {raw_path}\")\n",
    "\n",
    "    suffix = raw_path.suffix.lower()\n",
    "    if suffix == \".csv\":\n",
    "        df = pd.read_csv(raw_path)\n",
    "    elif suffix in {\".parquet\", \".pq\"}:\n",
    "        df = pd.read_parquet(raw_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {suffix}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Version registry + load log handling\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_artifact_paths(config: Dict[str, Any]) -> Dict[str, Path]:\n",
    "    artifacts_root = PROJECT_ROOT / config[\"PATHS\"].get(\"ARTIFACTS\", \"artifacts\")\n",
    "    artifacts_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    registry_path = artifacts_root / \"dataset_version_registry.csv\"\n",
    "    load_log_path = artifacts_root / \"dataset_load_log.csv\"\n",
    "\n",
    "    return {\n",
    "        \"artifacts_root\": artifacts_root,\n",
    "        \"registry_path\": registry_path,\n",
    "        \"load_log_path\": load_log_path,\n",
    "    }\n",
    "\n",
    "\n",
    "def upsert_version_registry(\n",
    "    registry_path: Path,\n",
    "    dataset_path: Path,\n",
    "    file_hash: str,\n",
    "    n_rows: int,\n",
    "    n_cols: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    1.5.2 Hash / Snapshot Validation\n",
    "\n",
    "    - Compare current file hash to existing registry.\n",
    "    - If seen before, reuse version_id.\n",
    "    - If new, create new version_id and append.\n",
    "    \"\"\"\n",
    "    now = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    if registry_path.exists():\n",
    "        registry = pd.read_csv(registry_path)\n",
    "    else:\n",
    "        registry = pd.DataFrame(columns=[\n",
    "            \"version_id\",\n",
    "            \"dataset_path\",\n",
    "            \"file_hash\",\n",
    "            \"first_seen_utc\",\n",
    "            \"last_seen_utc\",\n",
    "            \"n_rows\",\n",
    "            \"n_cols\",\n",
    "        ])\n",
    "\n",
    "    # If this hash already exists, reuse its version_id\n",
    "    existing = registry.loc[registry[\"file_hash\"] == file_hash]\n",
    "\n",
    "    if not existing.empty:\n",
    "        version_id = str(existing.iloc[0][\"version_id\"])\n",
    "        # Update last_seen_utc\n",
    "        registry.loc[registry[\"file_hash\"] == file_hash, \"last_seen_utc\"] = now\n",
    "    else:\n",
    "        # New version: increment version_id\n",
    "        if registry.empty:\n",
    "            next_id = 1\n",
    "        else:\n",
    "            # version_id might be string; coerce to int safely\n",
    "            max_id = pd.to_numeric(registry[\"version_id\"], errors=\"coerce\").fillna(0).max()\n",
    "            next_id = int(max_id) + 1\n",
    "\n",
    "        version_id = str(next_id)\n",
    "        new_row = pd.DataFrame(\n",
    "            {\n",
    "                \"version_id\": [version_id],\n",
    "                \"dataset_path\": [str(dataset_path)],\n",
    "                \"file_hash\": [file_hash],\n",
    "                \"first_seen_utc\": [now],\n",
    "                \"last_seen_utc\": [now],\n",
    "                \"n_rows\": [n_rows],\n",
    "                \"n_cols\": [n_cols],\n",
    "            }\n",
    "        )\n",
    "        registry = pd.concat([registry, new_row], ignore_index=True)\n",
    "\n",
    "    # Atomic write\n",
    "    tmp_path = registry_path.with_suffix(registry_path.suffix + \".tmp\")\n",
    "    registry.to_csv(tmp_path, index=False)\n",
    "    os.replace(tmp_path, registry_path)\n",
    "\n",
    "    return version_id\n",
    "\n",
    "\n",
    "def append_dataset_load_log(\n",
    "    load_log_path: Path,\n",
    "    dataset_path: Path,\n",
    "    version_id: str,\n",
    "    file_hash: str,\n",
    "    n_rows: int,\n",
    "    n_cols: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    1.5.3 Schema Version Logging\n",
    "\n",
    "    Append a single row to dataset_load_log.csv with:\n",
    "      - timestamp\n",
    "      - dataset_path\n",
    "      - version_id\n",
    "      - file_hash\n",
    "      - n_rows, n_cols\n",
    "    \"\"\"\n",
    "    now = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    log_row = pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp_utc\": [now],\n",
    "            \"dataset_path\": [str(dataset_path)],\n",
    "            \"version_id\": [version_id],\n",
    "            \"file_hash\": [file_hash],\n",
    "            \"n_rows\": [n_rows],\n",
    "            \"n_cols\": [n_cols],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if load_log_path.exists():\n",
    "        existing = pd.read_csv(load_log_path)\n",
    "        all_cols = pd.Index(existing.columns).union(log_row.columns)\n",
    "        out = pd.concat(\n",
    "            [existing.reindex(columns=all_cols),\n",
    "             log_row.reindex(columns=all_cols)],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    else:\n",
    "        out = log_row\n",
    "\n",
    "    tmp_path = load_log_path.with_suffix(load_log_path.suffix + \".tmp\")\n",
    "    out.to_csv(tmp_path, index=False)\n",
    "    os.replace(tmp_path, load_log_path)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main entrypoint\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main(config_path: Path = DEFAULT_CONFIG_PATH) -> pd.DataFrame:\n",
    "    # Load config\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    raw_path = PROJECT_ROOT / config[\"PATHS\"][\"RAW_DATA\"]\n",
    "    paths = get_artifact_paths(config)\n",
    "\n",
    "    print(f\"1.5.1) üì• Loading raw dataset from: {raw_path}\")\n",
    "    df = load_dataset(raw_path)\n",
    "    n_rows, n_cols = df.shape\n",
    "    print(f\"   ‚Üí Loaded dataset shape: {n_rows} rows √ó {n_cols} cols\")\n",
    "\n",
    "    print(\"1.5.2) üîê Computing file hash and updating version registry\")\n",
    "    file_hash = compute_file_hash(raw_path, algo=\"sha256\")\n",
    "    version_id = upsert_version_registry(\n",
    "        paths[\"registry_path\"],\n",
    "        raw_path,\n",
    "        file_hash,\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "    )\n",
    "    print(f\"   ‚Üí Dataset version_id: {version_id}\")\n",
    "\n",
    "    print(\"1.5.3) üßæ Appending to dataset_load_log.csv\")\n",
    "    append_dataset_load_log(\n",
    "        paths[\"load_log_path\"],\n",
    "        raw_path,\n",
    "        version_id,\n",
    "        file_hash,\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "    )\n",
    "    print(f\"   ‚Üí Load log updated at: {paths['load_log_path']}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_loaded = main()\n",
    "    # df_loaded is available here if you want to quickly inspect or debug\n",
    "```\n",
    "\n",
    "### How this maps to your outline\n",
    "\n",
    "* **1.5.1 Load Raw Dataset**\n",
    "\n",
    "  * `load_dataset(raw_path)` reads CSV/Parquet from `CONFIG[\"PATHS\"][\"RAW_DATA\"]`.\n",
    "\n",
    "* **1.5.2 Hash / Snapshot Validation**\n",
    "\n",
    "  * `compute_file_hash(raw_path, \"sha256\")`\n",
    "  * `upsert_version_registry(...)` reads/updates `dataset_version_registry.csv` and assigns a `version_id`.\n",
    "\n",
    "* **1.5.3 Schema Version Logging**\n",
    "\n",
    "  * `append_dataset_load_log(...)` appends a row to `dataset_load_log.csv` with timestamp, version, hash, n_rows, n_cols.\n",
    "\n",
    "You can now plug this script into your Section 1.5 dependency chain and reference the outputs (`dataset_version_registry.csv`, `dataset_load_log.csv`) in later sections (e.g., your data contract / DQ reports).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
