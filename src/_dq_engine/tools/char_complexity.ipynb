{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ad994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# /Telco/_T2/Level_3/notebooks/02_DQ_IF.ipynb\n",
    "NOTEBOOK_PATH = \"/Users/b/DATA/PROJECTS/Telco/_T2/Level_3/notebooks/02_DQ_IF.ipynb\"  # <-- Change to your notebook path\n",
    "\n",
    "THRESHOLDS = {\n",
    "    \"max_markdown_cells\": 100,\n",
    "    \"max_code_cells\": 150,\n",
    "    \"max_avg_markdown_length\": 500,\n",
    "    \"max_avg_code_length\": 700,\n",
    "    \"max_consecutive_markdown\": 5,\n",
    "    \"max_md_code_ratio\": 2.0,\n",
    "}\n",
    "\n",
    "# === ANALYSIS SCRIPT ===\n",
    "def analyze_notebook(path):\n",
    "    with open(path) as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    code_cells = []\n",
    "    md_cells = []\n",
    "    all_cells = nb.get(\"cells\", [])\n",
    "    md_streaks = []\n",
    "    current_md_streak = []\n",
    "\n",
    "    for i, cell in enumerate(all_cells):\n",
    "        if cell[\"cell_type\"] == \"markdown\":\n",
    "            md_cells.append((i, cell))\n",
    "            current_md_streak.append(i)\n",
    "        elif cell[\"cell_type\"] == \"code\":\n",
    "            code_cells.append((i, cell))\n",
    "            if current_md_streak:\n",
    "                md_streaks.append(current_md_streak)\n",
    "                current_md_streak = []\n",
    "    if current_md_streak:\n",
    "        md_streaks.append(current_md_streak)\n",
    "\n",
    "    report = []\n",
    "    md_lengths = [len(\"\".join(cell[\"source\"])) for _, cell in md_cells]\n",
    "    code_lengths = [len(\"\".join(cell[\"source\"])) for _, cell in code_cells]\n",
    "\n",
    "    md_count = len(md_cells)\n",
    "    code_count = len(code_cells)\n",
    "    avg_md_len = sum(md_lengths)/md_count if md_count else 0\n",
    "    avg_code_len = sum(code_lengths)/code_count if code_count else 0\n",
    "    md_code_ratio = md_count / max(code_count, 1)\n",
    "\n",
    "    # === REPORT CHECKS ===\n",
    "    if md_count > THRESHOLDS[\"max_markdown_cells\"]:\n",
    "        report.append(f\"‚ö†Ô∏è Markdown cells: {md_count} exceeds threshold ({THRESHOLDS['max_markdown_cells']})\")\n",
    "\n",
    "    if code_count > THRESHOLDS[\"max_code_cells\"]:\n",
    "        report.append(f\"‚ö†Ô∏è Code cells: {code_count} exceeds threshold ({THRESHOLDS['max_code_cells']})\")\n",
    "\n",
    "    if avg_md_len > THRESHOLDS[\"max_avg_markdown_length\"]:\n",
    "        long_md = [i for i, l in zip([i for i, _ in md_cells], md_lengths) if l > THRESHOLDS[\"max_avg_markdown_length\"]]\n",
    "        report.append(f\"‚ö†Ô∏è Avg Markdown cell length: {avg_md_len:.1f} exceeds {THRESHOLDS['max_avg_markdown_length']} (cells: {long_md})\")\n",
    "\n",
    "    if avg_code_len > THRESHOLDS[\"max_avg_code_length\"]:\n",
    "        long_code = [i for i, l in zip([i for i, _ in code_cells], code_lengths) if l > THRESHOLDS[\"max_avg_code_length\"]]\n",
    "        report.append(f\"‚ö†Ô∏è Avg Code cell length: {avg_code_len:.1f} exceeds {THRESHOLDS['max_avg_code_length']} (cells: {long_code})\")\n",
    "\n",
    "    long_streaks = [streak for streak in md_streaks if len(streak) > THRESHOLDS[\"max_consecutive_markdown\"]]\n",
    "    for streak in long_streaks:\n",
    "        report.append(f\"‚ö†Ô∏è Long markdown streak: {len(streak)} cells ‚Üí indices: {streak}\")\n",
    "\n",
    "    if md_code_ratio > THRESHOLDS[\"max_md_code_ratio\"]:\n",
    "        report.append(f\"‚ö†Ô∏è High markdown/code ratio: {md_code_ratio:.2f} exceeds {THRESHOLDS['max_md_code_ratio']}\")\n",
    "\n",
    "    print(\"\\nüìä Notebook Complexity Report\")\n",
    "    print(\"=\"*35)\n",
    "    print(f\"üìÑ Markdown cells: {md_count}\")\n",
    "    print(f\"üìü Code cells: {code_count}\")\n",
    "    print(f\"‚úèÔ∏è Avg markdown length: {avg_md_len:.1f}\")\n",
    "    print(f\"üßÆ Avg code length: {avg_code_len:.1f}\")\n",
    "    print(f\"üìä Markdown/code ratio: {md_code_ratio:.2f}\")\n",
    "    print(\"\\nüîç Issues Found:\")\n",
    "    if report:\n",
    "        for r in report:\n",
    "            print(r)\n",
    "    else:\n",
    "        print(\"‚úÖ No major complexity issues detected.\")\n",
    "\n",
    "# === RUN ===\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_notebook(NOTEBOOK_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython import get_ipython\n",
    "\n",
    "# 1) Aggregate DataFrames by size (FIXED: static globals copy)\n",
    "print(\"üîç PERFORMANCE AUDIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ‚úÖ FIXED: Create static copy to avoid mutation during iteration\n",
    "globals_copy = dict(globals())  # Static snapshot\n",
    "df_sizes = []\n",
    "for name, obj in globals_copy.items():\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        size_mb = obj.memory_usage(deep=True).sum() / (1024**2)\n",
    "        df_sizes.append({\"name\": name, \"shape\": obj.shape, \"size_mb\": size_mb})\n",
    "\n",
    "df_sizes_df = pd.DataFrame(df_sizes).sort_values(\"size_mb\", ascending=False)\n",
    "print(\"\\nüìä 1. DataFrames by SIZE (MB)\")\n",
    "display(df_sizes_df.head(10))\n",
    "\n",
    "total_df_mem = df_sizes_df[\"size_mb\"].sum()\n",
    "print(f\"üíæ TOTAL DataFrame memory: {total_df_mem:.1f} MB\")\n",
    "\n",
    "# 2) Heavy loops (unchanged)\n",
    "heavy_loops = []\n",
    "try:\n",
    "    notebook = get_ipython().history_manager.input_hist.raw_reset()\n",
    "    for i, cell in enumerate(notebook[-50:], 1):\n",
    "        lines = cell.split('\\n')\n",
    "        for line_num, line in enumerate(lines, 1):\n",
    "            if re.search(r'for\\s+\\w+\\s+in\\s+range?\\(', line) or 'for i in range' in line:\n",
    "                heavy_loops.append({\"cell\": i, \"line\": line_num, \"code\": line.strip()[:80]})\n",
    "            if any(x in line for x in ['for i in', '[i for i in', 'apply(', 'iterrows']):\n",
    "                heavy_loops.append({\"cell\": i, \"line\": line_num, \"code\": line.strip()[:80]})\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Could not access notebook history\")\n",
    "    \n",
    "print(f\"\\nüîÑ 2. Heavy LOOPS/CALCS: {len(heavy_loops)} found\")\n",
    "if heavy_loops:\n",
    "    display(pd.DataFrame(heavy_loops).drop_duplicates())\n",
    "\n",
    "# 3) Plots (simplified)\n",
    "plot_calls = []\n",
    "try:\n",
    "    for i, cell in enumerate(notebook[-50:], 1):\n",
    "        if any(x in cell.lower() for x in ['plt.plot', 'plt.scatter', 'sns', 'plotly']):\n",
    "            data_size = sum(len(re.findall(r'\\d+', cell)) for cell in cell.split(';'))\n",
    "            if data_size > 750:\n",
    "                plot_calls.append({\"cell\": i, \"code_snip\": cell[:100], \"est_points\": data_size})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nüìà 3. PLOTS >750 points: {len(plot_calls)} found\")\n",
    "if plot_calls:\n",
    "    display(pd.DataFrame(plot_calls))\n",
    "\n",
    "# 4) CSV calls (unchanged)\n",
    "csv_calls = Counter()\n",
    "try:\n",
    "    for i, cell in enumerate(notebook[-100:], 1):\n",
    "        csv_count = len(re.findall(r'pd\\.read_csv\\s*\\(', cell))\n",
    "        if csv_count > 0:\n",
    "            csv_calls[i] += csv_count\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nüìÇ 4. pd.read_csv() calls: {sum(csv_calls.values())} total\")\n",
    "csv_df = pd.DataFrame([{\"cell\": cell, \"count\": count} for cell, count in csv_calls.most_common()])\n",
    "if not csv_df.empty:\n",
    "    display(csv_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "if not df_sizes_df.empty:\n",
    "    top_dfs = df_sizes_df.head(10)\n",
    "    plt.barh(range(len(top_dfs)), top_dfs['size_mb'])\n",
    "    plt.yticks(range(len(top_dfs)), top_dfs['name'])\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.title('Top 10 DataFrames by Memory Usage')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ AUDIT COMPLETE!\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# üîß Set this to the *current* notebook filename\n",
    "NB_NAME = \"02_DQ_IF.ipynb\"   # change if your notebook name differs\n",
    "\n",
    "nb_path = Path(NB_NAME).resolve()\n",
    "print(\"Notebook path:\", nb_path)\n",
    "\n",
    "first_lines = []\n",
    "if nb_path.exists():\n",
    "    with nb_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        nb = json.load(f)\n",
    "\n",
    "    for i, cell in enumerate(nb.get(\"cells\", []), start=1):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "\n",
    "        src = cell.get(\"source\", [])\n",
    "        if isinstance(src, str):\n",
    "            lines = src.splitlines()\n",
    "        else:\n",
    "            lines = [str(x) for x in src]\n",
    "\n",
    "        first = lines[0].rstrip(\"\\n\") if lines else \"\"\n",
    "        first_lines.append({\"cell_index\": i, \"first_line\": first})\n",
    "\n",
    "    df_first_lines = pd.DataFrame(first_lines)\n",
    "    display(df_first_lines)\n",
    "else:\n",
    "    print(\"‚ùå Notebook file not found:\", nb_path)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "from IPython.core.getipython import get_ipython\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Set this to the current notebook file name\n",
    "NB_NAME = \"02_DQ_IF.ipynb\"   # <-- change if different\n",
    "nb_path = Path(NB_NAME).resolve()\n",
    "print(\"Notebook path:\", nb_path)\n",
    "\n",
    "if not nb_path.exists():\n",
    "    raise FileNotFoundError(f\"Notebook file not found: {nb_path}\")\n",
    "\n",
    "# 2) Collect first line of every code cell\n",
    "with nb_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "first_lines = []\n",
    "for i, cell in enumerate(nb.get(\"cells\", []), start=1):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "    src = cell.get(\"source\", [])\n",
    "    if isinstance(src, str):\n",
    "        lines = src.splitlines()\n",
    "    else:\n",
    "        lines = [str(x) for x in src]\n",
    "    first = lines[0].rstrip(\"\\n\") if lines else \"\"\n",
    "    first_lines.append((i, first))\n",
    "\n",
    "df_first_lines = pd.DataFrame(first_lines, columns=[\"cell_index\", \"first_line\"])\n",
    "display(df_first_lines)\n",
    "\n",
    "# 3) Build markdown list\n",
    "md_lines = [\"# Code cell first lines\", \"\"]\n",
    "for idx, line in first_lines:\n",
    "    safe = line.replace(\"\\n\", \" \").replace(\"|\", r\"\\|\")\n",
    "    md_lines.append(f\"- **Cell {idx}**: `{safe}`\")\n",
    "md_text = \"\\n\".join(md_lines)\n",
    "\n",
    "# 4) Inject markdown into the *next* cell\n",
    "shell = get_ipython()\n",
    "shell.set_next_input(md_text, replace=False)\n",
    "\n",
    "print(\"\\n‚úÖ A new cell has been pre-filled below.\")\n",
    "print(\"üëâ Change its type to *Markdown* (press 'M' in command mode) and run it.\")\n",
    "\n",
    "import nbformat\n",
    "from nbformat import read\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- config ---\n",
    "NB_PATH = Path(\"02_DQ_IF.ipynb\")  # change to your notebook path\n",
    "\n",
    "# --- heuristics for DataFrame creation patterns ---\n",
    "DF_PATTERNS = [\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*pd\\.read_csv\\(\",          # df = pd.read_csv(...)\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*pd\\.read_parquet\\(\",\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*pd\\.DataFrame\\(\",\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*\\w+\\.copy\\(\",\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*\\w+\\.merge\\(\",\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*\\w+\\.join\\(\",\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*\\w+\\.groupby\\(\",\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*\\w+\\.pivot\",             # pivot / pivot_table\n",
    "    r\"^\\s*(\\w+)\\s*=\\s*df_[a-zA-Z0-9_]+\\b\",     # df_x = df_y\n",
    "]\n",
    "\n",
    "compiled_patterns = [re.compile(p) for p in DF_PATTERNS]\n",
    "\n",
    "def find_dataframe_creations(nb_path: Path) -> pd.DataFrame:\n",
    "    with nb_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        nb = read(f, as_version=4)  # v4 is standard Jupyter format [web:274]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for cell_idx, cell in enumerate(nb.get(\"cells\", []), start=1):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "\n",
    "        src = cell.get(\"source\", \"\")\n",
    "        # nbformat may store source as list or str\n",
    "        if isinstance(src, list):\n",
    "            lines = src\n",
    "        else:\n",
    "            lines = src.splitlines()\n",
    "\n",
    "        for line_no, line in enumerate(lines, start=1):\n",
    "            for pat in compiled_patterns:\n",
    "                m = pat.match(line)\n",
    "                if m:\n",
    "                    var_name = m.group(1)\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"cell_index\": cell_idx,\n",
    "                            \"line_no\": line_no,\n",
    "                            \"var_name\": var_name,\n",
    "                            \"code_line\": line.strip(),\n",
    "                        }\n",
    "                    )\n",
    "                    break  # avoid double-counting same line\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_created = find_dataframe_creations(NB_PATH)\n",
    "print(f\"Found {len(df_created)} potential DataFrame creations\")\n",
    "display(df_created)\n",
    "\n",
    "# Agg all LIVE DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "engine_dfs = []\n",
    "\n",
    "for name, obj in globals().items():\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        engine_dfs.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"rows\": obj.shape[0],\n",
    "                \"cols\": obj.shape[1],\n",
    "                \"memory_mb\": obj.memory_usage(deep=True).sum() / (1024 ** 2),\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_registry = pd.DataFrame(engine_dfs).sort_values(\"memory_mb\", ascending=False)\n",
    "df_registry\n",
    "\n",
    "engine_registry = df_registry[\n",
    "    ~df_registry[\"name\"].str.contains(\"tmp|test|debug\", case=False, na=False)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "display(engine_registry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_nb = \"/Users/b/DATA/PROJECTS/Telco/Level_3/notebooks/02_DQ_IF.ipynb\"\n",
    "# out_nb = \"/Users/b/DATA/PROJECTS/Telco/Level_3/outputs/_AGG/02_DQ_IF_AGG.ipynb\"\n",
    "\n",
    "# aggregate_code_cells(src_nb, out_nb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
