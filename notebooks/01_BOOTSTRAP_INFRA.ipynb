{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c307757",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Is this infrastructure bootstrap worth turning into a project of its own?\n",
    "</summary>\n",
    "\n",
    "Yes ‚Äî **but only if you treat it as a *library*, not a second pipeline.** Turning `01_INFRA` into ‚Äúthe infra package‚Äù is worth it because it contains the exact stuff that separates a notebook experiment from a hireable data product:\n",
    "\n",
    "* deterministic **project root resolution**\n",
    "* **config loading + validation**\n",
    "* **run identity + run-scoped directories**\n",
    "* **logging + artifact paths**\n",
    "* environment ‚Äúpreflight‚Äù checks (deps, paths, versions)\n",
    "* a stable ‚Äúcontext‚Äù object you can pass around\n",
    "\n",
    "What‚Äôs *not* worth it: keeping `01_INFRA` as its own standalone notebook that people are expected to run first.\n",
    "\n",
    "### The hireable move\n",
    "\n",
    "Extract the good bits of `01_INFRA` into:\n",
    "\n",
    "* `src/dq_engine/bootstrap.py` (or `src/dq_engine/infra/‚Ä¶`)\n",
    "* optionally `src/dq_engine/config/‚Ä¶`\n",
    "\n",
    "Then make:\n",
    "\n",
    "* `02_DQ_IF.ipynb` the one public ‚Äúrunner notebook‚Äù\n",
    "* `01_INFRA.ipynb` moved to `notebooks/_archive/` as provenance\n",
    "\n",
    "### When it‚Äôs definitely worth it\n",
    "\n",
    "It‚Äôs worth it if you want any of these (and you do, if you‚Äôre aiming hireable):\n",
    "\n",
    "* **CI runs** (GitHub Actions can‚Äôt ‚Äúrun a notebook preamble‚Äù reliably)\n",
    "* **multi-dataset** support (Telco today, anything tomorrow)\n",
    "* **warehouse targets** (DuckDB local + Snowflake prod)\n",
    "* **dbt integration** (run dbt + store artifacts + score results)\n",
    "* clean CLI entrypoint: `dq run --config ...`\n",
    "\n",
    "### What ‚Äúmaking a project out of it‚Äù should mean\n",
    "\n",
    "Not a new project. A module.\n",
    "\n",
    "Minimum extraction list (high ROI):\n",
    "\n",
    "1. `discover_project_root()`\n",
    "2. `load_config(path) -> cfg`\n",
    "3. `make_run_context(cfg) -> ctx`\n",
    "\n",
    "   * run_id, run_ts\n",
    "   * run dirs (reports/artifacts/logs)\n",
    "   * config snapshot\n",
    "4. `preflight(ctx)`\n",
    "\n",
    "   * ensure dirs, warn on missing optional deps, etc.\n",
    "\n",
    "Everything else can stay in notebooks until you‚Äôre ready.\n",
    "\n",
    "### A simple gut-check\n",
    "\n",
    "If you delete `01_INFRA.ipynb` tomorrow, would you lose reusable engineering value?\n",
    "Yes. So keep the value ‚Äî **but move it into code** so it can be used by notebook, CLI, and CI.\n",
    "\n",
    "That‚Äôs the sweet spot: you get the maturity signal without multiplying notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6685e5c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Help converting the bootstrap into functions\n",
    "</summary>\n",
    "\n",
    "Help me move it into code. Please take a deep dive into the file and explain how to refactor in depth.\n",
    "\n",
    "Yep. I did a deep read of `01_INFRA.ipynb`, and the good news is: it already *contains* most of the ‚Äúhireable‚Äù infrastructure patterns ‚Äî they‚Äôre just trapped in notebook cells and a couple parts are doing too much.\n",
    "\n",
    "Below is an in-depth refactor plan that maps your notebook sections into a clean Python package, with a minimal, high-signal interface: **`bootstrap_context()` ‚Üí `load_dataset()` ‚Üí `profile_schema()` ‚Üí `write_setup_summary()`**.\n",
    "\n",
    "---\n",
    "\n",
    "## What `01_INFRA.ipynb` is doing (so we refactor the right stuff)\n",
    "\n",
    "### Section 1.1 ‚Äî Root + directories\n",
    "\n",
    "* **1.1.1** resolves `PROJECT_ROOT` via heuristic ‚Äúfolder named Telco‚Äù (and hints at a better heuristic: ‚Äúconfig exists above‚Äù).\n",
    "* **1.1.2** creates Level-root subfolders and a `DIRS` dict.\n",
    "* **1.1.3** does a ‚Äúcan I write here?‚Äù smoke test.\n",
    "\n",
    "‚úÖ Keep all of this, but **make it a reusable module** and stop hardcoding ‚ÄúTelco‚Äù.\n",
    "\n",
    "### Section 1.2 ‚Äî Config loader + `C()` dotted getter\n",
    "\n",
    "* Loads `project_config.yaml`\n",
    "* Stores `CONFIG` + read-only `CFG`\n",
    "* Adds `C(\"PATHS.RAW_DATA\")` dotted lookup\n",
    "* Writes `config_snapshot.json`\n",
    "\n",
    "‚úÖ This is solid. In code, it becomes a `Config` object + `get()` method + optional schema validation.\n",
    "\n",
    "### Section 1.3 ‚Äî Resolve config-driven paths + seeds + protected columns\n",
    "\n",
    "* Resolves RAW/PROCESSED dirs with fallback keys\n",
    "* Creates dirs\n",
    "* Reads target/id/ranges/data_quality/flags\n",
    "* Sets RNG seeds\n",
    "* Defines ‚Äúprotected columns‚Äù\n",
    "\n",
    "‚ö†Ô∏è Here‚Äôs the boundary issue: some of this is **infra** (paths, seeds), and some is **dataset semantics** (target/id/protected). We‚Äôll split those cleanly.\n",
    "\n",
    "### Section 1.4 ‚Äî Load dataset + version registry\n",
    "\n",
    "* Loads CSV/parquet using `READ_OPTS`\n",
    "* Hashes file\n",
    "* Maintains `dataset_version_registry.csv`\n",
    "* Appends `dataset_load_log.csv`\n",
    "\n",
    "‚úÖ Very hireable. In code, this becomes a `DatasetVersioning` utility.\n",
    "üí° In a warehouse-first world, you‚Äôll later swap file hashing for **table snapshot metadata** ‚Äî but keep the interface.\n",
    "\n",
    "### Section 1.5 ‚Äî Profiling + target normalization\n",
    "\n",
    "* Checks non-empty\n",
    "* Builds schema summary (type grouping, null%, n_unique)\n",
    "* Normalizes target labels and creates numeric flag\n",
    "* Writes target summary CSV\n",
    "\n",
    "‚ö†Ô∏è This is where you should start trimming: **in a dbt + warehouse version, target normalization should move to dbt** (staging/marts). But the *profiling + reporting* part stays.\n",
    "\n",
    "### Section 1.6 ‚Äî Environment capture + setup summary\n",
    "\n",
    "* Captures system/library versions\n",
    "* Writes `setup_summary.json`\n",
    "* Writes `core_lib_versions.json`\n",
    "\n",
    "‚úÖ Great ‚Äúproduction vibe‚Äù and easy to code-ify.\n",
    "\n",
    "---\n",
    "\n",
    "## The end state you want (hireable shape)\n",
    "\n",
    "**Notebook becomes a thin runner**. Your code becomes reusable from:\n",
    "\n",
    "* notebooks\n",
    "* CLI (`dq run --config ...`)\n",
    "* CI (GitHub Actions)\n",
    "\n",
    "A clean, readable call pattern:\n",
    "\n",
    "```python\n",
    "from dq_engine.bootstrap import bootstrap_context\n",
    "from dq_engine.dataset import load_local_dataset\n",
    "from dq_engine.profiling import profile_schema\n",
    "from dq_engine.reporting import write_setup_summary\n",
    "\n",
    "ctx = bootstrap_context(\"config/project_config.yaml\")\n",
    "df, ds_meta = load_local_dataset(ctx)   # later: load from warehouse\n",
    "schema_df = profile_schema(df, ctx)\n",
    "write_setup_summary(ctx, ds_meta, schema_df)\n",
    "```\n",
    "\n",
    "That‚Äôs the ‚Äúrecruiter can understand it in 90 seconds‚Äù version.\n",
    "\n",
    "---\n",
    "\n",
    "## Refactor plan (step-by-step) with a concrete module layout\n",
    "\n",
    "Create this structure:\n",
    "\n",
    "```\n",
    "src/dq_engine/\n",
    "  __init__.py\n",
    "  bootstrap.py\n",
    "  config.py\n",
    "  paths.py\n",
    "  env.py\n",
    "  dataset.py\n",
    "  versioning.py\n",
    "  profiling.py\n",
    "  reporting.py\n",
    "  types.py\n",
    "```\n",
    "\n",
    "### 1) `config.py` ‚Äî move 1.2.1‚Äì1.2.3 here\n",
    "\n",
    "Your notebook‚Äôs YAML loader + `C()` becomes a small class.\n",
    "\n",
    "```python\n",
    "# src/dq_engine/config.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from types import MappingProxyType\n",
    "from typing import Any, Optional\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    path: Path\n",
    "    data: MappingProxyType\n",
    "\n",
    "    def get(self, dotted: str, default: Any = None) -> Any:\n",
    "        node: Any = dict(self.data)\n",
    "        for part in dotted.split(\".\"):\n",
    "            if isinstance(node, dict) and part in node:\n",
    "                node = node[part]\n",
    "            else:\n",
    "                return default\n",
    "        return node\n",
    "\n",
    "def load_config(path: str | Path, snapshot_dir: Optional[Path] = None) -> Config:\n",
    "    p = Path(path).resolve()\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {p}\")\n",
    "    obj = yaml.safe_load(p.read_text(encoding=\"utf-8\")) or {}\n",
    "    cfg = Config(path=p, data=MappingProxyType(obj))\n",
    "    if snapshot_dir:\n",
    "        snapshot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (snapshot_dir / \"config_snapshot.json\").write_text(\n",
    "            json.dumps(obj, indent=2), encoding=\"utf-8\"\n",
    "        )\n",
    "    return cfg\n",
    "```\n",
    "\n",
    "**Why this matters:** you eliminate global `CONFIG/CFG/C()` and make config explicit and testable.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) `paths.py` ‚Äî move 1.1.1‚Äì1.1.3 and 1.3.1 (path resolution) here\n",
    "\n",
    "#### Replace the ‚ÄúTelco folder name‚Äù heuristic\n",
    "\n",
    "Make `discover_project_root()` support:\n",
    "\n",
    "* environment variable override\n",
    "* ‚Äúcontains config file‚Äù heuristic\n",
    "* fallback to ‚Äúgit root‚Äù if available\n",
    "\n",
    "```python\n",
    "# src/dq_engine/paths.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProjectPaths:\n",
    "    project_root: Path\n",
    "    level_root: Path\n",
    "    config_dir: Path\n",
    "    reports_dir: Path\n",
    "    artifacts_dir: Path\n",
    "    figures_dir: Path\n",
    "    models_dir: Path\n",
    "    outputs_dir: Path\n",
    "    raw_data_dir: Path\n",
    "    processed_dir: Path\n",
    "\n",
    "def _git_root(cwd: Path) -> Path | None:\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], cwd=cwd)\n",
    "        return Path(out.decode().strip()).resolve()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def discover_project_root(start: Path, config_rel: str = \"config/project_config.yaml\") -> Path:\n",
    "    env_root = os.getenv(\"DQ_PROJECT_ROOT\")\n",
    "    if env_root:\n",
    "        return Path(env_root).expanduser().resolve()\n",
    "\n",
    "    start = start.resolve()\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / config_rel).exists():\n",
    "            return parent\n",
    "    gr = _git_root(start)\n",
    "    if gr:\n",
    "        return gr\n",
    "    raise FileNotFoundError(f\"Could not discover project root from {start}\")\n",
    "\n",
    "def build_paths(project_root: Path, level_name: str, cfg) -> ProjectPaths:\n",
    "    level_root = project_root / level_name\n",
    "    config_dir = level_root / \"config\"\n",
    "\n",
    "    # config-driven fallbacks (your 1.3.1 logic)\n",
    "    raw_rel = cfg.get(\"PATHS.RAW_DATA\") or cfg.get(\"PATHS.RAW_DATA_DIR\")\n",
    "    proc_rel = cfg.get(\"PATHS.PROCESSED\") or cfg.get(\"PATHS.PROCESSED_DIR\")\n",
    "    if not raw_rel:\n",
    "        raise KeyError(\"Missing PATHS.RAW_DATA or PATHS.RAW_DATA_DIR\")\n",
    "    if not proc_rel:\n",
    "        raise KeyError(\"Missing PATHS.PROCESSED or PATHS.PROCESSED_DIR\")\n",
    "\n",
    "    return ProjectPaths(\n",
    "        project_root=project_root,\n",
    "        level_root=level_root,\n",
    "        config_dir=config_dir,\n",
    "        reports_dir=project_root / cfg.get(\"PATHS.REPORTS\"),\n",
    "        artifacts_dir=project_root / cfg.get(\"PATHS.ARTIFACTS\"),\n",
    "        figures_dir=project_root / cfg.get(\"PATHS.FIGURES\"),\n",
    "        models_dir=project_root / cfg.get(\"PATHS.MODELS\"),\n",
    "        outputs_dir=project_root / cfg.get(\"PATHS.OUTPUTS\"),\n",
    "        raw_data_dir=project_root / raw_rel,\n",
    "        processed_dir=project_root / proc_rel,\n",
    "    )\n",
    "\n",
    "def ensure_dirs(*paths: Path) -> None:\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def assert_writable(*paths: Path) -> None:\n",
    "    for p in paths:\n",
    "        test = p / \".write_test.tmp\"\n",
    "        test.write_text(\"ok\", encoding=\"utf-8\")\n",
    "        test.unlink(missing_ok=True)\n",
    "```\n",
    "\n",
    "This preserves your current behavior but makes it portable and ‚Äúagnostic.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### 3) `env.py` ‚Äî move 1.6.1 and core versions snapshot here\n",
    "\n",
    "```python\n",
    "# src/dq_engine/env.py\n",
    "from __future__ import annotations\n",
    "from datetime import datetime\n",
    "import platform, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def capture_env_info() -> dict:\n",
    "    return {\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        \"system\": {\n",
    "            \"platform\": platform.system(),\n",
    "            \"platform_release\": platform.release(),\n",
    "            \"platform_version\": platform.version(),\n",
    "            \"machine\": platform.machine(),\n",
    "            \"python_version\": sys.version,\n",
    "        },\n",
    "        \"libraries\": {\n",
    "            \"pandas\": pd.__version__,\n",
    "            \"numpy\": np.__version__,\n",
    "        },\n",
    "    }\n",
    "```\n",
    "\n",
    "Later you can optionally include seaborn/matplotlib if installed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) `versioning.py` ‚Äî move 1.4.2 + 1.4.3 here (hash registry + load log)\n",
    "\n",
    "This is one of your best ‚Äúhireable signals‚Äù because it shows **reproducibility**.\n",
    "\n",
    "```python\n",
    "# src/dq_engine/versioning.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import hashlib, os\n",
    "import pandas as pd\n",
    "\n",
    "def compute_file_hash(path: Path, algo: str = \"sha256\", chunk_size: int = 1 << 20) -> str:\n",
    "    h = hashlib.new(algo)\n",
    "    with path.open(\"rb\") as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetMeta:\n",
    "    dataset_path: str\n",
    "    file_hash: str\n",
    "    version_id: str\n",
    "    n_rows: int\n",
    "    n_cols: int\n",
    "    timestamp_utc: str\n",
    "\n",
    "def update_version_registry(registry_path: Path, dataset_path: Path, file_hash: str, n_rows: int, n_cols: int) -> str:\n",
    "    now_utc = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    if registry_path.exists():\n",
    "        reg = pd.read_csv(registry_path)\n",
    "    else:\n",
    "        reg = pd.DataFrame(columns=[\"version_id\",\"dataset_path\",\"file_hash\",\"first_seen_utc\",\"last_seen_utc\",\"n_rows\",\"n_cols\"])\n",
    "\n",
    "    existing = reg.loc[reg[\"file_hash\"] == file_hash]\n",
    "    if not existing.empty:\n",
    "        version_id = str(existing.iloc[0][\"version_id\"])\n",
    "        reg.loc[reg[\"file_hash\"] == file_hash, \"last_seen_utc\"] = now_utc\n",
    "    else:\n",
    "        next_id = 1 if reg.empty else int(pd.to_numeric(reg[\"version_id\"], errors=\"coerce\").fillna(0).max()) + 1\n",
    "        version_id = str(next_id)\n",
    "        reg = pd.concat([reg, pd.DataFrame([{\n",
    "            \"version_id\": version_id,\n",
    "            \"dataset_path\": str(dataset_path),\n",
    "            \"file_hash\": file_hash,\n",
    "            \"first_seen_utc\": now_utc,\n",
    "            \"last_seen_utc\": now_utc,\n",
    "            \"n_rows\": n_rows,\n",
    "            \"n_cols\": n_cols,\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    tmp = registry_path.with_suffix(\".tmp\")\n",
    "    reg.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, registry_path)\n",
    "    return version_id\n",
    "\n",
    "def append_load_log(load_log_path: Path, meta: DatasetMeta, read_opts: dict) -> None:\n",
    "    row = pd.DataFrame([{\n",
    "        \"timestamp_utc\": meta.timestamp_utc,\n",
    "        \"dataset_path\": meta.dataset_path,\n",
    "        \"version_id\": meta.version_id,\n",
    "        \"file_hash\": meta.file_hash,\n",
    "        \"n_rows\": meta.n_rows,\n",
    "        \"n_cols\": meta.n_cols,\n",
    "        \"read_opts\": str(read_opts),\n",
    "    }])\n",
    "    if load_log_path.exists():\n",
    "        existing = pd.read_csv(load_log_path)\n",
    "        all_cols = pd.Index(existing.columns).union(row.columns)\n",
    "        out = pd.concat([existing.reindex(columns=all_cols), row.reindex(columns=all_cols)], ignore_index=True)\n",
    "    else:\n",
    "        out = row\n",
    "    tmp = load_log_path.with_suffix(\".tmp\")\n",
    "    out.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, load_log_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5) `dataset.py` ‚Äî move 1.4.1 here (local load)\n",
    "\n",
    "This becomes replaceable later with a warehouse loader.\n",
    "\n",
    "```python\n",
    "# src/dq_engine/dataset.py\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_local_dataset(raw_path: Path, read_opts: dict | None = None) -> pd.DataFrame:\n",
    "    read_opts = read_opts or {}\n",
    "    suffix = raw_path.suffix.lower()\n",
    "    if suffix == \".csv\":\n",
    "        return pd.read_csv(raw_path, **read_opts)\n",
    "    if suffix in {\".parquet\", \".pq\"}:\n",
    "        return pd.read_parquet(raw_path)\n",
    "    raise ValueError(f\"Unsupported RAW_DATA format: {suffix}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6) `profiling.py` ‚Äî move the schema summary from 1.5.2 here\n",
    "\n",
    "Your schema summary is a perfect reusable ‚Äúprofiling‚Äù function.\n",
    "\n",
    "```python\n",
    "# src/dq_engine/profiling.py\n",
    "from __future__ import annotations\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def profile_schema(df: pd.DataFrame, section_id: str = \"1.5.2\") -> pd.DataFrame:\n",
    "    n_rows, n_cols = df.shape\n",
    "    run_ts = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    def classify(dtype_str: str) -> str:\n",
    "        s = dtype_str.lower()\n",
    "        if \"int\" in s or \"float\" in s: return \"numeric\"\n",
    "        if \"bool\" in s: return \"boolean\"\n",
    "        if \"datetime\" in s or \"date\" in s: return \"datetime\"\n",
    "        if \"category\" in s: return \"categorical\"\n",
    "        return \"string_like\"\n",
    "\n",
    "    rows = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        dtype_str = str(s.dtype)\n",
    "        non_null = int(s.notna().sum())\n",
    "        nulls = int(s.isna().sum())\n",
    "        n_unique = int(s.nunique(dropna=True))\n",
    "        null_pct = round(nulls / n_rows * 100, 3) if n_rows else 0.0\n",
    "        rows.append({\n",
    "            \"column\": col,\n",
    "            \"dtype\": dtype_str,\n",
    "            \"type_group\": classify(dtype_str),\n",
    "            \"non_null\": non_null,\n",
    "            \"nulls\": nulls,\n",
    "            \"null_pct\": null_pct,\n",
    "            \"n_unique\": n_unique,\n",
    "            \"run_ts\": run_ts,\n",
    "            \"n_rows\": n_rows,\n",
    "            \"n_cols\": n_cols,\n",
    "            \"section_id\": section_id,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7) `reporting.py` ‚Äî move 1.6.2 setup summary here\n",
    "\n",
    "This is the artifact that future ‚Äúagnostic engine‚Äù runs will always emit.\n",
    "\n",
    "```python\n",
    "# src/dq_engine/reporting.py\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "import json\n",
    "\n",
    "def write_setup_summary(env_dir: Path, payload: dict) -> Path:\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out = env_dir / \"setup_summary.json\"\n",
    "    out.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## The glue: `bootstrap.py` ‚Äî the single entrypoint that replaces your Section 1 infra cells\n",
    "\n",
    "```python\n",
    "# src/dq_engine/bootstrap.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from dq_engine.config import load_config, Config\n",
    "from dq_engine.paths import discover_project_root, build_paths, ensure_dirs, assert_writable\n",
    "from dq_engine.env import capture_env_info\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RunContext:\n",
    "    cfg: Config\n",
    "    paths: object  # ProjectPaths\n",
    "    env: dict\n",
    "\n",
    "def bootstrap_context(config_path: str | Path, *, level_name: str = \"Level_3\") -> RunContext:\n",
    "    config_path = Path(config_path)\n",
    "    project_root = discover_project_root(Path.cwd())\n",
    "    level_root = project_root / level_name\n",
    "    cfg = load_config(config_path, snapshot_dir=level_root / \"config\")\n",
    "\n",
    "    paths = build_paths(project_root, level_name, cfg)\n",
    "    ensure_dirs(\n",
    "        paths.config_dir, paths.reports_dir, paths.artifacts_dir, paths.figures_dir,\n",
    "        paths.models_dir, paths.outputs_dir, paths.raw_data_dir, paths.processed_dir\n",
    "    )\n",
    "    assert_writable(paths.reports_dir, paths.artifacts_dir)\n",
    "\n",
    "    env = capture_env_info()\n",
    "    return RunContext(cfg=cfg, paths=paths, env=env)\n",
    "```\n",
    "\n",
    "Now your notebook doesn‚Äôt need to define `PROJECT_ROOT`, `DIRS`, `CONFIG`, `C()`, etc. It calls one function.\n",
    "\n",
    "---\n",
    "\n",
    "## What to do with target normalization (1.5.3)\n",
    "\n",
    "This is important:\n",
    "\n",
    "* In **warehouse/dbt version**, target normalization belongs in dbt (`stg_*` or `mrt_*`), not in infra.\n",
    "* In local CSV mode, you can keep a helper function, but I‚Äôd treat it as a **dataset pack** (Telco pack), not core infra.\n",
    "\n",
    "So: move 1.5.3 into `src/dq_engine/packs/telco/standardize.py` later.\n",
    "\n",
    "That‚Äôs how you become ‚Äúagnostic‚Äù without deleting Telco usefulness.\n",
    "\n",
    "---\n",
    "\n",
    "## Refactor sequence (no drama, no rewrite spiral)\n",
    "\n",
    "1. **Create the module files** above (empty stubs first).\n",
    "2. Copy code from notebook sections into the right files *as-is*.\n",
    "3. Make `02_DQ_IF.ipynb` import and use:\n",
    "\n",
    "   * `bootstrap_context()`\n",
    "   * `load_local_dataset()`\n",
    "   * `update_version_registry()` / `append_load_log()`\n",
    "   * `profile_schema()`\n",
    "   * `write_setup_summary()`\n",
    "4. Delete/disable the old inline infra cells once outputs match.\n",
    "\n",
    "---\n",
    "\n",
    "## A tiny ‚Äúsuccess criteria‚Äù checklist\n",
    "\n",
    "You‚Äôll know you did it right when:\n",
    "\n",
    "* the notebook is < ~30% the size it is now\n",
    "* there is exactly **one** bootstrap call\n",
    "* artifacts still show up in the same places\n",
    "* you can run the same bootstrap from a CLI and from CI\n",
    "* adding Snowflake/dbt later does **not** require changing the checks ‚Äî only changing `load_dataset()` and result writers\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next we can do the same deep extraction for `02_DQ_IF.ipynb` but in the *right order*: first make checks accept `(ctx, table_or_df)` and write results to a standardized `DQ_RESULTS` model ‚Äî then you‚Äôre warehouse-ready without pain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d9fd293",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # AGGREGATE_PYTHON_CELLS_SCRIPT_NO_NOTES\n",
    "# \"\"\"\n",
    "# Aggregate all Python code cells from a Jupyter notebook into a single code cell\n",
    "# in a new notebook, skipping this script cell.\n",
    "\n",
    "# No headers or notes are injected into the aggregated code.\n",
    "# \"\"\"\n",
    "\n",
    "# import nbformat\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# # 1. CONFIG: paths\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# SOURCE_NOTEBOOK = Path(\"/Users/b/DATA/PROJECTS/Telco/Level_3/notebooks/01_EDA.ipynb\")\n",
    "\n",
    "# # Destination: one level up from \"notebooks/\" ‚Üí \"reports/\"\n",
    "# REPORTS_DIR = SOURCE_NOTEBOOK.parent.parent / \"reports\"\n",
    "# REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Marker used to identify this script cell so we can skip it\n",
    "# SCRIPT_MARKER = \"AGGREGATE_PYTHON_CELLS_SCRIPT_NO_NOTES\"\n",
    "\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# # 2. Load source notebook\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# nb = nbformat.read(SOURCE_NOTEBOOK, as_version=4)\n",
    "\n",
    "# code_snippets = []\n",
    "# total_cells = len(nb.cells)\n",
    "# skipped_cells = 0\n",
    "\n",
    "# for cell in nb.cells:\n",
    "#     if cell.cell_type != \"code\":\n",
    "#         continue\n",
    "\n",
    "#     src = cell.source if isinstance(cell.source, str) else \"\".join(cell.source)\n",
    "\n",
    "#     # Skip the aggregator script cell (any cell containing the marker)\n",
    "#     if SCRIPT_MARKER in src:\n",
    "#         skipped_cells += 1\n",
    "#         continue\n",
    "\n",
    "#     if src.strip():\n",
    "#         code_snippets.append(src.rstrip())\n",
    "\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# # 3. Build aggregated source (pure code)\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# # Join all code cells with two newlines between them ‚Äî no extra comments.\n",
    "# agg_source = \"\\n\\n\".join(code_snippets) + \"\\n\"\n",
    "\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# # 4. Create new notebook with single code cell\n",
    "# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# output_name = f\"AGG_ALL_{timestamp}.ipynb\"\n",
    "# OUTPUT_PATH = REPORTS_DIR / output_name\n",
    "\n",
    "# new_nb = nbformat.v4.new_notebook(\n",
    "#     metadata={\n",
    "#         \"kernelspec\": nb.metadata.get(\"kernelspec\", {}),\n",
    "#         \"language_info\": nb.metadata.get(\"language_info\", {}),\n",
    "#         \"aggregated_from\": str(SOURCE_NOTEBOOK),\n",
    "#         \"aggregated_at_utc\": timestamp,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Single big code cell (no notes)\n",
    "# new_nb.cells.append(nbformat.v4.new_code_cell(agg_source))\n",
    "\n",
    "# nbformat.write(new_nb, OUTPUT_PATH)\n",
    "\n",
    "# print(\"‚úÖ Aggregation complete (no notes inserted into cells)\")\n",
    "# print(f\"   Source notebook:      {SOURCE_NOTEBOOK}\")\n",
    "# print(f\"   Total cells scanned:  {total_cells}\")\n",
    "# print(f\"   Output notebook:      {OUTPUT_PATH}\")\n",
    "# print(f\"   Script cells skipped: {skipped_cells}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba716f29",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #e78229ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "TODO\n",
    "</summary>\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# 1.2.1 or 1.2.2?\n",
    "# print(f\"\\nüìÅ Project root:         {PROJECT_ROOT}\")\n",
    "# print(f\"üìÑ Raw dataset path:     {RAW_DATA}\")\n",
    "# print(f\"üìÇ Processed data dir:   {DATA_PROCESSED_DIR}\")\n",
    "# print(\"‚úÖ Project directories verified/created.\")\n",
    "\n",
    "# # Verify dataset presence and load a peek\n",
    "# if not TELCO_RAW.exists():\n",
    "#     raise FileNotFoundError(f\"‚ùå Missing dataset. Place it here:\\n   {TELCO_RAW}\")\n",
    "\n",
    "# # (Optional) You could write this to directory_structure_report.txt if desired\n",
    "######\n",
    "\n",
    "# TODO: CHANGE PANDAS DTYPES FOR DOWNSTREAM MODELING\n",
    "# TODO: CONVERT ALL PANDAS DTYPES. MAKE SURE TO DISPLAY DTYPES BEFORE ANY CONVERSIONS. MAKE SNAPSHOT BEFORE AND AFTER CHANGES.\n",
    "# TODO: CONVERT ALL PANDAS DTYPES TO APPROPRIATE TYPES\n",
    "# TODO: MAKE SURE df.info() is called before any conversions.\n",
    "# TODO: (1.7) Would you like me to also provide the **mini implementation skeleton** for 2.0 (the Python stubs that create directories, initialize logs, and validate configs) in the same style as your 2.3/2.8 code blueprints?\n",
    "# TODO: Would you like me to also provide the **mini implementation skeleton** for 2.0 (the Python stubs that create directories, initialize logs, and validate configs) in the same style as your 2.3/2.8 code blueprints\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "That `# TODO: proper desc? Load registry` is basically asking to describe this whole ‚Äúload-or-init version registry‚Äù behavior.\n",
    "\n",
    "Here‚Äôs a good, professional wording + a slightly tidied block you can drop in.\n",
    "\n",
    "### Suggested comment for that TODO\n",
    "\n",
    "Replace:\n",
    "\n",
    "```python\n",
    "# TODO: proper desc? Load registry\n",
    "if registry_path.exists():\n",
    "```\n",
    "\n",
    "with something like:\n",
    "\n",
    "```python\n",
    "# Load existing dataset version registry if present; otherwise initialize an empty one\n",
    "if registry_path.exists():\n",
    "```\n",
    "\n",
    "If you want to be even more explicit about semantics:\n",
    "\n",
    "```python\n",
    "# Load dataset version registry (file_hash ‚Üí version_id); initialize empty registry if not present\n",
    "if registry_path.exists():\n",
    "```\n",
    "\n",
    "### Slightly polished version of your whole block\n",
    "\n",
    "Here‚Äôs your code with that comment dropped in and minor tighten-ups:\n",
    "\n",
    "```python\n",
    "# 1.5.2 Hash / Snapshot Validation\n",
    "print(\"\\n1.5.2) üîê Hash / snapshot validation\")\n",
    "\n",
    "def compute_hash(path, algo: str = \"sha256\", chunk_size: int = 1 << 20) -> str:\n",
    "    \"\"\"Streaming file hash to avoid loading whole file into memory.\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "file_hash = compute_hash(RAW_DATA, \"sha256\")\n",
    "now_utc = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "# Ensure artifacts directory exists so atomic writes (tmp + replace) don't fail\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGISTRY_DIR = ARTIFACTS_DIR / \"registry\"\n",
    "REGISTRY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "registry_path = REGISTRY_DIR / \"dataset_version_registry.csv\"\n",
    "\n",
    "# Load dataset version registry (file_hash ‚Üí version_id); initialize empty registry if not present\n",
    "if registry_path.exists():\n",
    "    registry = pd.read_csv(registry_path)\n",
    "else:\n",
    "    registry = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"version_id\",\n",
    "            \"dataset_path\",\n",
    "            \"file_hash\",\n",
    "            \"first_seen_utc\",\n",
    "            \"last_seen_utc\",\n",
    "            \"n_rows\",\n",
    "            \"n_cols\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "existing = registry.loc[registry[\"file_hash\"] == file_hash]\n",
    "\n",
    "if not existing.empty:\n",
    "    version_id = str(existing.iloc[0][\"version_id\"])\n",
    "    registry.loc[registry[\"file_hash\"] == file_hash, \"last_seen_utc\"] = now_utc\n",
    "    print(f\"‚ÑπÔ∏è Existing dataset version found ‚Üí version_id={version_id}\")\n",
    "else:\n",
    "    if registry.empty:\n",
    "        next_id = 1\n",
    "    else:\n",
    "        max_existing = pd.to_numeric(registry[\"version_id\"], errors=\"coerce\").fillna(0).max()\n",
    "        next_id = int(max_existing) + 1\n",
    "\n",
    "    version_id = str(next_id)\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"version_id\": [version_id],\n",
    "            \"dataset_path\": [str(RAW_DATA)],\n",
    "            \"file_hash\": [file_hash],\n",
    "            \"first_seen_utc\": [now_utc],\n",
    "            \"last_seen_utc\": [now_utc],\n",
    "            \"n_rows\": [n_rows],\n",
    "            \"n_cols\": [n_cols],\n",
    "        }\n",
    "    )\n",
    "    registry = pd.concat([registry, new_row], ignore_index=True)\n",
    "    print(f\"üÜï New dataset version registered ‚Üí version_id={version_id}\")\n",
    "\n",
    "tmp_reg = registry_path.with_suffix(\".tmp\")\n",
    "registry.to_csv(tmp_reg, index=False)\n",
    "os.replace(tmp_reg, registry_path)\n",
    "print(f\"‚úÖ Version registry updated ‚Üí {registry_path}\")\n",
    "```\n",
    "\n",
    "One tiny note: `load_log_path` is now unused in this snippet; you can either:\n",
    "\n",
    "* Remove it from this section, or\n",
    "* Move its path + mkdir into the 1.5.3 ‚Äúload log‚Äù block where you actually use it.\n",
    "\n",
    "But for the TODO itself, ‚ÄúLoad dataset version registry (file_hash ‚Üí version_id); initialize empty registry if not present‚Äù is a very recruiter-friendly description of what‚Äôs going on.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3e4ce",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:7px solid #be4c2dff;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "NOTEBOOK SETUP - 11/15/2025\n",
    "</summary>\n",
    "Great question to ask *right now* instead of after Section 2 turns into a hydra üòÑ\n",
    "\n",
    "Given your **Type 3 ‚Üí Type 2 config pattern** and **Section 2 = Data Quality framework**, I‚Äôd organize notebooks around *pipeline stages*, not around ‚Äúwhatever I was playing with today‚Äù.\n",
    "\n",
    "Below is a concrete layout that fits exactly what you‚Äôve built so far.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Principles (for this Telco Level_3 setup)\n",
    "\n",
    "With your current design:\n",
    "\n",
    "* **Type 3 config** = `project_config.yaml` (source of truth).\n",
    "* **Type 2 config** = `CONFIG` dict + `C()` helper (in memory).\n",
    "* **Section 1** = environment + paths + config load + raw ingest.\n",
    "* **Section 2** = data quality & integrity (your big dependency chain).\n",
    "* **Section 3+** = feature engineering & modeling.\n",
    "\n",
    "So the notebooks should:\n",
    "\n",
    "1. **Never duplicate config.** Only one place loads YAML ‚Üí CONFIG; everything else just uses `C()` or imports.\n",
    "2. **Treat Section 2 as its own pipeline block.** One notebook (or two) = ‚ÄúDQ job‚Äù.\n",
    "3. **Keep notebooks thin over time.** Heavy logic migrates into `/telco_churn` modules; notebooks mostly orchestrate + display.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recommended notebook layout\n",
    "\n",
    "### 01_SECTION1_env_config_raw_load.ipynb\n",
    "\n",
    "**Scope:** everything up through the end of Section 1 (what you‚Äôve already written).\n",
    "\n",
    "**Content:**\n",
    "\n",
    "* 1.1 Imports & visualization defaults\n",
    "* 1.2 Project root & path setup\n",
    "* 1.3 Load YAML ‚Üí `CONFIG` + `C()` (Type 3 ‚Üí Type 2)\n",
    "* 1.4 Resolve config-driven paths, seeds, protected columns\n",
    "* 1.5 Raw dataset load + version registry + load log\n",
    "* 1.6 Basic structural validation, initial schema summary\n",
    "* 1.7 Environment summary & setup log\n",
    "\n",
    "**Key outcome:**\n",
    "By the end, you‚Äôve got:\n",
    "\n",
    "* `df`, `df_raw`\n",
    "* `CONFIG`, `C()`\n",
    "* `PROJECT_ROOT`, `REPORTS_DIR`, `ARTIFACTS_DIR`, etc.\n",
    "* `protected_columns`, `id_cols`, `target_name`, `raw_target`\n",
    "* artifacts like `dataset_version_registry.csv`, `dataset_load_log.csv`, etc.\n",
    "\n",
    "**Other notebooks *assume* this one ran successfully.**\n",
    "\n",
    "---\n",
    "\n",
    "### 02_SECTION2_data_quality_framework.ipynb\n",
    "\n",
    "This is your **entire Section 2 dependency chain** in one place. All the stuff you‚Äôve been carefully designing goes here.\n",
    "\n",
    "**2.0 ‚Äì Preflight & scaffold**\n",
    "\n",
    "* 2.0.1 Environment preflight & Section 2 report bootstrap\n",
    "* 2.0.2 Config & constants validation for Section 2\n",
    "* 2.0.3 Logging & run metadata snapshot\n",
    "* 2.0.4 Dataset snapshot & preview (reusing `df`)\n",
    "* 2.0.5 Lightweight baseline summary\n",
    "* 2.0.6 ID & protected columns snapshot\n",
    "* 2.0.7 Dependency registry build (`section2_registry.json`)\n",
    "* 2.0.8 Execution map printout / markdown summary\n",
    "\n",
    "**2.1 ‚Äì Base schema & consistency**\n",
    "\n",
    "* Schema expectation checks vs CONFIG\n",
    "* Column presence, dtype expectations, drift checks\n",
    "* Missingness baseline (2.1.8)\n",
    "* All outputs to `section2` reports/artifacts\n",
    "\n",
    "**2.2 ‚Äì Column type discovery (everything you just designed)**\n",
    "\n",
    "* 2.2.1 Auto-detect data types ‚Üí `type_detection_summary.csv`, `column_type_map.json`\n",
    "* 2.2.2 Coercion attempt & logging ‚Üí `coercion_log.csv` (behind a flag)\n",
    "* 2.2.3 Binary field detection ‚Üí `binary_field_report.csv`\n",
    "* 2.2.4 SeniorCitizen retype (config-driven, optional)\n",
    "* 2.2.5 Churn flag validation (type-level)\n",
    "* 2.2.6 ID & protected columns registration ‚Üí `protected_columns.json`\n",
    "* 2.2.7 Feature group classification ‚Üí `feature_group_registry.csv`\n",
    "* 2.2.8 Type distribution visualization\n",
    "\n",
    "**2.3+** (later):\n",
    "\n",
    "* Numeric integrity & outliers\n",
    "* Categorical domain / tokens\n",
    "* Logic checks\n",
    "* Apply phase QA\n",
    "\n",
    "All DQ artifacts live under:\n",
    "\n",
    "* `Level_3/resources/reports/section2/...`\n",
    "* `Level_3/resources/artifacts/...`\n",
    "\n",
    "**Important:**\n",
    "This notebook **does not load data or config**. It just:\n",
    "\n",
    "```python\n",
    "from telco_churn.config.loader import C, CONFIG\n",
    "from telco_churn.io.datasets import load_raw_telco  # later\n",
    "# or simply assumes Section 1 ran in the same kernel\n",
    "```\n",
    "\n",
    "For now in your Level_3 notebook world, it‚Äôs fine that this runs in the same kernel as Section 1; later, you‚Äôll refactor into scripts.\n",
    "\n",
    "---\n",
    "\n",
    "### 03_SECTION3_feature_engineering_and_baseline_model.ipynb\n",
    "\n",
    "**Scope:** once Section 2 says ‚Äúdata is clean & typed,‚Äù this is where you:\n",
    "\n",
    "* pull in only the **model-ready features** using `feature_group_registry.csv`\n",
    "* perform feature engineering\n",
    "* build baseline models\n",
    "* track model metrics\n",
    "\n",
    "It will **consume** artifacts from Section 2:\n",
    "\n",
    "* `feature_group_registry.csv`\n",
    "* `protected_columns.json`\n",
    "* maybe ‚Äúnumeric-only‚Äù list for scaling, etc.\n",
    "\n",
    "This is more for later, but the **important part** for organization right now is:\n",
    "**don‚Äôt mix Section 3 modeling with Section 2 DQ in the same notebook.**\n",
    "\n",
    "---\n",
    "\n",
    "### 99_sandbox.ipynb (optional)\n",
    "\n",
    "Your trash lab / scratchpad:\n",
    "\n",
    "* weird experiments\n",
    "* one-off plots\n",
    "* queries you might later formalize in Section 1/2/3\n",
    "\n",
    "This keeps your main notebooks clean and portfolio-friendly.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Where Type 3 ‚Üí Type 2 actually lives\n",
    "\n",
    "To keep the story clean for recruiters:\n",
    "\n",
    "* **Type 3 (YAML) lives in `/config/project_config.yaml`**\n",
    "* **Type 2 (CONFIG dict + C()) is created once in Section 1**\n",
    "  ‚Üê *and eventually moved into a module* like:\n",
    "\n",
    "```python\n",
    "# telco_churn/config/loader.py\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from types import MappingProxyType\n",
    "\n",
    "CONFIG_PATH = Path(__file__).resolve().parents[2] / \"config\" / \"project_config.yaml\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CONFIG = yaml.safe_load(f) or {}\n",
    "\n",
    "CFG = MappingProxyType(CONFIG)\n",
    "\n",
    "def C(path: str, default=None):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Then your notebooks do:\n",
    "\n",
    "```python\n",
    "from telco_churn.config.loader import CONFIG, C\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* **Section 1 notebook** = demonstration of using that loader + building env.\n",
    "* **Section 2 notebook** = consumer of `C()` + env, not re-declaring config.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. TL;DR notebook organization\n",
    "\n",
    "For your current design:\n",
    "\n",
    "* `notebooks/01_SECTION1_env_config_and_raw_load.ipynb`\n",
    "* `notebooks/02_SECTION2_data_quality_framework.ipynb`\n",
    "* `notebooks/03_SECTION3_feature_engineering_and_modeling.ipynb`\n",
    "* `notebooks/99_sandbox.ipynb` (optional)\n",
    "\n",
    "And every time you feel the urge to copy a big chunk of logic between notebooks, that‚Äôs your cue:\n",
    "\n",
    "> ‚ÄúThis belongs in `/telco_churn/...py`, not duplicated in two notebooks.‚Äù\n",
    "\n",
    "That‚Äôs how your **Type 3 ‚Üí Type 2 config chain + Section 2 as Data Quality** turns from ‚Äúcool notebook‚Äù into ‚Äúthis person thinks like a pipeline engineer.‚Äù\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Which notebook has the stats?\n",
    "</summary>\n",
    "\n",
    "Short answer: **the stats live mostly in 2.1.8, 2.2.x, 2.3, and 2.4.**\n",
    "Everything before that is mostly *plumbing / contracts / config*.\n",
    "\n",
    "Let me map it to your chain.\n",
    "\n",
    "---\n",
    "\n",
    "### Mostly *not* statistics (infra / contracts)\n",
    "\n",
    "These are engineering-focused:\n",
    "\n",
    "* **2.0.x ‚Äì Preflight & scaffolding**\n",
    "\n",
    "  * Env checks, paths, manifests, metadata, logging.\n",
    "* **2.1.1‚Äì2.1.7 ‚Äì Base schema & consistency**\n",
    "\n",
    "  * Column presence, dtype expectations, config roots.\n",
    "  * This is more ‚Äúdata contracts‚Äù than ‚Äústatistics‚Äù.\n",
    "\n",
    "They may compute some counts, but the *purpose* is structural, not statistical insight.\n",
    "\n",
    "---\n",
    "\n",
    "### Where the **statistics** start showing up\n",
    "\n",
    "Think: counts, proportions, distributions, outliers, frequency patterns, etc.\n",
    "\n",
    "#### 1. 2.1.8 ‚Äì Missingness Baseline\n",
    "\n",
    "This is your **first clearly statistical block**:\n",
    "\n",
    "* `% missing per column`\n",
    "* maybe top columns by null %, etc.\n",
    "\n",
    "That‚Äôs descriptive stats about completeness.\n",
    "\n",
    "#### 2. 2.2.x ‚Äì Column Type Discovery (light stats baked in)\n",
    "\n",
    "Even though 2.2 is ‚Äútype discovery‚Äù, it uses statistical ideas:\n",
    "\n",
    "* **2.2.1 Auto-Detect Types**\n",
    "\n",
    "  * `n_unique`, `unique_ratio`\n",
    "  * `pct_numeric_like`, `pct_boolean_like`, `pct_datetime_like`\n",
    "* **2.2.2 Coercion Attempt & Logging**\n",
    "\n",
    "  * success/fail rates of coercion (implicit stats on parseability)\n",
    "* **2.2.3 Binary Field Detection**\n",
    "\n",
    "  * columns with exactly 2 unique values, value counts, proportions\n",
    "\n",
    "This is still mostly *schema intelligence*, but it‚Äôs driven by simple descriptive stats.\n",
    "\n",
    "#### 3. 2.3 ‚Äì Numeric Integrity & Outliers\n",
    "\n",
    "This is your **first ‚Äúreal‚Äù stats-heavy section**:\n",
    "\n",
    "Typical contents:\n",
    "\n",
    "* Summary stats: mean, median, std, min, max, quantiles\n",
    "* Outlier rules: IQR, Z-score, Winsorization flags, etc.\n",
    "* Distribution checks: skew, kurtosis (if you include them)\n",
    "\n",
    "This is where you can brag ‚Äúwe do statistical verification of numeric fields.‚Äù\n",
    "\n",
    "#### 4. 2.4 ‚Äì Categorical Integrity & Domain Diagnostics\n",
    "\n",
    "This is statistical too, but on categories:\n",
    "\n",
    "* Frequency tables (`count`, `% share`)\n",
    "* Rare-category identification (`<1%`, `<0.5%`, etc.)\n",
    "* Domain frequency audits, dominant vs fragmented labels\n",
    "* Potentially association measures later (Cram√©r‚Äôs V, Theil‚Äôs U, etc.)\n",
    "\n",
    "All of that is **discrete / categorical statistics**.\n",
    "\n",
    "---\n",
    "\n",
    "### Mostly logical / rule-based (but standing on stats)\n",
    "\n",
    "* **2.5 Logic Checks**\n",
    "\n",
    "  * Business rules like `TotalCharges ‚âà MonthlyCharges * tenure`,\n",
    "    `tenure=0 ‚áí TotalCharges=0`, etc.\n",
    "    These *may* use thresholds or tolerances that come from stats (e.g. ‚Äúallow ¬±3œÉ‚Äù), but they‚Äôre conceptually logic/contract-based.\n",
    "\n",
    "* **2.6 Apply Phase QA**\n",
    "\n",
    "  * Comparing ‚Äúbefore vs after‚Äù distributions, counts of rows changed, etc.\n",
    "    That‚Äôs partly stats (distribution comparison) and partly pipeline QA.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR: if a recruiter asks ‚ÄúWhere‚Äôs the statistics part?‚Äù\n",
    "\n",
    "You can honestly say:\n",
    "\n",
    "* **Descriptive completeness stats**: 2.1.8\n",
    "* **Type-discovery stats (uniques, proportions, binary detection)**: 2.2.x\n",
    "* **Numeric distribution & outlier statistics**: 2.3\n",
    "* **Categorical frequency & rare-category statistics**: 2.4\n",
    "* **Optional multivariate / drift tests**: later 2.x if you add them (e.g., Cram√©r‚Äôs V, PSI, KS tests)\n",
    "\n",
    "Everything else in Section 2 is the **infrastructure that makes those statistics reproducible, traceable, and production-ready**.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7d263",
   "metadata": {},
   "source": [
    "---\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border: 1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:900;\">\n",
    "üìä Report Summary\n",
    "</summary>\n",
    "\n",
    "<h1>üìä IBM Telco Customer Churn ‚Äî Exploratory Data Analysis (EDA)</h1>\n",
    "\n",
    "**Author:** Brandon Hardison  \n",
    "**Role:** Analytics Engineering Student  \n",
    "**Notebook:** `01_EDA.ipynb`  \n",
    "**Version:** v1.0  \n",
    "**Date Completed:** 2025-10-31  \n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border: 1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:900;\">\n",
    "Purpose üéØ\n",
    "</summary>\n",
    "<div style=\"margin: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 10px;\">\n",
    "\n",
    "This notebook performs an in-depth **Exploratory Data Analysis (EDA)** on the IBM Telco Customer Churn dataset.  \n",
    "It focuses on data quality diagnostics, missing value handling, type coercion, categorical normalization,  \n",
    "and target preparation for downstream modeling and feature engineering.\n",
    "\n",
    "Its objectives are to:\n",
    "- Assess overall **data quality**, including missing values, type consistency, and categorical normalization.  \n",
    "- **Understand the dataset‚Äôs structure and feature distributions** through descriptive statistics and visualization.  \n",
    "- **Identify statistically significant predictors of customer churn** for downstream modeling.  \n",
    "\n",
    "The analysis is designed for both **business stakeholders** seeking actionable insights  \n",
    "and the **data science team** responsible for model development and feature engineering.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Dataset Summary\n",
    "- **Source:** IBM Telco Customer Churn (public Kaggle / IBM sample dataset)  \n",
    "- **Rows:** ~7,000 customer records  \n",
    "- **Columns:** 21 features  \n",
    "- **Target:** `Churn` (Yes/No) ‚Üí numeric flag `Churn_flag`  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† Report Scope\n",
    "This notebook covers:\n",
    "1. **Data Quality & Cleaning (Section 2)**  \n",
    "   - Missing value scan  \n",
    "   - Numeric validation & coercion  \n",
    "   - Categorical cleaning  \n",
    "   - Cross-field & business-rule consistency  \n",
    "2. **Preliminary Target & Demographic Diagnostics (Section 2.12)**  \n",
    "3. **Preparation for Modeling & Feature Engineering (next notebook)**\n",
    "\n",
    "---\n",
    "\n",
    "> _This report is designed for internal validation and reproducibility.  \n",
    "> All outputs are atomic (timestamped) and feed directly into Level_3 reports and resources._\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border: 1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:900;\">\n",
    "Deliverables\n",
    "</summary>\n",
    "<div style=\"margin: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 10px;\">\n",
    "\n",
    "\n",
    "‚úÖ **Deliverables from EDA Notebook**\n",
    "\n",
    "| Output Type       | Example File                  | Used In                 |\n",
    "| ----------------- | ----------------------------- | ----------------------- |\n",
    "| Clean EDA dataset | `telco_eda.parquet`           | Statistics & Modeling   |\n",
    "| EDA report        | `eda_summary.csv`             | Insights notebook       |\n",
    "| Visuals           | `figures/*.png`               | Insights presentation   |\n",
    "| Notes             | Inline markdown or `.md` file | Documentation & handoff |\n",
    "\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚û°Ô∏è 3.0  Descriptive Statistics & EDA\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border: 1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:900;\">\n",
    "Outlines\n",
    "</summary>\n",
    "<div style=\"margin: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 10px;\">\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border: 1px solid #e5e7eb;\n",
    "    padding:10px 12px;border-radius:10px;font-weight:900;\">\n",
    "Pros & Cons of full-inline pipeline\n",
    "</summary>\n",
    "\n",
    "Short version: doing it all inline is **awesome as a learning/spec pass**, and kind of terrible as a long-term implementation. üòÑ\n",
    "\n",
    "I‚Äôll break it down.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of a fully inline, no-`def` pipeline\n",
    "\n",
    "### 1. Maximum transparency & debuggability\n",
    "\n",
    "* You can **see every step** in the exact order it runs. No hunting for helper functions across files.\n",
    "* When something blows up, the traceback points straight at **the exact cell / section** instead of some deep helper.\n",
    "* It‚Äôs easier to answer: *‚ÄúWhat is this pipeline actually doing to the data?‚Äù* because the answer is on one page, in order, with concrete operations.\n",
    "\n",
    "### 2. Perfect ‚Äúexecutable spec‚Äù\n",
    "\n",
    "* This style gives you a **single, linear, executable specification** of Section 2.x / 3.x:\n",
    "\n",
    "  * What directories must exist\n",
    "  * What artifacts are produced and where\n",
    "  * What each check actually checks\n",
    "* For future you (or reviewers / interviewers), this file is a **golden reference**: ‚ÄúThis is the canonical behavior; all refactors must preserve this.‚Äù\n",
    "\n",
    "### 3. Great for learning & design\n",
    "\n",
    "* You‚Äôre forcing yourself to think about **data flow, state, and dependencies**:\n",
    "\n",
    "  * ‚ÄúThis needs `df` and `sec2_reports_dir` in globals.‚Äù\n",
    "  * ‚ÄúThis script assumes `CONFIG` and `C()` exist, but can still run without them.‚Äù\n",
    "* No abstractions to hide behind = you really internalize:\n",
    "\n",
    "  * How checks depend on each other\n",
    "  * How config gets resolved\n",
    "  * How Section 2/3 artifacts hang together\n",
    "\n",
    "For your analytics-engineering brain, this is basically a **design doc + implementation in one**.\n",
    "\n",
    "### 4. Easier to make dataset-agnostic *specs*\n",
    "\n",
    "* Doing it inline makes it obvious which pieces are:\n",
    "\n",
    "  * **Truly generic** (e.g., ‚Äúwrite CSV with tmp + `os.replace`‚Äù)\n",
    "  * **Schema-aware but dataset-agnostic** (e.g., ‚ÄúONEHOT.GROUPS‚Äù, ‚ÄúTOTALS.RULES‚Äù from CONFIG)\n",
    "  * **Telco-specific** (e.g., `tenure √ó MonthlyCharges` helpers)\n",
    "* That clarity helps when you later split into:\n",
    "\n",
    "  * `core/generic_dq.py`\n",
    "  * `projects/telco/logic_rules.yaml`\n",
    "\n",
    "### 5. Frictionless tweaks during exploration\n",
    "\n",
    "* Want to change a tolerance or add a column? You edit that one block and rerun. No worrying about function signatures, imports or modules being stale.\n",
    "* For ‚ÄúLevel_3‚Äù work, **speed of iteration** matters more than elegance.\n",
    "\n",
    "### 6. Great for portfolio / code review\n",
    "\n",
    "* A reviewer can:\n",
    "\n",
    "  * Scroll once and **see the entire logic stack**: bootstrap ‚Üí numeric checks ‚Üí logic ‚Üí anomaly context.\n",
    "  * Understand your data-quality philosophy without jumping across files.\n",
    "* Then you can say, ‚ÄúThis notebook is the **spec level**. In Level_4+ I factor it into a package.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## Cons of staying fully inline long term\n",
    "\n",
    "### 1. Zero reuse ‚Üí duplication everywhere\n",
    "\n",
    "* You already see this: path resolution, `SECTION2_REPORT_PATH` merging, temp-file writes, `if \"df\" not in globals()` checks‚Ä¶\n",
    "* Inline only means:\n",
    "\n",
    "  * If you want the same checks for EDGAR, Telco, and some 3rd dataset, you‚Äôll copy / paste and lightly edit.\n",
    "  * Bug fixes must be applied in multiple places, which is how pipelines drift out of sync and become untrustworthy.\n",
    "\n",
    "### 2. Hard to test properly\n",
    "\n",
    "* Unit tests want small, pure-ish functions: ‚ÄúGiven this config and df fragment, does `catnum_alignment` behave?‚Äù\n",
    "* With fully inline code:\n",
    "\n",
    "  * You can only really test by running the **whole script**, which is slow and noisy.\n",
    "  * It‚Äôs extremely hard to test edge cases without manually hacking global state in notebook cells.\n",
    "\n",
    "### 3. Brittle global state & ordering\n",
    "\n",
    "* Everything depends on:\n",
    "\n",
    "  * Correct **cell order**\n",
    "  * Correct **global variables** (`df`, `CONFIG`, `C`, `REPORTS_DIR`, etc.)\n",
    "* If you:\n",
    "\n",
    "  * Rerun cells out of order\n",
    "  * Import this notebook as a module\n",
    "  * Or try to call a ‚Äúpiece‚Äù of Section 2.5 from another notebook\n",
    "    it becomes fragile quickly.\n",
    "* Production systems (Airflow, Dagster, Prefect, etc.) **hate** this style because they need deterministic, parameterized tasks.\n",
    "\n",
    "### 4. Harder to parameterize & orchestrate\n",
    "\n",
    "* Want to:\n",
    "\n",
    "  * Run Section 2 on multiple datasets?\n",
    "  * Toggle checks on/off from the command line?\n",
    "  * Run only 2.5.7‚Äì2.5.11 for a quick regression?\n",
    "* With inline-only:\n",
    "\n",
    "  * You end up adding more and more `if run_this:` globals and config flags inside the giant script.\n",
    "  * That becomes just as complex as having functions, but without the structure.\n",
    "\n",
    "### 5. Maintenance cost explodes with size\n",
    "\n",
    "* Section 2.5 alone is already huge.\n",
    "* Imagine:\n",
    "\n",
    "  * Section 2 numeric + categorical\n",
    "  * Section 3 features\n",
    "  * Section 4 modeling\n",
    "    all inline in one place.\n",
    "* Keeping mental track of **dependencies, side effects, and artifacts** becomes a full-time job.\n",
    "* Refactors (e.g., changing anomaly severity scheme) are risky because you‚Äôre editing big blocks instead of well-scoped helpers.\n",
    "\n",
    "### 6. Integration with other tools is awkward\n",
    "\n",
    "* Want:\n",
    "\n",
    "  * A CLI: `telco-churn run section2 --level full`\n",
    "  * A Python package other notebooks can `import telco_churn.dq as dq` from\n",
    "  * A web UI that re-runs just the logic section\n",
    "* Inline only means:\n",
    "\n",
    "  * You basically have to re-implement the logic *again* in function form or script wrappers.\n",
    "  * Or you end up calling ‚Äúmagic notebook cells‚Äù via `papermill`, which is not fun.\n",
    "\n",
    "### 7. Cognitive load for teammates / future-you\n",
    "\n",
    "* For you today, the inline spec is crisp because you grew it.\n",
    "* For future you (or a teammate), 1500+ lines of inline code with repeated patterns is intimidating:\n",
    "\n",
    "  * Harder to see ‚Äúoh, this shape appears in 5 places, we should unify it.‚Äù\n",
    "  * Harder to tell which parts are safe to change without breaking everything.\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this style *strategically*\n",
    "\n",
    "The way you‚Äôre doing it actually fits a **nice maturity curve**:\n",
    "\n",
    "1. **Level_3‚Äìish: Full inline ‚Äúspec mode‚Äù (where you are)**\n",
    "\n",
    "   * Goal: get *everything* correct, explicit, and observable.\n",
    "   * You prioritize **clarity and traceability** over elegance.\n",
    "   * You discover the **natural function boundaries** by feeling the repetition and pain.\n",
    "\n",
    "2. **Level_4+: Extract patterns, keep the spec**\n",
    "\n",
    "   * You:\n",
    "\n",
    "     * Identify repeated shapes (**path resolution**, **append section2 report**, **config access**, **CSV/parquet writes**, **anomaly aggregation patterns**).\n",
    "     * Pull those into small, sharp helpers in a `telco_churn` package.\n",
    "   * But you **keep this inline file** as:\n",
    "\n",
    "     * Documentation\n",
    "     * A regression reference\n",
    "     * A teaching artifact / portfolio piece\n",
    "\n",
    "3. **Later: pure package / CLI oriented**\n",
    "\n",
    "   * At this point:\n",
    "\n",
    "     * Notebooks become **thin orchestration / visualization**.\n",
    "     * The real logic lives in reusable functions / classes.\n",
    "\n",
    "So:\n",
    "\n",
    "* **Pros**: unbeatable for learning, design clarity, auditing, and having an executable spec for the whole logic layer.\n",
    "* **Cons**: terrible for reuse, testing, orchestration, and long-term maintenance if you *never* factor out helpers.\n",
    "\n",
    "If you want, next step I can help you:\n",
    "\n",
    "* Walk through your inline 2.5.x and highlight **exact candidates** for the first 5‚Äì10 helper functions you‚Äôd eventually extract, while still honoring your ‚Äúno `def` until Level_3 is done‚Äù rule.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060b5f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #b95d39ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Section 1 üèóÔ∏è Environment, Imports & Project Setup: click to Jump to Section\n",
    "</summary>\n",
    "\n",
    "> ### Section 1 is your **foundation** ‚Äî the layer that ensures every later stage (2.0 Data Quality, 3.0 Modeling, etc.) runs reproducibly and coherently.\n",
    "> **üéØ Purpose:**\n",
    "> - Establish a reproducible, production-grade analytics environmen\n",
    "> - that guarantees consistency across machines, teammates, and time.\n",
    "> - All later sections (2.x‚Äì4.x) depend on this foundation.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #8470d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.1 üì¶ Imports & Environment Settings\n",
    "</summary>\n",
    "\n",
    "```groovy\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.1.1 Import Core Libraries\n",
    "‚îÇ        ‚Üí pandas, numpy, matplotlib, seaborn, scipy, pathlib, warnings\n",
    "‚îÇ        ‚Üí Establish reproducible numeric & plotting environment\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.1.2 Visualization Defaults\n",
    "‚îÇ        ‚Üí Apply consistent theme (whitegrid, font, figsize)\n",
    "‚îÇ        ‚Üí Centralizes aesthetic consistency across notebooks\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.1.3 Global Display & Print Settings\n",
    "‚îÇ        ‚Üí Configure pandas display, float precision, warning filters\n",
    "‚îÇ        ‚Üí Guarantees uniform console and report behavior\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: environment_log.txt (optional)\n",
    "    üü© *Hiring value:* Demonstrates environment reproducibility and\n",
    "        version-controlled dependency awareness ‚Äî key in team analytics.\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:5px solid #70b1d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.2 üìÅ Path Setup & Directory Management\n",
    "</summary>\n",
    "\n",
    "```py\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.2.1 Resolve Project Root\n",
    "‚îÇ        ‚Üí Dynamically detect `/Telco` or repo root path.\n",
    "‚îÇ        ‚Üí Supports portability across dev and prod.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.2.2 Define Core Subdirectories\n",
    "‚îÇ        ‚Üí /data/raw/, /data/processed/, /src/, /reports/, /config/, /resources/\n",
    "‚îÇ        ‚Üí Create missing directories automatically.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.2.3 Verify Writable Paths\n",
    "‚îÇ        ‚Üí Assert read/write permissions before runtime.\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: directory_structure_report.txt\n",
    "    üü© *Hiring value:* Signals infrastructure literacy (you treat notebooks like codebases).\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #8470d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.3 üß∞ Environment Configuration & Dependency Management\n",
    "</summary>\n",
    "\n",
    "```py\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.3.1 Virtual Environment / Kernel Registration\n",
    "‚îÇ        ‚Üí Create `venv` or `conda` env, register with Jupyter kernel.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.3.2 requirements.txt / setup_env.sh\n",
    "‚îÇ        ‚Üí Pin library versions for deterministic builds.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.3.3 CONFIG Loader\n",
    "‚îÇ        ‚Üí Load central YAML/JSON config (project constants, thresholds).\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: env_requirements_report.txt\n",
    "    üü© *Hiring value:* Shows MLOps maturity ‚Äî you know reproducibility is part of engineering.\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #70b1d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.4 ‚öôÔ∏è Global Constants & Config Mapping\n",
    "</summary>\n",
    "\n",
    "```groovy\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.4.1 Load CONFIG[\"PATHS\"], CONFIG[\"RANGES\"], CONFIG[\"DATA_CONTRACTS\"]\n",
    "‚îÇ        ‚Üí Provides shared context across all notebooks (2.x‚Äì3.x).\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.4.2 Set Random Seeds\n",
    "‚îÇ        ‚Üí Guarantees deterministic statistical results.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.4.3 Register Protected Columns\n",
    "‚îÇ        ‚Üí ID and target columns (customerID, Churn_flag) locked from mutation.\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: config_validation_report.csv\n",
    "    üü© *Hiring value:* Reflects enterprise-ready modularity ‚Äî you centralize constants cleanly.\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #8470d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.5 üß© Dataset Load & Version Verification\n",
    "</summary>\n",
    "\n",
    "```groovy\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.5.1 Load Raw Dataset (CSV or Parquet)\n",
    "‚îÇ        ‚Üí Reads from CONFIG[\"PATHS\"][\"RAW_DATA\"]\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.5.2 Hash / Snapshot Validation\n",
    "‚îÇ        ‚Üí Compare MD5/SHA hash of dataset to version registry.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.5.3 Schema Version Logging\n",
    "‚îÇ        ‚Üí Record dataset version, load timestamp, row/column count.\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: dataset_load_log.csv\n",
    "    üü© *Hiring value:* Proves you understand data lineage ‚Äî recruiters see governance mindset.\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #8470d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.6 üîç Quick Structural Validation (Pre-DQ Sanity)\n",
    "</summary>\n",
    "\n",
    "```groovy\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "1.6 üîç Quick Structural Validation (Pre-DQ Sanity)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.6.1 Verify Non-Empty Dataset\n",
    "‚îÇ        ‚Üí Assert df.shape > (0,0).\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.6.2 Basic Column-Type Introspection\n",
    "‚îÇ        ‚Üí preview df.dtypes, null counts, unique counts.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.6.3 Target Variable Registration\n",
    "‚îÇ        ‚Üí Map ‚ÄúYes‚Äù/‚ÄúNo‚Äù ‚Üí 1/0; create `Churn_flag`.\n",
    "‚îÇ        ‚Üí Ensures consistency for modeling (Section 3).\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: initial_schema_summary.csv\n",
    "    üü© *Hiring value:* You validate structure *before* analysis ‚Äî shows quality-first discipline.\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #8470d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.7 üßæ Environment Summary & Log Consolidation\n",
    "</summary>\n",
    "\n",
    "```groovy\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "1.7 üßæ Environment Summary & Log Consolidation\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.7.1 Capture System & Library Versions\n",
    "‚îÇ        ‚Üí Python, pandas, numpy, seaborn, matplotlib, etc.\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 1.7.2 Generate Setup Summary Report\n",
    "‚îÇ        ‚Üí Aggregates paths, env vars, library versions, config sources.\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ Output: setup_summary.json / markdown cell\n",
    "    üü© *Hiring value:* Gives recruiters a polished, verifiable record of engineering hygiene.\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:10px solid #8470d4ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\"> \n",
    "1.8 ‚úÖ Integration Points\n",
    "</summary>\n",
    "\n",
    "```groovy\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "‚úÖ Integration Points\n",
    "‚Üí Feeds: Section 2 (Data Quality & Integrity Framework)\n",
    "‚Üí Updates: CONFIG, PROJECT_ROOT, ENV_LOGS\n",
    "‚Üí Ensures all later notebooks can run deterministically and portably.\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ea15c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.0.0-1.0.2 | PART A üì¶ Imports & Environment Settings\n",
    "</summary>\n",
    "\n",
    ">### **1.0.0 Import Core Libraries**\n",
    "- pandas, numpy, matplotlib, seaborn, scipy, pathlib, warnings\n",
    "- Establish reproducible numeric & plotting environment\n",
    "\n",
    ">### **1.0.1 Visualization Defaults**\n",
    "- Apply consistent theme (whitegrid, font, figsize)\n",
    "- Centralizes aesthetic consistency across notebooks\n",
    "\n",
    ">### **1.0.2 Global Display & Print Settings**\n",
    "- Configure pandas display, float precision, warning filters\n",
    "- Guarantees uniform console and report behavior\n",
    "\n",
    "---\n",
    "\n",
    "- **Output:** environment_log.txt (optional)\n",
    "\n",
    "- üü© *Hiring value:* Demonstrates environment reproducibility and version-controlled dependency awareness ‚Äî key in team analytics.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7d938f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.0.0 Imports & Environment Settings\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import platform\n",
    "import hashlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from types import MappingProxyType\n",
    "from datetime import datetime, date\n",
    "import subprocess\n",
    "\n",
    "now = datetime.now()\n",
    "today = date.today()\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "# Local application (Create after first full build)\n",
    "# from telco_churn.config.loader import C, CONFIG\n",
    "# from telco_churn.io.datasets import load_raw_telco\n",
    "# from telco_churn.dq.schema_checks import run_schema_checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58c0611b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.0.1 Clean visualization defaults\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:,.4f}\".format)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (12, 6),\n",
    "        \"font.size\": 11,\n",
    "        \"axes.grid\": True,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ceb7e57f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Env & visual defaults set!\n",
      "   python: 3.12.6 | pandas: 2.3.3 | numpy: 2.3.5 | seaborn: 0.13.2 | matplotlib: 3.10.7 | pyyaml: 6.0.3\n"
     ]
    }
   ],
   "source": [
    "# 1.0.2 Core library version snapshot (match pins in requirements.txt)\n",
    "versions = {\n",
    "    \"python\":    sys.version.split()[0],\n",
    "    \"pandas\":    pd.__version__,\n",
    "    \"numpy\":     np.__version__,\n",
    "    \"seaborn\":   sns.__version__,\n",
    "    \"matplotlib\": mpl.__version__,\n",
    "    \"pyyaml\":    getattr(yaml, \"__version__\", \"unknown\"),\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Env & visual defaults set!\")\n",
    "print(\"   \" + \" | \".join(f\"{k}: {v}\" for k, v in versions.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ec2f6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.1.1-1.1.3 | PART B üìÅ Path Setup & Directory Management\n",
    "</summary>\n",
    "\n",
    "# 1.1.1-1.1.3 | PART B Path Setup & Directory Management\n",
    "# - \"Level_3\" centralized via LEVEL_NAME\n",
    "# - Case-insensitive project root detection\n",
    "# - Explicit prints for PROJECT_ROOT and LEVEL_ROOT\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.1.1 Resolve Project Root**\n",
    "- ‚Üí Dynamically detect `/Telco` (or repo root) via `Path.cwd().resolve()` walk-up.\n",
    "- ‚Üí Supports portability across dev, CI, and prod-like environments.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.1.2 Define Core Subdirectories**\n",
    "- ‚Üí `/resources/data/raw/`, `/resources/data/processed/`, `/Level_3/resources/reports/`, `/config/`, `/models/`, `/Level_3/resources/artifacts/`, etc.\n",
    "- ‚Üí Create missing directories automatically (`mkdir(parents=True, exist_ok=True)`).\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.1.3 Verify Writable Paths**\n",
    "- ‚Üí Assert read/write permissions for key paths (data, reports, artifacts).\n",
    "\n",
    "- ‚îî‚îÄ‚îÄ **Output:** `directory_structure_report.txt`\n",
    "- üü© *Hiring value:* Signals infrastructure literacy ‚Äî you treat notebooks like codebases with a real project root and folder structure.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508d203",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "# TODO: | PART B Path Setup & Directory Management / 1113_924\n",
    "</summary>\n",
    "\n",
    "This looks really solid already ‚Äî this is exactly the kind of ‚ÄúPart B: environment bootstrap‚Äù that makes a project feel real. There are just a few small things I‚Äôd tweak for clarity/maintainability and ‚Äúfuture-you won‚Äôt hate this‚Äù vibes.\n",
    "\n",
    "I‚Äôll point them out and then show a tightened version.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. PROJECT_ROOT resolution\n",
    "\n",
    "This part is good:\n",
    "\n",
    "```python\n",
    "CURRENT_PATH = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "\n",
    "for parent in [CURRENT_PATH] + list(CURRENT_PATH.parents):\n",
    "    # Heuristic 1: repo folder name is \"Telco\"\n",
    "    if parent.name == \"Telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "```\n",
    "\n",
    "Two small ideas:\n",
    "\n",
    "1. **Be case-insensitive**, just in case the folder gets renamed `telco` at some point:\n",
    "\n",
    "   ```python\n",
    "   if parent.name.lower() == \"telco\":\n",
    "   ```\n",
    "\n",
    "2. **Consider an override** (env var) later, but that can wait until you have the package; your comment about `config/project_config.yaml` is already a nice ‚Äúfuture heuristic.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Magic `\"Level_3\"` string everywhere\n",
    "\n",
    "Right now:\n",
    "\n",
    "```python\n",
    "CONFIG_DIR = PROJECT_ROOT / \"Level_3\" / \"config\"\n",
    "\n",
    "CONVENTION_DIRS = [\n",
    "    CONFIG_DIR,\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"reports\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"figures\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"models\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"outputs\",\n",
    "    PROJECT_ROOT / \"Level_3\" / \"resources\" / \"artifacts\",\n",
    "]\n",
    "```\n",
    "\n",
    "You‚Äôre going to copy this pattern for Level_4, Level_5, etc, so I‚Äôd pull `\"Level_3\"` into a tiny constant at the top of the notebook:\n",
    "\n",
    "```python\n",
    "LEVEL_NAME = \"Level_3\"\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "\n",
    "CONFIG_DIR = LEVEL_ROOT / \"config\"\n",
    "\n",
    "CONVENTION_DIRS = [\n",
    "    CONFIG_DIR,\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    LEVEL_ROOT / \"resources\" / \"reports\",\n",
    "    LEVEL_ROOT / \"resources\" / \"figures\",\n",
    "    LEVEL_ROOT / \"resources\" / \"models\",\n",
    "    LEVEL_ROOT / \"resources\" / \"outputs\",\n",
    "    LEVEL_ROOT / \"resources\" / \"artifacts\",\n",
    "]\n",
    "```\n",
    "\n",
    "Same behavior, much easier to generalize later.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Directory creation + writable check\n",
    "\n",
    "This bit is great:\n",
    "\n",
    "```python\n",
    "for d in CONVENTION_DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "```\n",
    "\n",
    "and:\n",
    "\n",
    "```python\n",
    "writable_checks = []\n",
    "for d in CONVENTION_DIRS:\n",
    "    try:\n",
    "        test_file = d / \".write_test.tmp\"\n",
    "        test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "        test_file.unlink(missing_ok=True)\n",
    "        writable_checks.append((str(d), True))\n",
    "    except Exception:\n",
    "        writable_checks.append((str(d), False))\n",
    "```\n",
    "\n",
    "Two micro-tweaks:\n",
    "\n",
    "1. **Narrow the exception** if you want (e.g. `OSError`), but `Exception` is fine for a ‚Äúhealth check‚Äù if you just log it.\n",
    "2. Add a tiny comment that explains *why* you‚Äôre doing the test write (you know now ‚Äî future-you might not):\n",
    "\n",
    "```python\n",
    "# 1.2.3 Verify Writable Paths (simple can-I-write-here check)\n",
    "```\n",
    "\n",
    "You already have the printout at the end, which is üëå.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. A slightly polished version\n",
    "\n",
    "Here‚Äôs your same logic with those tiny tweaks:\n",
    "\n",
    "```python\n",
    "# 1.2.1‚Äì1.2.3 | PART B Path Setup & Directory Management\n",
    "\n",
    "# 1.2.1 Resolve Project Root\n",
    "CURRENT_PATH = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "LEVEL_NAME = \"Level_3\"  # current working level\n",
    "LEVEL_ROOT = None\n",
    "\n",
    "for parent in [CURRENT_PATH] + list(CURRENT_PATH.parents):\n",
    "    # Heuristic 1: repo folder name is \"Telco\"\n",
    "    if parent.name.lower() == \"telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "        break\n",
    "    # Heuristic 2 (future): parent contains config/project_config.yaml\n",
    "    # if (parent / \"config\" / \"project_config.yaml\").exists():\n",
    "    #     PROJECT_ROOT = parent\n",
    "    #     LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "    #     break\n",
    "\n",
    "if PROJECT_ROOT is None or LEVEL_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Could not locate Telco project root from {CURRENT_PATH}. \"\n",
    "        \"Expected a 'Telco' folder or 'config/project_config.yaml' above.\"\n",
    "    )\n",
    "\n",
    "print(f\"üìÅ 1.2.1 Project root resolved   ‚Üí {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ 1.2.1 Level root resolved     ‚Üí {LEVEL_ROOT}\")\n",
    "\n",
    "# 1.2.2 Define Core Subdirectories (convention-level, before config)\n",
    "CONFIG_DIR = LEVEL_ROOT / \"config\"\n",
    "\n",
    "CONVENTION_DIRS = [\n",
    "    CONFIG_DIR,\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    LEVEL_ROOT / \"resources\" / \"reports\",\n",
    "    LEVEL_ROOT / \"resources\" / \"figures\",\n",
    "    LEVEL_ROOT / \"resources\" / \"models\",\n",
    "    LEVEL_ROOT / \"resources\" / \"outputs\",\n",
    "    LEVEL_ROOT / \"resources\" / \"artifacts\",\n",
    "]\n",
    "\n",
    "for d in CONVENTION_DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1.2.3 Verify Writable Paths\n",
    "writable_checks = []\n",
    "for d in CONVENTION_DIRS:\n",
    "    try:\n",
    "        test_file = d / \".write_test.tmp\"\n",
    "        test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "        test_file.unlink(missing_ok=True)\n",
    "        writable_checks.append((str(d), True))\n",
    "    except Exception:\n",
    "        writable_checks.append((str(d), False))\n",
    "\n",
    "print(\"\\nüìÇ 1.2.2‚Äì1.2.3 Directory structure & write access:\")\n",
    "for path_str, ok in writable_checks:\n",
    "    status = \"‚úÖ writable\" if ok else \"‚ö†Ô∏è not writable\"\n",
    "    print(f\"   {status}: {path_str}\")\n",
    "```\n",
    "\n",
    "Functionally identical to what you have, but:\n",
    "\n",
    "* `\"Level_3\"` is centralized\n",
    "* Case-insensitive root detection\n",
    "* Slightly more explicit prints\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next step I can help you turn this 1.2 block into a tiny `telco_churn.config.paths` module (e.g. `get_project_root()`, `get_level_root()`) so your notebooks can just call a function instead of carrying the for-loop logic inline.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f31d475a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ 1.1.1 Project root resolved ‚Üí /Users/b/DATA/PROJECTS/Telco\n"
     ]
    }
   ],
   "source": [
    "# 1.1.1  Resolve Project Root\n",
    "\n",
    "CURRENT_PATH = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "LEVEL_NAME = \"Level_3\"  # current working level\n",
    "LEVEL_ROOT = None\n",
    "\n",
    "for parent in [CURRENT_PATH] + list(CURRENT_PATH.parents):\n",
    "    # Heuristic 1: repo folder name is \"Telco\"\n",
    "    # TODO: consider override after create package; your comment about config/project_config.yaml is already a nice ‚Äúfuture heuristic.‚Äù\n",
    "    #  Be case insensitive in case folder renamed\n",
    "    if parent.name.lower() == \"telco\":\n",
    "        PROJECT_ROOT = parent\n",
    "        LEVEL_ROOT = PROJECT_ROOT / LEVEL_NAME\n",
    "        break\n",
    "    # Heuristic 2: parent contains config/project_config.yaml\n",
    "    # Data lives up a level from the project root\n",
    "    # if (parent / \"config\" / \"project_config.yaml\").exists():\n",
    "    #     PROJECT_ROOT = parent\n",
    "    #     break\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Could not locate Telco project root from {CURRENT_PATH}. \"\n",
    "        \"Expected a 'Telco' folder or 'config/project_config.yaml' above.\"\n",
    "    )\n",
    "\n",
    "print(f\"üìÅ 1.1.1 Project root resolved ‚Üí {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1d44d09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ 1.1.2 Level root resolved     ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3\n"
     ]
    }
   ],
   "source": [
    "# 1.1.2 Define Core Subdirectories (convention-level, before config)\n",
    "\n",
    "CONFIG_DIR = LEVEL_ROOT / \"config\"\n",
    "\n",
    "DIRS = {\n",
    "    \"CONFIG_DIR\":        CONFIG_DIR,\n",
    "    \"RAW_DATA_DIR\":      PROJECT_ROOT / \"resources\" / \"data\" / \"raw\",\n",
    "    \"PROCESSED_DATA_DIR\":PROJECT_ROOT / \"resources\" / \"data\" / \"processed\",\n",
    "    \"REPORTS_DIR\":       LEVEL_ROOT / \"resources\" / \"reports\",\n",
    "    \"FIGURES_DIR\":       LEVEL_ROOT / \"resources\" / \"figures\",\n",
    "    \"MODELS_DIR\":        LEVEL_ROOT / \"resources\" / \"models\",\n",
    "    \"OUTPUTS_DIR\":       LEVEL_ROOT / \"resources\" / \"outputs\",\n",
    "    \"ARTIFACTS_DIR\":     LEVEL_ROOT / \"resources\" / \"artifacts\",\n",
    "    \"DASHBOARDS_DIR\":     LEVEL_ROOT / \"resources\" / \"_dash\"\n",
    "}\n",
    "\n",
    "# Create directories (dirs only)\n",
    "for path in DIRS.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optionally expose key paths as globals for later sections\n",
    "REPORTS_DIR   = DIRS[\"REPORTS_DIR\"]\n",
    "ARTIFACTS_DIR = DIRS[\"ARTIFACTS_DIR\"]\n",
    "FIGURES_DIR   = DIRS[\"FIGURES_DIR\"]\n",
    "MODELS_DIR    = DIRS[\"MODELS_DIR\"]\n",
    "OUTPUTS_DIR   = DIRS[\"OUTPUTS_DIR\"]\n",
    "DASHBOARDS_DIR  = DIRS[\"DASHBOARDS_DIR\"]\n",
    "\n",
    "print(f\"üìÅ 1.1.2 Level root resolved     ‚Üí {LEVEL_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6179f6d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ 1.1.3 Directory structure & write access:\n",
      "   ‚úÖ writable: CONFIG_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/config\n",
      "   ‚úÖ writable: RAW_DATA_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/resources/data/raw\n",
      "   ‚úÖ writable: PROCESSED_DATA_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/resources/data/processed\n",
      "   ‚úÖ writable: REPORTS_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports\n",
      "   ‚úÖ writable: FIGURES_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/figures\n",
      "   ‚úÖ writable: MODELS_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/models\n",
      "   ‚úÖ writable: OUTPUTS_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/outputs\n",
      "   ‚úÖ writable: ARTIFACTS_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts\n",
      "   ‚úÖ writable: DASHBOARDS_DIR ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/_dash\n"
     ]
    }
   ],
   "source": [
    "# 1.1.3 Verify writable paths (fail fast if directories are not usable)\n",
    "\n",
    "# Simple \"can-I-write-here?\" smoke test so later sections fail fast\n",
    "writable_checks = []\n",
    "\n",
    "for name, path in DIRS.items():\n",
    "    try:\n",
    "        test_file = path / \".write_test.tmp\"\n",
    "        test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "        test_file.unlink(missing_ok=True)\n",
    "        writable_checks.append((name, path, True))\n",
    "    except Exception:\n",
    "        writable_checks.append((name, path, False))\n",
    "\n",
    "print(\"\\nüìÇ 1.1.3 Directory structure & write access:\")\n",
    "for name, path_obj, ok in writable_checks:\n",
    "    status = \"‚úÖ writable\" if ok else \"‚ö†Ô∏è not writable\"\n",
    "    print(f\"   {status}: {name} ‚Üí {path_obj}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57144c26",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.2.1-1.2.3 | PART C üß∞ Env Config & Dependency Management | Hybrid Config Loader (Type 3 ‚Üí Type 2)\n",
    "\n",
    "</summary>\n",
    "\n",
    "# 1.2.1-1.3.3 | PART C Load Environment Configuration & Hybrid Config Loader (Type 3 ‚Üí Type 2)\n",
    "\n",
    ">**Goal:** Load a **YAML config file** as the single source of truth (Type 3), then expose it as a safe, ergonomic in-memory mapping (Type 2-style API).\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.2.1 Load External Config (`project_config.yaml`)**\n",
    "- Read `config/project_config.yaml` using `yaml.safe_load`.\n",
    "- Centralizes:\n",
    "- `PATHS` (RAW_DATA, PROCESSED, ARTIFACTS, REPORTS, FIGURES, MODELS, OUTPUTS)\n",
    "- `TARGET` (target column, raw target, positive/negative labels)\n",
    "- `ID_COLUMNS`\n",
    "- `RANGES` (tenure, MonthlyCharges, TotalCharges)\n",
    "- `DATA_QUALITY` thresholds (high-cardinality, rare categories, etc.).\n",
    "- ‚Üí YAML lives in version control and can be overridden per environment.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.2.2 Create In-Memory CONFIG + Read-Only View (`CFG`)**\n",
    "- Store YAML contents in a Python `CONFIG` dict.\n",
    "- Wrap it in `MappingProxyType` ‚Üí `CFG` to prevent accidental mutation.\n",
    "- This becomes the **in-memory config object** used throughout the notebook.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.2.3 Add Dotted-Path Helper `C()`**\n",
    "- Implement `C(\"PATHS.RAW_DATA\")` / `C(\"RANGES.tenure.max\")` style lookup.\n",
    "- ‚Üí Keeps code clean and readable when accessing deeply nested config entries.\n",
    "\n",
    "- ‚îî‚îÄ‚îÄ **Output:** optional `config_snapshot.json` (for debugging / documentation)\n",
    "- üü© *Hiring value:* Shows you combine file-based, versioned config with a clean runtime API ‚Äî exactly how production pipelines manage configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bd157ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.2.1 Load external config file\n",
    "\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG_PATH = CONFIG_DIR / \"project_config.yaml\"\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    # üí° CHANGED: Expanded error message with a hint about expected location.\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Config file not found at: {CONFIG_PATH}\\n\"\n",
    "        \"Expected a project_config.yaml under LEVEL_ROOT/config.\"\n",
    "    )\n",
    "\n",
    "# üí° Use CONFIG_PATH.open(...) > instead of `bare open() for style consistency with Path.\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    config_data = yaml.safe_load(f) or {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e323695a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.2.2 Load in-memory config TODO: inspect yaml vars\n",
    "\n",
    "CONFIG = config_data\n",
    "CFG = MappingProxyType(CONFIG)  # üîí read-only view to prevent accidental mutation\n",
    "\n",
    "#\n",
    "# inspect new config.yaml vars\n",
    "strict_map   = CONFIG.get(\"SCHEMA_EXPECTED_DTYPES_STRICT\", {})\n",
    "semantic_map = CONFIG.get(\"SCHEMA_EXPECTED_DTYPES_SEMANTIC\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f780773",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 1.2 Config loaded & wrapped as read-only mapping.\n",
      "   CONFIG_PATH:    /Users/b/DATA/PROJECTS/Telco/Level_3/config/project_config.yaml\n",
      "   Snapshot saved: /Users/b/DATA/PROJECTS/Telco/Level_3/config/config_snapshot.json\n",
      "   Project name:   Telco Customer Churn\n",
      "   Target column:  Churn_flag (raw: Churn)\n",
      "   RAW_DATA path:  None\n"
     ]
    }
   ],
   "source": [
    "# 1.2.3 Add dotted helper - def\n",
    "\n",
    "def C(path: str, default=None):\n",
    "    \"\"\"\n",
    "    Dotted-path lookup into CONFIG.\n",
    "\n",
    "    Example:\n",
    "        C(\"PATHS.RAW_DATA\")\n",
    "        C(\"RANGES.tenure.max\")\n",
    "        C(\"TARGET.POSITIVE_CLASS\", default=\"Yes\")\n",
    "    \"\"\"\n",
    "    node = CONFIG\n",
    "    for part in path.split(\".\"):\n",
    "        if isinstance(node, dict) and part in node:\n",
    "            node = node[part]\n",
    "        else:\n",
    "            return default\n",
    "    return node\n",
    "\n",
    "# Optional snapshot\n",
    "snapshot_path = CONFIG_DIR / \"config_snapshot.json\"\n",
    "with snapshot_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ 1.2 Config loaded & wrapped as read-only mapping.\")\n",
    "print(f\"   CONFIG_PATH:    {CONFIG_PATH}\")\n",
    "print(f\"   Snapshot saved: {snapshot_path}\")\n",
    "print(f\"   Project name:   {C('META.PROJECT_NAME')}\")\n",
    "print(f\"   Target column:  {C('TARGET.COLUMN')} (raw: {C('TARGET.RAW_COLUMN')})\")\n",
    "print(f\"   RAW_DATA path:  {C('PATHS.RAW_DATA')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b56127",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.3.1-1.3.3 | PART D ‚öôÔ∏è Global Constants & Config Mapping\n",
    "</summary>\n",
    "\n",
    "##### **Goal:** Map key entries from `CONFIG` into convenient runtime constants for the rest of the project.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.3.1 Resolve Config-Driven Paths & Columns**\n",
    "- `RAW_DATA      = PROJECT_ROOT / C(\"PATHS.RAW_DATA\")`\n",
    "- `PROCESSED_DIR = PROJECT_ROOT / C(\"PATHS.PROCESSED\")`\n",
    "- `ARTIFACTS_DIR = PROJECT_ROOT / C(\"PATHS.ARTIFACTS\")`\n",
    "- `REPORTS_DIR`, `FIGURES_DIR`, `MODELS_DIR`, `OUTPUTS_DIR` similarly.\n",
    "- `TARGET_NAME   = C(\"TARGET.COLUMN\")`\n",
    "- `RAW_TARGET    = C(\"TARGET.RAW_COLUMN\")`\n",
    "- `ID_COLS       = C(\"ID_COLUMNS\", [])`\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.3.2 Set Random Seeds (Config-Driven)**\n",
    "- Optionally read seed values from `CONFIG[\"SEEDS\"]` or use default seeds.\n",
    "- Set seeds for `numpy`, `random`, and any ML libraries you‚Äôll use.\n",
    "- Guarantees deterministic statistical results across runs.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.3.3 Register Protected Columns & DQ Ranges**\n",
    "- `PROTECTED_COLUMNS = set(ID_COLS) ‚à™ {TARGET_NAME}`\n",
    "- Load `RANGES  = C(\"RANGES\")`\n",
    "- Load `DQ_OPTS = C(\"DATA_QUALITY\")` (thresholds, suspect tokens, etc.).\n",
    "- These values govern Section 2‚Äôs checks and are all **config-driven**, not hard-coded.\n",
    "- ‚îî‚îÄ‚îÄ **Output:** `config_validation_report.csv`\n",
    "- üü© *Hiring value:* Reflects enterprise-ready modularity ‚Äî constants are centralized and derived from a versioned YAML config, not scattered magic numbers.\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d026c0fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå 1.3.1 Resolved config-driven paths & columns:\n",
      "   RAW_DATA:      /Users/b/DATA/PROJECTS/Telco/data/_raw\n",
      "   PROCESSED_DIR: /Users/b/DATA/PROJECTS/Telco/data/processed\n",
      "   ARTIFACTS_DIR: /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts\n",
      "   REPORTS_DIR:   /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports\n",
      "   FIGURES_DIR:   /Users/b/DATA/PROJECTS/Telco/Level_3/resources/figures\n",
      "   MODELS_DIR:    /Users/b/DATA/PROJECTS/Telco/Level_3/resources/models\n",
      "   OUTPUTS_DIR:   /Users/b/DATA/PROJECTS/Telco/Level_3/outputs\n",
      "   target_name:   Churn_flag (raw: Churn)\n",
      "   id_cols:       ['customerID']\n",
      "   ranges:        {'tenure': {'min': 0, 'max': 120}, 'MonthlyCharges': {'min': 0, 'max': 1000}, 'TotalCharges': {'min': 0, 'max': 100000}}\n",
      "   data_quality:  {'NUMERIC_LIKE_THRESHOLD': 0.95, 'RARE_PCT_THRESHOLD': 0.5, 'RARE_MIN_COUNT': 5, 'HIGH_CARD_THRESHOLD': 50, 'NEARLY_CONST_THRESHOLD': 0.98, 'SUSPECT_TOKENS': ['?', 'unknown', 'unk', 'n/a', 'na', 'n.a.', 'null', 'none', 'missing', '-', '--', '_', 'tbd']}\n"
     ]
    }
   ],
   "source": [
    "# 1.3.1 Resolve Config-Driven Paths & Columns\n",
    "# TODO: def\n",
    "# FIXME: (FIX_ID: 11132019): allow PROJECT_ROOT override via env/config instead of Telco-only heuristic\n",
    "# FIXME: (FIX_ID: 11141430): allow C() override via env/config instead of Telco-only heuristic\n",
    "# FIXME: (FIX_ID: 11141431): Keep or remove 1.3.4 / place all parts appropriately if keeping.\n",
    "# FIXME(FIX_ID: 2025-11-13-01): RAW_DATA path is hard-coded to Telco layout.\n",
    "#   Future: allow overriding PROJECT_ROOT via env var or CLI arg.\n",
    "\n",
    "# RAW_DATA: support RAW_DATA then RAW_DATA_DIR\n",
    "raw_data_rel = C(\"PATHS.RAW_DATA\", None)\n",
    "if raw_data_rel is None:\n",
    "    raw_data_rel = C(\"PATHS.RAW_DATA_DIR\", None)\n",
    "if raw_data_rel is None:\n",
    "    raise KeyError(\n",
    "        \"‚ùå Missing config for RAW_DATA path. \"\n",
    "        \"Tried PATHS.RAW_DATA and PATHS.RAW_DATA_DIR.\"\n",
    "    )\n",
    "RAW_DATA_DIR = PROJECT_ROOT / raw_data_rel\n",
    "\n",
    "# PROCESSED_DIR: support PROCESSED then PROCESSED_DIR\n",
    "processed_rel = C(\"PATHS.PROCESSED\", None)\n",
    "if processed_rel is None:\n",
    "    processed_rel = C(\"PATHS.PROCESSED_DIR\", None)\n",
    "if processed_rel is None:\n",
    "    raise KeyError(\n",
    "        \"‚ùå Missing config for PROCESSED path. \"\n",
    "        \"Tried PATHS.PROCESSED and PATHS.PROCESSED_DIR.\"\n",
    "    )\n",
    "PROCESSED_DIR = PROJECT_ROOT / processed_rel\n",
    "\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / C(\"PATHS.ARTIFACTS\")\n",
    "REPORTS_DIR   = PROJECT_ROOT / C(\"PATHS.REPORTS\")\n",
    "FIGURES_DIR   = PROJECT_ROOT / C(\"PATHS.FIGURES\")\n",
    "MODELS_DIR    = PROJECT_ROOT / C(\"PATHS.MODELS\")\n",
    "OUTPUTS_DIR   = PROJECT_ROOT / C(\"PATHS.OUTPUTS\")\n",
    "\n",
    "# old pattern\n",
    "# RAW_DATA      = PROJECT_ROOT / C(\"PATHS.RAW_DATA\")\n",
    "# PROCESSED_DIR = PROJECT_ROOT / C(\"PATHS.PROCESSED\")\n",
    "# ARTIFACTS_DIR = PROJECT_ROOT / C(\"PATHS.ARTIFACTS\")\n",
    "# REPORTS_DIR   = PROJECT_ROOT / C(\"PATHS.REPORTS\")\n",
    "# FIGURES_DIR   = PROJECT_ROOT / C(\"PATHS.FIGURES\")\n",
    "# MODELS_DIR    = PROJECT_ROOT / C(\"PATHS.MODELS\")\n",
    "# OUTPUTS_DIR   = PROJECT_ROOT / C(\"PATHS.OUTPUTS\")\n",
    "\n",
    "# old 2\n",
    "# RAW_DATA      = _cfg_path(\"PATHS.RAW_DATA\")\n",
    "# PROCESSED_DIR = _cfg_path(\"PATHS.PROCESSED\")\n",
    "# ARTIFACTS_DIR = _cfg_path(\"PATHS.ARTIFACTS\")\n",
    "# REPORTS_DIR   = _cfg_path(\"PATHS.REPORTS\")\n",
    "# FIGURES_DIR   = _cfg_path(\"PATHS.FIGURES\")\n",
    "# MODELS_DIR    = _cfg_path(\"PATHS.MODELS\")\n",
    "# OUTPUTS_DIR   = _cfg_path(\"PATHS.OUTPUTS\")\n",
    "\n",
    "for d in [PROCESSED_DIR, ARTIFACTS_DIR, REPORTS_DIR, FIGURES_DIR, MODELS_DIR, OUTPUTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "target_name = C(\"TARGET.COLUMN\")\n",
    "raw_target  = C(\"TARGET.RAW_COLUMN\")\n",
    "id_cols     = C(\"ID_COLUMNS\", []) or []\n",
    "ranges      = C(\"RANGES\", {})\n",
    "dq_opts     = C(\"DATA_QUALITY\", {})\n",
    "flags       = C(\"FLAGS\", {})\n",
    "\n",
    "print(\"üìå 1.3.1 Resolved config-driven paths & columns:\")\n",
    "print(f\"   RAW_DATA:      {RAW_DATA}\")\n",
    "print(f\"   PROCESSED_DIR: {PROCESSED_DIR}\")\n",
    "print(f\"   ARTIFACTS_DIR: {ARTIFACTS_DIR}\")\n",
    "print(f\"   REPORTS_DIR:   {REPORTS_DIR}\")\n",
    "print(f\"   FIGURES_DIR:   {FIGURES_DIR}\")\n",
    "print(f\"   MODELS_DIR:    {MODELS_DIR}\")\n",
    "print(f\"   OUTPUTS_DIR:   {OUTPUTS_DIR}\")\n",
    "print(f\"   target_name:   {target_name} (raw: {raw_target})\")\n",
    "print(f\"   id_cols:       {id_cols}\")\n",
    "print(f\"   ranges:        {ranges}\")\n",
    "print(f\"   data_quality:  {dq_opts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef0872f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé≤ 1.3.2 Seeds set from config (if provided): {'numpy': 42, 'python': 42}\n"
     ]
    }
   ],
   "source": [
    "# 1.3.2 Set Random Seeds (Config-Driven)\n",
    "\n",
    "seeds = C(\"SEEDS\", {\"numpy\": 42, \"python\": 42})\n",
    "np.random.seed(seeds.get(\"numpy\", 42))\n",
    "random.seed(seeds.get(\"python\", 42))\n",
    "print(f\"\\nüé≤ 1.3.2 Seeds set from config (if provided): {seeds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a16a5f75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ°Ô∏è 1.3.3 Protected columns (not to be mutated): ['Churn_flag', 'customerID']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>category</th>\n",
       "      <th>required</th>\n",
       "      <th>ok</th>\n",
       "      <th>value</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PATHS.RAW_DATA</td>\n",
       "      <td>paths</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/Users/b/DATA/PROJECTS/Telco/data/_raw</td>\n",
       "      <td>file should exist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PATHS.PROCESSED</td>\n",
       "      <td>paths</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/Users/b/DATA/PROJECTS/Telco/data/processed</td>\n",
       "      <td>directory should exist (auto-created if missing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATHS.ARTIFACTS</td>\n",
       "      <td>paths</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/Users/b/DATA/PROJECTS/Telco/Level_3/resources...</td>\n",
       "      <td>directory should exist (auto-created if missing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PATHS.REPORTS</td>\n",
       "      <td>paths</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>/Users/b/DATA/PROJECTS/Telco/Level_3/resources...</td>\n",
       "      <td>directory should exist (auto-created if missing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TARGET.COLUMN</td>\n",
       "      <td>target</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Churn_flag</td>\n",
       "      <td>numeric flag used across Sections 2‚Äì3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TARGET.RAW_COLUMN</td>\n",
       "      <td>target</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Churn</td>\n",
       "      <td>raw label column before encoding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_COLUMNS</td>\n",
       "      <td>ids</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['customerID']</td>\n",
       "      <td>at least one primary identifier expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RANGES</td>\n",
       "      <td>ranges</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>['tenure', 'MonthlyCharges', 'TotalCharges']</td>\n",
       "      <td>optional but recommended for Telco numeric fields</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DATA_QUALITY</td>\n",
       "      <td>data_quality</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>['NUMERIC_LIKE_THRESHOLD', 'RARE_PCT_THRESHOLD...</td>\n",
       "      <td>optional thresholds for Section 2 checks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               check      category  required    ok  \\\n",
       "0     PATHS.RAW_DATA         paths      True  True   \n",
       "1    PATHS.PROCESSED         paths      True  True   \n",
       "2    PATHS.ARTIFACTS         paths      True  True   \n",
       "3      PATHS.REPORTS         paths      True  True   \n",
       "4      TARGET.COLUMN        target      True  True   \n",
       "5  TARGET.RAW_COLUMN        target      True  True   \n",
       "6         ID_COLUMNS           ids      True  True   \n",
       "7             RANGES        ranges     False  True   \n",
       "8       DATA_QUALITY  data_quality     False  True   \n",
       "\n",
       "                                               value  \\\n",
       "0             /Users/b/DATA/PROJECTS/Telco/data/_raw   \n",
       "1        /Users/b/DATA/PROJECTS/Telco/data/processed   \n",
       "2  /Users/b/DATA/PROJECTS/Telco/Level_3/resources...   \n",
       "3  /Users/b/DATA/PROJECTS/Telco/Level_3/resources...   \n",
       "4                                         Churn_flag   \n",
       "5                                              Churn   \n",
       "6                                     ['customerID']   \n",
       "7       ['tenure', 'MonthlyCharges', 'TotalCharges']   \n",
       "8  ['NUMERIC_LIKE_THRESHOLD', 'RARE_PCT_THRESHOLD...   \n",
       "\n",
       "                                                note  \n",
       "0                                  file should exist  \n",
       "1   directory should exist (auto-created if missing)  \n",
       "2   directory should exist (auto-created if missing)  \n",
       "3   directory should exist (auto-created if missing)  \n",
       "4              numeric flag used across Sections 2‚Äì3  \n",
       "5                   raw label column before encoding  \n",
       "6           at least one primary identifier expected  \n",
       "7  optional but recommended for Telco numeric fields  \n",
       "8           optional thresholds for Section 2 checks  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 1.3.3 Config validation report saved ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/config_validation_report.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.3.3 Register Protected Columns & DQ Ranges\n",
    "\n",
    "protected_columns = set(id_cols)\n",
    "if target_name:\n",
    "    protected_columns.add(target_name)\n",
    "print(f\"\\nüõ°Ô∏è 1.3.3 Protected columns (not to be mutated): {sorted(protected_columns)}\")\n",
    "\n",
    "\n",
    "# Config validation report\n",
    "\n",
    "checks = []\n",
    "\n",
    "def add_check(name, ok, value=None, category=\"general\", required=True, note=\"\"):\n",
    "    checks.append(\n",
    "        {\n",
    "            \"check\": name,\n",
    "            \"category\": category,\n",
    "            \"required\": required,\n",
    "            \"ok\": bool(ok),\n",
    "            \"value\": str(value),\n",
    "            \"note\": note,\n",
    "        }\n",
    "    )\n",
    "\n",
    "add_check(\n",
    "    \"PATHS.RAW_DATA\",\n",
    "    RAW_DATA.exists(),\n",
    "    value=RAW_DATA,\n",
    "    category=\"paths\",\n",
    "    note=\"file should exist\",\n",
    ")\n",
    "add_check(\n",
    "    \"PATHS.PROCESSED\",\n",
    "    PROCESSED_DIR.exists(),\n",
    "    value=PROCESSED_DIR,\n",
    "    category=\"paths\",\n",
    "    note=\"directory should exist (auto-created if missing)\",\n",
    ")\n",
    "add_check(\n",
    "    \"PATHS.ARTIFACTS\",\n",
    "    ARTIFACTS_DIR.exists(),\n",
    "    value=ARTIFACTS_DIR,\n",
    "    category=\"paths\",\n",
    "    note=\"directory should exist (auto-created if missing)\",\n",
    ")\n",
    "add_check(\n",
    "    \"PATHS.REPORTS\",\n",
    "    REPORTS_DIR.exists(),\n",
    "    value=REPORTS_DIR,\n",
    "    category=\"paths\",\n",
    "    note=\"directory should exist (auto-created if missing)\",\n",
    ")\n",
    "\n",
    "add_check(\n",
    "    \"TARGET.COLUMN\",\n",
    "    target_name is not None,\n",
    "    value=target_name,\n",
    "    category=\"target\",\n",
    "    note=\"numeric flag used across Sections 2‚Äì3\",\n",
    ")\n",
    "add_check(\n",
    "    \"TARGET.RAW_COLUMN\",\n",
    "    raw_target is not None,\n",
    "    value=raw_target,\n",
    "    category=\"target\",\n",
    "    note=\"raw label column before encoding\",\n",
    ")\n",
    "add_check(\n",
    "    \"ID_COLUMNS\",\n",
    "    len(id_cols) > 0,\n",
    "    value=id_cols,\n",
    "    category=\"ids\",\n",
    "    note=\"at least one primary identifier expected\",\n",
    ")\n",
    "\n",
    "add_check(\n",
    "    \"RANGES\",\n",
    "    bool(ranges),\n",
    "    value=list(ranges.keys()),\n",
    "    category=\"ranges\",\n",
    "    required=False,\n",
    "    note=\"optional but recommended for Telco numeric fields\",\n",
    ")\n",
    "add_check(\n",
    "    \"DATA_QUALITY\",\n",
    "    bool(dq_opts),\n",
    "    value=list(dq_opts.keys()),\n",
    "    category=\"data_quality\",\n",
    "    required=False,\n",
    "    note=\"optional thresholds for Section 2 checks\",\n",
    ")\n",
    "\n",
    "config_validation_df = pd.DataFrame(checks)\n",
    "display(config_validation_df)\n",
    "\n",
    "config_validation_path = REPORTS_DIR / \"config_validation_report.csv\"\n",
    "config_validation_df.to_csv(config_validation_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ 1.3.3 Config validation report saved ‚Üí {config_validation_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38371f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.4.1-1.4.3 | PART E üìä Dataset Load & Version Verification\n",
    "</summary>\n",
    "\n",
    "> **GOAL:**\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.4.1 Load Raw Dataset (CSV or Parquet)**\n",
    "- Reads from CONFIG[\"PATHS\"][\"RAW_DATA\"]\n",
    "\n",
    ">### **1.4.2 Hash / Snapshot Validation**\n",
    "- Compare MD5/SHA hash of dataset to version registry.\n",
    "\n",
    ">### **1.4.3 Schema Version Logging**\n",
    "- Record dataset version, load timestamp, row/column count.\n",
    "  - ‚îî‚îÄ‚îÄ Output: dataset_load_log.csv\n",
    "- üü© *Hiring value:* Proves you understand data lineage ‚Äî recruiters see governance mindset.\n",
    "\n",
    "---\n",
    "\n",
    "**Options:**\n",
    "1. Download the dataset from kaggle\n",
    "2. Use Kaggle API\n",
    "**Download the dataset:** https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "\n",
    "```py\n",
    "%pip install kagglehub\n",
    "import os\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "# Set KaggleHub cache to current working directory\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = str(Path.cwd())\n",
    "\n",
    "# Download dataset directly into current working directory\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "\n",
    "print(\"üìÇ Dataset downloaded to:\", path)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b3ac0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 üß© DATASET LOAD & VERSION VERIFICATION\n",
      "1.4.1) üì• Loading raw dataset from: /Users/b/DATA/PROJECTS/Telco/data/_raw\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ùå Unsupported file format for RAW_DATA: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m     df_raw = pd.read_parquet(RAW_DATA_DIR)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Unsupported file format for RAW_DATA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m n_rows, n_cols = df_raw.shape\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows √ó \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_cols\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: ‚ùå Unsupported file format for RAW_DATA: "
     ]
    }
   ],
   "source": [
    "# 1.4.1 Load Raw Dataset (CSV: Config-Driven, with READ_OPTS) / (PARQUET: )\n",
    "print(\"1.4 üß© DATASET LOAD & VERSION VERIFICATION\")\n",
    "print(f\"1.4.1) üì• Loading raw dataset from: {RAW_DATA}\")\n",
    "#FIXME: 1113_1013\n",
    "\n",
    "if not RAW_DATA.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå RAW_DATA file not found at {RAW_DATA}\")\n",
    "\n",
    "# Pull CSV read options from config (READ_OPTS is optional)\n",
    "READ_OPTS = C(\"READ_OPTS\", {}) or {}\n",
    "\n",
    "suffix = RAW_DATA.suffix.lower()\n",
    "if suffix == \".csv\":\n",
    "    print(f\"   Using READ_OPTS from config: {READ_OPTS}\")\n",
    "    df_raw = pd.read_csv(RAW_DATA, **READ_OPTS)\n",
    "elif suffix in {\".parquet\", \".pq\"}:\n",
    "    df_raw = pd.read_parquet(RAW_DATA)\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå Unsupported file format for RAW_DATA: {suffix}\")\n",
    "\n",
    "n_rows, n_cols = df_raw.shape\n",
    "print(f\"‚úÖ Loaded dataset: {n_rows:,} rows √ó {n_cols:,} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4402b83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.4.2) üîê Hash / snapshot validation\n",
      "‚ÑπÔ∏è Existing dataset version found ‚Üí version_id=1\n",
      "‚úÖ Version registry updated ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts/registry/dataset_version_registry.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.4.2 Hash / Snapshot Validation\n",
    "print(\"\\n1.4.2) üîê Hash / snapshot validation\")\n",
    "\n",
    "def compute_hash(path, algo: str = \"sha256\", chunk_size: int = 1 << 20) -> str:\n",
    "    \"\"\"Streaming file hash to avoid loading whole file into memory.\"\"\"\n",
    "    h = hashlib.new(algo)\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "file_hash = compute_hash(RAW_DATA, \"sha256\")\n",
    "now_utc = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "registry_path = ARTIFACTS_DIR / \"dataset_version_registry.csv\"\n",
    "load_log_path = ARTIFACTS_DIR / \"dataset_load_log.csv\"\n",
    "\n",
    "# Ensure artifacts directory exists so atomic writes (tmp + replace) don't fail\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGISTRY_DIR = ARTIFACTS_DIR / \"registry\"\n",
    "REGISTRY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "registry_path = REGISTRY_DIR / \"dataset_version_registry.csv\"\n",
    "\n",
    "# Load dataset version registry (file_hash ‚Üí version_id)\n",
    "# ; initialize empty registry if it doesn't exist yet\n",
    "# Load existing dataset version registry,\n",
    "# or start a new one if this is the first run\n",
    "if registry_path.exists():\n",
    "    registry = pd.read_csv(registry_path)\n",
    "else:\n",
    "    registry = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"version_id\",\n",
    "            \"dataset_path\",\n",
    "            \"file_hash\",\n",
    "            \"first_seen_utc\",\n",
    "            \"last_seen_utc\",\n",
    "            \"n_rows\",\n",
    "            \"n_cols\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "existing = registry.loc[registry[\"file_hash\"] == file_hash]\n",
    "\n",
    "if not existing.empty:\n",
    "    version_id = str(existing.iloc[0][\"version_id\"])\n",
    "    registry.loc[registry[\"file_hash\"] == file_hash, \"last_seen_utc\"] = now_utc\n",
    "    print(f\"‚ÑπÔ∏è Existing dataset version found ‚Üí version_id={version_id}\")\n",
    "else:\n",
    "    if registry.empty:\n",
    "        next_id = 1\n",
    "    else:\n",
    "        max_existing = pd.to_numeric(registry[\"version_id\"], errors=\"coerce\").fillna(0).max()\n",
    "        next_id = int(max_existing) + 1\n",
    "\n",
    "    version_id = str(next_id)\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"version_id\": [version_id],\n",
    "            \"dataset_path\": [str(RAW_DATA)],\n",
    "            \"file_hash\": [file_hash],\n",
    "            \"first_seen_utc\": [now_utc],\n",
    "            \"last_seen_utc\": [now_utc],\n",
    "            \"n_rows\": [n_rows],\n",
    "            \"n_cols\": [n_cols],\n",
    "        }\n",
    "    )\n",
    "    registry = pd.concat([registry, new_row], ignore_index=True)\n",
    "    print(f\"üÜï New dataset version registered ‚Üí version_id={version_id}\")\n",
    "\n",
    "tmp_reg = registry_path.with_suffix(\".tmp\")\n",
    "registry.to_csv(tmp_reg, index=False)\n",
    "os.replace(tmp_reg, registry_path)\n",
    "print(f\"‚úÖ Version registry updated ‚Üí {registry_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b0bc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.4.3) üßæ Appending load metadata to dataset_load_log.csv\n",
      "‚úÖ Load log updated ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts/dataset_load_log.csv\n",
      "üìä Dataset hash: 88be4b93fbe0... | version_id: 1\n"
     ]
    }
   ],
   "source": [
    "# 1.4.3 Schema Version Logging\n",
    "\n",
    "print(\"\\n1.4.3) üßæ Appending load metadata to dataset_load_log.csv\")\n",
    "\n",
    "log_row = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp_utc\": [now_utc],\n",
    "        \"dataset_path\": [str(RAW_DATA)],\n",
    "        \"version_id\": [version_id],\n",
    "        \"file_hash\": [file_hash],\n",
    "        \"n_rows\": [n_rows],\n",
    "        \"n_cols\": [n_cols],\n",
    "        \"read_opts\": [str(READ_OPTS)],\n",
    "    }\n",
    ")\n",
    "\n",
    "if load_log_path.exists():\n",
    "    existing_log = pd.read_csv(load_log_path)\n",
    "    all_cols = pd.Index(existing_log.columns).union(log_row.columns)\n",
    "    out_log = pd.concat(\n",
    "        [existing_log.reindex(columns=all_cols), log_row.reindex(columns=all_cols)],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "else:\n",
    "    out_log = log_row\n",
    "\n",
    "tmp_log = load_log_path.with_suffix(\".tmp\")\n",
    "out_log.to_csv(tmp_log, index=False)\n",
    "os.replace(tmp_log, load_log_path)\n",
    "\n",
    "print(f\"‚úÖ Load log updated ‚Üí {load_log_path}\")\n",
    "print(f\"üìä Dataset hash: {file_hash[:12]}... | version_id: {version_id}\")\n",
    "\n",
    "# df_raw is your canonical raw DataFrame for 1.6+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4097a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created deep working copy 'df' from 'df_raw'\n",
      "   Memory usage: 6.82 MB\n",
      "   Shape: 7,043 rows √ó 21 columns\n"
     ]
    }
   ],
   "source": [
    "# 1.4.4 Defensive Copy for Processing\n",
    "df = df_raw.copy(deep=True)\n",
    "print(\"\\n‚úÖ Created deep working copy 'df' from 'df_raw'\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]:,} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db131c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.4.5 metadata snapshot\n",
    "\n",
    "raw_info_path = ARTIFACTS_DIR / \"df_raw_info.txt\"\n",
    "working_info_path = ARTIFACTS_DIR / \"df_working_info.txt\"\n",
    "\n",
    "with raw_info_path.open(\"w\") as f:\n",
    "    df_raw.info(buf=f)\n",
    "\n",
    "with working_info_path.open(\"w\") as f:\n",
    "    df.info(buf=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d6b1c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:3px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.5.1-1.5.3 | PART A üîç Quick Structural Validation (Pre-DQ Sanity)\n",
    "</summary>\n",
    "\n",
    "**GOAL:**\n",
    "\n",
    "\n",
    ">### **1.5.1 Verify Non-Empty Dataset**\n",
    "- Assert df.shape > (0,0).\n",
    "\n",
    "- Assert that df exists and df.shape has:\n",
    "    - n_rows > 0\n",
    "    - n_cols > 0\n",
    "      - If empty, raise a clear error and stop Section 1.\n",
    "\n",
    "Ensures downstream DQ logic never runs on an empty or malformed dataset.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.5.2 Basic Column-Type Introspection**\n",
    "\n",
    "**Preview:**\n",
    "- df.dtypes\n",
    "- non-null / null counts per column\n",
    "- number of unique values per column\n",
    "\n",
    "* **Save a lightweight schema snapshot for auditability.**\n",
    "- **Output:** initial_schema_summary.csv under REPORTS_DIR.\n",
    "- This becomes the baseline view for later type/coercion decisions.\n",
    "\n",
    "---\n",
    "\n",
    ">### **1.5.3 Target Variable Registration**\n",
    "\n",
    "**Normalize the raw churn label:**\n",
    "- Map ‚ÄúYes‚Äù/‚ÄúNo‚Äù ‚Üí 1/0 (or config-driven labels)to canonical text; create numeric `Churn_flag`\n",
    "\n",
    "**Ensure:**\n",
    "- Churn_flag exists\n",
    "- dtype is an efficient integer type (e.g. Int8)\n",
    "- consistency for modeling (Section 3).\n",
    "\n",
    "This guarantees the target is consistent and model-ready for Section 3.\n",
    "\n",
    "**Section Output:** initial_schema_summary.csv (+- target sanity prints)\n",
    "\n",
    "üü© Hiring value: validate structure before deeper analysis ‚Äî a quality-first, production-minded habit. Shows quality-first discipline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb02128",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #e74f29ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "TODO: Short answer: **B is the better 1.6.2** for your Telco ‚Äúhiring-ready‚Äù story\n",
    "</summary>\n",
    "\n",
    "‚Äî it‚Äôs section-aware, config-aware, has emojis/type_groups, exports CSVs, and carries metadata.\n",
    "\n",
    "But **A has really useful per-column metrics** (non_null, nulls, null_pct, n_unique) and a nice simple `initial_schema_df` that you *don‚Äôt* want to lose.\n",
    "\n",
    "So the move is:\n",
    "\n",
    "> ‚úÖ Keep **B as the canonical 1.6.2**,\n",
    "> ‚úÖ **Merge A‚Äôs metrics and `initial_schema_df` into B**,\n",
    "> ‚ùå Delete the old A block and the duplicate ‚Äúinitial_schema summary‚Äù block at the bottom.\n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs good in each\n",
    "\n",
    "### A (simple 1.6.2)\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "* Straightforward per-column stats:\n",
    "\n",
    "  * `non_null`, `nulls`, `null_pct`, `n_unique`\n",
    "* A clean `initial_schema_df` that‚Äôs perfect for:\n",
    "\n",
    "  * sanity checks\n",
    "  * downstream comparisons (before/after dtype changes)\n",
    "* Simple, readable `display(initial_schema_df.head(10))`\n",
    "\n",
    "**Weaknesses:**\n",
    "\n",
    "* No section metadata (`SECTION_ID`, `run_ts`, etc.).\n",
    "* No ‚Äútype_group‚Äù (numeric / string_like / etc.).\n",
    "* No export to a canonical CSV path.\n",
    "* No emoji/color groupings.\n",
    "\n",
    "---\n",
    "\n",
    "### B (Telco-ready 1.6.2)\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "* Section metadata: `SECTION_ID`, `SECTION_NAME`, `run_ts`, `n_rows`, `n_cols`.\n",
    "* `type_group` (numeric/boolean/datetime/categorical/string_like).\n",
    "* Emoji + background color per type.\n",
    "* Wide ‚Äúone row per dtype‚Äù view with columns horizontally.\n",
    "* Writes a **schema CSV** for the pipeline.\n",
    "\n",
    "**Weaknesses:**\n",
    "\n",
    "* It **does not compute** `non_null`, `nulls`, `null_pct`, `n_unique`.\n",
    "* It references `initial_schema_df` later, but never actually builds it (it still depends on A).\n",
    "* It re-resolves `PROJECT_ROOT` even though you already have it from 1.2 (not wrong, just redundant for this notebook).\n",
    "* You have **two different blocks** that try to persist `initial_schema_summary.csv` (one via `REPORTS_DIR`, one via `SECTION1_REPORT_DIR`).\n",
    "\n",
    "---\n",
    "\n",
    "## Suggested refactor: ‚ÄúB + A‚Äôs brains‚Äù\n",
    "\n",
    "Here‚Äôs the structural plan:\n",
    "\n",
    "1. **Use B as the base** for 1.6.2.\n",
    "2. While building `schema_df`, **add A‚Äôs metrics**:\n",
    "\n",
    "   * `non_null`, `nulls`, `null_pct`, `n_unique`\n",
    "3. Build `initial_schema_df` from `schema_df`.\n",
    "4. Use **one consistent report directory**:\n",
    "\n",
    "   ```python\n",
    "   SECTION1_REPORT_DIR = REPORTS_DIR / \"section1\"\n",
    "   ```\n",
    "\n",
    "   (since `REPORTS_DIR` is already config-driven).\n",
    "5. Save:\n",
    "\n",
    "   * `section1_6_2_schema.csv` (full, with type_group & metadata)\n",
    "   * `initial_schema_summary.csv` (column-level metrics table)\n",
    "6. Delete:\n",
    "\n",
    "   * The old A block.\n",
    "   * The duplicate `initial_schema_df` persistence block at the bottom.\n",
    "\n",
    "---\n",
    "\n",
    "## Example merged 1.6.2 (sketch)\n",
    "\n",
    "You don‚Äôt have to paste this verbatim, but this is the shape I‚Äôd aim for:\n",
    "\n",
    "```python\n",
    "# 1.6.2 üß± Basic column-type introspection (merged A + B)\n",
    "\n",
    "print(\"\\n1.6.2) üß± Basic column-type introspection\")\n",
    "\n",
    "SECTION_ID   = \"1.6.2\"\n",
    "SECTION_NAME = \"Basic column-type introspection\"\n",
    "\n",
    "# n_rows / n_cols from df\n",
    "n_rows, n_cols = df.shape\n",
    "run_ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Use config-driven REPORTS_DIR from earlier and create section1 subfolder\n",
    "SECTION1_REPORT_DIR = REPORTS_DIR / \"section1\"\n",
    "SECTION1_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_PATH = SECTION1_REPORT_DIR / \"section1_6_2_schema.csv\"\n",
    "\n",
    "# 1) Helper (define once somewhere global if you prefer)\n",
    "def classify_dtype(dtype_str: str) -> str:\n",
    "    s = dtype_str.lower()\n",
    "    if \"int\" in s or \"float\" in s:\n",
    "        return \"numeric\"\n",
    "    if \"bool\" in s:\n",
    "        return \"boolean\"\n",
    "    if \"datetime\" in s or \"date\" in s:\n",
    "        return \"datetime\"\n",
    "    if \"category\" in s:\n",
    "        return \"categorical\"\n",
    "    return \"string_like\"\n",
    "\n",
    "TYPE_EMOJI = {\n",
    "    \"numeric\":      \"üî¢\",\n",
    "    \"boolean\":      \"üîò\",\n",
    "    \"datetime\":     \"‚è±Ô∏è\",\n",
    "    \"categorical\":  \"üè∑Ô∏è\",\n",
    "    \"string_like\":  \"üî§\",\n",
    "}\n",
    "TYPE_COLOR = {\n",
    "    \"numeric\":      \"#c7ffd4\",\n",
    "    \"boolean\":      \"#e4ffe9\",\n",
    "    \"datetime\":     \"#ffd6f6\",\n",
    "    \"categorical\":  \"#fff7ae\",\n",
    "    \"string_like\":  \"#ffe4d1\",\n",
    "}\n",
    "\n",
    "# 2) Build long schema table with BOTH A + B info\n",
    "rows = []\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    dtype_str = str(s.dtype)\n",
    "    type_group = classify_dtype(dtype_str)\n",
    "\n",
    "    non_null = int(s.notna().sum())\n",
    "    nulls = int(s.isna().sum())\n",
    "    n_unique = int(s.nunique(dropna=True))\n",
    "    null_pct = round(nulls / n_rows * 100, 3) if n_rows else 0.0\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"column\":      col,\n",
    "            \"dtype\":       dtype_str,\n",
    "            \"type_group\":  type_group,\n",
    "            \"emoji\":       TYPE_EMOJI.get(type_group, \"‚ùì\"),\n",
    "            \"bg_color\":    TYPE_COLOR.get(type_group, \"#f2f2f2\"),\n",
    "            \"non_null\":    non_null,\n",
    "            \"nulls\":       nulls,\n",
    "            \"null_pct\":    null_pct,\n",
    "            \"n_unique\":    n_unique,\n",
    "            \"section_id\":   SECTION_ID,\n",
    "            \"section_name\": SECTION_NAME,\n",
    "            \"run_ts\":       run_ts,\n",
    "            \"n_rows\":       n_rows,\n",
    "            \"n_cols\":       n_cols,\n",
    "        }\n",
    "    )\n",
    "\n",
    "schema_df = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values([\"type_group\", \"dtype\", \"column\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nüìã dtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nüìä Column-level summary (head):\")\n",
    "display(schema_df[[\"column\", \"dtype\", \"type_group\", \"non_null\", \"nulls\", \"null_pct\", \"n_unique\"]].head(10))\n",
    "\n",
    "# 3) Wide view: one row per dtype, columns horizontally (from B)\n",
    "grouped = {}\n",
    "for _, r in schema_df.iterrows():\n",
    "    key = (r[\"dtype\"], r[\"type_group\"], r[\"emoji\"], r[\"bg_color\"])\n",
    "    grouped.setdefault(key, []).append(r[\"column\"])\n",
    "\n",
    "rows_wide = []\n",
    "for (dtype_str, type_group, emoji, bg_color), cols in grouped.items():\n",
    "    row = {\n",
    "        \"dtype\":      dtype_str,\n",
    "        \"type_group\": type_group,\n",
    "        \"emoji\":      emoji,\n",
    "        \"bg_color\":   bg_color,\n",
    "    }\n",
    "    for i, colname in enumerate(cols, start=1):\n",
    "        row[f\"col_{i}\"] = colname\n",
    "    rows_wide.append(row)\n",
    "\n",
    "wide_df = (\n",
    "    pd.DataFrame(rows_wide)\n",
    "    .sort_values([\"type_group\", \"dtype\"])\n",
    "    .reset_index(drop=True)\n",
    "    .fillna(\"\")\n",
    ")\n",
    "\n",
    "def row_bg_colors(row):\n",
    "    color = row[\"bg_color\"]\n",
    "    return [f\"background-color: {color}\" for _ in row]\n",
    "\n",
    "styled_wide = (\n",
    "    wide_df\n",
    "    .style\n",
    "    .apply(row_bg_colors, axis=1)\n",
    "    .hide(axis=\"columns\", subset=[\"bg_color\"])\n",
    ")\n",
    "display(styled_wide)\n",
    "\n",
    "# 4) Export for pipeline\n",
    "export_cols = [\n",
    "    \"column\",\n",
    "    \"dtype\",\n",
    "    \"type_group\",\n",
    "    \"emoji\",\n",
    "    \"section_id\",\n",
    "    \"section_name\",\n",
    "    \"run_ts\",\n",
    "    \"n_rows\",\n",
    "    \"n_cols\",\n",
    "    \"non_null\",\n",
    "    \"nulls\",\n",
    "    \"null_pct\",\n",
    "    \"n_unique\",\n",
    "]\n",
    "\n",
    "schema_df[export_cols].to_csv(CSV_PATH, index=False)\n",
    "print(f\"\\nüíæ Exported schema summary to:\\n   {CSV_PATH}\")\n",
    "print(f\"   (columns={n_cols}, rows={n_rows}, run_ts={run_ts}, section={SECTION_ID})\")\n",
    "\n",
    "# 5) initial_schema_df for downstream summaries (what A was doing)\n",
    "initial_schema_df = schema_df[\n",
    "    [\"column\", \"dtype\", \"type_group\", \"non_null\", \"nulls\", \"null_pct\", \"n_unique\"]\n",
    "].copy()\n",
    "\n",
    "initial_schema_df.insert(0, \"n_rows\", n_rows)\n",
    "initial_schema_df.insert(1, \"n_cols\", n_cols)\n",
    "\n",
    "initial_schema_path = SECTION1_REPORT_DIR / \"initial_schema_summary.csv\"\n",
    "initial_schema_df.to_csv(initial_schema_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ 1.6 initial schema summary saved ‚Üí {initial_schema_path}\")\n",
    "```\n",
    "\n",
    "Then you can safely:\n",
    "\n",
    "* **Delete** the earlier simple A version.\n",
    "* **Delete** the second duplicate `initial_schema_df` block that uses `SECTION1_REPORT_DIR` separately.\n",
    "* Keep 1.6.3+ as-is (they will now see a properly defined `initial_schema_df` if they care, and the pipeline will have both CSVs).\n",
    "\n",
    "If you want, next we can go through **1.6.5** and make sure it leans on the *same* `classify_dtype` + type_group logic (single source of truth for type semantics across Section 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f246f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.1) ‚úÖ Verify non-empty dataset\n",
      "   Shape: 7,043 rows √ó 21 columns\n",
      "‚úÖ Dataset is non-empty and ready for structural checks.\n"
     ]
    }
   ],
   "source": [
    "# 1.5.1 Verify Non-Empty Dataset\n",
    "\n",
    "RAISE_ON_CRITICAL = bool(C(\"FLAGS.RAISE_ON_CRITICAL\", True))\n",
    "\n",
    "print(\"\\n1.5.1) ‚úÖ Verify non-empty dataset\")\n",
    "\n",
    "if \"df\" not in globals():\n",
    "    raise NameError(\"‚ùå Working DataFrame 'df' is not defined. Run Section 1.4/1.5 first.\")\n",
    "\n",
    "n_rows, n_cols = df.shape\n",
    "print(f\"   Shape: {n_rows:,} rows √ó {n_cols:,} columns\")\n",
    "\n",
    "if n_rows == 0 or n_cols == 0:\n",
    "    msg = f\"‚ùå Dataset is empty: shape={df.shape}\"\n",
    "    if RAISE_ON_CRITICAL:\n",
    "        raise ValueError(msg)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è \" + msg)\n",
    "else:\n",
    "    print(\"‚úÖ Dataset is non-empty and ready for structural checks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87f98e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.2) üß± Basic column-type introspection\n",
      "\n",
      "üìã dtypes:\n",
      "customerID           object\n",
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner              object\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "MultipleLines        object\n",
      "InternetService      object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "PaperlessBilling     object\n",
      "PaymentMethod        object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges        float64\n",
      "Churn                object\n",
      "dtype: object\n",
      "\n",
      "üìä Column-level summary (head):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>type_group</th>\n",
       "      <th>non_null</th>\n",
       "      <th>nulls</th>\n",
       "      <th>null_pct</th>\n",
       "      <th>n_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MonthlyCharges</td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TotalCharges</td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>7032</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>6530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SeniorCitizen</td>\n",
       "      <td>int64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tenure</td>\n",
       "      <td>int64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Churn</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Contract</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dependents</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DeviceProtection</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>InternetService</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MultipleLines</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OnlineBackup</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OnlineSecurity</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PaperlessBilling</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Partner</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PaymentMethod</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PhoneService</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StreamingMovies</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StreamingTV</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TechSupport</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>customerID</td>\n",
       "      <td>object</td>\n",
       "      <td>string_like</td>\n",
       "      <td>7043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column    dtype   type_group  non_null  nulls  null_pct  \\\n",
       "0     MonthlyCharges  float64      numeric      7043      0    0.0000   \n",
       "1       TotalCharges  float64      numeric      7032     11    0.1560   \n",
       "2      SeniorCitizen    int64      numeric      7043      0    0.0000   \n",
       "3             tenure    int64      numeric      7043      0    0.0000   \n",
       "4              Churn   object  string_like      7043      0    0.0000   \n",
       "5           Contract   object  string_like      7043      0    0.0000   \n",
       "6         Dependents   object  string_like      7043      0    0.0000   \n",
       "7   DeviceProtection   object  string_like      7043      0    0.0000   \n",
       "8    InternetService   object  string_like      7043      0    0.0000   \n",
       "9      MultipleLines   object  string_like      7043      0    0.0000   \n",
       "10      OnlineBackup   object  string_like      7043      0    0.0000   \n",
       "11    OnlineSecurity   object  string_like      7043      0    0.0000   \n",
       "12  PaperlessBilling   object  string_like      7043      0    0.0000   \n",
       "13           Partner   object  string_like      7043      0    0.0000   \n",
       "14     PaymentMethod   object  string_like      7043      0    0.0000   \n",
       "15      PhoneService   object  string_like      7043      0    0.0000   \n",
       "16   StreamingMovies   object  string_like      7043      0    0.0000   \n",
       "17       StreamingTV   object  string_like      7043      0    0.0000   \n",
       "18       TechSupport   object  string_like      7043      0    0.0000   \n",
       "19        customerID   object  string_like      7043      0    0.0000   \n",
       "\n",
       "    n_unique  \n",
       "0       1585  \n",
       "1       6530  \n",
       "2          2  \n",
       "3         73  \n",
       "4          2  \n",
       "5          3  \n",
       "6          2  \n",
       "7          3  \n",
       "8          3  \n",
       "9          3  \n",
       "10         3  \n",
       "11         3  \n",
       "12         2  \n",
       "13         2  \n",
       "14         4  \n",
       "15         2  \n",
       "16         3  \n",
       "17         3  \n",
       "18         3  \n",
       "19      7043  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_800b7_row0_col0, #T_800b7_row0_col1, #T_800b7_row0_col2, #T_800b7_row0_col4, #T_800b7_row0_col5, #T_800b7_row0_col6, #T_800b7_row0_col7, #T_800b7_row0_col8, #T_800b7_row0_col9, #T_800b7_row0_col10, #T_800b7_row0_col11, #T_800b7_row0_col12, #T_800b7_row0_col13, #T_800b7_row0_col14, #T_800b7_row0_col15, #T_800b7_row0_col16, #T_800b7_row0_col17, #T_800b7_row0_col18, #T_800b7_row0_col19, #T_800b7_row0_col20, #T_800b7_row1_col0, #T_800b7_row1_col1, #T_800b7_row1_col2, #T_800b7_row1_col4, #T_800b7_row1_col5, #T_800b7_row1_col6, #T_800b7_row1_col7, #T_800b7_row1_col8, #T_800b7_row1_col9, #T_800b7_row1_col10, #T_800b7_row1_col11, #T_800b7_row1_col12, #T_800b7_row1_col13, #T_800b7_row1_col14, #T_800b7_row1_col15, #T_800b7_row1_col16, #T_800b7_row1_col17, #T_800b7_row1_col18, #T_800b7_row1_col19, #T_800b7_row1_col20 {\n",
       "  background-color: #c7ffd4;\n",
       "}\n",
       "#T_800b7_row2_col0, #T_800b7_row2_col1, #T_800b7_row2_col2, #T_800b7_row2_col4, #T_800b7_row2_col5, #T_800b7_row2_col6, #T_800b7_row2_col7, #T_800b7_row2_col8, #T_800b7_row2_col9, #T_800b7_row2_col10, #T_800b7_row2_col11, #T_800b7_row2_col12, #T_800b7_row2_col13, #T_800b7_row2_col14, #T_800b7_row2_col15, #T_800b7_row2_col16, #T_800b7_row2_col17, #T_800b7_row2_col18, #T_800b7_row2_col19, #T_800b7_row2_col20 {\n",
       "  background-color: #ffe4d1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_800b7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_800b7_level0_col0\" class=\"col_heading level0 col0\" >dtype</th>\n",
       "      <th id=\"T_800b7_level0_col1\" class=\"col_heading level0 col1\" >type_group</th>\n",
       "      <th id=\"T_800b7_level0_col2\" class=\"col_heading level0 col2\" >emoji</th>\n",
       "      <th id=\"T_800b7_level0_col4\" class=\"col_heading level0 col4\" >col_1</th>\n",
       "      <th id=\"T_800b7_level0_col5\" class=\"col_heading level0 col5\" >col_2</th>\n",
       "      <th id=\"T_800b7_level0_col6\" class=\"col_heading level0 col6\" >col_3</th>\n",
       "      <th id=\"T_800b7_level0_col7\" class=\"col_heading level0 col7\" >col_4</th>\n",
       "      <th id=\"T_800b7_level0_col8\" class=\"col_heading level0 col8\" >col_5</th>\n",
       "      <th id=\"T_800b7_level0_col9\" class=\"col_heading level0 col9\" >col_6</th>\n",
       "      <th id=\"T_800b7_level0_col10\" class=\"col_heading level0 col10\" >col_7</th>\n",
       "      <th id=\"T_800b7_level0_col11\" class=\"col_heading level0 col11\" >col_8</th>\n",
       "      <th id=\"T_800b7_level0_col12\" class=\"col_heading level0 col12\" >col_9</th>\n",
       "      <th id=\"T_800b7_level0_col13\" class=\"col_heading level0 col13\" >col_10</th>\n",
       "      <th id=\"T_800b7_level0_col14\" class=\"col_heading level0 col14\" >col_11</th>\n",
       "      <th id=\"T_800b7_level0_col15\" class=\"col_heading level0 col15\" >col_12</th>\n",
       "      <th id=\"T_800b7_level0_col16\" class=\"col_heading level0 col16\" >col_13</th>\n",
       "      <th id=\"T_800b7_level0_col17\" class=\"col_heading level0 col17\" >col_14</th>\n",
       "      <th id=\"T_800b7_level0_col18\" class=\"col_heading level0 col18\" >col_15</th>\n",
       "      <th id=\"T_800b7_level0_col19\" class=\"col_heading level0 col19\" >col_16</th>\n",
       "      <th id=\"T_800b7_level0_col20\" class=\"col_heading level0 col20\" >col_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_800b7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_800b7_row0_col0\" class=\"data row0 col0\" >float64</td>\n",
       "      <td id=\"T_800b7_row0_col1\" class=\"data row0 col1\" >numeric</td>\n",
       "      <td id=\"T_800b7_row0_col2\" class=\"data row0 col2\" >üî¢</td>\n",
       "      <td id=\"T_800b7_row0_col4\" class=\"data row0 col4\" >MonthlyCharges</td>\n",
       "      <td id=\"T_800b7_row0_col5\" class=\"data row0 col5\" >TotalCharges</td>\n",
       "      <td id=\"T_800b7_row0_col6\" class=\"data row0 col6\" ></td>\n",
       "      <td id=\"T_800b7_row0_col7\" class=\"data row0 col7\" ></td>\n",
       "      <td id=\"T_800b7_row0_col8\" class=\"data row0 col8\" ></td>\n",
       "      <td id=\"T_800b7_row0_col9\" class=\"data row0 col9\" ></td>\n",
       "      <td id=\"T_800b7_row0_col10\" class=\"data row0 col10\" ></td>\n",
       "      <td id=\"T_800b7_row0_col11\" class=\"data row0 col11\" ></td>\n",
       "      <td id=\"T_800b7_row0_col12\" class=\"data row0 col12\" ></td>\n",
       "      <td id=\"T_800b7_row0_col13\" class=\"data row0 col13\" ></td>\n",
       "      <td id=\"T_800b7_row0_col14\" class=\"data row0 col14\" ></td>\n",
       "      <td id=\"T_800b7_row0_col15\" class=\"data row0 col15\" ></td>\n",
       "      <td id=\"T_800b7_row0_col16\" class=\"data row0 col16\" ></td>\n",
       "      <td id=\"T_800b7_row0_col17\" class=\"data row0 col17\" ></td>\n",
       "      <td id=\"T_800b7_row0_col18\" class=\"data row0 col18\" ></td>\n",
       "      <td id=\"T_800b7_row0_col19\" class=\"data row0 col19\" ></td>\n",
       "      <td id=\"T_800b7_row0_col20\" class=\"data row0 col20\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_800b7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_800b7_row1_col0\" class=\"data row1 col0\" >int64</td>\n",
       "      <td id=\"T_800b7_row1_col1\" class=\"data row1 col1\" >numeric</td>\n",
       "      <td id=\"T_800b7_row1_col2\" class=\"data row1 col2\" >üî¢</td>\n",
       "      <td id=\"T_800b7_row1_col4\" class=\"data row1 col4\" >SeniorCitizen</td>\n",
       "      <td id=\"T_800b7_row1_col5\" class=\"data row1 col5\" >tenure</td>\n",
       "      <td id=\"T_800b7_row1_col6\" class=\"data row1 col6\" ></td>\n",
       "      <td id=\"T_800b7_row1_col7\" class=\"data row1 col7\" ></td>\n",
       "      <td id=\"T_800b7_row1_col8\" class=\"data row1 col8\" ></td>\n",
       "      <td id=\"T_800b7_row1_col9\" class=\"data row1 col9\" ></td>\n",
       "      <td id=\"T_800b7_row1_col10\" class=\"data row1 col10\" ></td>\n",
       "      <td id=\"T_800b7_row1_col11\" class=\"data row1 col11\" ></td>\n",
       "      <td id=\"T_800b7_row1_col12\" class=\"data row1 col12\" ></td>\n",
       "      <td id=\"T_800b7_row1_col13\" class=\"data row1 col13\" ></td>\n",
       "      <td id=\"T_800b7_row1_col14\" class=\"data row1 col14\" ></td>\n",
       "      <td id=\"T_800b7_row1_col15\" class=\"data row1 col15\" ></td>\n",
       "      <td id=\"T_800b7_row1_col16\" class=\"data row1 col16\" ></td>\n",
       "      <td id=\"T_800b7_row1_col17\" class=\"data row1 col17\" ></td>\n",
       "      <td id=\"T_800b7_row1_col18\" class=\"data row1 col18\" ></td>\n",
       "      <td id=\"T_800b7_row1_col19\" class=\"data row1 col19\" ></td>\n",
       "      <td id=\"T_800b7_row1_col20\" class=\"data row1 col20\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_800b7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_800b7_row2_col0\" class=\"data row2 col0\" >object</td>\n",
       "      <td id=\"T_800b7_row2_col1\" class=\"data row2 col1\" >string_like</td>\n",
       "      <td id=\"T_800b7_row2_col2\" class=\"data row2 col2\" >üî§</td>\n",
       "      <td id=\"T_800b7_row2_col4\" class=\"data row2 col4\" >Churn</td>\n",
       "      <td id=\"T_800b7_row2_col5\" class=\"data row2 col5\" >Contract</td>\n",
       "      <td id=\"T_800b7_row2_col6\" class=\"data row2 col6\" >Dependents</td>\n",
       "      <td id=\"T_800b7_row2_col7\" class=\"data row2 col7\" >DeviceProtection</td>\n",
       "      <td id=\"T_800b7_row2_col8\" class=\"data row2 col8\" >InternetService</td>\n",
       "      <td id=\"T_800b7_row2_col9\" class=\"data row2 col9\" >MultipleLines</td>\n",
       "      <td id=\"T_800b7_row2_col10\" class=\"data row2 col10\" >OnlineBackup</td>\n",
       "      <td id=\"T_800b7_row2_col11\" class=\"data row2 col11\" >OnlineSecurity</td>\n",
       "      <td id=\"T_800b7_row2_col12\" class=\"data row2 col12\" >PaperlessBilling</td>\n",
       "      <td id=\"T_800b7_row2_col13\" class=\"data row2 col13\" >Partner</td>\n",
       "      <td id=\"T_800b7_row2_col14\" class=\"data row2 col14\" >PaymentMethod</td>\n",
       "      <td id=\"T_800b7_row2_col15\" class=\"data row2 col15\" >PhoneService</td>\n",
       "      <td id=\"T_800b7_row2_col16\" class=\"data row2 col16\" >StreamingMovies</td>\n",
       "      <td id=\"T_800b7_row2_col17\" class=\"data row2 col17\" >StreamingTV</td>\n",
       "      <td id=\"T_800b7_row2_col18\" class=\"data row2 col18\" >TechSupport</td>\n",
       "      <td id=\"T_800b7_row2_col19\" class=\"data row2 col19\" >customerID</td>\n",
       "      <td id=\"T_800b7_row2_col20\" class=\"data row2 col20\" >gender</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13a99e840>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Exported schema summary to:\n",
      "   /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section1/section1_5_2_schema.csv\n",
      "   (columns=21, rows=7043, run_ts=2025-11-20 13:59:12, section=1.5.2)\n",
      "\n",
      "‚úÖ 1.5 initial schema summary saved ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section1/initial_schema_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.5.2 Basic column-type introspection (merged A + B)\n",
    "\n",
    "print(\"\\n1.5.2) üß± Basic column-type introspection\")\n",
    "\n",
    "SECTION_ID   = \"1.5.2\"\n",
    "SECTION_NAME = \"Basic column-type introspection\"\n",
    "\n",
    "# 0) quick dtypes preview\n",
    "print(\"\\nüìã dtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 1) Env & metadata | n_rows / n_cols from df\n",
    "n_rows, n_cols = df.shape\n",
    "run_ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Use config-driven REPORTS_DIR from earlier and create section1 subfolder\n",
    "SECTION1_REPORT_DIR = REPORTS_DIR / \"section1\"\n",
    "SECTION1_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_PATH = SECTION1_REPORT_DIR / \"section1_5_2_schema.csv\"\n",
    "\n",
    "# 2) Helper (define once somewhere global if you prefer)\n",
    "def classify_dtype(dtype_str: str) -> str:\n",
    "    s = dtype_str.lower()\n",
    "    if \"int\" in s or \"float\" in s:\n",
    "        return \"numeric\"\n",
    "    if \"bool\" in s:\n",
    "        return \"boolean\"\n",
    "    if \"datetime\" in s or \"date\" in s:\n",
    "        return \"datetime\"\n",
    "    if \"category\" in s:\n",
    "        return \"categorical\"\n",
    "    return \"string_like\"\n",
    "\n",
    "TYPE_EMOJI = {\n",
    "    \"numeric\":      \"üî¢\",\n",
    "    \"boolean\":      \"üîò\",\n",
    "    \"datetime\":     \"‚è±Ô∏è\",\n",
    "    \"categorical\":  \"üè∑Ô∏è\",\n",
    "    \"string_like\":  \"üî§\",\n",
    "}\n",
    "TYPE_COLOR = {\n",
    "    \"numeric\":      \"#c7ffd4\",\n",
    "    \"boolean\":      \"#e4ffe9\",\n",
    "    \"datetime\":     \"#ffd6f6\",\n",
    "    \"categorical\":  \"#fff7ae\",\n",
    "    \"string_like\":  \"#ffe4d1\",\n",
    "}\n",
    "\n",
    "# --- 3) Long schema table with counts + type grouping\n",
    "# 3) Build long schema table with BOTH A + B info\n",
    "rows = []\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    dtype_str = str(s.dtype)\n",
    "    type_group = classify_dtype(dtype_str)\n",
    "\n",
    "    non_null = int(s.notna().sum())\n",
    "    nulls = int(s.isna().sum())\n",
    "    n_unique = int(s.nunique(dropna=True))\n",
    "    null_pct = round(nulls / n_rows * 100, 3) if n_rows else 0.0\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"column\":      col,\n",
    "            \"dtype\":       dtype_str,\n",
    "            \"type_group\":  type_group,\n",
    "            \"emoji\":       TYPE_EMOJI.get(type_group, \"‚ùì\"),\n",
    "            \"bg_color\":    TYPE_COLOR.get(type_group, \"#f2f2f2\"),\n",
    "            \"non_null\":    non_null,\n",
    "            \"nulls\":       nulls,\n",
    "            \"null_pct\":    null_pct,\n",
    "            \"n_unique\":    n_unique,\n",
    "            \"section_id\":   SECTION_ID,\n",
    "            \"section_name\": SECTION_NAME,\n",
    "            \"run_ts\":       run_ts,\n",
    "            \"n_rows\":       n_rows,\n",
    "            \"n_cols\":       n_cols,\n",
    "        }\n",
    "    )\n",
    "\n",
    "schema_df = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values([\"type_group\", \"dtype\", \"column\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Column-level summary (head):\")\n",
    "display(schema_df[[\"column\", \"dtype\", \"type_group\", \"non_null\", \"nulls\", \"null_pct\", \"n_unique\"]].head(20))\n",
    "\n",
    "# 4) Wide view: one row per dtype, columns horizontally (from B)\n",
    "grouped = {}\n",
    "for _, r in schema_df.iterrows():\n",
    "    key = (r[\"dtype\"], r[\"type_group\"], r[\"emoji\"], r[\"bg_color\"])\n",
    "    grouped.setdefault(key, []).append(r[\"column\"])\n",
    "\n",
    "rows_wide = []\n",
    "for (dtype_str, type_group, emoji, bg_color), cols in grouped.items():\n",
    "    row = {\n",
    "        \"dtype\":      dtype_str,\n",
    "        \"type_group\": type_group,\n",
    "        \"emoji\":      emoji,\n",
    "        \"bg_color\":   bg_color,\n",
    "    }\n",
    "    for i, colname in enumerate(cols, start=1):\n",
    "        row[f\"col_{i}\"] = colname\n",
    "    rows_wide.append(row)\n",
    "\n",
    "wide_df = (\n",
    "    pd.DataFrame(rows_wide)\n",
    "    .sort_values([\"type_group\", \"dtype\"])\n",
    "    .reset_index(drop=True)\n",
    "    .fillna(\"\")\n",
    ")\n",
    "\n",
    "def row_bg_colors(row):\n",
    "    color = row[\"bg_color\"]\n",
    "    return [f\"background-color: {color}\" for _ in row]\n",
    "\n",
    "styled_wide = (\n",
    "    wide_df\n",
    "    .style\n",
    "    .apply(row_bg_colors, axis=1)\n",
    "    .hide(axis=\"columns\", subset=[\"bg_color\"])\n",
    ")\n",
    "display(styled_wide)\n",
    "\n",
    "# 5) Export for pipeline\n",
    "export_cols = [\n",
    "    \"column\",\n",
    "    \"dtype\",\n",
    "    \"type_group\",\n",
    "    \"emoji\",\n",
    "    \"section_id\",\n",
    "    \"section_name\",\n",
    "    \"run_ts\",\n",
    "    \"n_rows\",\n",
    "    \"n_cols\",\n",
    "    \"non_null\",\n",
    "    \"nulls\",\n",
    "    \"null_pct\",\n",
    "    \"n_unique\",\n",
    "]\n",
    "\n",
    "schema_df[export_cols].to_csv(CSV_PATH, index=False)\n",
    "print(f\"\\nüíæ Exported schema summary to:\\n   {CSV_PATH}\")\n",
    "print(f\"   (columns={n_cols}, rows={n_rows}, run_ts={run_ts}, section={SECTION_ID})\")\n",
    "\n",
    "# 6) initial_schema_df for downstream summaries (what A was doing)\n",
    "initial_schema_df = schema_df[\n",
    "    [\"column\", \"dtype\", \"type_group\", \"non_null\", \"nulls\", \"null_pct\", \"n_unique\"]\n",
    "].copy()\n",
    "\n",
    "initial_schema_df.insert(0, \"n_rows\", n_rows)\n",
    "initial_schema_df.insert(1, \"n_cols\", n_cols)\n",
    "\n",
    "initial_schema_path = SECTION1_REPORT_DIR / \"initial_schema_summary.csv\"\n",
    "initial_schema_df.to_csv(initial_schema_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ 1.5 initial schema summary saved ‚Üí {initial_schema_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120cf19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.3) üéØ Target variable registration & normalization\n",
      "üéØ Target registered: RAW_TARGET='Churn' ‚Üí FLAG='Churn_flag'\n",
      "\n",
      "Churn value counts (normalized labels):\n",
      "Churn\n",
      "No     5174\n",
      "Yes    1869\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target flag distribution (0=negative, 1=positive):\n",
      "Churn_flag\n",
      "0    5174\n",
      "1    1869\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "üßæ Target field summary written ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section1/target_field_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.5.3 Target Variable Registration\n",
    "print(\"\\n1.5.3) üéØ Target variable registration & normalization\")\n",
    "\n",
    "# FIXME: where does this belong the best?\n",
    "#  Ensure we have a section1 reports subfolder\n",
    "SECTION1_REPORT_DIR = REPORTS_DIR / \"section1\"\n",
    "SECTION1_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "TARGET_NAME = C(\"TARGET.COLUMN\")\n",
    "RAW_TARGET  = C(\"TARGET.RAW_COLUMN\")\n",
    "POS_CLASS   = C(\"TARGET.POSITIVE_CLASS\", \"Yes\")\n",
    "NEG_CLASS   = C(\"TARGET.NEGATIVE_CLASS\", \"No\")\n",
    "\n",
    "if RAW_TARGET is None:\n",
    "    msg = \"‚ùå TARGET.RAW_COLUMN not defined in config.\"\n",
    "    if RAISE_ON_CRITICAL:\n",
    "        raise KeyError(msg)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è \" + msg)\n",
    "elif RAW_TARGET not in df.columns:\n",
    "    msg = f\"‚ùå Raw target column '{RAW_TARGET}' not found in df.\"\n",
    "    if RAISE_ON_CRITICAL:\n",
    "        raise KeyError(msg)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è \" + msg)\n",
    "else:\n",
    "    # normalize raw labels\n",
    "    raw_series = df[RAW_TARGET].astype(\"string\")\n",
    "    norm = raw_series.str.strip().str.casefold()\n",
    "\n",
    "    pos_norm = str(POS_CLASS).strip().casefold()\n",
    "    neg_norm = str(NEG_CLASS).strip().casefold()\n",
    "\n",
    "    # map normalized labels to canonical text\n",
    "    text_map = {\n",
    "        pos_norm: POS_CLASS,\n",
    "        neg_norm: NEG_CLASS,\n",
    "    }\n",
    "    df[RAW_TARGET] = norm.map(text_map).fillna(raw_series)\n",
    "    df[RAW_TARGET] = df[RAW_TARGET].astype(\"category\")\n",
    "\n",
    "    # create numeric flag (TARGET_NAME) = 1 for positive, 0 for negative\n",
    "    if TARGET_NAME is None:\n",
    "        TARGET_NAME = \"Churn_flag\"  # fallback name if config missing\n",
    "        print(f\"‚ÑπÔ∏è TARGET.COLUMN not set in config ‚Äî using default '{TARGET_NAME}'\")\n",
    "\n",
    "    num_map = {\n",
    "        neg_norm: 0,\n",
    "        pos_norm: 1,\n",
    "    }\n",
    "    df[TARGET_NAME] = norm.map(num_map).astype(\"Int8\")\n",
    "\n",
    "    print(f\"üéØ Target registered: RAW_TARGET='{RAW_TARGET}' ‚Üí FLAG='{TARGET_NAME}'\")\n",
    "    print(\"\\nChurn value counts (normalized labels):\")\n",
    "    print(df[RAW_TARGET].value_counts(dropna=False))\n",
    "\n",
    "    print(\"\\nTarget flag distribution (0=negative, 1=positive):\")\n",
    "    print(df[TARGET_NAME].value_counts(dropna=False))\n",
    "\n",
    "    # quick sanity: unmapped rows\n",
    "    unmapped_mask = df[TARGET_NAME].isna()\n",
    "    if unmapped_mask.any():\n",
    "        print(\"\\n‚ö†Ô∏è Some rows did not map to 0/1. Normalized raw values (top 10):\")\n",
    "        print(norm[unmapped_mask].value_counts(dropna=False).head(10))\n",
    "\n",
    "\n",
    " # üìäüìä Build a tiny summary artifact for Section 1 / later 2.2.5 validation\n",
    "    run_ts = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    tgt_summary_rows = []\n",
    "\n",
    "    vc_raw = df[RAW_TARGET].value_counts(dropna=False)\n",
    "    vc_flag = df[TARGET_NAME].value_counts(dropna=False)\n",
    "\n",
    "    for label, count in vc_raw.items():\n",
    "        tgt_summary_rows.append(\n",
    "            {\n",
    "                \"column\":          RAW_TARGET,\n",
    "                \"kind\":            \"raw_label\",\n",
    "                \"value\":           str(label),\n",
    "                \"count\":           int(count),\n",
    "                \"pct\":             float(count) / len(df) if len(df) else 0.0,\n",
    "                \"target_name\":     TARGET_NAME,\n",
    "                \"timestamp_utc\":   run_ts,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for flag_val, count in vc_flag.items():\n",
    "        tgt_summary_rows.append(\n",
    "            {\n",
    "                \"column\":          TARGET_NAME,\n",
    "                \"kind\":            \"flag\",\n",
    "                \"value\":           str(flag_val),\n",
    "                \"count\":           int(count),\n",
    "                \"pct\":             float(count) / len(df) if len(df) else 0.0,\n",
    "                \"target_name\":     TARGET_NAME,\n",
    "                \"timestamp_utc\":   run_ts,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    target_summary_df = pd.DataFrame(tgt_summary_rows)\n",
    "\n",
    "    # üíæüíæ Persist summary to Section 1 reports for downstream checks (2.2.5)\n",
    "    target_summary_path = SECTION1_REPORT_DIR / \"target_field_summary.csv\"\n",
    "    tmp_path = target_summary_path.with_suffix(\".tmp.csv\")\n",
    "\n",
    "    try:\n",
    "        target_summary_df.to_csv(tmp_path, index=False)\n",
    "        os.replace(tmp_path, target_summary_path)\n",
    "        print(f\"\\nüßæ Target field summary written ‚Üí {target_summary_path}\")\n",
    "    except Exception as e:\n",
    "        if tmp_path.exists():\n",
    "            try:\n",
    "                tmp_path.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"‚ö†Ô∏è Could not write target_field_summary.csv: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120f751",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.4) üíæ Persist initial schema summary\n",
      "‚úÖ 1.5 initial schema summary saved ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section1/initial_schema_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.5.4 üíæ Persist initial schema summary (single source of truth)\n",
    "\n",
    "print(\"\\n1.5.4) üíæ Persist initial schema summary\")\n",
    "\n",
    "# Ensure we have a section1 reports subfolder\n",
    "SECTION1_REPORT_DIR = REPORTS_DIR / \"section1\"\n",
    "SECTION1_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add dataset-shape columns if not already present\n",
    "if \"n_rows\" not in initial_schema_df.columns:\n",
    "    initial_schema_df.insert(0, \"n_rows\", n_rows)\n",
    "if \"n_cols\" not in initial_schema_df.columns:\n",
    "    initial_schema_df.insert(1, \"n_cols\", n_cols)\n",
    "\n",
    "initial_schema_path = SECTION1_REPORT_DIR / \"initial_schema_summary.csv\"\n",
    "tmp_path = initial_schema_path.with_suffix(\".tmp.csv\")\n",
    "\n",
    "try:\n",
    "    # Atomic write: temp ‚Üí replace\n",
    "    initial_schema_df.to_csv(tmp_path, index=False)\n",
    "    os.replace(tmp_path, initial_schema_path)\n",
    "    print(f\"‚úÖ 1.5 initial schema summary saved ‚Üí {initial_schema_path}\")\n",
    "except Exception as e:\n",
    "    # Best-effort cleanup\n",
    "    if tmp_path.exists():\n",
    "        try:\n",
    "            tmp_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\"‚ö†Ô∏è Could not write initial_schema_summary.csv: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04afbb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.5) üß¨ Dtype snapshot (pre‚ÄìData Quality, no coercion)\n",
      "   üíæ Saved pre‚ÄìSection 2 dtype snapshot ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts/dtypes/dtypes_pre_section2.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.5.5) üß¨ Dtype snapshot (read-only, pre‚ÄìData Quality)\n",
    "\n",
    "print(\"\\n1.5.5) üß¨ Dtype snapshot (pre‚ÄìData Quality, no coercion)\")\n",
    "\n",
    "DTYPE_DIR = ARTIFACTS_DIR / \"dtypes\"\n",
    "DTYPE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dtype_snapshot = (\n",
    "    df.dtypes.astype(str)\n",
    "      .rename(\"dtype\")\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\": \"column\"})\n",
    ")\n",
    "\n",
    "dtype_snapshot[\"n_unique\"] = [df[c].nunique(dropna=True) for c in df.columns]\n",
    "dtype_snapshot[\"null_pct\"] = [df[c].isna().mean() * 100 for c in df.columns]\n",
    "dtype_snapshot[\"snapshot_ts_utc\"] = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "before_path = DTYPE_DIR / \"dtypes_pre_section2.csv\"\n",
    "tmp_path = before_path.with_suffix(\".tmp.csv\")\n",
    "\n",
    "try:\n",
    "    dtype_snapshot.to_csv(tmp_path, index=False)\n",
    "    os.replace(tmp_path, before_path)\n",
    "    print(f\"   üíæ Saved pre‚ÄìSection 2 dtype snapshot ‚Üí {before_path}\")\n",
    "except Exception as e:\n",
    "    if tmp_path.exists():\n",
    "        try:\n",
    "            tmp_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\"   ‚ö†Ô∏è Could not write dtype snapshot: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f12418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7032 non-null   float64\n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(2), int64(2), object(17)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b81111",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:5px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "NEW! 1.6.5 üè∑Ô∏è Categorical Registration & Manifest\n",
    "</summary>\n",
    "\n",
    "This block assumes you‚Äôve run everything you pasted above (including the `schema_df` / `initial_schema_df` / `dtype_before` logic from earlier 1.6.x).\n",
    "\n",
    ">#### drop this **right after 1.6.4 (dtypes snapshot)** and before you enter Section 2.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.5) üè∑Ô∏è Categorical registration & manifest\n",
    "\n",
    "### How this answers your earlier dtype questions\n",
    "\n",
    "* **‚Äúobject / string_like üî§ Contract, Dependents, DeviceProtection, ‚Ä¶ Aren‚Äôt these categorical?‚Äù**\n",
    "  Yep. The Telco-ish block `DEFAULT_BINARY_CANDIDATES` targets exactly those ‚ÄúYes/No / No service‚Äù style columns and forces them to `category`.\n",
    "\n",
    "* **‚ÄúShould I be converting all those columns to categorical?‚Äù**\n",
    "  Generally: yes for things like `Partner`, `Dependents`, `PhoneService`, etc. They‚Äôre discrete labels and perfect for `category`. The block above makes it **config-first**, but also has heuristics so you don‚Äôt have to hand-curate everything.\n",
    "\n",
    "* **‚ÄúWhich dtype is best for these?‚Äù**\n",
    "  For **binary / low-cardinality labels** in modeling pipelines:\n",
    "\n",
    "  * `category` for the raw label column (`Yes`/`No`/`No internet service`)\n",
    "  * Optionally **separate numeric flags** when needed (like your `Churn_flag` = 0/1).\n",
    "\n",
    "* **‚ÄúWhy isn‚Äôt `category` in primitive/scalar data types?‚Äù**\n",
    "  Because ‚Äúprimitive/scalar types‚Äù is usually a *language-level* concept (int, float, char, bool).\n",
    "  `category` is a **pandas logical / semantic dtype**, built on top of underlying arrays, not a low-level primitive like `int32` or `float64`. It‚Äôs more of a storage + semantics optimization in the DataFrame world.\n",
    "\n",
    "If you‚Äôd like, next pass we can wire up **Section 2** so it explicitly reads:\n",
    "\n",
    "* `categorical_manifest_1_6_5.csv`\n",
    "* `dtypes_before.csv` / `dtypes_after.csv`\n",
    "\n",
    "and uses them to drive:\n",
    "\n",
    "* which columns get frequency tables,\n",
    "* which get association metrics (Cram√©r‚Äôs V / Theil‚Äôs U),\n",
    "* which ones are ‚Äúprotected‚Äù from encoding until later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa29e45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.6) üè∑Ô∏è Categorical hints & light profiling\n",
      "   Config categorical columns:  []\n",
      "   Config boolean columns:      []\n",
      "   Default binary candidates:   ['Dependents', 'DeviceProtection', 'MultipleLines', 'OnlineBackup', 'OnlineSecurity', 'PaperlessBilling', 'Partner', 'PhoneService', 'StreamingMovies', 'StreamingTV', 'TechSupport']\n",
      "   Heuristic candidates (‚â§50 uniques, ‚â§0.50 ratio): ['Contract', 'Dependents', 'DeviceProtection', 'InternetService', 'MultipleLines', 'OnlineBackup', 'OnlineSecurity', 'PaperlessBilling', 'Partner', 'PaymentMethod', 'PhoneService', 'StreamingMovies', 'StreamingTV', 'TechSupport', 'gender']\n",
      "   üíæ Categorical hints profile saved ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section1/categorical_hints_1_5_6.csv\n",
      "\n",
      "‚úÖ 1.5.6 Categorical hints & light profiling completed (no dtype coercion).\n"
     ]
    }
   ],
   "source": [
    "# 1.5.6 üè∑Ô∏è Categorical hints & light profiling (no coercion)\n",
    "print(\"\\n1.5.6) üè∑Ô∏è Categorical hints & light profiling\")\n",
    "\n",
    "SECTION_ID   = \"1.5.6\"\n",
    "SECTION_NAME = \"Categorical hints & light profiling\"\n",
    "\n",
    "# 1Ô∏è‚É£ Config-driven categorical hints\n",
    "cfg_cat_cols  = set(C(\"CATEGORICAL_COLUMNS\", []) or [])\n",
    "cfg_bool_cols = set(C(\"BOOLEAN_COLUMNS\", []) or [])\n",
    "cfg_id_cols   = set(C(\"ID_COLUMNS\", []) or [])\n",
    "\n",
    "DEFAULT_BINARY_CANDIDATES = {\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"MultipleLines\",\n",
    "    \"DeviceProtection\",\n",
    "    \"OnlineBackup\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingMovies\",\n",
    "    \"StreamingTV\",\n",
    "}\n",
    "\n",
    "cfg_binary_candidates = set(C(\"BINARY_CANDIDATES\", []) or [])\n",
    "binary_candidates = DEFAULT_BINARY_CANDIDATES | cfg_binary_candidates\n",
    "\n",
    "print(\"   Config categorical columns: \", sorted(cfg_cat_cols))\n",
    "print(\"   Config boolean columns:     \", sorted(cfg_bool_cols))\n",
    "print(\"   Default binary candidates:  \", sorted(binary_candidates))\n",
    "\n",
    "# 2Ô∏è‚É£ Column-level profile used for later heuristics (Section 2.2)\n",
    "n_rows, n_cols = df.shape\n",
    "\n",
    "def classify_dtype(dtype_str: str) -> str:\n",
    "    s = dtype_str.lower()\n",
    "    if \"int\" in s or \"float\" in s:\n",
    "        return \"numeric\"\n",
    "    if \"bool\" in s:\n",
    "        return \"boolean\"\n",
    "    if \"datetime\" in s or \"date\" in s:\n",
    "        return \"datetime\"\n",
    "    if \"category\" in s:\n",
    "        return \"categorical\"\n",
    "    return \"string_like\"\n",
    "\n",
    "col_meta_rows = []\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    dtype_str = str(s.dtype)\n",
    "    type_group = classify_dtype(dtype_str)\n",
    "\n",
    "    non_null = s.notna().sum()\n",
    "    n_null   = s.isna().sum()\n",
    "    n_unique = s.nunique(dropna=True)\n",
    "    unique_ratio = float(n_unique) / n_rows if n_rows else 0.0\n",
    "\n",
    "    col_meta_rows.append(\n",
    "        {\n",
    "            \"column\":       col,\n",
    "            \"dtype\":        dtype_str,\n",
    "            \"type_group\":   type_group,\n",
    "            \"n_unique\":     int(n_unique),\n",
    "            \"unique_ratio\": round(unique_ratio, 4),\n",
    "            \"null_pct\":     round(n_null / n_rows * 100, 3) if n_rows else 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "col_meta = (\n",
    "    pd.DataFrame(col_meta_rows)\n",
    "    .sort_values([\"type_group\", \"dtype\", \"column\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Heuristic ‚Äúsmall-cardinality string-like‚Äù candidates (for Section 2.2)\n",
    "MAX_UNIQUE_FOR_CATEGORY = int(C(\"CATEGORICAL_HEURISTICS.MAX_UNIQUE\", 50))\n",
    "MAX_RATIO_FOR_CATEGORY  = float(C(\"CATEGORICAL_HEURISTICS.MAX_RATIO\", 0.5))\n",
    "\n",
    "heuristic_candidates = set(\n",
    "    col_meta.loc[\n",
    "        (col_meta[\"type_group\"] == \"string_like\")\n",
    "        & (col_meta[\"n_unique\"] <= MAX_UNIQUE_FOR_CATEGORY)\n",
    "        & (col_meta[\"unique_ratio\"] <= MAX_RATIO_FOR_CATEGORY),\n",
    "        \"column\",\n",
    "    ].tolist()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"   Heuristic candidates (‚â§{MAX_UNIQUE_FOR_CATEGORY} uniques, \"\n",
    "    f\"‚â§{MAX_RATIO_FOR_CATEGORY:.2f} ratio): {sorted(heuristic_candidates)}\"\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Optional: persist a tiny artifact for Section 2.2 to reuse\n",
    "section1_dir = REPORTS_DIR / \"section1\"\n",
    "section1_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cat_hints_path = section1_dir / \"categorical_hints_1_5_6.csv\"\n",
    "col_meta.to_csv(cat_hints_path, index=False)\n",
    "print(f\"   üíæ Categorical hints profile saved ‚Üí {cat_hints_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ 1.5.6 Categorical hints & light profiling completed (no dtype coercion).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0489f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.5.6) üè∑Ô∏è Categorical registration & manifest\n",
      "   Config categorical columns:  []\n",
      "   Config boolean columns:      []\n",
      "   Default binary candidates:   ['Dependents', 'DeviceProtection', 'MultipleLines', 'OnlineBackup', 'OnlineSecurity', 'PaperlessBilling', 'Partner', 'PhoneService', 'StreamingMovies', 'StreamingTV', 'TechSupport']\n",
      "   Heuristic category candidates (‚â§50 uniques, ‚â§0.50 ratio): ['Contract', 'Dependents', 'DeviceProtection', 'InternetService', 'MultipleLines', 'OnlineBackup', 'OnlineSecurity', 'PaperlessBilling', 'Partner', 'PaymentMethod', 'PhoneService', 'StreamingMovies', 'StreamingTV', 'TechSupport', 'gender']\n",
      "\n",
      "   ‚úÖ Forced to categorical: ['Contract', 'Dependents', 'DeviceProtection', 'InternetService', 'MultipleLines', 'OnlineBackup', 'OnlineSecurity', 'PaperlessBilling', 'Partner', 'PaymentMethod', 'PhoneService', 'StreamingMovies', 'StreamingTV', 'TechSupport', 'gender']\n",
      "   ‚úÖ Forced to boolean:    []\n",
      "\n",
      "   üíæ Categorical manifest written ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts/categorical/categorical_manifest_1_5_6.csv\n",
      "   üìÑ Markdown overview written ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/reports/section1/1_5_6_categorical_overview.md\n",
      "\n",
      "‚úÖ 1.5.6 Categorical registration & manifest completed.\n"
     ]
    }
   ],
   "source": [
    "# 1.5.6 Categorical Candidate Registration & Manifest\n",
    "print(\"\\n1.5.6) üè∑Ô∏è Categorical registration & manifest\")\n",
    "\n",
    "# Optionally: save a read-only artifact like section1/categorical_candidates.csv:\n",
    "# columns: column, type_group, n_unique, unique_ratio, is_config_categorical, is_config_boolean, is_binary_candidate, is_heuristic_candidate\n",
    "# No astype(...) calls, no mutation.\n",
    "from collections import defaultdict\n",
    "\n",
    "SECTION_ID   = \"1.5.6\"\n",
    "SECTION_NAME = \"Categorical registration & manifest\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) Pull config-driven categorical hints\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "cfg_cat_cols   = set(C(\"CATEGORICAL_COLUMNS\", []) or [])\n",
    "cfg_bool_cols  = set(C(\"BOOLEAN_COLUMNS\", []) or [])\n",
    "cfg_id_cols    = set(C(\"ID_COLUMNS\", []) or [])\n",
    "\n",
    "DEFAULT_BINARY_CANDIDATES = {\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"MultipleLines\",\n",
    "    \"DeviceProtection\",\n",
    "    \"OnlineBackup\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingMovies\",\n",
    "    \"StreamingTV\",\n",
    "}\n",
    "\n",
    "cfg_binary_candidates = set(C(\"BINARY_CANDIDATES\", []) or [])\n",
    "binary_candidates = DEFAULT_BINARY_CANDIDATES | cfg_binary_candidates\n",
    "\n",
    "print(\"   Config categorical columns: \", sorted(cfg_cat_cols))\n",
    "print(\"   Config boolean columns:     \", sorted(cfg_bool_cols))\n",
    "print(\"   Default binary candidates:  \", sorted(binary_candidates))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) Inference helpers: type_group, uniqueness, tiny-cardinality strings\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "n_rows, n_cols = df.shape\n",
    "\n",
    "def classify_dtype(dtype_str: str) -> str:\n",
    "    s = dtype_str.lower()\n",
    "    if \"int\" in s or \"float\" in s:\n",
    "        return \"numeric\"\n",
    "    if \"bool\" in s:\n",
    "        return \"boolean\"\n",
    "    if \"datetime\" in s or \"date\" in s:\n",
    "        return \"datetime\"\n",
    "    if \"category\" in s:\n",
    "        return \"categorical\"\n",
    "    return \"string_like\"\n",
    "\n",
    "col_meta_rows = []\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    dtype_str = str(s.dtype)\n",
    "    type_group = classify_dtype(dtype_str)\n",
    "\n",
    "    non_null = s.notna().sum()\n",
    "    n_null = s.isna().sum()\n",
    "    n_unique = s.nunique(dropna=True)\n",
    "    unique_ratio = float(n_unique) / n_rows if n_rows else 0.0\n",
    "\n",
    "    col_meta_rows.append(\n",
    "        {\n",
    "            \"column\": col,\n",
    "            \"dtype\": dtype_str,\n",
    "            \"type_group\": type_group,\n",
    "            \"n_unique\": int(n_unique),\n",
    "            \"unique_ratio\": round(unique_ratio, 4),\n",
    "            \"null_pct\": round(n_null / n_rows * 100, 3) if n_rows else 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "col_meta = pd.DataFrame(col_meta_rows)\n",
    "\n",
    "MAX_UNIQUE_FOR_CATEGORY = int(C(\"CATEGORICAL_HEURISTICS.MAX_UNIQUE\", 50))\n",
    "MAX_RATIO_FOR_CATEGORY  = float(C(\"CATEGORICAL_HEURISTICS.MAX_RATIO\", 0.5))\n",
    "\n",
    "heuristic_candidates = set(\n",
    "    col_meta.loc[\n",
    "        (col_meta[\"type_group\"] == \"string_like\")\n",
    "        & (col_meta[\"n_unique\"] <= MAX_UNIQUE_FOR_CATEGORY)\n",
    "        & (col_meta[\"unique_ratio\"] <= MAX_RATIO_FOR_CATEGORY),\n",
    "        \"column\",\n",
    "    ].tolist()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"   Heuristic category candidates (‚â§{MAX_UNIQUE_FOR_CATEGORY} uniques, \"\n",
    "    f\"‚â§{MAX_RATIO_FOR_CATEGORY:.2f} ratio): {sorted(heuristic_candidates)}\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) Apply categorical / boolean conversions\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "forced_categories = set()\n",
    "forced_booleans = set()\n",
    "conversion_errors = []\n",
    "\n",
    "# (a) Boolean columns from config\n",
    "for col in cfg_bool_cols:\n",
    "    if col not in df.columns:\n",
    "        conversion_errors.append((col, \"bool\", \"missing column\"))\n",
    "        continue\n",
    "    try:\n",
    "        df[col] = df[col].map(\n",
    "            {\n",
    "                True: True,\n",
    "                False: False,\n",
    "                1: True,\n",
    "                0: False,\n",
    "                \"Yes\": True,\n",
    "                \"No\": False,\n",
    "                \"yes\": True,\n",
    "                \"no\": False,\n",
    "                \"Y\": True,\n",
    "                \"N\": False,\n",
    "            }\n",
    "        ).astype(\"boolean\")\n",
    "        forced_booleans.add(col)\n",
    "    except Exception as e:\n",
    "        conversion_errors.append((col, \"bool\", str(e)))\n",
    "\n",
    "# (b) Config categorical columns\n",
    "for col in cfg_cat_cols:\n",
    "    if col not in df.columns:\n",
    "        conversion_errors.append((col, \"category\", \"missing column\"))\n",
    "        continue\n",
    "    try:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        forced_categories.add(col)\n",
    "    except Exception as e:\n",
    "        conversion_errors.append((col, \"category\", str(e)))\n",
    "\n",
    "# (c) Default binary Yes/No-style candidates\n",
    "for col in binary_candidates:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "    if str(df[col].dtype) == \"category\":\n",
    "        continue\n",
    "    try:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        forced_categories.add(col)\n",
    "    except Exception as e:\n",
    "        conversion_errors.append((col, \"category\", str(e)))\n",
    "\n",
    "# (d) Heuristic small-cardinality string-like candidates (if not in ID columns)\n",
    "auto_cat_candidates = heuristic_candidates - cfg_id_cols - forced_categories\n",
    "for col in auto_cat_candidates:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "    if str(df[col].dtype) == \"category\":\n",
    "        continue\n",
    "    try:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        forced_categories.add(col)\n",
    "    except Exception as e:\n",
    "        conversion_errors.append((col, \"category\", str(e)))\n",
    "\n",
    "print(f\"\\n   ‚úÖ Forced to categorical: {sorted(forced_categories)}\")\n",
    "print(f\"   ‚úÖ Forced to boolean:    {sorted(forced_booleans)}\")\n",
    "\n",
    "if conversion_errors:\n",
    "    print(\"\\n   ‚ö†Ô∏è Conversion issues encountered:\")\n",
    "    for col, target, msg in conversion_errors[:10]:\n",
    "        print(f\"     ‚Ä¢ {col} ‚Üí {target}: {msg}\")\n",
    "    if len(conversion_errors) > 10:\n",
    "        print(f\"     ‚Ä¶ and {len(conversion_errors) - 10} more\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) Build categorical manifest\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "run_ts = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "cat_cols = [c for c in df.columns if \"category\" in str(df[c].dtype).lower()]\n",
    "\n",
    "manifest_rows = []\n",
    "for col in cat_cols:\n",
    "    s = df[col]\n",
    "    n_unique = s.nunique(dropna=True)\n",
    "    value_counts = (\n",
    "        s.value_counts(dropna=False)\n",
    "         .head(10)\n",
    "         .to_dict()\n",
    "    )\n",
    "\n",
    "    if col in cfg_cat_cols:\n",
    "        source = \"config:categorical\"\n",
    "    elif col in binary_candidates:\n",
    "        source = \"default_binary\"\n",
    "    elif col in heuristic_candidates:\n",
    "        source = \"heuristic_small_cardinality\"\n",
    "    else:\n",
    "        source = \"existing_category\"\n",
    "\n",
    "    manifest_rows.append(\n",
    "        {\n",
    "            \"column\": col,\n",
    "            \"is_config_categorical\": col in cfg_cat_cols,\n",
    "            \"is_binary_candidate\": col in binary_candidates,\n",
    "            \"source\": source,\n",
    "            \"n_unique\": int(n_unique),\n",
    "            \"sample_values\": json.dumps(value_counts),\n",
    "            \"section_id\": SECTION_ID,\n",
    "            \"section_name\": SECTION_NAME,\n",
    "            \"run_ts_utc\": run_ts,\n",
    "        }\n",
    "    )\n",
    "\n",
    "categorical_manifest = (\n",
    "    pd.DataFrame(manifest_rows)\n",
    "      .sort_values(\"column\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Persist manifest\n",
    "cat_dir = ARTIFACTS_DIR / \"categorical\"\n",
    "cat_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cat_manifest_path = cat_dir / \"categorical_manifest_1_5_6.csv\"\n",
    "tmp_path = cat_manifest_path.with_suffix(\".tmp.csv\")\n",
    "\n",
    "try:\n",
    "    categorical_manifest.to_csv(tmp_path, index=False)\n",
    "    os.replace(tmp_path, cat_manifest_path)\n",
    "    print(f\"\\n   üíæ Categorical manifest written ‚Üí {cat_manifest_path}\")\n",
    "except Exception as e:\n",
    "    if tmp_path.exists():\n",
    "        try:\n",
    "            tmp_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\"   ‚ö†Ô∏è Could not write categorical manifest: {e}\")\n",
    "\n",
    "# Markdown snapshot\n",
    "section1_dir = REPORTS_DIR / \"section1\"\n",
    "section1_dir.mkdir(parents=True, exist_ok=True)\n",
    "cat_md_path = section1_dir / \"1_5_6_categorical_overview.md\"\n",
    "\n",
    "with cat_md_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"# {SECTION_ID} {SECTION_NAME}\\n\\n\")\n",
    "    f.write(f\"- Run timestamp: {run_ts}\\n\")\n",
    "    f.write(f\"- Total columns: {n_cols}\\n\")\n",
    "    f.write(f\"- Categorical columns: {len(cat_cols)}\\n\\n\")\n",
    "    f.write(\"## Categorical Columns\\n\\n\")\n",
    "    for _, row in categorical_manifest.iterrows():\n",
    "        f.write(f\"### {row['column']}\\n\")\n",
    "        f.write(f\"- Source: `{row['source']}`\\n\")\n",
    "        f.write(f\"- n_unique: {row['n_unique']}\\n\")\n",
    "        f.write(f\"- Config categorical: {row['is_config_categorical']}\\n\")\n",
    "        f.write(f\"- Binary candidate: {row['is_binary_candidate']}\\n\\n\")\n",
    "\n",
    "print(f\"   üìÑ Markdown overview written ‚Üí {cat_md_path}\")\n",
    "print(\"\\n‚úÖ 1.5.6 Categorical registration & manifest completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020cbd1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:5px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "1.6.1 - 1.6.2 üßæ Environment Summary & Log Consolidation\n",
    "</summary>\n",
    "\n",
    "# 1.6.1-1.6.2 üßæ Environment Summary & Log Consolidation\n",
    "\n",
    "1.6.1 Capture System & Library Versions\n",
    "- Python, pandas, numpy, seaborn, matplotlib, etc.\n",
    "\n",
    "1.6.2 Generate Setup Summary Report\n",
    "- Aggregates paths, env vars, library versions, config sources.\n",
    "\n",
    "- **Output:** setup_summary.json / markdown cell\n",
    "\n",
    "- üü© *Hiring value:* Gives recruiters a polished, verifiable record of engineering hygiene.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4132b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6 üßæ ENVIRONMENT SUMMARY & LOG CONSOLIDATION\n",
      "\n",
      "1.6.1) üß¨ Capturing system & library versions\n",
      "   System:\n",
      "     ‚Ä¢ Platform: Darwin 21.6.0\n",
      "     ‚Ä¢ Python:   3.12.6\n",
      "   Libraries:\n",
      "     ‚Ä¢ pandas: 2.3.2\n",
      "     ‚Ä¢ numpy: 2.3.3\n",
      "     ‚Ä¢ seaborn: 0.13.2\n",
      "     ‚Ä¢ matplotlib: 3.10.6\n"
     ]
    }
   ],
   "source": [
    "# 1.6.1 Capture System & Library Versions\n",
    "print(\"1.6 üßæ ENVIRONMENT SUMMARY & LOG CONSOLIDATION\")\n",
    "print(\"\\n1.6.1) üß¨ Capturing system & library versions\")\n",
    "\n",
    "env_info = {\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"system\": {\n",
    "        \"platform\": platform.system(),\n",
    "        \"platform_release\": platform.release(),\n",
    "        \"platform_version\": platform.version(),\n",
    "        \"machine\": platform.machine(),\n",
    "        \"python_version\": sys.version,\n",
    "    },\n",
    "    \"libraries\": {\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"seaborn\": sns.__version__,\n",
    "        \"matplotlib\": mpl.__version__,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"   System:\")\n",
    "print(f\"     ‚Ä¢ Platform: {env_info['system']['platform']} \"\n",
    "      f\"{env_info['system']['platform_release']}\")\n",
    "print(f\"     ‚Ä¢ Python:   {env_info['system']['python_version'].split()[0]}\")\n",
    "\n",
    "print(\"   Libraries:\")\n",
    "for lib, ver in env_info[\"libraries\"].items():\n",
    "    print(f\"     ‚Ä¢ {lib}: {ver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc06d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.6.2 \n",
      "\n",
      "üßæ Generating setup summary report..\n",
      "\n",
      "üìå Setup snapshot:\n",
      "   ‚Ä¢ Project: Telco Customer Churn (v1.0)\n",
      "   ‚Ä¢ Root:    /Users/b/DATA/PROJECTS/Telco\n",
      "   ‚Ä¢ Raw:     /Users/b/DATA/PROJECTS/Telco/resources/data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "   ‚Ä¢ Target:  Churn_flag (raw: Churn)\n",
      "   ‚Ä¢ IDs:     ['customerID']\n",
      "\n",
      "‚úÖ Setup summary written ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts/env/setup_summary.json\n",
      "‚úÖ 1.6 Environment summary & setup log completed.\n"
     ]
    }
   ],
   "source": [
    "# 1.6.2 üßæ Generating setup summary report\n",
    "# Right now, 1.6 is strong enough that you don‚Äôt strictly need a separate 1.3.4.\n",
    "# Consider 1.6.2 a ‚Äújust-in-time‚Äù version of 1.3.4 / Or downsize 1.6\n",
    "print(\"\\n1.6.2 \\n\\nüßæ Generating setup summary report..\")\n",
    "\n",
    "\n",
    "\n",
    "setup_summary = {\n",
    "    \"timestamp_utc\": env_info[\"timestamp_utc\"],\n",
    "    \"project\": {\n",
    "        \"project_root\":   str(PROJECT_ROOT),\n",
    "        \"project_name\":   C(\"META.PROJECT_NAME\", \"Telco Churn Project\"),\n",
    "        \"project_version\": C(\"META.VERSION\", \"0.1.0\"),\n",
    "        \"config_path\":    str(CONFIG_PATH),\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"raw_data\":      str(RAW_DATA),\n",
    "        \"processed_dir\": str(PROCESSED_DIR),\n",
    "        \"artifacts_dir\": str(ARTIFACTS_DIR),\n",
    "        \"reports_dir\":   str(REPORTS_DIR),\n",
    "        \"figures_dir\":   str(FIGURES_DIR),\n",
    "        \"models_dir\":    str(MODELS_DIR),\n",
    "        \"outputs_dir\":   str(OUTPUTS_DIR),\n",
    "    },\n",
    "    \"target_and_ids\": {\n",
    "        \"target_column\":      C(\"TARGET.COLUMN\"),\n",
    "        \"raw_target_column\":  C(\"TARGET.RAW_COLUMN\"),\n",
    "        \"positive_class\":     C(\"TARGET.POSITIVE_CLASS\"),\n",
    "        \"negative_class\":     C(\"TARGET.NEGATIVE_CLASS\"),\n",
    "        \"id_columns\":         C(\"ID_COLUMNS\", []),\n",
    "    },\n",
    "    \"data_quality_config\": {\n",
    "        \"ranges\":                 C(\"RANGES\", {}),\n",
    "        \"data_quality_thresholds\": C(\"DATA_QUALITY\", {}),\n",
    "        \"flags\":                  C(\"FLAGS\", {}),\n",
    "    },\n",
    "    \"environment\": env_info,\n",
    "}\n",
    "\n",
    "# Decide where to store env/setup info\n",
    "env_dir = ARTIFACTS_DIR / \"env\"\n",
    "env_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "setup_summary_path = env_dir / \"setup_summary.json\"\n",
    "\n",
    "with setup_summary_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(setup_summary, f, indent=2)\n",
    "\n",
    "# Optional: small human-readable snapshot for the notebook\n",
    "print(\"\\nüìå Setup snapshot:\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Project: {setup_summary['project']['project_name']} \"\n",
    "    f\"(v{setup_summary['project']['project_version']})\"\n",
    ")\n",
    "print(f\"   ‚Ä¢ Root:    {setup_summary['project']['project_root']}\")\n",
    "print(f\"   ‚Ä¢ Raw:     {setup_summary['paths']['raw_data']}\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Target:  {setup_summary['target_and_ids']['target_column']} \"\n",
    "    f\"(raw: {setup_summary['target_and_ids']['raw_target_column']})\"\n",
    ")\n",
    "print(f\"   ‚Ä¢ IDs:     {setup_summary['target_and_ids']['id_columns']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup summary written ‚Üí {setup_summary_path}\")\n",
    "print(\"‚úÖ 1.6 Environment summary & setup log completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9a7a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ 1.6.3 Core library versions snapshot saved ‚Üí /Users/b/DATA/PROJECTS/Telco/Level_3/resources/artifacts/env/core_lib_versions.json\n"
     ]
    }
   ],
   "source": [
    "# 1.6.3 Core library version snapshot (optional, but nice hiring signal)\n",
    "\n",
    "# OLD-1.3.4 snapshot core lib version (match pins in requirements.txt)\n",
    "# TODO: remove this if not needed\n",
    "\n",
    "def write_core_env_snapshot(artifacts_dir: Path, versions: dict) -> Path:\n",
    "    \"\"\"Write core library versions + timestamp as a JSON artifact.\"\"\"\n",
    "    env_dir = artifacts_dir / \"env\"\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    env_snapshot_path = env_dir / \"core_lib_versions.json\"\n",
    "    env_snapshot = {\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        \"versions\": versions,\n",
    "    }\n",
    "\n",
    "    with env_snapshot_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(env_snapshot, f, indent=2)\n",
    "\n",
    "    return env_snapshot_path\n",
    "\n",
    "env_snapshot_path = write_core_env_snapshot(ARTIFACTS_DIR, versions)\n",
    "print(f\"üßæ 1.6.3 Core library versions snapshot saved ‚Üí {env_snapshot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584e2af",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"\n",
    "    cursor:pointer;background:#f7f7fb;border:2px solid #297be7ff;\n",
    "padding:10px 12px;border-radius:10px;font-weight:700;\">\n",
    "Section-1 / Section-2 boundary check!\n",
    "</summary>\n",
    "\n",
    "# Section-1 / Section-2 boundary check!\n",
    "\n",
    "Important sanity check with your new DQ framework:\n",
    "\n",
    "Section 1:\n",
    "Loads config (Type-3 YAML ‚Üí Type-2 C()).\n",
    "Resolves paths & constants.\n",
    "Loads df_raw, hashes it, registers version, logs load.\n",
    "Creates df (working copy).\n",
    "Does non-mutating structural checks and simple, domain-obvious normalization (target label to Churn_flag).\n",
    "\n",
    "Emits:\n",
    "config_validation_report.csv\n",
    "dataset_version_registry.csv\n",
    "dataset_load_log.csv\n",
    "df_raw_info.txt / df_working_info.txt\n",
    "section1_5_2_schema.csv\n",
    "initial_schema_summary.csv\n",
    "target_field_summary.csv\n",
    "dtypes_pre_section2.csv\n",
    "\n",
    "Section 2:\n",
    "+ Owns typed schema intelligence + coercion\n",
    "+ DQ (2.1‚Äì2.4 pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
